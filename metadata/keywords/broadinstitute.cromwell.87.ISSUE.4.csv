id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/cromwell/issues/2997:641,Testability,test,test,641,Link to spec:; - http://www.commonwl.org/v1.0/CommandLineTool.html#EnvVarRequirement. Example conformance test(s):. - <span>#</span>41 [v1.0/env-wf1.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/env-wf1.cwl); - <span>#</span>42 [v1.0/env-wf2.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/env-wf2.cwl); - <span>#</span>43 [v1.0/env-wf3.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/env-wf3.cwl). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2997
https://github.com/broadinstitute/cromwell/issues/2998:112,Testability,test,test,112,Link to spec:; - http://www.commonwl.org/v1.0/CommandLineTool.html#ShellCommandRequirement. Example conformance test(s):. - <span>#</span>10 [v1.0/stderr.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/stderr.cwl); - <span>#</span>11 [v1.0/stderr-shortcut.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/stderr-shortcut.cwl); - <span>#</span>12 [v1.0/stderr-mediumcut.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/stderr-mediumcut.cwl); - <span>#</span>107 [v1.0/shellchar.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/shellchar.cwl); - <span>#</span>108 [v1.0/shellchar2.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/shellchar2.cwl). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2998
https://github.com/broadinstitute/cromwell/issues/2998:977,Testability,test,test,977,Link to spec:; - http://www.commonwl.org/v1.0/CommandLineTool.html#ShellCommandRequirement. Example conformance test(s):. - <span>#</span>10 [v1.0/stderr.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/stderr.cwl); - <span>#</span>11 [v1.0/stderr-shortcut.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/stderr-shortcut.cwl); - <span>#</span>12 [v1.0/stderr-mediumcut.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/stderr-mediumcut.cwl); - <span>#</span>107 [v1.0/shellchar.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/shellchar.cwl); - <span>#</span>108 [v1.0/shellchar2.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/shellchar2.cwl). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2998
https://github.com/broadinstitute/cromwell/issues/2999:107,Testability,test,test,107,Link to spec:; - http://www.commonwl.org/v1.0/Workflow.html#ScatterFeatureRequirement. Example conformance test(s):. - <span>#</span>30 [v1.0/scatter-wf1.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/scatter-wf1.cwl); - <span>#</span>31 [v1.0/scatter-wf2.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/scatter-wf2.cwl); - <span>#</span>32 [v1.0/scatter-wf3.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/scatter-wf3.cwl); - <span>#</span>33 [v1.0/scatter-wf4.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/scatter-wf4.cwl). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2999
https://github.com/broadinstitute/cromwell/issues/2999:817,Testability,test,test,817,Link to spec:; - http://www.commonwl.org/v1.0/Workflow.html#ScatterFeatureRequirement. Example conformance test(s):. - <span>#</span>30 [v1.0/scatter-wf1.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/scatter-wf1.cwl); - <span>#</span>31 [v1.0/scatter-wf2.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/scatter-wf2.cwl); - <span>#</span>32 [v1.0/scatter-wf3.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/scatter-wf3.cwl); - <span>#</span>33 [v1.0/scatter-wf4.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/scatter-wf4.cwl). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2999
https://github.com/broadinstitute/cromwell/issues/3000:112,Testability,test,test,112,Link to spec:; - http://www.commonwl.org/v1.0/Workflow.html#StepInputExpressionRequirement. Example conformance test(s):. - <span>#</span>62 [v1.0/step-valuefrom-wf.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/step-valuefrom-wf.cwl); - <span>#</span>63 [v1.0/step-valuefrom2-wf.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/step-valuefrom2-wf.cwl); - <span>#</span>64 [v1.0/step-valuefrom3-wf.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/step-valuefrom3-wf.cwl). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3000
https://github.com/broadinstitute/cromwell/issues/3000:711,Testability,test,test,711,Link to spec:; - http://www.commonwl.org/v1.0/Workflow.html#StepInputExpressionRequirement. Example conformance test(s):. - <span>#</span>62 [v1.0/step-valuefrom-wf.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/step-valuefrom-wf.cwl); - <span>#</span>63 [v1.0/step-valuefrom2-wf.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/step-valuefrom2-wf.cwl); - <span>#</span>64 [v1.0/step-valuefrom3-wf.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/step-valuefrom3-wf.cwl). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3000
https://github.com/broadinstitute/cromwell/issues/3001:107,Testability,test,test,107,Link to spec:; - http://www.commonwl.org/v1.0/Workflow.html#InitialWorkDirRequirement. Example conformance test(s):. - <span>#</span>51 [v1.0/rename.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/rename.cwl); - <span>#</span>81 [v1.0/stagefile.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/stagefile.cwl) (also uses docker). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3001
https://github.com/broadinstitute/cromwell/issues/3001:520,Testability,test,test,520,Link to spec:; - http://www.commonwl.org/v1.0/Workflow.html#InitialWorkDirRequirement. Example conformance test(s):. - <span>#</span>51 [v1.0/rename.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/rename.cwl); - <span>#</span>81 [v1.0/stagefile.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/stagefile.cwl) (also uses docker). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3001
https://github.com/broadinstitute/cromwell/issues/3002:113,Testability,test,test,113,Link to spec:; - http://www.commonwl.org/v1.0/Workflow.html#MultipleInputFeatureRequirement. Example conformance test(s):. - <span>#</span>26 [v1.0/count-lines7-wf.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/count-lines7-wf.cwl); - <span>#</span>63 [v1.0/step-valuefrom2-wf.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/step-valuefrom2-wf.cwl) (also uses StepInputExpressionRequirement). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3002
https://github.com/broadinstitute/cromwell/issues/3002:586,Testability,test,test,586,Link to spec:; - http://www.commonwl.org/v1.0/Workflow.html#MultipleInputFeatureRequirement. Example conformance test(s):. - <span>#</span>26 [v1.0/count-lines7-wf.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/count-lines7-wf.cwl); - <span>#</span>63 [v1.0/step-valuefrom2-wf.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/step-valuefrom2-wf.cwl) (also uses StepInputExpressionRequirement). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3002
https://github.com/broadinstitute/cromwell/pull/3003:38,Testability,test,test,38,New womgraph for the WDL in #2992:. ![test](https://user-images.githubusercontent.com/13006282/33627312-20999554-d9cb-11e7-9fef-e876f6714bb2.png),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3003
https://github.com/broadinstitute/cromwell/issues/3004:155,Testability,test,test,155,Link to specs:; - http://www.commonwl.org/v1.0/CommandLineTool.html#Directory; - http://www.commonwl.org/v1.0/Workflow.html#Directory. Example conformance test(s):. - <span>#</span>78 [v1.0/dir3.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/dir3.cwl); - <span>#</span>85 [v1.0/dir6.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/dir6.cwl); - <span>#</span>93 [v1.0/dir7.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/dir7.cwl); - <span>#</span>99 [v1.0/recursive-input-directory.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/recursive-input-directory.cwl). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3004
https://github.com/broadinstitute/cromwell/issues/3004:851,Testability,test,test,851,Link to specs:; - http://www.commonwl.org/v1.0/CommandLineTool.html#Directory; - http://www.commonwl.org/v1.0/Workflow.html#Directory. Example conformance test(s):. - <span>#</span>78 [v1.0/dir3.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/dir3.cwl); - <span>#</span>85 [v1.0/dir6.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/dir6.cwl); - <span>#</span>93 [v1.0/dir7.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/dir7.cwl); - <span>#</span>99 [v1.0/recursive-input-directory.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/v1.0/recursive-input-directory.cwl). A/C: These or similar CWL should run to `WorkflowSucceeded` via centaur; Bonus: CWL conformance test passes (may also be a future ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3004
https://github.com/broadinstitute/cromwell/issues/3007:585,Availability,echo,echo,585,"On behalf of @leepc12, opening this as a new issue (previously a comment in #2992). Another if-scatter bug.; i built a new `cromwell-31-d716fd2-SNAP.jar` from your `develop` branch.; ```wdl; workflow test {; 	Boolean b0 = true; 	Boolean b1 = true; 	Boolean b2 = true; 	scatter( i in range(3) ) {; 		if ( b0 ) {; 			call t0 as t1 { input: i=i }; 		}; 	}; 	if ( b1 ) {; 		scatter( i in range(3) ) {; 			call t0 as t2 { input: i=t1.out[i] }; 		}; 	}; 	if ( b1 && b2 ) {; 		scatter( i in range(3) ) {; 			call t0 as t3 { input: i=t2.out[i] }; 		}; 	}; }. task t0 {; 	Int? i; 	command {; 		echo ${i}; 	}; 	output {; 		Int out = read_int(stdout()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:655,Availability,error,error,655,"On behalf of @leepc12, opening this as a new issue (previously a comment in #2992). Another if-scatter bug.; i built a new `cromwell-31-d716fd2-SNAP.jar` from your `develop` branch.; ```wdl; workflow test {; 	Boolean b0 = true; 	Boolean b1 = true; 	Boolean b2 = true; 	scatter( i in range(3) ) {; 		if ( b0 ) {; 			call t0 as t1 { input: i=i }; 		}; 	}; 	if ( b1 ) {; 		scatter( i in range(3) ) {; 			call t0 as t2 { input: i=t1.out[i] }; 		}; 	}; 	if ( b1 && b2 ) {; 		scatter( i in range(3) ) {; 			call t0 as t3 { input: i=t2.out[i] }; 		}; 	}; }. task t0 {; 	Int? i; 	command {; 		echo ${i}; 	}; 	output {; 		Int out = read_int(stdout()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:2585,Availability,error,error,2585,"nitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23,82] [info] SingleWorkflowRunnerActor: Workflow submitted 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,82] [info] 1 new workflows fetched; [2017-12-05 20:11:23,82] [info] WorkflowManagerActor Starting workflow 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 20:11:24,82] [error] WorkflowManagerActor Workflow 159210e6-fa6a-4a99-b386-5931ae245324 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Can't index (ArrayOrMapLookup:; lhs=(MemberAccess:; lhs=<string:19:29 identifier ""dDI="">,; rhs=<string:19:32 identifier ""b3V0"">; ),; rhs=<string:19:36 identifier ""aQ=="">; ); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Can't index (ArrayOrMapLookup:; lhs=(MemberAccess:; lhs=<string:19:29 identifier ""dDI="">,; rhs=<string:19:32 identifier ""b3V0"">; ),; rhs=<string:19:36 identifier ""aQ=="">; ); at cromwell.engine.workflow.lifecycle.materialization.Materializ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:6297,Deployability,configurat,configuration,6297,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:6640,Deployability,configurat,configuration,6640,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:1566,Energy Efficiency,monitor,monitor,1566,"()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23,82] [info] SingleWorkflowRunnerActor: Workflow submitted 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,82] [info] 1 new workflows fetched; [2017-12-05 20:11:23,82] [info] WorkflowManagerActor Starting workflow 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 20:11:24,82] [error] WorkflowManagerActor Workflow 159210e6-fa6a-4a9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:6079,Integrability,Message,Message,6079,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:6422,Integrability,Message,Message,6422,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:1680,Modifiability,config,configured,1680,"()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23,82] [info] SingleWorkflowRunnerActor: Workflow submitted 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,82] [info] 1 new workflows fetched; [2017-12-05 20:11:23,82] [info] WorkflowManagerActor Starting workflow 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 20:11:24,82] [error] WorkflowManagerActor Workflow 159210e6-fa6a-4a9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:1815,Modifiability,config,configured,1815,"-Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23,82] [info] SingleWorkflowRunnerActor: Workflow submitted 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,82] [info] 1 new workflows fetched; [2017-12-05 20:11:23,82] [info] WorkflowManagerActor Starting workflow 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 20:11:24,82] [error] WorkflowManagerActor Workflow 159210e6-fa6a-4a99-b386-5931ae245324 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:6297,Modifiability,config,configuration,6297,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:6640,Modifiability,config,configuration,6640,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:200,Testability,test,test,200,"On behalf of @leepc12, opening this as a new issue (previously a comment in #2992). Another if-scatter bug.; i built a new `cromwell-31-d716fd2-SNAP.jar` from your `develop` branch.; ```wdl; workflow test {; 	Boolean b0 = true; 	Boolean b1 = true; 	Boolean b2 = true; 	scatter( i in range(3) ) {; 		if ( b0 ) {; 			call t0 as t1 { input: i=i }; 		}; 	}; 	if ( b1 ) {; 		scatter( i in range(3) ) {; 			call t0 as t2 { input: i=t1.out[i] }; 		}; 	}; 	if ( b1 && b2 ) {; 		scatter( i in range(3) ) {; 			call t0 as t3 { input: i=t2.out[i] }; 		}; 	}; }. task t0 {; 	Int? i; 	command {; 		echo ${i}; 	}; 	output {; 		Int out = read_int(stdout()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:661,Testability,log,log,661,"On behalf of @leepc12, opening this as a new issue (previously a comment in #2992). Another if-scatter bug.; i built a new `cromwell-31-d716fd2-SNAP.jar` from your `develop` branch.; ```wdl; workflow test {; 	Boolean b0 = true; 	Boolean b1 = true; 	Boolean b2 = true; 	scatter( i in range(3) ) {; 		if ( b0 ) {; 			call t0 as t1 { input: i=i }; 		}; 	}; 	if ( b1 ) {; 		scatter( i in range(3) ) {; 			call t0 as t2 { input: i=t1.out[i] }; 		}; 	}; 	if ( b1 && b2 ) {; 		scatter( i in range(3) ) {; 			call t0 as t3 { input: i=t2.out[i] }; 		}; 	}; }. task t0 {; 	Int? i; 	command {; 		echo ${i}; 	}; 	output {; 		Int out = read_int(stdout()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:4351,Testability,Log,LoggingFSM,4351,"s:; lhs=<string:19:29 identifier ""dDI="">,; rhs=<string:19:32 identifier ""b3V0"">; ),; rhs=<string:19:36 identifier ""aQ=="">; ); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:665); at akka.actor.FSM.processEvent$(FSM.scala:662); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:801); at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:4443,Testability,Log,LoggingFSM,4443,"r ""b3V0"">; ),; rhs=<string:19:36 identifier ""aQ=="">; ); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:665); at akka.actor.FSM.processEvent$(FSM.scala:662); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:801); at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:4497,Testability,Log,LoggingFSM,4497,; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:665); at akka.actor.FSM.processEvent$(FSM.scala:662); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:801); at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.di,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:6254,Testability,log,logging,6254,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:6326,Testability,log,log-dead-letters,6326,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:6354,Testability,log,log-dead-letters-during-shutdown,6354,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:6597,Testability,log,logging,6597,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:6669,Testability,log,log-dead-letters,6669,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/issues/3007:6697,Testability,log,log-dead-letters-during-shutdown,6697,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007
https://github.com/broadinstitute/cromwell/pull/3008:39,Integrability,message,message,39,"For the example in #3007 replaces this message:; ```; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Can't index (ArrayOrMapLookup:; lhs=(MemberAccess:; lhs=<string:19:29 identifier ""dDI="">,; rhs=<string:19:32 identifier ""b3V0"">; ),; rhs=<string:19:36 identifier ""aQ=="">; ); ```. With this one:; ```; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': ; Invalid indexing target. You cannot index a value of type 'Array[Int]?'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3008
https://github.com/broadinstitute/cromwell/issues/3010:12,Safety,abort,abort,12,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010
https://github.com/broadinstitute/cromwell/issues/3010:551,Safety,abort,abort,551,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010
https://github.com/broadinstitute/cromwell/issues/3010:666,Safety,abort,abort,666,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010
https://github.com/broadinstitute/cromwell/issues/3010:816,Safety,Abort,Aborted,816,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010
https://github.com/broadinstitute/cromwell/issues/3010:0,Testability,Test,Tests,0,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010
https://github.com/broadinstitute/cromwell/issues/3010:101,Testability,test,test,101,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010
https://github.com/broadinstitute/cromwell/issues/3010:186,Testability,test,test,186,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010
https://github.com/broadinstitute/cromwell/issues/3010:263,Testability,test,tests,263,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010
https://github.com/broadinstitute/cromwell/issues/3010:348,Testability,test,tests,348,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010
https://github.com/broadinstitute/cromwell/issues/3010:623,Testability,test,test,623,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010
https://github.com/broadinstitute/cromwell/issues/3010:1146,Testability,test,tests,1146,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010
https://github.com/broadinstitute/cromwell/issues/3010:1199,Testability,test,test,1199,"Tests like `abort.scheduled_abort` should run also run sequentially, inline with the current Restart test cases. Otherwise, Travis+Centaur produces false negative results, requiring the test to be re-run, and delaying PR merges. There are currently two suites of tests: those that run in parallel, and those that run sequentially. The ""sequential"" tests are currently _only_ of the category ""start a workflow job, restart, then ensure that job still succeeds"". [Here ](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L2955) is an example`abort.scheduled_abort` (wf 6d64cc05) running in parallel with a restart test case. The workflow job is supposed to abort. But because cromwell restarts during the middle of the job, TES is unable to tell if the job was running and marks it as [`Failed` instead of `Aborted`](https://travis-ci.org/broadinstitute/cromwell/jobs/312225757#L3534). The [`RestartTestCaseSpec`](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/it/scala/centaur/RestartTestCaseSpec.scala) could be renamed to `Sync` or `Sequential` and also used for these additional tests. In theory this should only take a half-day to test and fix and would save frustration across multiple PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3010
https://github.com/broadinstitute/cromwell/issues/3011:89,Integrability,depend,dependent,89,"On a restart, especially in a `NoNewCalls` state, cromwell shouldn't try to reconnect to dependent jobs if it isn't even readyToStart, aka the parent job is still running. In the `no_new_calls` test, `delayedTask1` should never be reconnected. - The workflow is submitted with options specifying [""workflowFailureMode"": ""NoNewCalls""](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/main/resources/standardTestCases/no_new_calls/no_new_calls.options#L2); - Both `shouldSucceed` and `boundToFail` start ~simultaneously; - `boundToFail` fails quickly, putting the workflow in a state of not-starting-any-new-calls; - cromwell restarts; - shouldSucceed finishes minutes later; - A/C: cromwell should never try to reconnect to `delayedTask1`, meaning that no metadata key [should be found](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/main/resources/standardTestCases/no_new_calls.test#L21-L33)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3011
https://github.com/broadinstitute/cromwell/issues/3011:194,Testability,test,test,194,"On a restart, especially in a `NoNewCalls` state, cromwell shouldn't try to reconnect to dependent jobs if it isn't even readyToStart, aka the parent job is still running. In the `no_new_calls` test, `delayedTask1` should never be reconnected. - The workflow is submitted with options specifying [""workflowFailureMode"": ""NoNewCalls""](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/main/resources/standardTestCases/no_new_calls/no_new_calls.options#L2); - Both `shouldSucceed` and `boundToFail` start ~simultaneously; - `boundToFail` fails quickly, putting the workflow in a state of not-starting-any-new-calls; - cromwell restarts; - shouldSucceed finishes minutes later; - A/C: cromwell should never try to reconnect to `delayedTask1`, meaning that no metadata key [should be found](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/main/resources/standardTestCases/no_new_calls.test#L21-L33)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3011
https://github.com/broadinstitute/cromwell/issues/3011:985,Testability,test,test,985,"On a restart, especially in a `NoNewCalls` state, cromwell shouldn't try to reconnect to dependent jobs if it isn't even readyToStart, aka the parent job is still running. In the `no_new_calls` test, `delayedTask1` should never be reconnected. - The workflow is submitted with options specifying [""workflowFailureMode"": ""NoNewCalls""](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/main/resources/standardTestCases/no_new_calls/no_new_calls.options#L2); - Both `shouldSucceed` and `boundToFail` start ~simultaneously; - `boundToFail` fails quickly, putting the workflow in a state of not-starting-any-new-calls; - cromwell restarts; - shouldSucceed finishes minutes later; - A/C: cromwell should never try to reconnect to `delayedTask1`, meaning that no metadata key [should be found](https://github.com/broadinstitute/cromwell/blob/4a37f95cc49567505ad50907f85c4fa046ac596e/centaur/src/main/resources/standardTestCases/no_new_calls.test#L21-L33)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3011
https://github.com/broadinstitute/cromwell/issues/3012:765,Availability,ERROR,ERROR,765,"There is evidence in centaur that CWL fully qualified names may not be stored in the database in a way CWL workflows survive a restart. Currently this occurs because the suite-of-restart-tests are running sequentially, while at the same time the suite-of-_non_-restart tests are running. If a restart test accidentally restarts during `three_step_cwl`, one might see the stack trace below. . A/C: Explicit centaur test that restarts in the middle of a CWL workflow. Example stack trace:; ```java; 2017-12-06 04:38:35,249 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowExecutionActor-20f2c75f-5250-4525-8e30-2330f25dbbec [UUID(20f2c75f)]: Restarting calls: ps:NA:1; 2017-12-06 04:38:35,277 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - JobStoreReadFailure; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:3344,Availability,ERROR,ERROR,3344,"t scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:35,304 cromwell-system-akka.dispatchers.engine-dispatcher-30 WARN - Couldn't find a suitable DSN, defaulting to a Noop one.; 2017-12-06 04:38:35,336 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - Using noop to send events.; 2017-12-06 04:38:35,447 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - EJEA_20f2c75f:ps:-1:1: Error reading from JobStore; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Su",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:3375,Availability,Error,Error,3375,"t scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:35,304 cromwell-system-akka.dispatchers.engine-dispatcher-30 WARN - Couldn't find a suitable DSN, defaulting to a Noop one.; 2017-12-06 04:38:35,336 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - Using noop to send events.; 2017-12-06 04:38:35,447 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - EJEA_20f2c75f:ps:-1:1: Error reading from JobStore; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Su",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:5698,Availability,ERROR,ERROR,5698,"hingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:5824,Availability,failure,failure,5824,"hingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:5919,Availability,failure,failure,5919,".apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.do",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:6364,Availability,Fault,FaultHandling,6364,"ask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.En",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:6413,Availability,Fault,FaultHandling,6413,"orkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobEx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:6441,Availability,Fault,FaultHandling,6441,"JoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:14",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:6490,Availability,Fault,FaultHandling,6490,"Pool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:143); 	at cromwell.engine.workflow.lifecycle.execution.job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:6519,Availability,Fault,FaultHandling,6519,"inPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:143); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionAct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:5860,Energy Efficiency,monitor,monitoring,5860,"hingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:5955,Energy Efficiency,monitor,monitoring,5955,".apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.do",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:1846,Performance,concurren,concurrent,1846,t; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:1908,Performance,concurren,concurrent,1908,); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:1975,Performance,concurren,concurrent,1975,s$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(Fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:2049,Performance,concurren,concurrent,2049,"fun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:35,304 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:2374,Performance,concurren,concurrent,2374,"re.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:35,304 cromwell-system-akka.dispatchers.engine-dispatcher-30 WARN - Couldn't find a suitable DSN, defaulting to a Noop one.; 2017-12-06 04:38:35,336 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - Using noop to send events.; 2017-12-06 04:38:35,447 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - EJEA_20f2c75f:ps:-1:1: Error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:4456,Performance,concurren,concurrent,4456,t; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:4518,Performance,concurren,concurrent,4518,); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:4585,Performance,concurren,concurrent,4585,s$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(Fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:4659,Performance,concurren,concurrent,4659,"fun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:4984,Performance,concurren,concurrent,4984,"re.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.en",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:9815,Performance,concurren,concurrent,9815,257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	... 5 more; Caused by: java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	... 4 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:9877,Performance,concurren,concurrent,9877,257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	... 5 more; Caused by: java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	... 4 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:9944,Performance,concurren,concurrent,9944,257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	... 5 more; Caused by: java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	... 4 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:10018,Performance,concurren,concurrent,10018,257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	... 5 more; Caused by: java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	... 4 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:10343,Performance,concurren,concurrent,10343,257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	... 5 more; Caused by: java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	... 4 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:187,Testability,test,tests,187,"There is evidence in centaur that CWL fully qualified names may not be stored in the database in a way CWL workflows survive a restart. Currently this occurs because the suite-of-restart-tests are running sequentially, while at the same time the suite-of-_non_-restart tests are running. If a restart test accidentally restarts during `three_step_cwl`, one might see the stack trace below. . A/C: Explicit centaur test that restarts in the middle of a CWL workflow. Example stack trace:; ```java; 2017-12-06 04:38:35,249 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowExecutionActor-20f2c75f-5250-4525-8e30-2330f25dbbec [UUID(20f2c75f)]: Restarting calls: ps:NA:1; 2017-12-06 04:38:35,277 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - JobStoreReadFailure; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:269,Testability,test,tests,269,"There is evidence in centaur that CWL fully qualified names may not be stored in the database in a way CWL workflows survive a restart. Currently this occurs because the suite-of-restart-tests are running sequentially, while at the same time the suite-of-_non_-restart tests are running. If a restart test accidentally restarts during `three_step_cwl`, one might see the stack trace below. . A/C: Explicit centaur test that restarts in the middle of a CWL workflow. Example stack trace:; ```java; 2017-12-06 04:38:35,249 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowExecutionActor-20f2c75f-5250-4525-8e30-2330f25dbbec [UUID(20f2c75f)]: Restarting calls: ps:NA:1; 2017-12-06 04:38:35,277 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - JobStoreReadFailure; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:301,Testability,test,test,301,"There is evidence in centaur that CWL fully qualified names may not be stored in the database in a way CWL workflows survive a restart. Currently this occurs because the suite-of-restart-tests are running sequentially, while at the same time the suite-of-_non_-restart tests are running. If a restart test accidentally restarts during `three_step_cwl`, one might see the stack trace below. . A/C: Explicit centaur test that restarts in the middle of a CWL workflow. Example stack trace:; ```java; 2017-12-06 04:38:35,249 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowExecutionActor-20f2c75f-5250-4525-8e30-2330f25dbbec [UUID(20f2c75f)]: Restarting calls: ps:NA:1; 2017-12-06 04:38:35,277 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - JobStoreReadFailure; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:414,Testability,test,test,414,"There is evidence in centaur that CWL fully qualified names may not be stored in the database in a way CWL workflows survive a restart. Currently this occurs because the suite-of-restart-tests are running sequentially, while at the same time the suite-of-_non_-restart tests are running. If a restart test accidentally restarts during `three_step_cwl`, one might see the stack trace below. . A/C: Explicit centaur test that restarts in the middle of a CWL workflow. Example stack trace:; ```java; 2017-12-06 04:38:35,249 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowExecutionActor-20f2c75f-5250-4525-8e30-2330f25dbbec [UUID(20f2c75f)]: Restarting calls: ps:NA:1; 2017-12-06 04:38:35,277 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - JobStoreReadFailure; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:7849,Testability,Log,LoggingFSM,7849,.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:143); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:134); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobExecutionActor.scala:44); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:801); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor.processEvent(EngineJobExecutionActor.scala:44); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor.aroundReceive(EngineJobExecutionActor.scala:44); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	... 5 more; Caused by: java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Ma,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:7930,Testability,Log,LoggingFSM,7930,k.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:143); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:134); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobExecutionActor.scala:44); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:801); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor.processEvent(EngineJobExecutionActor.scala:44); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor.aroundReceive(EngineJobExecutionActor.scala:44); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	... 5 more; Caused by: java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonf,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:7985,Testability,Log,LoggingFSM,7985,join.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:143); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:134); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobExecutionActor.scala:44); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:801); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor.processEvent(EngineJobExecutionActor.scala:44); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor.aroundReceive(EngineJobExecutionActor.scala:44); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	... 5 more; Caused by: java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:934,Usability,simpl,simpleton,934,"There is evidence in centaur that CWL fully qualified names may not be stored in the database in a way CWL workflows survive a restart. Currently this occurs because the suite-of-restart-tests are running sequentially, while at the same time the suite-of-_non_-restart tests are running. If a restart test accidentally restarts during `three_step_cwl`, one might see the stack trace below. . A/C: Explicit centaur test that restarts in the middle of a CWL workflow. Example stack trace:; ```java; 2017-12-06 04:38:35,249 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowExecutionActor-20f2c75f-5250-4525-8e30-2330f25dbbec [UUID(20f2c75f)]: Restarting calls: ps:NA:1; 2017-12-06 04:38:35,277 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - JobStoreReadFailure; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:1382,Usability,simpl,simpleton,1382,"w. . A/C: Explicit centaur test that restarts in the middle of a CWL workflow. Example stack trace:; ```java; 2017-12-06 04:38:35,249 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowExecutionActor-20f2c75f-5250-4525-8e30-2330f25dbbec [UUID(20f2c75f)]: Restarting calls: ps:NA:1; 2017-12-06 04:38:35,277 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - JobStoreReadFailure; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.Bl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:1467,Usability,simpl,simpleton,1467,"e stack trace:; ```java; 2017-12-06 04:38:35,249 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowExecutionActor-20f2c75f-5250-4525-8e30-2330f25dbbec [UUID(20f2c75f)]: Restarting calls: ps:NA:1; 2017-12-06 04:38:35,277 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - JobStoreReadFailure; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:3544,Usability,simpl,simpleton,3544,"ation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:35,304 cromwell-system-akka.dispatchers.engine-dispatcher-30 WARN - Couldn't find a suitable DSN, defaulting to a Noop one.; 2017-12-06 04:38:35,336 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - Using noop to send events.; 2017-12-06 04:38:35,447 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - EJEA_20f2c75f:ps:-1:1: Error reading from JobStore; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedT",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:3992,Usability,simpl,simpleton,3992,"ad.java:107); 2017-12-06 04:38:35,304 cromwell-system-akka.dispatchers.engine-dispatcher-30 WARN - Couldn't find a suitable DSN, defaulting to a Noop one.; 2017-12-06 04:38:35,336 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - Using noop to send events.; 2017-12-06 04:38:35,447 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - EJEA_20f2c75f:ps:-1:1: Error reading from JobStore; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.Bl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:4077,Usability,simpl,simpleton,4077,"her-30 WARN - Couldn't find a suitable DSN, defaulting to a Noop one.; 2017-12-06 04:38:35,336 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - Using noop to send events.; 2017-12-06 04:38:35,447 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - EJEA_20f2c75f:ps:-1:1: Error reading from JobStore; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:8903,Usability,simpl,simpleton,8903,a:44); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:801); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor.processEvent(EngineJobExecutionActor.scala:44); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor.aroundReceive(EngineJobExecutionActor.scala:44); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	... 5 more; Caused by: java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedT,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:9351,Usability,simpl,simpleton,9351,undReceive$(Actor.scala:512); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor.aroundReceive(EngineJobExecutionActor.scala:44); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	... 5 more; Caused by: java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.Bl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/issues/3012:9436,Usability,simpl,simpleton,9436,gineJobExecutionActor.aroundReceive(EngineJobExecutionActor.scala:44); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	... 5 more; Caused by: java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecut,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012
https://github.com/broadinstitute/cromwell/pull/3014:2,Deployability,Update,Updates,2,* Updates the TES backend to use the [v0.3 schema](https://github.com/ga4gh/task-execution-schemas/releases/tag/v0.3). ; * Updates the integration tests to use the latest Funnel binary,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3014
https://github.com/broadinstitute/cromwell/pull/3014:99,Deployability,release,releases,99,* Updates the TES backend to use the [v0.3 schema](https://github.com/ga4gh/task-execution-schemas/releases/tag/v0.3). ; * Updates the integration tests to use the latest Funnel binary,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3014
https://github.com/broadinstitute/cromwell/pull/3014:123,Deployability,Update,Updates,123,* Updates the TES backend to use the [v0.3 schema](https://github.com/ga4gh/task-execution-schemas/releases/tag/v0.3). ; * Updates the integration tests to use the latest Funnel binary,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3014
https://github.com/broadinstitute/cromwell/pull/3014:135,Deployability,integrat,integration,135,* Updates the TES backend to use the [v0.3 schema](https://github.com/ga4gh/task-execution-schemas/releases/tag/v0.3). ; * Updates the integration tests to use the latest Funnel binary,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3014
https://github.com/broadinstitute/cromwell/pull/3014:135,Integrability,integrat,integration,135,* Updates the TES backend to use the [v0.3 schema](https://github.com/ga4gh/task-execution-schemas/releases/tag/v0.3). ; * Updates the integration tests to use the latest Funnel binary,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3014
https://github.com/broadinstitute/cromwell/pull/3014:147,Testability,test,tests,147,* Updates the TES backend to use the [v0.3 schema](https://github.com/ga4gh/task-execution-schemas/releases/tag/v0.3). ; * Updates the integration tests to use the latest Funnel binary,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3014
https://github.com/broadinstitute/cromwell/pull/3015:64,Availability,down,download,64,The coursier plugin is a drop-in replacement for resolution and download of ivy artifacts. Resolution and downloading are done in parallel. https://github.com/coursier/coursier,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3015
https://github.com/broadinstitute/cromwell/pull/3015:106,Availability,down,downloading,106,The coursier plugin is a drop-in replacement for resolution and download of ivy artifacts. Resolution and downloading are done in parallel. https://github.com/coursier/coursier,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3015
https://github.com/broadinstitute/cromwell/pull/3015:13,Modifiability,plugin,plugin,13,The coursier plugin is a drop-in replacement for resolution and download of ivy artifacts. Resolution and downloading are done in parallel. https://github.com/coursier/coursier,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3015
https://github.com/broadinstitute/cromwell/issues/3018:334,Usability,simpl,simple,334,"Talk to me first, I have already started the parsing in my subworkflows branch. `ExpressionTool`s are top level CWLs that can also be called from workflow steps. The CWL examples include messing around in filesystems so setting up the appropriate conditions to run these could be as involved as running a task (i.e. these aren't just simple `InLineExpression`s).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3018
https://github.com/broadinstitute/cromwell/issues/3020:20,Testability,test,test,20,"Unlike WDL, our CWL test set relies on input files already existing (especially the one-step CLTs).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3020
https://github.com/broadinstitute/cromwell/pull/3022:36,Availability,error,errors,36,this will spit out more informative errors when a user encounters a parsing error:. ![image](https://user-images.githubusercontent.com/165320/33690130-25f3b10c-dab0-11e7-8221-275ea9abec20.png). Unfortunately this has to be done by hand as the `Coproduct` parser in circe only returns the `CNil` by virtue of its automaticness. As such we will continue to hit `CNil` in deeper parts of the code(as seen in this example!) unless we write `Decoder`s by hand for all of our coproducts or figure out a way to implement error accumulation in Coproduct decoder derivation and submit a patch to Circe. For reference the offending code is [here](https://github.com/circe/circe/blob/master/modules/shapes/src/main/scala/io/circe/shapes/CoproductInstances.scala#L18),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3022
https://github.com/broadinstitute/cromwell/pull/3022:76,Availability,error,error,76,this will spit out more informative errors when a user encounters a parsing error:. ![image](https://user-images.githubusercontent.com/165320/33690130-25f3b10c-dab0-11e7-8221-275ea9abec20.png). Unfortunately this has to be done by hand as the `Coproduct` parser in circe only returns the `CNil` by virtue of its automaticness. As such we will continue to hit `CNil` in deeper parts of the code(as seen in this example!) unless we write `Decoder`s by hand for all of our coproducts or figure out a way to implement error accumulation in Coproduct decoder derivation and submit a patch to Circe. For reference the offending code is [here](https://github.com/circe/circe/blob/master/modules/shapes/src/main/scala/io/circe/shapes/CoproductInstances.scala#L18),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3022
https://github.com/broadinstitute/cromwell/pull/3022:514,Availability,error,error,514,this will spit out more informative errors when a user encounters a parsing error:. ![image](https://user-images.githubusercontent.com/165320/33690130-25f3b10c-dab0-11e7-8221-275ea9abec20.png). Unfortunately this has to be done by hand as the `Coproduct` parser in circe only returns the `CNil` by virtue of its automaticness. As such we will continue to hit `CNil` in deeper parts of the code(as seen in this example!) unless we write `Decoder`s by hand for all of our coproducts or figure out a way to implement error accumulation in Coproduct decoder derivation and submit a patch to Circe. For reference the offending code is [here](https://github.com/circe/circe/blob/master/modules/shapes/src/main/scala/io/circe/shapes/CoproductInstances.scala#L18),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3022
https://github.com/broadinstitute/cromwell/pull/3022:578,Deployability,patch,patch,578,this will spit out more informative errors when a user encounters a parsing error:. ![image](https://user-images.githubusercontent.com/165320/33690130-25f3b10c-dab0-11e7-8221-275ea9abec20.png). Unfortunately this has to be done by hand as the `Coproduct` parser in circe only returns the `CNil` by virtue of its automaticness. As such we will continue to hit `CNil` in deeper parts of the code(as seen in this example!) unless we write `Decoder`s by hand for all of our coproducts or figure out a way to implement error accumulation in Coproduct decoder derivation and submit a patch to Circe. For reference the offending code is [here](https://github.com/circe/circe/blob/master/modules/shapes/src/main/scala/io/circe/shapes/CoproductInstances.scala#L18),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3022
https://github.com/broadinstitute/cromwell/issues/3023:483,Deployability,update,updated,483,"The ~wdl4s~ cromwell-wdl docs were deleted in bc3f754966c09a35b456aa1c047f7b5b81fa497e, while the example code used within the docs currently remains, unused, in the source tree. Delete the [cromwell-wdl examples folder](https://github.com/broadinstitute/cromwell/tree/9aed75028ddf99c6996fa20a830f914accce488d/wdl/src/main/scala/wdl/examples) -or- restore [the old docs](https://github.com/broadinstitute/cromwell/blob/216224925486f0e45d0d03bd0cc40e31054af5ee/wdl4s/README.md) to an updated entry under [/docs](https://github.com/broadinstitute/cromwell/tree/9aed75028ddf99c6996fa20a830f914accce488d/docs).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3023
https://github.com/broadinstitute/cromwell/issues/3024:246,Testability,test,test,246,Currently we only handle inputs and outputs that are specified to a `CwlType`. We should also support ; * `RecordSchema`; * `EnumSchema`; * `ArraySchema`. in both the input and output situations. Coincidentally this will help fix Cwl conformance test #4,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3024
https://github.com/broadinstitute/cromwell/issues/3025:32,Performance,cache,cache,32,Right now every attempt to call cache throws an exception and falls back to running the task.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3025
https://github.com/broadinstitute/cromwell/issues/3031:209,Deployability,release,releases,209,"Our WOMTool (previously WDLTool) has moved into the mega-mono-repo of Cromwell, but there's no way for Jamie User (like me) to find the latest of the WOMTool. . AC; - [x] have the WOMTool jar on the [Cromwell releases page](https://github.com/broadinstitute/cromwell/releases); - [x] update the [WOMtool docs](https://cromwell.readthedocs.io/en/develop/WOMtool/) to point to the releases page for the latest tool",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3031
https://github.com/broadinstitute/cromwell/issues/3031:267,Deployability,release,releases,267,"Our WOMTool (previously WDLTool) has moved into the mega-mono-repo of Cromwell, but there's no way for Jamie User (like me) to find the latest of the WOMTool. . AC; - [x] have the WOMTool jar on the [Cromwell releases page](https://github.com/broadinstitute/cromwell/releases); - [x] update the [WOMtool docs](https://cromwell.readthedocs.io/en/develop/WOMtool/) to point to the releases page for the latest tool",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3031
https://github.com/broadinstitute/cromwell/issues/3031:284,Deployability,update,update,284,"Our WOMTool (previously WDLTool) has moved into the mega-mono-repo of Cromwell, but there's no way for Jamie User (like me) to find the latest of the WOMTool. . AC; - [x] have the WOMTool jar on the [Cromwell releases page](https://github.com/broadinstitute/cromwell/releases); - [x] update the [WOMtool docs](https://cromwell.readthedocs.io/en/develop/WOMtool/) to point to the releases page for the latest tool",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3031
https://github.com/broadinstitute/cromwell/issues/3031:379,Deployability,release,releases,379,"Our WOMTool (previously WDLTool) has moved into the mega-mono-repo of Cromwell, but there's no way for Jamie User (like me) to find the latest of the WOMTool. . AC; - [x] have the WOMTool jar on the [Cromwell releases page](https://github.com/broadinstitute/cromwell/releases); - [x] update the [WOMtool docs](https://cromwell.readthedocs.io/en/develop/WOMtool/) to point to the releases page for the latest tool",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3031
https://github.com/broadinstitute/cromwell/issues/3032:531,Availability,error,error,531,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:1991,Deployability,pipeline,pipeline-workflows,1991,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:2165,Deployability,pipeline,pipeline-workflows,2165,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:141,Energy Efficiency,adapt,adapters,141,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:278,Energy Efficiency,adapt,adapters,278,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:403,Energy Efficiency,adapt,adapters,403,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:523,Energy Efficiency,adapt,adapter,523,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:731,Energy Efficiency,adapt,adapters,731,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:754,Energy Efficiency,adapt,adapters,754,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:867,Energy Efficiency,adapt,adapter,867,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:2142,Energy Efficiency,adapt,adapters,2142,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:2331,Energy Efficiency,adapt,adapter,2331,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:141,Integrability,adapter,adapters,141,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:278,Integrability,adapter,adapters,278,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:403,Integrability,adapter,adapters,403,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:523,Integrability,adapter,adapter,523,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:731,Integrability,adapter,adapters,731,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:754,Integrability,adapter,adapters,754,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:867,Integrability,adapter,adapter,867,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:2142,Integrability,adapter,adapters,2142,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:2331,Integrability,adapter,adapter,2331,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:141,Modifiability,adapt,adapters,141,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:278,Modifiability,adapt,adapters,278,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:403,Modifiability,adapt,adapters,403,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:523,Modifiability,adapt,adapter,523,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:731,Modifiability,adapt,adapters,731,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:754,Modifiability,adapt,adapters,754,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:867,Modifiability,adapt,adapter,867,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:2142,Modifiability,adapt,adapters,2142,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:2331,Modifiability,adapt,adapter,2331,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:391,Safety,detect,detect,391,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:860,Safety,detect,detect-adapter,860,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3032:2324,Safety,detect,detect-adapter,2324,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032
https://github.com/broadinstitute/cromwell/issues/3035:59,Testability,test,test,59,"Seemingly the cause of many of my centaur woes: if any CWL test happens to be caught up in a restart, the JobStore lookup fails and so does the workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3035
https://github.com/broadinstitute/cromwell/issues/3038:448,Deployability,upgrade,upgraded,448,- cromwell 30; - JES backend. ** This WDL worked just fine in cromwell 28.2 **. I have a workflow calling a sub-workflow. The parent workflow has an optional parameter. Cromwell is refusing to run the workflow stating that the optional parameter has no value. Even though the task has a `{$default= ....` statement. The optional parameter is given by the input json and does not interact with the subworkflow at all. The only difference was that I upgraded cromwell.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3038
https://github.com/broadinstitute/cromwell/issues/3039:34,Availability,failure,failures,34,"- cromwell 30; - JES backend. ```""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""key not found: <cnv_somatic_pair_workflow.wdl:63:64 identifier \""UHJlcHJvY2Vzc0ludGVydmFscw==\"">""; }; ],; ""message"": ""Workflow input processing failed""; }; ],```. That WDL file (subworkflow) is packaged correctly as a subworkflow and being submitted to cromwell. I'm pretty sure that this worked just fine in cromwell 28.2. I've had to do a bunch of workarounds for issues in cromwell 30, so confounds abound.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3039
https://github.com/broadinstitute/cromwell/issues/3039:87,Integrability,message,message,87,"- cromwell 30; - JES backend. ```""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""key not found: <cnv_somatic_pair_workflow.wdl:63:64 identifier \""UHJlcHJvY2Vzc0ludGVydmFscw==\"">""; }; ],; ""message"": ""Workflow input processing failed""; }; ],```. That WDL file (subworkflow) is packaged correctly as a subworkflow and being submitted to cromwell. I'm pretty sure that this worked just fine in cromwell 28.2. I've had to do a bunch of workarounds for issues in cromwell 30, so confounds abound.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3039
https://github.com/broadinstitute/cromwell/issues/3039:205,Integrability,message,message,205,"- cromwell 30; - JES backend. ```""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""key not found: <cnv_somatic_pair_workflow.wdl:63:64 identifier \""UHJlcHJvY2Vzc0ludGVydmFscw==\"">""; }; ],; ""message"": ""Workflow input processing failed""; }; ],```. That WDL file (subworkflow) is packaged correctly as a subworkflow and being submitted to cromwell. I'm pretty sure that this worked just fine in cromwell 28.2. I've had to do a bunch of workarounds for issues in cromwell 30, so confounds abound.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3039
https://github.com/broadinstitute/cromwell/pull/3040:591,Availability,failure,failures,591,"This does appear to work for 3 step but I'd like to add metadata assertions to the existing Centaur test assuming the PR doesn't get gulled out of existence. TODO. - [x] Set parent workflow into subworkflows; - [x] Fix input path localization; - [x] Make ""please turn on Docker"" an explicit thing in `core`; - [x] Unbreak all the conformance tests that are broken on this branch; - [x] Centaur assertions that the expected Docker images are being used; - [ ] Turn on conformance tests in backends that require Docker; - [ ] Fix globs on JES; - [ ] Fix inputs on JES; - [ ] Return validation failures to the caller",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3040
https://github.com/broadinstitute/cromwell/pull/3040:580,Security,validat,validation,580,"This does appear to work for 3 step but I'd like to add metadata assertions to the existing Centaur test assuming the PR doesn't get gulled out of existence. TODO. - [x] Set parent workflow into subworkflows; - [x] Fix input path localization; - [x] Make ""please turn on Docker"" an explicit thing in `core`; - [x] Unbreak all the conformance tests that are broken on this branch; - [x] Centaur assertions that the expected Docker images are being used; - [ ] Turn on conformance tests in backends that require Docker; - [ ] Fix globs on JES; - [ ] Fix inputs on JES; - [ ] Return validation failures to the caller",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3040
https://github.com/broadinstitute/cromwell/pull/3040:65,Testability,assert,assertions,65,"This does appear to work for 3 step but I'd like to add metadata assertions to the existing Centaur test assuming the PR doesn't get gulled out of existence. TODO. - [x] Set parent workflow into subworkflows; - [x] Fix input path localization; - [x] Make ""please turn on Docker"" an explicit thing in `core`; - [x] Unbreak all the conformance tests that are broken on this branch; - [x] Centaur assertions that the expected Docker images are being used; - [ ] Turn on conformance tests in backends that require Docker; - [ ] Fix globs on JES; - [ ] Fix inputs on JES; - [ ] Return validation failures to the caller",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3040
https://github.com/broadinstitute/cromwell/pull/3040:100,Testability,test,test,100,"This does appear to work for 3 step but I'd like to add metadata assertions to the existing Centaur test assuming the PR doesn't get gulled out of existence. TODO. - [x] Set parent workflow into subworkflows; - [x] Fix input path localization; - [x] Make ""please turn on Docker"" an explicit thing in `core`; - [x] Unbreak all the conformance tests that are broken on this branch; - [x] Centaur assertions that the expected Docker images are being used; - [ ] Turn on conformance tests in backends that require Docker; - [ ] Fix globs on JES; - [ ] Fix inputs on JES; - [ ] Return validation failures to the caller",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3040
https://github.com/broadinstitute/cromwell/pull/3040:342,Testability,test,tests,342,"This does appear to work for 3 step but I'd like to add metadata assertions to the existing Centaur test assuming the PR doesn't get gulled out of existence. TODO. - [x] Set parent workflow into subworkflows; - [x] Fix input path localization; - [x] Make ""please turn on Docker"" an explicit thing in `core`; - [x] Unbreak all the conformance tests that are broken on this branch; - [x] Centaur assertions that the expected Docker images are being used; - [ ] Turn on conformance tests in backends that require Docker; - [ ] Fix globs on JES; - [ ] Fix inputs on JES; - [ ] Return validation failures to the caller",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3040
https://github.com/broadinstitute/cromwell/pull/3040:394,Testability,assert,assertions,394,"This does appear to work for 3 step but I'd like to add metadata assertions to the existing Centaur test assuming the PR doesn't get gulled out of existence. TODO. - [x] Set parent workflow into subworkflows; - [x] Fix input path localization; - [x] Make ""please turn on Docker"" an explicit thing in `core`; - [x] Unbreak all the conformance tests that are broken on this branch; - [x] Centaur assertions that the expected Docker images are being used; - [ ] Turn on conformance tests in backends that require Docker; - [ ] Fix globs on JES; - [ ] Fix inputs on JES; - [ ] Return validation failures to the caller",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3040
https://github.com/broadinstitute/cromwell/pull/3040:479,Testability,test,tests,479,"This does appear to work for 3 step but I'd like to add metadata assertions to the existing Centaur test assuming the PR doesn't get gulled out of existence. TODO. - [x] Set parent workflow into subworkflows; - [x] Fix input path localization; - [x] Make ""please turn on Docker"" an explicit thing in `core`; - [x] Unbreak all the conformance tests that are broken on this branch; - [x] Centaur assertions that the expected Docker images are being used; - [ ] Turn on conformance tests in backends that require Docker; - [ ] Fix globs on JES; - [ ] Fix inputs on JES; - [ ] Return validation failures to the caller",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3040
https://github.com/broadinstitute/cromwell/issues/3043:28,Testability,test,tests,28,"Some of the CWL conformance tests submit workflow files that contain an Array of Workflows/CLTs instead of a single one. The file path has a ""tag"" at the end specifying which workflow/CLT needs to be run in the array. Example: https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/conformance_test_v1.0.yaml#L250. To pass those tests we need to ; 1) Be able to parse arrays instead of single Workflow/CLT; 2) submit the file to Cromwell and (separately ?) the name of the Workflow?CLT to be run",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3043
https://github.com/broadinstitute/cromwell/issues/3043:360,Testability,test,tests,360,"Some of the CWL conformance tests submit workflow files that contain an Array of Workflows/CLTs instead of a single one. The file path has a ""tag"" at the end specifying which workflow/CLT needs to be run in the array. Example: https://github.com/common-workflow-language/common-workflow-language/blob/master/v1.0/conformance_test_v1.0.yaml#L250. To pass those tests we need to ; 1) Be able to parse arrays instead of single Workflow/CLT; 2) submit the file to Cromwell and (separately ?) the name of the Workflow?CLT to be run",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3043
https://github.com/broadinstitute/cromwell/issues/3044:82,Deployability,pipeline,pipelines,82,"I recently got into a trouble because of wrong caching in cromwell. In most of my pipelines I copy the results of last tasks to some folder which I define in the variable (unfortunately the option.json feature of cromwell to kind'of ""copy workflow results"" is useless for us as it copies all the nested trash, like ""id/execution"" instead of just resulting files). ; Unfortunately, my task that does the copying was cached even if the task on which it depends had different inputs (and was not cached). That means that we have a false positive caching and the copying task copies the same file from cache all over again.; Here is an example of my WDL. However, it fails in all WDLs where I use the copy-task for results; ```wdl; workflow Diamond_Blast {. Int threads; File db; File query; String result_name; String results_folder; String mode = ""blastx"". call diamond_blast {; input:; threads = threads,; database = db,; name = result_name,; query = query,; mode = mode. }. call copy as copy_results {; input:; files = [diamond_blast.out],; destination = results_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:1747,Deployability,pipeline,pipelines,1747,"reads; File db; File query; String result_name; String results_folder; String mode = ""blastx"". call diamond_blast {; input:; threads = threads,; database = db,; name = result_name,; query = query,; mode = mode. }. call copy as copy_results {; input:; files = [diamond_blast.out],; destination = results_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:1904,Deployability,pipeline,pipelines,1904,"= result_name,; query = query,; mode = mode. }. call copy as copy_results {; input:; files = [diamond_blast.out],; destination = results_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:1998,Deployability,pipeline,pipelines,1998,"ts_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that shows that the task diamond_blast recomputes while the copy task copies the cached result, in other words it sends the same file as pr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:2258,Deployability,pipeline,pipelines,2258,"atabase; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that shows that the task diamond_blast recomputes while the copy task copies the cached result, in other words it sends the same file as previous time, totally ignoring the changes of output in the diamond_blast task from which it depends!!!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:2388,Deployability,pipeline,pipelines,2388,"atabase; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that shows that the task diamond_blast recomputes while the copy task copies the cached result, in other words it sends the same file as previous time, totally ignoring the changes of output in the diamond_blast task from which it depends!!!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:2639,Deployability,pipeline,pipelines,2639,"atabase; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that shows that the task diamond_blast recomputes while the copy task copies the cached result, in other words it sends the same file as previous time, totally ignoring the changes of output in the diamond_blast task from which it depends!!!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:2770,Deployability,pipeline,pipelines,2770,"atabase; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that shows that the task diamond_blast recomputes while the copy task copies the cached result, in other words it sends the same file as previous time, totally ignoring the changes of output in the diamond_blast task from which it depends!!!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:451,Integrability,depend,depends,451,"I recently got into a trouble because of wrong caching in cromwell. In most of my pipelines I copy the results of last tasks to some folder which I define in the variable (unfortunately the option.json feature of cromwell to kind'of ""copy workflow results"" is useless for us as it copies all the nested trash, like ""id/execution"" instead of just resulting files). ; Unfortunately, my task that does the copying was cached even if the task on which it depends had different inputs (and was not cached). That means that we have a false positive caching and the copying task copies the same file from cache all over again.; Here is an example of my WDL. However, it fails in all WDLs where I use the copy-task for results; ```wdl; workflow Diamond_Blast {. Int threads; File db; File query; String result_name; String results_folder; String mode = ""blastx"". call diamond_blast {; input:; threads = threads,; database = db,; name = result_name,; query = query,; mode = mode. }. call copy as copy_results {; input:; files = [diamond_blast.out],; destination = results_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:3153,Integrability,depend,depends,3153,"atabase; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that shows that the task diamond_blast recomputes while the copy task copies the cached result, in other words it sends the same file as previous time, totally ignoring the changes of output in the diamond_blast task from which it depends!!!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:162,Modifiability,variab,variable,162,"I recently got into a trouble because of wrong caching in cromwell. In most of my pipelines I copy the results of last tasks to some folder which I define in the variable (unfortunately the option.json feature of cromwell to kind'of ""copy workflow results"" is useless for us as it copies all the nested trash, like ""id/execution"" instead of just resulting files). ; Unfortunately, my task that does the copying was cached even if the task on which it depends had different inputs (and was not cached). That means that we have a false positive caching and the copying task copies the same file from cache all over again.; Here is an example of my WDL. However, it fails in all WDLs where I use the copy-task for results; ```wdl; workflow Diamond_Blast {. Int threads; File db; File query; String result_name; String results_folder; String mode = ""blastx"". call diamond_blast {; input:; threads = threads,; database = db,; name = result_name,; query = query,; mode = mode. }. call copy as copy_results {; input:; files = [diamond_blast.out],; destination = results_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:415,Performance,cache,cached,415,"I recently got into a trouble because of wrong caching in cromwell. In most of my pipelines I copy the results of last tasks to some folder which I define in the variable (unfortunately the option.json feature of cromwell to kind'of ""copy workflow results"" is useless for us as it copies all the nested trash, like ""id/execution"" instead of just resulting files). ; Unfortunately, my task that does the copying was cached even if the task on which it depends had different inputs (and was not cached). That means that we have a false positive caching and the copying task copies the same file from cache all over again.; Here is an example of my WDL. However, it fails in all WDLs where I use the copy-task for results; ```wdl; workflow Diamond_Blast {. Int threads; File db; File query; String result_name; String results_folder; String mode = ""blastx"". call diamond_blast {; input:; threads = threads,; database = db,; name = result_name,; query = query,; mode = mode. }. call copy as copy_results {; input:; files = [diamond_blast.out],; destination = results_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:493,Performance,cache,cached,493,"I recently got into a trouble because of wrong caching in cromwell. In most of my pipelines I copy the results of last tasks to some folder which I define in the variable (unfortunately the option.json feature of cromwell to kind'of ""copy workflow results"" is useless for us as it copies all the nested trash, like ""id/execution"" instead of just resulting files). ; Unfortunately, my task that does the copying was cached even if the task on which it depends had different inputs (and was not cached). That means that we have a false positive caching and the copying task copies the same file from cache all over again.; Here is an example of my WDL. However, it fails in all WDLs where I use the copy-task for results; ```wdl; workflow Diamond_Blast {. Int threads; File db; File query; String result_name; String results_folder; String mode = ""blastx"". call diamond_blast {; input:; threads = threads,; database = db,; name = result_name,; query = query,; mode = mode. }. call copy as copy_results {; input:; files = [diamond_blast.out],; destination = results_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:598,Performance,cache,cache,598,"I recently got into a trouble because of wrong caching in cromwell. In most of my pipelines I copy the results of last tasks to some folder which I define in the variable (unfortunately the option.json feature of cromwell to kind'of ""copy workflow results"" is useless for us as it copies all the nested trash, like ""id/execution"" instead of just resulting files). ; Unfortunately, my task that does the copying was cached even if the task on which it depends had different inputs (and was not cached). That means that we have a false positive caching and the copying task copies the same file from cache all over again.; Here is an example of my WDL. However, it fails in all WDLs where I use the copy-task for results; ```wdl; workflow Diamond_Blast {. Int threads; File db; File query; String result_name; String results_folder; String mode = ""blastx"". call diamond_blast {; input:; threads = threads,; database = db,; name = result_name,; query = query,; mode = mode. }. call copy as copy_results {; input:; files = [diamond_blast.out],; destination = results_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:2177,Performance,cache,cache,2177,"atabase; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that shows that the task diamond_blast recomputes while the copy task copies the cached result, in other words it sends the same file as previous time, totally ignoring the changes of output in the diamond_blast task from which it depends!!!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:2517,Performance,Cache,Cache,2517,"atabase; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that shows that the task diamond_blast recomputes while the copy task copies the cached result, in other words it sends the same file as previous time, totally ignoring the changes of output in the diamond_blast task from which it depends!!!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:2900,Performance,Cache,Cache,2900,"atabase; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that shows that the task diamond_blast recomputes while the copy task copies the cached result, in other words it sends the same file as previous time, totally ignoring the changes of output in the diamond_blast task from which it depends!!!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/issues/3044:3003,Performance,cache,cached,3003,"atabase; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that shows that the task diamond_blast recomputes while the copy task copies the cached result, in other words it sends the same file as previous time, totally ignoring the changes of output in the diamond_blast task from which it depends!!!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044
https://github.com/broadinstitute/cromwell/pull/3045:274,Testability,test,tests,274,This is based off of Dan's branch as it needs it to parse array types correctly. I'll rebase on develop when `db_test_4` is merged but targeting it for now so this PR only shows the scatter changes.; There are a few other features missing to get all the scatter conformance tests passing but some of them do already.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3045
https://github.com/broadinstitute/cromwell/pull/3048:99,Availability,error,error,99,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048
https://github.com/broadinstitute/cromwell/pull/3048:204,Availability,error,error,204,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048
https://github.com/broadinstitute/cromwell/pull/3048:297,Availability,error,error,297,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048
https://github.com/broadinstitute/cromwell/pull/3048:351,Availability,error,error,351,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048
https://github.com/broadinstitute/cromwell/pull/3048:390,Availability,error,error,390,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048
https://github.com/broadinstitute/cromwell/pull/3048:317,Deployability,hotfix,hotfix,317,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048
https://github.com/broadinstitute/cromwell/pull/3048:105,Integrability,message,message,105,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048
https://github.com/broadinstitute/cromwell/pull/3048:210,Integrability,message,message,210,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048
https://github.com/broadinstitute/cromwell/pull/3048:303,Integrability,message,message,303,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048
https://github.com/broadinstitute/cromwell/pull/3048:357,Integrability,message,message,357,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048
https://github.com/broadinstitute/cromwell/pull/3048:457,Integrability,message,message,457,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048
https://github.com/broadinstitute/cromwell/issues/3055:52,Availability,error,error,52,"I validated `cnv_param_sweep.wdl` and received this error:; ```; ERROR: Missing value or call: Couldn't find value or call with name 'CollectAllelicCountsNormal' in workflow (line 136):. Int model_segments_disk = ceil(size(DenoiseReadCountsTumor.denoised_copy_ratios, ""GB"")) + ceil(size(CollectAllelicCountsTumor.allelic_counts, ""GB"")) + ceil(size(CollectAllelicCountsNormal.allelic_counts, ""GB"")) + disk_pad; ```; `cnv_param_sweep.wdl` imports 3 WDLs, and the error was actually in `cnv_somatic_pair_workflow.wdl`. It would be great to know what WDL the error is in, especially if it's not the primary WDL.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3055
https://github.com/broadinstitute/cromwell/issues/3055:65,Availability,ERROR,ERROR,65,"I validated `cnv_param_sweep.wdl` and received this error:; ```; ERROR: Missing value or call: Couldn't find value or call with name 'CollectAllelicCountsNormal' in workflow (line 136):. Int model_segments_disk = ceil(size(DenoiseReadCountsTumor.denoised_copy_ratios, ""GB"")) + ceil(size(CollectAllelicCountsTumor.allelic_counts, ""GB"")) + ceil(size(CollectAllelicCountsNormal.allelic_counts, ""GB"")) + disk_pad; ```; `cnv_param_sweep.wdl` imports 3 WDLs, and the error was actually in `cnv_somatic_pair_workflow.wdl`. It would be great to know what WDL the error is in, especially if it's not the primary WDL.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3055
https://github.com/broadinstitute/cromwell/issues/3055:461,Availability,error,error,461,"I validated `cnv_param_sweep.wdl` and received this error:; ```; ERROR: Missing value or call: Couldn't find value or call with name 'CollectAllelicCountsNormal' in workflow (line 136):. Int model_segments_disk = ceil(size(DenoiseReadCountsTumor.denoised_copy_ratios, ""GB"")) + ceil(size(CollectAllelicCountsTumor.allelic_counts, ""GB"")) + ceil(size(CollectAllelicCountsNormal.allelic_counts, ""GB"")) + disk_pad; ```; `cnv_param_sweep.wdl` imports 3 WDLs, and the error was actually in `cnv_somatic_pair_workflow.wdl`. It would be great to know what WDL the error is in, especially if it's not the primary WDL.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3055
https://github.com/broadinstitute/cromwell/issues/3055:555,Availability,error,error,555,"I validated `cnv_param_sweep.wdl` and received this error:; ```; ERROR: Missing value or call: Couldn't find value or call with name 'CollectAllelicCountsNormal' in workflow (line 136):. Int model_segments_disk = ceil(size(DenoiseReadCountsTumor.denoised_copy_ratios, ""GB"")) + ceil(size(CollectAllelicCountsTumor.allelic_counts, ""GB"")) + ceil(size(CollectAllelicCountsNormal.allelic_counts, ""GB"")) + disk_pad; ```; `cnv_param_sweep.wdl` imports 3 WDLs, and the error was actually in `cnv_somatic_pair_workflow.wdl`. It would be great to know what WDL the error is in, especially if it's not the primary WDL.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3055
https://github.com/broadinstitute/cromwell/issues/3055:2,Security,validat,validated,2,"I validated `cnv_param_sweep.wdl` and received this error:; ```; ERROR: Missing value or call: Couldn't find value or call with name 'CollectAllelicCountsNormal' in workflow (line 136):. Int model_segments_disk = ceil(size(DenoiseReadCountsTumor.denoised_copy_ratios, ""GB"")) + ceil(size(CollectAllelicCountsTumor.allelic_counts, ""GB"")) + ceil(size(CollectAllelicCountsNormal.allelic_counts, ""GB"")) + disk_pad; ```; `cnv_param_sweep.wdl` imports 3 WDLs, and the error was actually in `cnv_somatic_pair_workflow.wdl`. It would be great to know what WDL the error is in, especially if it's not the primary WDL.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3055
https://github.com/broadinstitute/cromwell/issues/3060:15,Availability,error,error,15,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060
https://github.com/broadinstitute/cromwell/issues/3060:327,Availability,error,error,327,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060
https://github.com/broadinstitute/cromwell/issues/3060:341,Availability,failure,failures,341,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060
https://github.com/broadinstitute/cromwell/issues/3060:161,Deployability,pipeline,pipeline-tools,161,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060
https://github.com/broadinstitute/cromwell/issues/3060:602,Deployability,pipeline,pipelines,602,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060
https://github.com/broadinstitute/cromwell/issues/3060:217,Energy Efficiency,adapt,adapter,217,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060
https://github.com/broadinstitute/cromwell/issues/3060:217,Integrability,adapter,adapter,217,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060
https://github.com/broadinstitute/cromwell/issues/3060:394,Integrability,message,message,394,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060
https://github.com/broadinstitute/cromwell/issues/3060:509,Integrability,message,message,509,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060
https://github.com/broadinstitute/cromwell/issues/3060:217,Modifiability,adapt,adapter,217,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060
https://github.com/broadinstitute/cromwell/issues/3060:306,Security,validat,validation,306,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060
https://github.com/broadinstitute/cromwell/issues/3065:118,Availability,echo,echo,118,"Given this workflow:; ```; workflow w {; 	call t {}; 	output { t.* }; }. task t {; 	; 	command {; 		touch file.txt; 		echo ""file.txt"" > filename.txt; 	}. 	output {; 		File f1 = ""filename.txt""; 		File f2 = read_string(f1)		; 	}. 	runtime {; 	 docker: ""ubuntu""; 	}; }; ```; One would expect 2 files to be marked for delocalization for `call t`, yet only ""filename.txt"" is properly localized, but not ""file.txt"". This behavior is only observed when running against the JES backend, and works as expected on the SharedFileSystem backend. This is a regression introduced in Cromwell v30. When checking Google's operation metadata, the outputs are listed as:; ```; outputs:; filename.txt: gs://rm-dev/w/6b29d52a-d665-4ce9-b042-8cd4e4374473/call-t/filename.txt; t-rc.txt: gs://rm-dev/w/6b29d52a-d665-4ce9-b042-8cd4e4374473/call-t/t-rc.txt; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3065
https://github.com/broadinstitute/cromwell/pull/3069:287,Deployability,hotfix,hotfix,287,"Stumbled upon this while trying to abort a CWL workflow that was failing to submit to JES.; Aborting it was not short circuiting the ""try 10 times"" mechanism of creating JES runs or polling.; The workflow do end up in `Aborted` state after the 10 retries so we don't necessarily have to hotfix it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3069
https://github.com/broadinstitute/cromwell/pull/3069:35,Safety,abort,abort,35,"Stumbled upon this while trying to abort a CWL workflow that was failing to submit to JES.; Aborting it was not short circuiting the ""try 10 times"" mechanism of creating JES runs or polling.; The workflow do end up in `Aborted` state after the 10 retries so we don't necessarily have to hotfix it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3069
https://github.com/broadinstitute/cromwell/pull/3069:92,Safety,Abort,Aborting,92,"Stumbled upon this while trying to abort a CWL workflow that was failing to submit to JES.; Aborting it was not short circuiting the ""try 10 times"" mechanism of creating JES runs or polling.; The workflow do end up in `Aborted` state after the 10 retries so we don't necessarily have to hotfix it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3069
https://github.com/broadinstitute/cromwell/pull/3069:219,Safety,Abort,Aborted,219,"Stumbled upon this while trying to abort a CWL workflow that was failing to submit to JES.; Aborting it was not short circuiting the ""try 10 times"" mechanism of creating JES runs or polling.; The workflow do end up in `Aborted` state after the 10 retries so we don't necessarily have to hotfix it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3069
https://github.com/broadinstitute/cromwell/pull/3072:385,Usability,simpl,simple,385,"It seems that sometimes the request to write the auth file fails but it still ends up being written on GCS. However because we write it in ""fail if this file already exists mode"", every subsequent retry to write the file fails.; Instead check for existence before and act accordingly. This is less clean as the ""check if exist then write"" is not atomic anymore but I don't see another simple way to fix this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3072
https://github.com/broadinstitute/cromwell/issues/3074:912,Availability,recover,recover,912,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/issues/3074:1355,Availability,failure,failures,1355,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/issues/3074:168,Performance,cache,cache,168,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/issues/3074:701,Performance,cache,cache,701,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/issues/3074:826,Performance,cache,cache,826,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/issues/3074:984,Performance,cache,cache,984,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/issues/3074:1445,Performance,cache,cache,1445,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/issues/3074:912,Safety,recover,recover,912,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/issues/3074:220,Security,hash,hashes,220,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/issues/3074:305,Security,hash,hashes,305,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/issues/3074:587,Security,hash,hashes,587,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/issues/3074:750,Security,hash,hash,750,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/issues/3074:229,Usability,simpl,simpletons,229,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074
https://github.com/broadinstitute/cromwell/pull/3075:3,Deployability,hotfix,hotfix,3,30 hotfix version of https://github.com/broadinstitute/cromwell/pull/3072,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3075
https://github.com/broadinstitute/cromwell/pull/3077:198,Availability,avail,available,198,"Validates `cpuMin`, `cpuMax`, `ramMin` and `ramMax` as valid cpu / memory runtime attributes; Maps `cpuMin` and `ramMin` to `cpu` and `memory` (respectively); All the resource requirements are then available in the runtime attributes map for the backend to play with if it wants to / can.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3077
https://github.com/broadinstitute/cromwell/pull/3077:0,Security,Validat,Validates,0,"Validates `cpuMin`, `cpuMax`, `ramMin` and `ramMax` as valid cpu / memory runtime attributes; Maps `cpuMin` and `ramMin` to `cpu` and `memory` (respectively); All the resource requirements are then available in the runtime attributes map for the backend to play with if it wants to / can.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3077
https://github.com/broadinstitute/cromwell/pull/3079:0,Deployability,Update,Updates,0,Updates to WomFile equals/add.; Future work will add CWL parsing and cromwell generation/usage.; Updates to other parts of code to support changes to WOM.; IntelliJ recommended updates to use lengthCompare.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3079
https://github.com/broadinstitute/cromwell/pull/3079:97,Deployability,Update,Updates,97,Updates to WomFile equals/add.; Future work will add CWL parsing and cromwell generation/usage.; Updates to other parts of code to support changes to WOM.; IntelliJ recommended updates to use lengthCompare.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3079
https://github.com/broadinstitute/cromwell/pull/3079:177,Deployability,update,updates,177,Updates to WomFile equals/add.; Future work will add CWL parsing and cromwell generation/usage.; Updates to other parts of code to support changes to WOM.; IntelliJ recommended updates to use lengthCompare.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3079
https://github.com/broadinstitute/cromwell/issues/3083:344,Security,access,access,344,"It would be helpful if the /query endpoint included total page count when requesting paginated results. **Use case:**; Currently, the response of `/query` endpoint returns workflows from oldest to newest. The requirements for the Job Manager are such that they require information about newest workflows first. One way that the Job Manager can access newest workflows first is they could paginate through the results in reverse order, which is possible if they have access to the total page count. . The other solution for this use case is if there was an option to provide a sort param and sort direction for the query endpoint. A separate [issue] (https://github.com/broadinstitute/cromwell/issues/3082) has been filed for that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3083
https://github.com/broadinstitute/cromwell/issues/3083:466,Security,access,access,466,"It would be helpful if the /query endpoint included total page count when requesting paginated results. **Use case:**; Currently, the response of `/query` endpoint returns workflows from oldest to newest. The requirements for the Job Manager are such that they require information about newest workflows first. One way that the Job Manager can access newest workflows first is they could paginate through the results in reverse order, which is possible if they have access to the total page count. . The other solution for this use case is if there was an option to provide a sort param and sort direction for the query endpoint. A separate [issue] (https://github.com/broadinstitute/cromwell/issues/3082) has been filed for that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3083
https://github.com/broadinstitute/cromwell/issues/3085:606,Modifiability,config,config,606,"When specifying a `user_service_account` in Cromwell, the most common scenario is that the same service account should be used for `google_compute_service_account`. We currently make the user supply the JSON for the former (which includes the account name) and then the account name again for the latter.... and it's not obvious you have to do that, and people will mess it up and be confused. Instead we should use the service account from `user_service_account` as the default service account for `google_compute_service_account` unless the latter is explicitly overridden by specifying in either in the config or the workflow options",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3085
https://github.com/broadinstitute/cromwell/pull/3086:537,Modifiability,config,config,537,"All the tests that pass on the Local backend currently pass on PAPI with the hackery documented below. This does not attempt to run tests that fail on Local because there's no realistic reason to believe a conformance test that fails Local would succeed on PAPI and the PAPI conformance test run would take hours. On the topic of slowness, the PAPI conformance run will probably have to be converted to a cron job once more conformance tests start passing. The hacks:; - Cromwell now allows for a default Docker image to be specified in config. This is required for those conformance tests that don't specify a `DockerRequirement`.; - Cromwell allows for a ""GCS default input prefix"" of any input files when using the JES backend. This allows for the conformance input JSONs to be used as-is. It also works to hack the input JSONs to specify the GCS paths where input files are staged and not bother with the ""GCS default input prefix"".; - I had to turn off call caching since the hashing actor was looking at the non-GCS paths of files. It's possible this could be worked around if anyone felt it was worth the effort, but I didn't.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3086
https://github.com/broadinstitute/cromwell/pull/3086:981,Security,hash,hashing,981,"All the tests that pass on the Local backend currently pass on PAPI with the hackery documented below. This does not attempt to run tests that fail on Local because there's no realistic reason to believe a conformance test that fails Local would succeed on PAPI and the PAPI conformance test run would take hours. On the topic of slowness, the PAPI conformance run will probably have to be converted to a cron job once more conformance tests start passing. The hacks:; - Cromwell now allows for a default Docker image to be specified in config. This is required for those conformance tests that don't specify a `DockerRequirement`.; - Cromwell allows for a ""GCS default input prefix"" of any input files when using the JES backend. This allows for the conformance input JSONs to be used as-is. It also works to hack the input JSONs to specify the GCS paths where input files are staged and not bother with the ""GCS default input prefix"".; - I had to turn off call caching since the hashing actor was looking at the non-GCS paths of files. It's possible this could be worked around if anyone felt it was worth the effort, but I didn't.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3086
https://github.com/broadinstitute/cromwell/pull/3086:8,Testability,test,tests,8,"All the tests that pass on the Local backend currently pass on PAPI with the hackery documented below. This does not attempt to run tests that fail on Local because there's no realistic reason to believe a conformance test that fails Local would succeed on PAPI and the PAPI conformance test run would take hours. On the topic of slowness, the PAPI conformance run will probably have to be converted to a cron job once more conformance tests start passing. The hacks:; - Cromwell now allows for a default Docker image to be specified in config. This is required for those conformance tests that don't specify a `DockerRequirement`.; - Cromwell allows for a ""GCS default input prefix"" of any input files when using the JES backend. This allows for the conformance input JSONs to be used as-is. It also works to hack the input JSONs to specify the GCS paths where input files are staged and not bother with the ""GCS default input prefix"".; - I had to turn off call caching since the hashing actor was looking at the non-GCS paths of files. It's possible this could be worked around if anyone felt it was worth the effort, but I didn't.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3086
https://github.com/broadinstitute/cromwell/pull/3086:132,Testability,test,tests,132,"All the tests that pass on the Local backend currently pass on PAPI with the hackery documented below. This does not attempt to run tests that fail on Local because there's no realistic reason to believe a conformance test that fails Local would succeed on PAPI and the PAPI conformance test run would take hours. On the topic of slowness, the PAPI conformance run will probably have to be converted to a cron job once more conformance tests start passing. The hacks:; - Cromwell now allows for a default Docker image to be specified in config. This is required for those conformance tests that don't specify a `DockerRequirement`.; - Cromwell allows for a ""GCS default input prefix"" of any input files when using the JES backend. This allows for the conformance input JSONs to be used as-is. It also works to hack the input JSONs to specify the GCS paths where input files are staged and not bother with the ""GCS default input prefix"".; - I had to turn off call caching since the hashing actor was looking at the non-GCS paths of files. It's possible this could be worked around if anyone felt it was worth the effort, but I didn't.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3086
https://github.com/broadinstitute/cromwell/pull/3086:218,Testability,test,test,218,"All the tests that pass on the Local backend currently pass on PAPI with the hackery documented below. This does not attempt to run tests that fail on Local because there's no realistic reason to believe a conformance test that fails Local would succeed on PAPI and the PAPI conformance test run would take hours. On the topic of slowness, the PAPI conformance run will probably have to be converted to a cron job once more conformance tests start passing. The hacks:; - Cromwell now allows for a default Docker image to be specified in config. This is required for those conformance tests that don't specify a `DockerRequirement`.; - Cromwell allows for a ""GCS default input prefix"" of any input files when using the JES backend. This allows for the conformance input JSONs to be used as-is. It also works to hack the input JSONs to specify the GCS paths where input files are staged and not bother with the ""GCS default input prefix"".; - I had to turn off call caching since the hashing actor was looking at the non-GCS paths of files. It's possible this could be worked around if anyone felt it was worth the effort, but I didn't.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3086
https://github.com/broadinstitute/cromwell/pull/3086:287,Testability,test,test,287,"All the tests that pass on the Local backend currently pass on PAPI with the hackery documented below. This does not attempt to run tests that fail on Local because there's no realistic reason to believe a conformance test that fails Local would succeed on PAPI and the PAPI conformance test run would take hours. On the topic of slowness, the PAPI conformance run will probably have to be converted to a cron job once more conformance tests start passing. The hacks:; - Cromwell now allows for a default Docker image to be specified in config. This is required for those conformance tests that don't specify a `DockerRequirement`.; - Cromwell allows for a ""GCS default input prefix"" of any input files when using the JES backend. This allows for the conformance input JSONs to be used as-is. It also works to hack the input JSONs to specify the GCS paths where input files are staged and not bother with the ""GCS default input prefix"".; - I had to turn off call caching since the hashing actor was looking at the non-GCS paths of files. It's possible this could be worked around if anyone felt it was worth the effort, but I didn't.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3086
https://github.com/broadinstitute/cromwell/pull/3086:436,Testability,test,tests,436,"All the tests that pass on the Local backend currently pass on PAPI with the hackery documented below. This does not attempt to run tests that fail on Local because there's no realistic reason to believe a conformance test that fails Local would succeed on PAPI and the PAPI conformance test run would take hours. On the topic of slowness, the PAPI conformance run will probably have to be converted to a cron job once more conformance tests start passing. The hacks:; - Cromwell now allows for a default Docker image to be specified in config. This is required for those conformance tests that don't specify a `DockerRequirement`.; - Cromwell allows for a ""GCS default input prefix"" of any input files when using the JES backend. This allows for the conformance input JSONs to be used as-is. It also works to hack the input JSONs to specify the GCS paths where input files are staged and not bother with the ""GCS default input prefix"".; - I had to turn off call caching since the hashing actor was looking at the non-GCS paths of files. It's possible this could be worked around if anyone felt it was worth the effort, but I didn't.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3086
https://github.com/broadinstitute/cromwell/pull/3086:584,Testability,test,tests,584,"All the tests that pass on the Local backend currently pass on PAPI with the hackery documented below. This does not attempt to run tests that fail on Local because there's no realistic reason to believe a conformance test that fails Local would succeed on PAPI and the PAPI conformance test run would take hours. On the topic of slowness, the PAPI conformance run will probably have to be converted to a cron job once more conformance tests start passing. The hacks:; - Cromwell now allows for a default Docker image to be specified in config. This is required for those conformance tests that don't specify a `DockerRequirement`.; - Cromwell allows for a ""GCS default input prefix"" of any input files when using the JES backend. This allows for the conformance input JSONs to be used as-is. It also works to hack the input JSONs to specify the GCS paths where input files are staged and not bother with the ""GCS default input prefix"".; - I had to turn off call caching since the hashing actor was looking at the non-GCS paths of files. It's possible this could be worked around if anyone felt it was worth the effort, but I didn't.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3086
https://github.com/broadinstitute/cromwell/issues/3088:277,Availability,failure,failure,277,"As part of my expression evaluation in `InitialWorkDirRequirement` I happened to accidentally write this:; ```yml; listing:; - entryname: $(script_name); ```; Instead of this:; ```yml; listing:; - entryname: $(inputs.script_name); ```. Instead of the expected runtime workflow failure (static checking of JS is too much), the workflow ran forever, repeatedly printing out the same expression evaluation error. Alas!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3088
https://github.com/broadinstitute/cromwell/issues/3088:403,Availability,error,error,403,"As part of my expression evaluation in `InitialWorkDirRequirement` I happened to accidentally write this:; ```yml; listing:; - entryname: $(script_name); ```; Instead of this:; ```yml; listing:; - entryname: $(inputs.script_name); ```. Instead of the expected runtime workflow failure (static checking of JS is too much), the workflow ran forever, repeatedly printing out the same expression evaluation error. Alas!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3088
https://github.com/broadinstitute/cromwell/pull/3089:244,Security,access,access,244,Motivating case: [link](https://github.com/broadinstitute/cromwell/blob/cjl_initial_work_dir_requirement_3/centaur/src/main/resources/standardTestCases/InitialWorkDirRequirement/input_string.cwl). The entry field of a Dirent can now be used to access inputs and evaluate ECMAScript expressions. - [x] Rebase and target develop after merging https://github.com/broadinstitute/cromwell/pull/3087,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3089
https://github.com/broadinstitute/cromwell/issues/3090:834,Integrability,depend,depends,834,"EDIT: to safe yourself some data entry, you can use branch [cjl_initial_work_dir_requirement_4](https://github.com/broadinstitute/cromwell/tree/cjl_initial_work_dir_requirement_4) as an entry point with the centaur test and a Spec already added. This seems to be a pretty common pattern but relies on `JSON.stringify(inputs)` working in our expression evaluator:; ```yml; # A common use case: stringy the inputs JSON and provide that file as another input file. cwlVersion: v1.0; $graph:; - id: stringify_inputs; class: CommandLineTool; baseCommand: ['grep', 'number', 'inputs.json']; requirements:; - class: DockerRequirement; dockerPull: ""python:3.5.0""; - class: InitialWorkDirRequirement; listing:; - entryname: 'inputs.json'; entry: $(JSON.stringify(inputs)). stdout: ""number_field"". # TODO CWL: Set the types more appropriately (depends on issue #3059); inputs:; - id: number; type: string; default: 27; - id: str; type: string; default: wooooo; - id: boolean; type: string; default: True; outputs:; - id: number_field_output; type: string; outputBinding:; glob: number_field; loadContents: true; outputEval: $(self[0].contents.trim()); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090
https://github.com/broadinstitute/cromwell/issues/3090:1082,Performance,load,loadContents,1082,"EDIT: to safe yourself some data entry, you can use branch [cjl_initial_work_dir_requirement_4](https://github.com/broadinstitute/cromwell/tree/cjl_initial_work_dir_requirement_4) as an entry point with the centaur test and a Spec already added. This seems to be a pretty common pattern but relies on `JSON.stringify(inputs)` working in our expression evaluator:; ```yml; # A common use case: stringy the inputs JSON and provide that file as another input file. cwlVersion: v1.0; $graph:; - id: stringify_inputs; class: CommandLineTool; baseCommand: ['grep', 'number', 'inputs.json']; requirements:; - class: DockerRequirement; dockerPull: ""python:3.5.0""; - class: InitialWorkDirRequirement; listing:; - entryname: 'inputs.json'; entry: $(JSON.stringify(inputs)). stdout: ""number_field"". # TODO CWL: Set the types more appropriately (depends on issue #3059); inputs:; - id: number; type: string; default: 27; - id: str; type: string; default: wooooo; - id: boolean; type: string; default: True; outputs:; - id: number_field_output; type: string; outputBinding:; glob: number_field; loadContents: true; outputEval: $(self[0].contents.trim()); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090
https://github.com/broadinstitute/cromwell/issues/3090:9,Safety,safe,safe,9,"EDIT: to safe yourself some data entry, you can use branch [cjl_initial_work_dir_requirement_4](https://github.com/broadinstitute/cromwell/tree/cjl_initial_work_dir_requirement_4) as an entry point with the centaur test and a Spec already added. This seems to be a pretty common pattern but relies on `JSON.stringify(inputs)` working in our expression evaluator:; ```yml; # A common use case: stringy the inputs JSON and provide that file as another input file. cwlVersion: v1.0; $graph:; - id: stringify_inputs; class: CommandLineTool; baseCommand: ['grep', 'number', 'inputs.json']; requirements:; - class: DockerRequirement; dockerPull: ""python:3.5.0""; - class: InitialWorkDirRequirement; listing:; - entryname: 'inputs.json'; entry: $(JSON.stringify(inputs)). stdout: ""number_field"". # TODO CWL: Set the types more appropriately (depends on issue #3059); inputs:; - id: number; type: string; default: 27; - id: str; type: string; default: wooooo; - id: boolean; type: string; default: True; outputs:; - id: number_field_output; type: string; outputBinding:; glob: number_field; loadContents: true; outputEval: $(self[0].contents.trim()); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090
https://github.com/broadinstitute/cromwell/issues/3090:215,Testability,test,test,215,"EDIT: to safe yourself some data entry, you can use branch [cjl_initial_work_dir_requirement_4](https://github.com/broadinstitute/cromwell/tree/cjl_initial_work_dir_requirement_4) as an entry point with the centaur test and a Spec already added. This seems to be a pretty common pattern but relies on `JSON.stringify(inputs)` working in our expression evaluator:; ```yml; # A common use case: stringy the inputs JSON and provide that file as another input file. cwlVersion: v1.0; $graph:; - id: stringify_inputs; class: CommandLineTool; baseCommand: ['grep', 'number', 'inputs.json']; requirements:; - class: DockerRequirement; dockerPull: ""python:3.5.0""; - class: InitialWorkDirRequirement; listing:; - entryname: 'inputs.json'; entry: $(JSON.stringify(inputs)). stdout: ""number_field"". # TODO CWL: Set the types more appropriately (depends on issue #3059); inputs:; - id: number; type: string; default: 27; - id: str; type: string; default: wooooo; - id: boolean; type: string; default: True; outputs:; - id: number_field_output; type: string; outputBinding:; glob: number_field; loadContents: true; outputEval: $(self[0].contents.trim()); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090
https://github.com/broadinstitute/cromwell/pull/3091:487,Integrability,interface,interface,487,"The gist is:. The `ioActor` that is already wired through from the CromwellRoot into the system has been swapped for an `IoActorEndpoint`. This endpoint just forwards existing requests to the IoActor so nothing changes on that side. The only difference is it forwards `IoCommandWithPromise` requests to a proxy actor that will communicate with the IoActor and complete the promise when the answer comes back, thus providing a globally accessible (even outside of an actor) `Future`-like interface that performs operations through the I/O actor.; This is then used in the IoFunctionSet to add retries, throttling and GCS optimizations to it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3091
https://github.com/broadinstitute/cromwell/pull/3091:502,Performance,perform,performs,502,"The gist is:. The `ioActor` that is already wired through from the CromwellRoot into the system has been swapped for an `IoActorEndpoint`. This endpoint just forwards existing requests to the IoActor so nothing changes on that side. The only difference is it forwards `IoCommandWithPromise` requests to a proxy actor that will communicate with the IoActor and complete the promise when the answer comes back, thus providing a globally accessible (even outside of an actor) `Future`-like interface that performs operations through the I/O actor.; This is then used in the IoFunctionSet to add retries, throttling and GCS optimizations to it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3091
https://github.com/broadinstitute/cromwell/pull/3091:620,Performance,optimiz,optimizations,620,"The gist is:. The `ioActor` that is already wired through from the CromwellRoot into the system has been swapped for an `IoActorEndpoint`. This endpoint just forwards existing requests to the IoActor so nothing changes on that side. The only difference is it forwards `IoCommandWithPromise` requests to a proxy actor that will communicate with the IoActor and complete the promise when the answer comes back, thus providing a globally accessible (even outside of an actor) `Future`-like interface that performs operations through the I/O actor.; This is then used in the IoFunctionSet to add retries, throttling and GCS optimizations to it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3091
https://github.com/broadinstitute/cromwell/pull/3091:435,Security,access,accessible,435,"The gist is:. The `ioActor` that is already wired through from the CromwellRoot into the system has been swapped for an `IoActorEndpoint`. This endpoint just forwards existing requests to the IoActor so nothing changes on that side. The only difference is it forwards `IoCommandWithPromise` requests to a proxy actor that will communicate with the IoActor and complete the promise when the answer comes back, thus providing a globally accessible (even outside of an actor) `Future`-like interface that performs operations through the I/O actor.; This is then used in the IoFunctionSet to add retries, throttling and GCS optimizations to it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3091
https://github.com/broadinstitute/cromwell/issues/3092:20,Testability,test,tests,20,"The CWL conformance tests have a new addition that is globbing everything in the current directory and expecting files based on what the command produced. It fails because in addition to those files Cromwell also has the script file, rc file etc...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3092
https://github.com/broadinstitute/cromwell/issues/3093:954,Energy Efficiency,adapt,adapted,954,"In the following code snippet:; ```WDL; scatter(centrifuge in centrifugeList){; call centrifugeDownload {; input:; centrifugeOutput= centrifuge.outputDir,; domain=centrifuge.domain,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnable",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093
https://github.com/broadinstitute/cromwell/issues/3093:954,Modifiability,adapt,adapted,954,"In the following code snippet:; ```WDL; scatter(centrifuge in centrifugeList){; call centrifugeDownload {; input:; centrifugeOutput= centrifuge.outputDir,; domain=centrifuge.domain,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnable",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093
https://github.com/broadinstitute/cromwell/issues/3093:1059,Security,Validat,Validated,1059,"ntrifugeList){; call centrifugeDownload {; input:; centrifugeOutput= centrifuge.outputDir,; domain=centrifuge.domain,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:447); at c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093
https://github.com/broadinstitute/cromwell/issues/3093:1074,Security,Validat,Validated,1074,"){; call centrifugeDownload {; input:; centrifugeOutput= centrifuge.outputDir,; domain=centrifuge.domain,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:447); at cromwell.engin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093
https://github.com/broadinstitute/cromwell/issues/3093:1108,Security,Validat,Validated,1108,"; centrifugeOutput= centrifuge.outputDir,; domain=centrifuge.domain,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:447); at cromwell.engine.workflow.lifecycle.execution.Workf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093
https://github.com/broadinstitute/cromwell/issues/3093:1124,Security,Validat,Validated,1124,"utput= centrifuge.outputDir,; domain=centrifuge.domain,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:447); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093
https://github.com/broadinstitute/cromwell/issues/3093:1159,Security,Validat,Validated,1159,"entrifuge.domain,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:447); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$4(Wor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093
https://github.com/broadinstitute/cromwell/issues/3093:1173,Security,Validat,Validated,1173,"main,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:447); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$4(WorkflowExecuti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093
https://github.com/broadinstitute/cromwell/issues/3093:3504,Testability,Log,LoggingFSM,3504,es.ListInstances$$anon$1.traverse(list.scala:64); at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); at cats.Traverse$Ops.traverse(Traverse.scala:19); at cats.Traverse$Ops.traverse$(Traverse.scala:19); at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:416); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:148); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:146); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:168); at akka.actor.FSM.processEvent(FSM.scala:668); at akka.actor.FSM.processEvent$(FSM.scala:662); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:41); at akka.actor.LoggingFSM.processEvent(FSM.scala:801); at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:41); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:41); at akka.actor.Timers.aroundReceive(Timers.scala:40); at akka.actor.Timers.aroundReceive$(Timers.scala:36); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:41); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093
https://github.com/broadinstitute/cromwell/issues/3093:3583,Testability,Log,LoggingFSM,3583,nces.ListInstances$$anon$1.traverse(list.scala:12); at cats.Traverse$Ops.traverse(Traverse.scala:19); at cats.Traverse$Ops.traverse$(Traverse.scala:19); at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:416); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:148); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:146); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:168); at akka.actor.FSM.processEvent(FSM.scala:668); at akka.actor.FSM.processEvent$(FSM.scala:662); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:41); at akka.actor.LoggingFSM.processEvent(FSM.scala:801); at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:41); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:41); at akka.actor.Timers.aroundReceive(Timers.scala:40); at akka.actor.Timers.aroundReceive$(Timers.scala:36); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:41); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.M,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093
https://github.com/broadinstitute/cromwell/issues/3093:3637,Testability,Log,LoggingFSM,3637, cats.Traverse$Ops.traverse(Traverse.scala:19); at cats.Traverse$Ops.traverse$(Traverse.scala:19); at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:416); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:148); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:146); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:168); at akka.actor.FSM.processEvent(FSM.scala:668); at akka.actor.FSM.processEvent$(FSM.scala:662); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:41); at akka.actor.LoggingFSM.processEvent(FSM.scala:801); at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:41); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:41); at akka.actor.Timers.aroundReceive(Timers.scala:40); at akka.actor.Timers.aroundReceive$(Timers.scala:36); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:41); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093
https://github.com/broadinstitute/cromwell/issues/3094:125,Availability,error,error,125,"- v29; - Local backend. `java -jar cromwell.jar test.wdl test.json` is missing the `-i` flag and yet cromwell returns a zero error code. Since this is invalid, it should return non-zero.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3094
https://github.com/broadinstitute/cromwell/issues/3094:48,Testability,test,test,48,"- v29; - Local backend. `java -jar cromwell.jar test.wdl test.json` is missing the `-i` flag and yet cromwell returns a zero error code. Since this is invalid, it should return non-zero.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3094
https://github.com/broadinstitute/cromwell/issues/3094:57,Testability,test,test,57,"- v29; - Local backend. `java -jar cromwell.jar test.wdl test.json` is missing the `-i` flag and yet cromwell returns a zero error code. Since this is invalid, it should return non-zero.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3094
https://github.com/broadinstitute/cromwell/issues/3095:112,Deployability,release,releases,112,"- [x] fix API docs that mention WDLtool; - [x] On WOMtool docs, say where to find the WOMtool artifact (now the releases page)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3095
https://github.com/broadinstitute/cromwell/pull/3097:0,Modifiability,Refactor,Refactors,0,"Refactors `ParameterContext` to allow values of various types. See the scaladoc for more info, generally speaking an internal untyped map is being used instead of the old `WomValue`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3097
https://github.com/broadinstitute/cromwell/pull/3100:62,Modifiability,config,config,62,`actor-factory` should be defined within `SGE` not within SGE.config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3100
https://github.com/broadinstitute/cromwell/pull/3101:46,Integrability,interface,interface,46,1. BCS supported backend.; 2. OSS storage nio interface.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3101
https://github.com/broadinstitute/cromwell/issues/3102:146,Deployability,patch,patches,146,"`centaurCwlConformancePAPI` doesn't have the same credential check as `centaurJes`, and so is failing when users external users try to contribute patches. - Example [checking for creds](https://github.com/broadinstitute/cromwell/blob/e7b0833d53a305039af7224879cd26ceab1f1881/src/bin/travis/testCentaurJes.sh#L3-L12) and NOP passing: ; https://travis-ci.org/broadinstitute/cromwell/jobs/324545050; - Example [missing](https://github.com/broadinstitute/cromwell/blob/develop/src/bin/travis/testCentaurCwlConformancePAPI.sh) the above check and failing CI: ; https://travis-ci.org/broadinstitute/cromwell/jobs/324545054",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3102
https://github.com/broadinstitute/cromwell/issues/3102:290,Testability,test,testCentaurJes,290,"`centaurCwlConformancePAPI` doesn't have the same credential check as `centaurJes`, and so is failing when users external users try to contribute patches. - Example [checking for creds](https://github.com/broadinstitute/cromwell/blob/e7b0833d53a305039af7224879cd26ceab1f1881/src/bin/travis/testCentaurJes.sh#L3-L12) and NOP passing: ; https://travis-ci.org/broadinstitute/cromwell/jobs/324545050; - Example [missing](https://github.com/broadinstitute/cromwell/blob/develop/src/bin/travis/testCentaurCwlConformancePAPI.sh) the above check and failing CI: ; https://travis-ci.org/broadinstitute/cromwell/jobs/324545054",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3102
https://github.com/broadinstitute/cromwell/issues/3102:488,Testability,test,testCentaurCwlConformancePAPI,488,"`centaurCwlConformancePAPI` doesn't have the same credential check as `centaurJes`, and so is failing when users external users try to contribute patches. - Example [checking for creds](https://github.com/broadinstitute/cromwell/blob/e7b0833d53a305039af7224879cd26ceab1f1881/src/bin/travis/testCentaurJes.sh#L3-L12) and NOP passing: ; https://travis-ci.org/broadinstitute/cromwell/jobs/324545050; - Example [missing](https://github.com/broadinstitute/cromwell/blob/develop/src/bin/travis/testCentaurCwlConformancePAPI.sh) the above check and failing CI: ; https://travis-ci.org/broadinstitute/cromwell/jobs/324545054",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3102
https://github.com/broadinstitute/cromwell/issues/3104:29,Security,Validat,ValidateWorkflowId,29,"CromwellApiService issues a `ValidateWorkflowId` to the metadata service, breaking our rule of not reading from metadata internally. This is problematic if e.g. one is using a write-only metadata implementation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3104
https://github.com/broadinstitute/cromwell/pull/3106:258,Testability,test,tests,258,See [here](https://github.com/broadinstitute/cromwell/blob/cjl_initial_work_dir_requirement_6/centaur/src/main/resources/standardTestCases/InitialWorkDirRequirement/file_array_listing.cwl#L10) for the motivating use case. . This needs improving as a centaur tests because we need to generate the file so that it can be passed in (which needs a WF rather than a simple CLT).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3106
https://github.com/broadinstitute/cromwell/pull/3106:361,Usability,simpl,simple,361,See [here](https://github.com/broadinstitute/cromwell/blob/cjl_initial_work_dir_requirement_6/centaur/src/main/resources/standardTestCases/InitialWorkDirRequirement/file_array_listing.cwl#L10) for the motivating use case. . This needs improving as a centaur tests because we need to generate the file so that it can be passed in (which needs a WF rather than a simple CLT).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3106
https://github.com/broadinstitute/cromwell/issues/3107:69,Testability,test,test,69,"Discovered during EnvVarRequirement work, and there is a conformance test that checks this in the context of EnvVarRequirements.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3107
https://github.com/broadinstitute/cromwell/pull/3108:168,Testability,test,tests,168,"Whilst working on env vars I noticed the Run / WorkflowStep / Workflow relationship wasn't modeled correctly wrt Requirements. That's something the env var conformance tests in particular will check, but it's generally useful to have this be correct so here it is in its own wee PR.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3108
https://github.com/broadinstitute/cromwell/issues/3109:899,Availability,down,download,899,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:1330,Availability,down,download,1330,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:1763,Availability,down,down,1763,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:55,Deployability,configurat,configuration,55,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:1754,Deployability,pipeline,pipeline,1754,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:55,Modifiability,config,configuration,55,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:182,Modifiability,config,config,182,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:200,Modifiability,config,config,200,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:243,Modifiability,config,config,243,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:408,Modifiability,config,config,408,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:195,Testability,test,test,195,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:238,Testability,test,test,238,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:292,Testability,test,test,292,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:1017,Testability,test,test,1017,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3109:1457,Testability,test,test,1457,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109
https://github.com/broadinstitute/cromwell/issues/3110:369,Testability,test,test,369,"The CWL Spec [says](http://www.commonwl.org/v1.0/CommandLineTool.html#File) that if a file named `cwl.outputs.json` is present in the output directory of the task, the `outputBinding` should be ignored and this file should be used as the output of the task.; It's hard to tell how extensively this functionality is used by real CWL workflows but at least 1 conformance test makes use of it (#2).; Implementing this might be a bit challenging (especially in JES where we need to know in advance what to delocalize, maybe we'll have to use some sort of special glob ?..)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3110
https://github.com/broadinstitute/cromwell/issues/3111:41,Modifiability,variab,variable,41,Bash and WDL have overlapping syntax for variable definitions: `${VARNAME}` is valid in both. This is a problem if you need to do variable manipulation in bash (e.g. get the length of a variable via `${#VARNAME}`). Cromwell should support escaping this statement so that it can be passed through and interpreted in bash.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3111
https://github.com/broadinstitute/cromwell/issues/3111:130,Modifiability,variab,variable,130,Bash and WDL have overlapping syntax for variable definitions: `${VARNAME}` is valid in both. This is a problem if you need to do variable manipulation in bash (e.g. get the length of a variable via `${#VARNAME}`). Cromwell should support escaping this statement so that it can be passed through and interpreted in bash.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3111
https://github.com/broadinstitute/cromwell/issues/3111:186,Modifiability,variab,variable,186,Bash and WDL have overlapping syntax for variable definitions: `${VARNAME}` is valid in both. This is a problem if you need to do variable manipulation in bash (e.g. get the length of a variable via `${#VARNAME}`). Cromwell should support escaping this statement so that it can be passed through and interpreted in bash.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3111
https://github.com/broadinstitute/cromwell/pull/3113:36,Modifiability,config,config,36,Still does not honor the read limit config for WDL but makes it possible.; For CWL it does only stream the first 64KB. - [ ] Add a centaur test ?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3113
https://github.com/broadinstitute/cromwell/pull/3113:139,Testability,test,test,139,Still does not honor the read limit config for WDL but makes it possible.; For CWL it does only stream the first 64KB. - [ ] Add a centaur test ?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3113
https://github.com/broadinstitute/cromwell/issues/3115:155,Availability,error,error,155,"On cromwell version 30.1, when making a request to the query endpoint that includes an `additionalQueryResultFields` parameter, the database will throw an error if the query matches ~1000 results. This seems to be because cromwell is querying the db to get that field's value for each workflow in the result set. Example GET request:; ""https://cromwell.mint-dev.broadinstitute.org/api/workflows/v1/query?additionalQueryResultFields=parentWorkflowId"". Error message: ; ```{; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@349b84c3 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@37b87f04[Running, pool size = 200, active threads = 200, queued tasks = 1000, completed tasks = 3928972]""; }; ```. This does not occur when paginating the results (as long as the page size is not 1000+)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115
https://github.com/broadinstitute/cromwell/issues/3115:451,Availability,Error,Error,451,"On cromwell version 30.1, when making a request to the query endpoint that includes an `additionalQueryResultFields` parameter, the database will throw an error if the query matches ~1000 results. This seems to be because cromwell is querying the db to get that field's value for each workflow in the result set. Example GET request:; ""https://cromwell.mint-dev.broadinstitute.org/api/workflows/v1/query?additionalQueryResultFields=parentWorkflowId"". Error message: ; ```{; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@349b84c3 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@37b87f04[Running, pool size = 200, active threads = 200, queued tasks = 1000, completed tasks = 3928972]""; }; ```. This does not occur when paginating the results (as long as the page size is not 1000+)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115
https://github.com/broadinstitute/cromwell/issues/3115:457,Integrability,message,message,457,"On cromwell version 30.1, when making a request to the query endpoint that includes an `additionalQueryResultFields` parameter, the database will throw an error if the query matches ~1000 results. This seems to be because cromwell is querying the db to get that field's value for each workflow in the result set. Example GET request:; ""https://cromwell.mint-dev.broadinstitute.org/api/workflows/v1/query?additionalQueryResultFields=parentWorkflowId"". Error message: ; ```{; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@349b84c3 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@37b87f04[Running, pool size = 200, active threads = 200, queued tasks = 1000, completed tasks = 3928972]""; }; ```. This does not occur when paginating the results (as long as the page size is not 1000+)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115
https://github.com/broadinstitute/cromwell/issues/3115:494,Integrability,message,message,494,"On cromwell version 30.1, when making a request to the query endpoint that includes an `additionalQueryResultFields` parameter, the database will throw an error if the query matches ~1000 results. This seems to be because cromwell is querying the db to get that field's value for each workflow in the result set. Example GET request:; ""https://cromwell.mint-dev.broadinstitute.org/api/workflows/v1/query?additionalQueryResultFields=parentWorkflowId"". Error message: ; ```{; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@349b84c3 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@37b87f04[Running, pool size = 200, active threads = 200, queued tasks = 1000, completed tasks = 3928972]""; }; ```. This does not occur when paginating the results (as long as the page size is not 1000+)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115
https://github.com/broadinstitute/cromwell/issues/3115:676,Performance,queue,queued,676,"On cromwell version 30.1, when making a request to the query endpoint that includes an `additionalQueryResultFields` parameter, the database will throw an error if the query matches ~1000 results. This seems to be because cromwell is querying the db to get that field's value for each workflow in the result set. Example GET request:; ""https://cromwell.mint-dev.broadinstitute.org/api/workflows/v1/query?additionalQueryResultFields=parentWorkflowId"". Error message: ; ```{; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@349b84c3 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@37b87f04[Running, pool size = 200, active threads = 200, queued tasks = 1000, completed tasks = 3928972]""; }; ```. This does not occur when paginating the results (as long as the page size is not 1000+)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115
https://github.com/broadinstitute/cromwell/issues/3116:124,Availability,error,error,124,"Running the `cd` command anywhere in the command block causes output files to not be found and for the workflow to throw an error:. [2018-01-08 15:05:21,44] [error] WorkflowManagerActor Workflow 6575132f-3c01-498f-ac7a-2a418568f002 failed (during ExecutingWorkflowState): Could not process output, file not found: /Users/jonn/Development/cromwell/cromwell-executions/Funcotator/6575132f-3c01-498f-ac7a-2a418568f002/call-MakeItFunky/execution/out.vcf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3116
https://github.com/broadinstitute/cromwell/issues/3116:158,Availability,error,error,158,"Running the `cd` command anywhere in the command block causes output files to not be found and for the workflow to throw an error:. [2018-01-08 15:05:21,44] [error] WorkflowManagerActor Workflow 6575132f-3c01-498f-ac7a-2a418568f002 failed (during ExecutingWorkflowState): Could not process output, file not found: /Users/jonn/Development/cromwell/cromwell-executions/Funcotator/6575132f-3c01-498f-ac7a-2a418568f002/call-MakeItFunky/execution/out.vcf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3116
https://github.com/broadinstitute/cromwell/issues/3120:1545,Availability,error,error,1545,"I gave Cromwell the following JSON when I had to specify the pair workflow:. {; ""CNVSomaticPairWorkflow.common_sites"": ""gs://broad-dsde-methods/mkanaszn/iota_small_exac_grch38.interval_list"",; ""CNVSomaticPairWorkflow.gatk_docker"": ""samuelklee/gatk:sl_wgs_acnv_headers"",; ""CNVSomaticPairWorkflow.intervals"": ""gs://shlee-dev/CNV/intervals/pad_parmY_int_agilent_1kg_exotars_logrch38.tsv"",; ""CNVSomaticPairWorkflow.normal_bam"": ""gs://shlee-dev/hcc/hcc1143_N_clean.bam"",; ""CNVSomaticPairWorkflow.normal_bam_idx"": ""gs://shlee-dev/hcc/hcc1143_N_clean.bai"",; ""CNVSomaticPairWorkflow.read_count_pon"": ""gs://broad-dsde-methods/cromwell-execution-29/CNVSomaticPanelWorkflow/b36a6e5a-d31e-45e1-9935-b44b87690b6e/call-CreateReadCountPanelOfNormals/1kg_tutorial.pon.hdf5"",; ""CNVSomaticPairWorkflow.ref_fasta_dict"": ""gs://shlee-dev/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.dict"",; ""CNVSomaticPairWorkflow.ref_fasta_fai"": ""gs://shlee-dev/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa.fai"",; ""CNVSomaticPairWorkflow.ref_fasta"": ""gs://shlee-dev/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa"",; ""CNVSomaticPairWorkflow.tumor_bam"": ""gs://shlee-dev/hcc/hcc1143_T_clean.bam"",; ""CNVSomaticPairWorkflow.tumor_bam_idx"": ""gs://shlee-dev/hcc/hcc1143_T_clean.bai"",; ""CNVSomaticPanelWorkflow.PreprocessIntervals.bin_length"": ""0"",; ""CNVSomaticPanelWorkflow.PreprocessIntervals.padding"": ""250""; }. As you can see, I accidentally wrote ""CNVSomaticPanelWorkflow"" instead of ""CNVSomaticPairWorkflow"" in the last two entries. I didn't get an error for this, which might have been useful though. This might be a duplicate issue but I didn't find another one related to this problem. Sorry if it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3120
https://github.com/broadinstitute/cromwell/pull/3124:279,Availability,Error,ErrorOr,279,"Added methods for retrieving cwl File/Directory parts like basename.; Added a directory listing stub close to the existing glob listing code.; Updated CWL types to reuse more Polys and to parse ecmascripts with embedded newlines and extra whitespace.; Minor cleanup around throw/ErrorOr/Try conversions especially around Javascript processing.; JsUtil encoding now encodes WomMap/WomArray into instances of JsObject instead of java.util.Map/java.lang.Array.; Hacked JsUtil to support reading in ""structs"" of mixed types.; Replaced all usages of .stripMargin.replaceAll with .stripMargin.replace so that the replacements aren't processed like regexs.; Removed docs/.Rhistory and ./Running empty files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3124
https://github.com/broadinstitute/cromwell/pull/3124:143,Deployability,Update,Updated,143,"Added methods for retrieving cwl File/Directory parts like basename.; Added a directory listing stub close to the existing glob listing code.; Updated CWL types to reuse more Polys and to parse ecmascripts with embedded newlines and extra whitespace.; Minor cleanup around throw/ErrorOr/Try conversions especially around Javascript processing.; JsUtil encoding now encodes WomMap/WomArray into instances of JsObject instead of java.util.Map/java.lang.Array.; Hacked JsUtil to support reading in ""structs"" of mixed types.; Replaced all usages of .stripMargin.replaceAll with .stripMargin.replace so that the replacements aren't processed like regexs.; Removed docs/.Rhistory and ./Running empty files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3124
https://github.com/broadinstitute/cromwell/pull/3124:96,Testability,stub,stub,96,"Added methods for retrieving cwl File/Directory parts like basename.; Added a directory listing stub close to the existing glob listing code.; Updated CWL types to reuse more Polys and to parse ecmascripts with embedded newlines and extra whitespace.; Minor cleanup around throw/ErrorOr/Try conversions especially around Javascript processing.; JsUtil encoding now encodes WomMap/WomArray into instances of JsObject instead of java.util.Map/java.lang.Array.; Hacked JsUtil to support reading in ""structs"" of mixed types.; Replaced all usages of .stripMargin.replaceAll with .stripMargin.replace so that the replacements aren't processed like regexs.; Removed docs/.Rhistory and ./Running empty files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3124
https://github.com/broadinstitute/cromwell/issues/3125:80,Availability,error,error,80,"Until we work this out, we shouldn't risk accidentally CCing, or spamming awful error messages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3125
https://github.com/broadinstitute/cromwell/issues/3125:86,Integrability,message,messages,86,"Until we work this out, we shouldn't risk accidentally CCing, or spamming awful error messages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3125
https://github.com/broadinstitute/cromwell/issues/3125:37,Safety,risk,risk,37,"Until we work this out, we shouldn't risk accidentally CCing, or spamming awful error messages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3125
https://github.com/broadinstitute/cromwell/pull/3126:165,Security,expose,expose,165,Motivation: This is a feature requested by the Mint team. They want the total number of results matching a query to be included in the query response. Mint wants to expose the total results count through the Job-Manager UI.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3126
https://github.com/broadinstitute/cromwell/issues/3133:68,Integrability,rout,route,68,"Like globbing, listing a directory should be 1) asynchronous and 2) route through the IoActor. See [this comment](https://github.com/broadinstitute/cromwell/pull/3124#pullrequestreview-87884613) for more context.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3133
https://github.com/broadinstitute/cromwell/pull/3134:292,Usability,Simpl,Simpletons,292,"This might be controversial hence the PREGULL.; The problem is `WomFile` can now take different forms.; `WomSingleFile` is just a file path (for now); `WomMaybePopulatedFile` can have a whole bunch of attributes, including a `List[WomFile]`; Same for `WomMaybeListedDirectory`.; `WomFile` => Simpletons is not the problem since we can just simpletonize a ""complex file"" as an object.; Simpletons => `WomFile` is harder because we don't necessarily know which kind of `WomFile` it is that we're trying to build.; The solution in this PR is to leave a ""marker"" in the simpleton key to give us a hint that what's following is a `WomMaybeListedDirectory` or a `WomMaybePopulatedFile` and re-build it appropriately.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3134
https://github.com/broadinstitute/cromwell/pull/3134:340,Usability,simpl,simpletonize,340,"This might be controversial hence the PREGULL.; The problem is `WomFile` can now take different forms.; `WomSingleFile` is just a file path (for now); `WomMaybePopulatedFile` can have a whole bunch of attributes, including a `List[WomFile]`; Same for `WomMaybeListedDirectory`.; `WomFile` => Simpletons is not the problem since we can just simpletonize a ""complex file"" as an object.; Simpletons => `WomFile` is harder because we don't necessarily know which kind of `WomFile` it is that we're trying to build.; The solution in this PR is to leave a ""marker"" in the simpleton key to give us a hint that what's following is a `WomMaybeListedDirectory` or a `WomMaybePopulatedFile` and re-build it appropriately.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3134
https://github.com/broadinstitute/cromwell/pull/3134:385,Usability,Simpl,Simpletons,385,"This might be controversial hence the PREGULL.; The problem is `WomFile` can now take different forms.; `WomSingleFile` is just a file path (for now); `WomMaybePopulatedFile` can have a whole bunch of attributes, including a `List[WomFile]`; Same for `WomMaybeListedDirectory`.; `WomFile` => Simpletons is not the problem since we can just simpletonize a ""complex file"" as an object.; Simpletons => `WomFile` is harder because we don't necessarily know which kind of `WomFile` it is that we're trying to build.; The solution in this PR is to leave a ""marker"" in the simpleton key to give us a hint that what's following is a `WomMaybeListedDirectory` or a `WomMaybePopulatedFile` and re-build it appropriately.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3134
https://github.com/broadinstitute/cromwell/pull/3134:566,Usability,simpl,simpleton,566,"This might be controversial hence the PREGULL.; The problem is `WomFile` can now take different forms.; `WomSingleFile` is just a file path (for now); `WomMaybePopulatedFile` can have a whole bunch of attributes, including a `List[WomFile]`; Same for `WomMaybeListedDirectory`.; `WomFile` => Simpletons is not the problem since we can just simpletonize a ""complex file"" as an object.; Simpletons => `WomFile` is harder because we don't necessarily know which kind of `WomFile` it is that we're trying to build.; The solution in this PR is to leave a ""marker"" in the simpleton key to give us a hint that what's following is a `WomMaybeListedDirectory` or a `WomMaybePopulatedFile` and re-build it appropriately.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3134
https://github.com/broadinstitute/cromwell/pull/3140:171,Testability,test,tests,171,"Javascript requirements have the option of declaring JS functions to be used in expressions thereafter. One may also include a file containing functions, as a conformance tests demonstrate. Most of the changes here are the ripple effect of passing this library around. I was forced to disable the ""strict"" javascript option in order to pass a conformance test. The library it imported used a ""with"" clause that is evidently verboten in strict world.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3140
https://github.com/broadinstitute/cromwell/pull/3140:355,Testability,test,test,355,"Javascript requirements have the option of declaring JS functions to be used in expressions thereafter. One may also include a file containing functions, as a conformance tests demonstrate. Most of the changes here are the ripple effect of passing this library around. I was forced to disable the ""strict"" javascript option in order to pass a conformance test. The library it imported used a ""with"" clause that is evidently verboten in strict world.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3140
https://github.com/broadinstitute/cromwell/issues/3141:145,Testability,test,test,145,"The spec declares that ECMAscript should be evaluated using ""strict mode"". (http://www.commonwl.org/v1.0/Workflow.html#Expressions). Conformance test of description ""Test InitialWorkDirRequirement ExpressionEngineRequirement.engineConfig feature"" (number 6 as of this writing) imports the library `underscore.js`, which in turn uses a `with` clause that is apparently forbidden in strict mode, at least according to the JDK Nashorn JS Engine. Thus, in order to pass this conformance test, *strict mode is not enabled* currently (1/16/18). So do we enable strict mode according to spec or do we do as people passing the conformance test do and not enable it?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3141
https://github.com/broadinstitute/cromwell/issues/3141:166,Testability,Test,Test,166,"The spec declares that ECMAscript should be evaluated using ""strict mode"". (http://www.commonwl.org/v1.0/Workflow.html#Expressions). Conformance test of description ""Test InitialWorkDirRequirement ExpressionEngineRequirement.engineConfig feature"" (number 6 as of this writing) imports the library `underscore.js`, which in turn uses a `with` clause that is apparently forbidden in strict mode, at least according to the JDK Nashorn JS Engine. Thus, in order to pass this conformance test, *strict mode is not enabled* currently (1/16/18). So do we enable strict mode according to spec or do we do as people passing the conformance test do and not enable it?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3141
https://github.com/broadinstitute/cromwell/issues/3141:483,Testability,test,test,483,"The spec declares that ECMAscript should be evaluated using ""strict mode"". (http://www.commonwl.org/v1.0/Workflow.html#Expressions). Conformance test of description ""Test InitialWorkDirRequirement ExpressionEngineRequirement.engineConfig feature"" (number 6 as of this writing) imports the library `underscore.js`, which in turn uses a `with` clause that is apparently forbidden in strict mode, at least according to the JDK Nashorn JS Engine. Thus, in order to pass this conformance test, *strict mode is not enabled* currently (1/16/18). So do we enable strict mode according to spec or do we do as people passing the conformance test do and not enable it?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3141
https://github.com/broadinstitute/cromwell/issues/3141:631,Testability,test,test,631,"The spec declares that ECMAscript should be evaluated using ""strict mode"". (http://www.commonwl.org/v1.0/Workflow.html#Expressions). Conformance test of description ""Test InitialWorkDirRequirement ExpressionEngineRequirement.engineConfig feature"" (number 6 as of this writing) imports the library `underscore.js`, which in turn uses a `with` clause that is apparently forbidden in strict mode, at least according to the JDK Nashorn JS Engine. Thus, in order to pass this conformance test, *strict mode is not enabled* currently (1/16/18). So do we enable strict mode according to spec or do we do as people passing the conformance test do and not enable it?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3141
https://github.com/broadinstitute/cromwell/issues/3142:53,Integrability,depend,depending,53,"The value of ""self"" in ECMAscript expression changes depending on the context. Currently we support an array of inputs only.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3142
https://github.com/broadinstitute/cromwell/issues/3143:131,Availability,failure,failures,131,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:4349,Availability,error,error,4349,"un$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I couldn't get the error to pop up again so I'm definitely missing some nuances of the cause. The root workflow passes womtool-30.1.jar validation. Root workflow - [SomaticPairedSingleSampleWf.txt](https://github.com/broadinstitute/cromwell/files/1635810/SomaticPairedSingleSampleWf.txt). Sub workflow - [SplitLargeRG.txt](https://github.com/broadinstitute/cromwell/files/1635814/SplitLargeRG.txt). Dependencies zip - [SomaticPairedSingleSampleWfDependencies.zip](https://github.com/broadinstitute/cromwell/files/1635815/SomaticPairedSingleSampleWfDependencies.zip). Let me know if theres any more information that might be useful.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:103,Deployability,Pipeline,Pipelines,103,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:179,Integrability,message,message,179,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:221,Integrability,depend,dependency,221,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:308,Integrability,message,message,308,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:391,Integrability,message,message,391,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:475,Integrability,message,message,475,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:581,Integrability,message,message,581,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:676,Integrability,message,message,676,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:767,Integrability,message,message,767,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:863,Integrability,message,message,863,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:961,Integrability,message,message,961,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:1048,Integrability,message,message,1048,"d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecyc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:1149,Integrability,message,message,1149,"s: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materializat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:1250,Integrability,message,message,1250,"PairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:1450,Integrability,message,message,1450,"scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:1529,Integrability,message,message,1529,"ed$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$fla",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:1729,Integrability,message,message,1729,"dlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Callback",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:1808,Integrability,message,message,1808,"rkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.B",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:1996,Integrability,message,message,1996,"womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:2274,Integrability,message,message,2274,"ell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:2475,Integrability,message,message,2475,".util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.Fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:2571,Integrability,message,message,2571,"ow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:2679,Integrability,message,message,2679,"WorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:2775,Integrability,message,message,2775,"Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:2895,Integrability,message,message,2895,"lowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; caus",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:3018,Integrability,message,message,3018,"omwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Work",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:3124,Integrability,message,message,3124,"w$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:3230,Integrability,message,message,3230,"owDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:3342,Integrability,message,message,3342,"on.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:3439,Integrability,message,message,3439,"ptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I couldn't get the error to pop up again so I'm definitely missing some nuances of the cause. The r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:3578,Integrability,message,message,3578,",; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I couldn't get the error to pop up again so I'm definitely missing some nuances of the cause. The root workflow passes womtool-30.1.jar validation. Root workflow - [SomaticPairedSingleSampleWf.txt](https://github.com/broadinstitute/cromwe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:3679,Integrability,message,message,3679,"By: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I couldn't get the error to pop up again so I'm definitely missing some nuances of the cause. The root workflow passes womtool-30.1.jar validation. Root workflow - [SomaticPairedSingleSampleWf.txt](https://github.com/broadinstitute/cromwell/files/1635810/SomaticPairedSingleSampleWf.txt). Sub workflow - [SplitLargeRG.txt](https://github.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:3792,Integrability,message,message,3792,"e: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I couldn't get the error to pop up again so I'm definitely missing some nuances of the cause. The root workflow passes womtool-30.1.jar validation. Root workflow - [SomaticPairedSingleSampleWf.txt](https://github.com/broadinstitute/cromwell/files/1635810/SomaticPairedSingleSampleWf.txt). Sub workflow - [SplitLargeRG.txt](https://github.com/broadinstitute/cromwell/files/1635814/SplitLargeRG.txt). Dependencies zip - [SomaticPairedSingleSampleWfDepend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:3897,Integrability,message,message,3897,"y: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I couldn't get the error to pop up again so I'm definitely missing some nuances of the cause. The root workflow passes womtool-30.1.jar validation. Root workflow - [SomaticPairedSingleSampleWf.txt](https://github.com/broadinstitute/cromwell/files/1635810/SomaticPairedSingleSampleWf.txt). Sub workflow - [SplitLargeRG.txt](https://github.com/broadinstitute/cromwell/files/1635814/SplitLargeRG.txt). Dependencies zip - [SomaticPairedSingleSampleWfDependencies.zip](https://github.com/broadinstitute/cromwell/files/1635815/SomaticPairedSingleSampleWfDependenc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:3995,Integrability,message,message,3995,"un$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I couldn't get the error to pop up again so I'm definitely missing some nuances of the cause. The root workflow passes womtool-30.1.jar validation. Root workflow - [SomaticPairedSingleSampleWf.txt](https://github.com/broadinstitute/cromwell/files/1635810/SomaticPairedSingleSampleWf.txt). Sub workflow - [SplitLargeRG.txt](https://github.com/broadinstitute/cromwell/files/1635814/SplitLargeRG.txt). Dependencies zip - [SomaticPairedSingleSampleWfDependencies.zip](https://github.com/broadinstitute/cromwell/files/1635815/SomaticPairedSingleSampleWfDependencies.zip). Let me know if theres any more information that might be useful.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:4729,Integrability,Depend,Dependencies,4729,"un$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I couldn't get the error to pop up again so I'm definitely missing some nuances of the cause. The root workflow passes womtool-30.1.jar validation. Root workflow - [SomaticPairedSingleSampleWf.txt](https://github.com/broadinstitute/cromwell/files/1635810/SomaticPairedSingleSampleWf.txt). Sub workflow - [SplitLargeRG.txt](https://github.com/broadinstitute/cromwell/files/1635814/SplitLargeRG.txt). Dependencies zip - [SomaticPairedSingleSampleWfDependencies.zip](https://github.com/broadinstitute/cromwell/files/1635815/SomaticPairedSingleSampleWfDependencies.zip). Let me know if theres any more information that might be useful.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:2491,Performance,concurren,concurrent,2491,"la:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:2587,Performance,concurren,concurrent,2587,"terializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:2695,Performance,concurren,concurrent,2695,"91)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:3140,Performance,concurren,concurrent,3140,"terializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:1355,Security,validat,validateWdlNamespace,1355,"dBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:1634,Security,validat,validateWdlNamespace,1634," causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:1904,Security,validat,validateWdlNamespace,1904,"73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:4466,Security,validat,validation,4466,"un$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I couldn't get the error to pop up again so I'm definitely missing some nuances of the cause. The root workflow passes womtool-30.1.jar validation. Root workflow - [SomaticPairedSingleSampleWf.txt](https://github.com/broadinstitute/cromwell/files/1635810/SomaticPairedSingleSampleWf.txt). Sub workflow - [SplitLargeRG.txt](https://github.com/broadinstitute/cromwell/files/1635814/SplitLargeRG.txt). Dependencies zip - [SomaticPairedSingleSampleWfDependencies.zip](https://github.com/broadinstitute/cromwell/files/1635815/SomaticPairedSingleSampleWfDependencies.zip). Let me know if theres any more information that might be useful.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/issues/3143:4307,Usability,simpl,simple,4307,"un$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I couldn't get the error to pop up again so I'm definitely missing some nuances of the cause. The root workflow passes womtool-30.1.jar validation. Root workflow - [SomaticPairedSingleSampleWf.txt](https://github.com/broadinstitute/cromwell/files/1635810/SomaticPairedSingleSampleWf.txt). Sub workflow - [SplitLargeRG.txt](https://github.com/broadinstitute/cromwell/files/1635814/SplitLargeRG.txt). Dependencies zip - [SomaticPairedSingleSampleWfDependencies.zip](https://github.com/broadinstitute/cromwell/files/1635815/SomaticPairedSingleSampleWfDependencies.zip). Let me know if theres any more information that might be useful.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143
https://github.com/broadinstitute/cromwell/pull/3146:31,Testability,test,test,31,🤞 at least one new conformance test passes...,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3146
https://github.com/broadinstitute/cromwell/pull/3147:32,Testability,test,test,32,"Wee PR that fixes a conformance test. This turns on quoting around `StringCommandPart`s which still isn't 100% right since it doesn't pay attention to `shellQuote` and also doesn't turn on quoting for any other command part. But quoting on is the default CWL behavior and this does allow more conformance tests to pass (one here, more in the next PR).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3147
https://github.com/broadinstitute/cromwell/pull/3147:305,Testability,test,tests,305,"Wee PR that fixes a conformance test. This turns on quoting around `StringCommandPart`s which still isn't 100% right since it doesn't pay attention to `shellQuote` and also doesn't turn on quoting for any other command part. But quoting on is the default CWL behavior and this does allow more conformance tests to pass (one here, more in the next PR).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3147
https://github.com/broadinstitute/cromwell/pull/3148:27,Integrability,depend,depends,27,"EnvVarRequirement support, depends on #3147.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3148
https://github.com/broadinstitute/cromwell/pull/3149:471,Testability,test,test,471,"CWL has an interesting way of denoting optional types, which is to declare an array where a single null type indicates that the accompanying non-null type is optional. There exists syntactic sugar to declare optionals, e.g. `File?` and `File[]?`. Luckly the SALAD pre-processing step de-sugars this for us into the aformentioned array. reference: http://www.commonwl.org/v1.0/Workflow.html#Document_preprocessing. Credit goes to @Horneth who wrote the code, I fixed some test and added new ones.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3149
https://github.com/broadinstitute/cromwell/issues/3151:96,Usability,simpl,simpletons,96,`WomMap` technically support any `WomValue`s as keys.; (De)Serialization of such maps from / to simpletons is not currently supported.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3151
https://github.com/broadinstitute/cromwell/pull/3153:0,Deployability,Update,Updated,0,"Updated some backend usages of WomSingleFile(womFile.value) with womFile.mapFile(_).; CWL glob creation temporarily creating WomSingleFile if the path doesn't contain '*' or '?'.; When generating secondary paths without '^' be sure to prefix the primary path.; Added directory outputs for un-nested directories.; Directory inputs are also incomplete, specifically paths don't localize to a user-specified parent directory.; IntelliJ suggested edits to BcsAsyncBackendJobExecutionActor.; Globs are now processed by CwlJsEncoder.; Added comments to CommandOutputBinding.generateOutputWomValue/coerceWomValue.; Bug fixes to CWL javascript directory encoding/decoding.; MetadataDatabaseAccessSpec uses unique names so that it can be run twice on the same db.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3153
https://github.com/broadinstitute/cromwell/issues/3154:21,Usability,clear,clearer,21,Make cromwell output clearer with regards to the CWL that it will handle and whether the submitted workflow is within those boundaries.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3154
https://github.com/broadinstitute/cromwell/issues/3156:4098,Availability,error,error,4098,"d 1 workflows from the WorkflowStoreActor; 2018-01-17 20:38:36,970 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - MaterializeWorkflowDescriptorActor [UUID(53c058fc)]: Call-to-Backend assignments: SplitLargeRG.Alignment -> JES, SplitLargeRG.SamSplitter -> JES, SomaticPairedEndSingleSampleWorkflow.GetBwaVersion -> JES, SomaticPairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba -> JES, SplitLargeRG.GatherBamFiless -> JES, SplitLargeRG.SumSplitAlignedSizes -> JES; 2018-01-17 20:38:38,399 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - WorkflowExecutionActor-53c058fc-65db-4347-9433-cc0753614776 [UUID(53c058fc)]: Starting calls: SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1; 2018-01-17 20:38:41,234 cromwell-system-akka.dispatchers.backend-dispatcher-47 INFO - JesAsyncBackendJobExecutionActor [UUID(53c058fc)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: `# not setting set -o pipefail here because /bwa has a rc=1 and we dont want to allow rc=1 to succeed because; # the sed may also fail with that error and that is something we actually want to fail on.; /usr/gitc/bwa 2>&1 | \; grep -e '^Version' | \; sed 's/Version: //'`; 2018-01-17 20:38:49,163 cromwell-system-akka.dispatchers.backend-dispatcher-47 INFO - JesAsyncBackendJobExecutionActor [UUID(53c058fc)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: job id: operations/ENrHrLeQLBiG_cWio8C8naYBIKngm4KOFSoPcHJvZHVjdGlvblF1ZXVl; 2018-01-17 20:39:01,789 cromwell-system-akka.dispatchers.backend-dispatcher-47 INFO - JesAsyncBackendJobExecutionActor [UUID(53c058fc)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: Status change from - to Running; 2018-01-17 20:41:59,809 cromwell-system-akka.dispatchers.backend-dispatcher-60 INFO - JesAsyncBackendJobExecutionActor [UUID(53c058fc)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: Status change from Running to Success; ```. Alternatively if I remove the scatter. ```; call GetBwaVersion. # Get the size of the s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156
https://github.com/broadinstitute/cromwell/issues/3156:9204,Availability,error,error,9204," 1 workflows from the WorkflowStoreActor; 2018-01-17 20:52:54,947 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - MaterializeWorkflowDescriptorActor [UUID(e71c769c)]: Call-to-Backend assignments: SomaticPairedEndSingleSampleWorkflow.GetBwaVersion -> JES, SplitLargeRG.SumSplitAlignedSizes -> JES, SplitLargeRG.GatherBamFiless -> JES, SomaticPairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba -> JES, SplitLargeRG.SamSplitter -> JES, SplitLargeRG.Alignment -> JES; 2018-01-17 20:52:56,323 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-e71c769c-948f-4bd7-8cbe-064a18375966 [UUID(e71c769c)]: Starting calls: SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1; 2018-01-17 20:53:02,487 cromwell-system-akka.dispatchers.backend-dispatcher-44 INFO - JesAsyncBackendJobExecutionActor [UUID(e71c769c)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: `# not setting set -o pipefail here because /bwa has a rc=1 and we dont want to allow rc=1 to succeed because; # the sed may also fail with that error and that is something we actually want to fail on.; /usr/gitc/bwa 2>&1 | \; grep -e '^Version' | \; sed 's/Version: //'`; 2018-01-17 20:53:04,348 cromwell-system-akka.dispatchers.backend-dispatcher-56 INFO - JesAsyncBackendJobExecutionActor [UUID(e71c769c)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: job id: operations/EPXh4LeQLBjT9Z2WvIiz-QggqeCbgo4VKg9wcm9kdWN0aW9uUXVldWU; 2018-01-17 20:53:15,636 cromwell-system-akka.dispatchers.backend-dispatcher-56 INFO - JesAsyncBackendJobExecutionActor [UUID(e71c769c)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: Status change from - to Running; 2018-01-17 20:56:25,285 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(e71c769c)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: Status change from Running to Success; 2018-01-17 20:56:28,223 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - Workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156
https://github.com/broadinstitute/cromwell/issues/3156:12252,Availability,ping,ping,12252,":56:30,264 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - 384e88c5-eba8-400c-aaef-5d618ffdce88-SubWorkflowActor-SubWorkflow-SplitRG:-1:1 [UUID(384e88c5)]: Starting calls: SplitLargeRG.SamSplitter:NA:1; 2018-01-17 20:56:30,293 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: `set -e; mkdir output_dir. total_reads=$(samtools view -c /cromwell_root/broad-dsp-spec-ops-cromwell-execution/CramToUnmappedBams/7db4d00c-0d04-43c5-b480-3cfe6080a3e3/call-SortSam/shard-0/0.1.unmapped.bam). java -Dsamjdk.compression_level=2 -Xms3000m -jar /usr/gitc/picard.jar SplitSamByNumberOfReads \; INPUT=/cromwell_root/broad-dsp-spec-ops-cromwell-execution/CramToUnmappedBams/7db4d00c-0d04-43c5-b480-3cfe6080a3e3/call-SortSam/shard-0/0.1.unmapped.bam \; OUTPUT=output_dir \; SPLIT_TO_N_READS=48000000 \; TOTAL_READS_IN_INPUT=$total_reads`; 2018-01-17 20:56:36,955 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: job id: operations/EOvc7beQLBiwi6fk-aX5yBEgqeCbgo4VKg9wcm9kdWN0aW9uUXVldWU; 2018-01-17 20:56:48,780 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: Status change from - to Running; ```. Here is the scattered [SomaticPairedSingleSampleWf.scattered.txt](https://github.com/broadinstitute/cromwell/files/1641317/SomaticPairedSingleSampleWf.scattered.txt) runnable version that gets stuck running. and the non scattered [SomaticPairedSingleSampleWf.single.txt](https://github.com/broadinstitute/cromwell/files/1641318/SomaticPairedSingleSampleWf.single.txt) runnable version that works great. Here is the dependencies zip [SomaticPairedSingleSampleWfDependencies.zip](https://github.com/broadinstitute/cromwell/files/1641320/SomaticPairedSingleSampleWfDependencies.zip). @kcibul i was asked to ping you on this issue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156
https://github.com/broadinstitute/cromwell/issues/3156:12063,Integrability,depend,dependencies,12063,":56:30,264 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - 384e88c5-eba8-400c-aaef-5d618ffdce88-SubWorkflowActor-SubWorkflow-SplitRG:-1:1 [UUID(384e88c5)]: Starting calls: SplitLargeRG.SamSplitter:NA:1; 2018-01-17 20:56:30,293 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: `set -e; mkdir output_dir. total_reads=$(samtools view -c /cromwell_root/broad-dsp-spec-ops-cromwell-execution/CramToUnmappedBams/7db4d00c-0d04-43c5-b480-3cfe6080a3e3/call-SortSam/shard-0/0.1.unmapped.bam). java -Dsamjdk.compression_level=2 -Xms3000m -jar /usr/gitc/picard.jar SplitSamByNumberOfReads \; INPUT=/cromwell_root/broad-dsp-spec-ops-cromwell-execution/CramToUnmappedBams/7db4d00c-0d04-43c5-b480-3cfe6080a3e3/call-SortSam/shard-0/0.1.unmapped.bam \; OUTPUT=output_dir \; SPLIT_TO_N_READS=48000000 \; TOTAL_READS_IN_INPUT=$total_reads`; 2018-01-17 20:56:36,955 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: job id: operations/EOvc7beQLBiwi6fk-aX5yBEgqeCbgo4VKg9wcm9kdWN0aW9uUXVldWU; 2018-01-17 20:56:48,780 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: Status change from - to Running; ```. Here is the scattered [SomaticPairedSingleSampleWf.scattered.txt](https://github.com/broadinstitute/cromwell/files/1641317/SomaticPairedSingleSampleWf.scattered.txt) runnable version that gets stuck running. and the non scattered [SomaticPairedSingleSampleWf.single.txt](https://github.com/broadinstitute/cromwell/files/1641318/SomaticPairedSingleSampleWf.single.txt) runnable version that works great. Here is the dependencies zip [SomaticPairedSingleSampleWfDependencies.zip](https://github.com/broadinstitute/cromwell/files/1641320/SomaticPairedSingleSampleWfDependencies.zip). @kcibul i was asked to ping you on this issue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156
https://github.com/broadinstitute/cromwell/issues/3156:2587,Testability,log,logs,2587,",; disk_multiplier = bwa_disk_multiplier,; unmapped_bam_size = unmapped_bam_size; }; }. if (unmapped_bam_size <= cutoff_for_large_rg_in_gb) {; # Map reads to reference; call commonTasks.SamToFastqAndBwaMemAndMba {; input:; input_bam = unmapped_bam,; bwa_commandline = bwa_commandline,; output_bam_basename = sub_sub + "".aligned.unsorted"",; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; ref_alt = ref_alt,; ref_bwt = ref_bwt,; ref_amb = ref_amb,; ref_ann = ref_ann,; ref_pac = ref_pac,; ref_sa = ref_sa,; bwa_version = GetBwaVersion.version,; # The merged bam can be bigger than only the aligned bam,; # so account for the output size by multiplying the input size by 2.75.; disk_size = unmapped_bam_size + bwa_ref_size + (bwa_disk_multiplier * unmapped_bam_size) + additional_disk,; compression_level = compression_level,; preemptible_tries = preemptible_tries; }; }; }; }; ```. `GetBwaVersion` will run and succeed but no `SplitRG` workflow will launch afterwards and the workflow will just be stuck in ""Running"" , this is what the server logs show. ```; 2018-01-17 20:38:35,611 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - WorkflowManagerActor Starting workflow UUID(53c058fc-65db-4347-9433-cc0753614776); 2018-01-17 20:38:35,614 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - WorkflowManagerActor Successfully started WorkflowActor-53c058fc-65db-4347-9433-cc0753614776; 2018-01-17 20:38:35,614 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-01-17 20:38:36,970 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - MaterializeWorkflowDescriptorActor [UUID(53c058fc)]: Call-to-Backend assignments: SplitLargeRG.Alignment -> JES, SplitLargeRG.SamSplitter -> JES, SomaticPairedEndSingleSampleWorkflow.GetBwaVersion -> JES, SomaticPairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba -> JES, SplitLargeRG.GatherBamFiless -> JES, SplitLargeRG.SumSplitAligned",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156
https://github.com/broadinstitute/cromwell/issues/3156:7436,Testability,log,log,7436,"ef_size,; disk_multiplier = bwa_disk_multiplier,; unmapped_bam_size = unmapped_bam_size; }; }. if (unmapped_bam_size <= cutoff_for_large_rg_in_gb) {; # Map reads to reference; call commonTasks.SamToFastqAndBwaMemAndMba {; input:; input_bam = unmapped_bam,; bwa_commandline = bwa_commandline,; output_bam_basename = sub_sub + "".aligned.unsorted"",; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; ref_alt = ref_alt,; ref_bwt = ref_bwt,; ref_amb = ref_amb,; ref_ann = ref_ann,; ref_pac = ref_pac,; ref_sa = ref_sa,; bwa_version = GetBwaVersion.version,; # The merged bam can be bigger than only the aligned bam,; # so account for the output size by multiplying the input size by 2.75.; disk_size = unmapped_bam_size + bwa_ref_size + (bwa_disk_multiplier * unmapped_bam_size) + additional_disk,; compression_level = compression_level,; preemptible_tries = preemptible_tries; }; }; }; ```. `GetBwaVersion` runs and succeeds and then launches the subworkflow. Server log for comparison. ```; 2018-01-17 20:52:50,403 cromwell-system-akka.dispatchers.api-dispatcher-23 INFO - Workflow e71c769c-948f-4bd7-8cbe-064a18375966 submitted.; 2018-01-17 20:52:53,874 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - 1 new workflows fetched; 2018-01-17 20:52:53,874 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(e71c769c-948f-4bd7-8cbe-064a18375966); 2018-01-17 20:52:53,874 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-e71c769c-948f-4bd7-8cbe-064a18375966; 2018-01-17 20:52:53,875 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-01-17 20:52:54,947 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - MaterializeWorkflowDescriptorActor [UUID(e71c769c)]: Call-to-Backend assignments: SomaticPairedEndSingleSampleWorkflow.GetBwaVersion -> JES, SplitLargeRG.SumSplitAlig",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156
https://github.com/broadinstitute/cromwell/issues/3157:486,Availability,Error,Error,486,"Here is the current situation from Google from https://partnerissuetracker.corp.google.com/issues/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:793,Availability,error,error,793,"Here is the current situation from Google from https://partnerissuetracker.corp.google.com/issues/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1003,Availability,Error,Error,1003,"m Google from https://partnerissuetracker.corp.google.com/issues/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Mes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1164,Availability,error,error,1164,"a@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1181,Availability,Error,Error,1181,"orted Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1320,Availability,error,error,1320,"our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGn",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1380,Availability,down,down,1380,"ticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:2042,Availability,failure,failures,2042,"=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EMPehaqLLBiS7p7OzdzYu5wBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl. > ------------------------------- ; > kcibul@broadinstitute.org <kcibul@broadinstitute.org> ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:3777,Availability,failure,failures,3777,"aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EMPehaqLLBiS7p7OzdzYu5wBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl. > ------------------------------- ; > kcibul@broadinstitute.org <kcibul@broadinstitute.org> #2 Jan 8, 2018 03:52PM ; > This is important to understand so Cromwell can do the right thing. It ; > hasn't been clear in the past why we sometimes get 13s on these preemptible ; > jobs ; > ; > Kristian Cibulskis ; > Director of Platform Engineering, Data Sciences Platform ; > Broad Institute of MIT and Harvard ; > kcibul@broadinstitute.org ; > ; > ; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:4226,Availability,failure,failure,4226," > jobs ; > ; > Kristian Cibulskis ; > Director of Platform Engineering, Data Sciences Platform ; > Broad Institute of MIT and Harvard ; > kcibul@broadinstitute.org ; > ; > ; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:4348,Availability,failure,failures,4348,"vard ; > kcibul@broadinstitute.org ; > ; > ; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:4456,Availability,failure,failures,4456,"titute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:4663,Availability,failure,failure,4663,"- our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either a GCE preemption policy change or some other resourcing issue. Mike, can you reach out to the GCE team on this?; > ; > Garret, let's look at some of the operations in #1 and see if we can s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:5694,Availability,error,error,5694,"stand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either a GCE preemption policy change or some other resourcing issue. Mike, can you reach out to the GCE team on this?; > ; > Garret, let's look at some of the operations in #1 and see if we can see any differences that point to the 13/14 error codes. > ------------------------------- ; > maltarace@google.com <maltarace@google.com> #7 Jan 17, 2018 11:45AM ; > Henry, can I get a project name and recent most time this issue occurred?. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #8 Jan 17, 2018 12:12PM ; > Mike,; > ; > Will get some more recent ones - but here is the project, start, end times, instance name, zone/machinetype for the opids listed in ticket. As you can see diff projects, diff region/zones, diff machine types; > ; > broad-wgs-prod2, 2018-01-02T03:00:06Z, 2018-01-02T03:53:16Z, ggp-17542369260334007071, us-east1-c/n1-standard-2; > broad-wgs-prod2, 2018-01-02T04:05:53Z, 2018-01-03T04:21:41Z, ggp-18057230858599167003, us-central1-f/n1-standard-2; > broad-wgs-prod2, 2018-01-03T04:26:14Z, 2018-01-03T04:46:10Z, ggp-7937324860344917086, us-central1-b/n1-standard-2; > broad-wgs-prod5, 2018-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:9693,Availability,down,down,9693,", ggp-1817239588482439788, us-east1-c/n1-standard-2; > broad-wgs-prod5, 2018-01-16T23:46:08Z, 2018-01-17T14:57:19Z, ggp-3754421722448645101, us-east1-d/n1-standard-2. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #11 Jan 17, 2018 12:32PM ; > Mike, ; > ; > For comparison - the previous reported workflow also had a Message 14: type pre-emption. Which is what cromwell normally detects as pre-emption. Not sure what is difference between the above pre-emptions and the one below. Info as follows:; > ; > OPSID; > operations/ENOi-PyPLBioyJKO-s3GhY0BIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > ; > broad-wgs-prod5, 2018-01-16T15:37:17Z, 2018-01-16T16:43:22Z, ggp-10163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've bee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:10016,Availability,down,down,10016," what cromwell normally detects as pre-emption. Not sure what is difference between the above pre-emptions and the one below. Info as follows:; > ; > OPSID; > operations/ENOi-PyPLBioyJKO-s3GhY0BIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > ; > broad-wgs-prod5, 2018-01-16T15:37:17Z, 2018-01-16T16:43:22Z, ggp-10163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:11297,Availability,error,error,11297,"n also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:11366,Availability,down,down,11366,"n also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:11390,Availability,error,error,11390,"n also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:12282,Availability,error,errors,12282," instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error to include a message that ; > can indicate it's really a preemption? As an example, error code 2 will ; > sometimes indicate it was a preemption. ; > ; > J . @hjfbynara tagging you if you have anything you want to add",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:12337,Availability,error,errors,12337," instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error to include a message that ; > can indicate it's really a preemption? As an example, error code 2 will ; > sometimes indicate it was a preemption. ; > ; > J . @hjfbynara tagging you if you have anything you want to add",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:12500,Availability,error,error,12500," instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error to include a message that ; > can indicate it's really a preemption? As an example, error code 2 will ; > sometimes indicate it was a preemption. ; > ; > J . @hjfbynara tagging you if you have anything you want to add",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:12590,Availability,error,error,12590," instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error to include a message that ; > can indicate it's really a preemption? As an example, error code 2 will ; > sometimes indicate it was a preemption. ; > ; > J . @hjfbynara tagging you if you have anything you want to add",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:5271,Deployability,pipeline,pipeline,5271,"e entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either a GCE preemption policy change or some other resourcing issue. Mike, can you reach out to the GCE team on this?; > ; > Garret, let's look at some of the operations in #1 and see if we can see any differences that point to the 13/14 error codes. > ------------------------------- ; > maltarace@google.com <maltarace@google.com> #7 Jan 17, 2018 11:45AM ; > Henry, can I get a project name and recent most time this issue occurred?. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #8 Jan 17, 2018 12:12PM ; > Mike,; > ; > Will get some more recent ones - but here is the project, start, end times, instance name, zone/machinetype for the opids listed in ticket. As you can see diff projects, diff region/zones, diff machine types; > ; > broad-wgs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:5415,Deployability,Pipeline,Pipelines,5415,"h the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either a GCE preemption policy change or some other resourcing issue. Mike, can you reach out to the GCE team on this?; > ; > Garret, let's look at some of the operations in #1 and see if we can see any differences that point to the 13/14 error codes. > ------------------------------- ; > maltarace@google.com <maltarace@google.com> #7 Jan 17, 2018 11:45AM ; > Henry, can I get a project name and recent most time this issue occurred?. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #8 Jan 17, 2018 12:12PM ; > Mike,; > ; > Will get some more recent ones - but here is the project, start, end times, instance name, zone/machinetype for the opids listed in ticket. As you can see diff projects, diff region/zones, diff machine types; > ; > broad-wgs-prod2, 2018-01-02T03:00:06Z, 2018-01-02T03:53:16Z, ggp-17542369260334007071, us-east1-c/n1-standard-2; > broad-wgs-prod2, 2018-01-02T04:05:53Z, 2018-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:501,Integrability,Message,Message,501,"Here is the current situation from Google from https://partnerissuetracker.corp.google.com/issues/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:705,Integrability,message,message,705,"Here is the current situation from Google from https://partnerissuetracker.corp.google.com/issues/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1066,Integrability,Message,Message,1066,"s/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1196,Integrability,Message,Message,1196,"orted Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1241,Integrability,message,message,1241,"orted Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1335,Integrability,Message,Message,1335,"ticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1450,Integrability,Message,Message,1450," showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1481,Integrability,Message,Message,1481," showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1740,Integrability,Message,Message,1740,"mToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1782,Integrability,Message,Message,1782,"mToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1852,Integrability,Message,Message,1852,"mToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1946,Integrability,Message,Message,1946,"mToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1987,Integrability,Message,Message,1987,"mToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:2031,Integrability,Message,Message,2031,"=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EMPehaqLLBiS7p7OzdzYu5wBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl. > ------------------------------- ; > kcibul@broadinstitute.org <kcibul@broadinstitute.org> ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:2059,Integrability,MESSAGE,MESSAGE,2059," > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EMPehaqLLBiS7p7OzdzYu5wBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl. > ------------------------------- ; > kcibul@broadinstitute.org <kcibul@broadinstitute.org> #2 Jan 8, 2018 03:52PM ; > This is important to understand so Cromwell can do the right thing. It ; > hasn't been clear in the past why we sometimes get 13s on these preemptible ; > jobs ; > ; > Kristian Cibulski",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:2426,Integrability,MESSAGE,MESSAGE,2426,"le"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EMPehaqLLBiS7p7OzdzYu5wBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl. > ------------------------------- ; > kcibul@broadinstitute.org <kcibul@broadinstitute.org> #2 Jan 8, 2018 03:52PM ; > This is important to understand so Cromwell can do the right thing. It ; > hasn't been clear in the past why we sometimes get 13s on these preemptible ; > jobs ; > ; > Kristian Cibulskis ; > Director of Platform Engineering, Data Sciences Platform ; > Broad Institute of MIT and Harvard ; > kcibul@broadinstitute.org ; > ; > ; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff repor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:3764,Integrability,Message,Message,3764,"aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EMPehaqLLBiS7p7OzdzYu5wBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl. > ------------------------------- ; > kcibul@broadinstitute.org <kcibul@broadinstitute.org> #2 Jan 8, 2018 03:52PM ; > This is important to understand so Cromwell can do the right thing. It ; > hasn't been clear in the past why we sometimes get 13s on these preemptible ; > jobs ; > ; > Kristian Cibulskis ; > Director of Platform Engineering, Data Sciences Platform ; > Broad Institute of MIT and Harvard ; > kcibul@broadinstitute.org ; > ; > ; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:4429,Integrability,Message,Message,4429,"titute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:4557,Integrability,Message,Message,4557,"- our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either a GCE preemption policy change or some other resourcing issue. Mike, can you reach out to the GCE team on this?; > ; > Garret, let's look at some of the operations in #1 and see if we can s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:4651,Integrability,Message,Message,4651,"- our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either a GCE preemption policy change or some other resourcing issue. Mike, can you reach out to the GCE team on this?; > ; > Garret, let's look at some of the operations in #1 and see if we can s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:8995,Integrability,Message,Message,8995,"ldWU; > operations/EOCf-4iQLBjstJf68LiInBkgx_mw9zYqD3Byb2R1Y3Rpb25RdWV1ZQ; > operations/EOjA9oqQLBjtl7_8otWYjTQgx_mw9zYqD3Byb2R1Y3Rpb25RdWV1ZQ; > ; > broad-wgs-prod5, 2018-01-16T15:37:16Z, 2018-01-16T17:45:43Z, ggp-1801918915849415035, us-central1-c/n1-standard-16; > broad-wgs-prod5, 2018-01-16T15:37:16Z, 2018-01-16T15:46:25Z, ggp-1347601243842424591, us-east1-c/n1-standard-16; > broad-wgs-prod5, 2018-01-16T15:37:16Z, 2018-01-16T17:14:27Z, ggp-17952768368412969986, us-east1-c/n1-standard-16; > broad-wgs-prod5, 2018-01-16T20:41:42Z, 2018-01-16T22:28:14Z, ggp-17459223747282221022, us-central1-b/n1-standard-2; > broad-wgs-prod5, 2018-01-16T22:37:32Z, 2018-01-16T23:38:28Z, ggp-1817239588482439788, us-east1-c/n1-standard-2; > broad-wgs-prod5, 2018-01-16T23:46:08Z, 2018-01-17T14:57:19Z, ggp-3754421722448645101, us-east1-d/n1-standard-2. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #11 Jan 17, 2018 12:32PM ; > Mike, ; > ; > For comparison - the previous reported workflow also had a Message 14: type pre-emption. Which is what cromwell normally detects as pre-emption. Not sure what is difference between the above pre-emptions and the one below. Info as follows:; > ; > OPSID; > operations/ENOi-PyPLBioyJKO-s3GhY0BIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > ; > broad-wgs-prod5, 2018-01-16T15:37:17Z, 2018-01-16T16:43:22Z, ggp-10163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:10238,Integrability,Message,Message,10238,"0163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI server",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:10356,Integrability,Message,Message,10356,"0163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI server",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:10433,Integrability,Message,Message,10433,"0163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI server",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:10645,Integrability,Message,Message,10645,"simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:10882,Integrability,message,messages,10882,"r of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:11198,Integrability,Message,Message,11198," 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; >",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:11505,Integrability,message,message,11505,"---- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:11731,Integrability,Message,Message,11731," instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error to include a message that ; > can indicate it's really a preemption? As an example, error code 2 will ; > sometimes indicate it was a preemption. ; > ; > J . @hjfbynara tagging you if you have anything you want to add",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:11787,Integrability,Message,Message,11787," instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error to include a message that ; > can indicate it's really a preemption? As an example, error code 2 will ; > sometimes indicate it was a preemption. ; > ; > J . @hjfbynara tagging you if you have anything you want to add",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:11997,Integrability,message,message,11997," instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error to include a message that ; > can indicate it's really a preemption? As an example, error code 2 will ; > sometimes indicate it was a preemption. ; > ; > J . @hjfbynara tagging you if you have anything you want to add",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:12519,Integrability,message,message,12519," instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error to include a message that ; > can indicate it's really a preemption? As an example, error code 2 will ; > sometimes indicate it was a preemption. ; > ; > J . @hjfbynara tagging you if you have anything you want to add",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1026,Safety,ABORT,ABORTED,1026,"m Google from https://partnerissuetracker.corp.google.com/issues/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Mes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:9057,Safety,detect,detects,9057,"LBjtl7_8otWYjTQgx_mw9zYqD3Byb2R1Y3Rpb25RdWV1ZQ; > ; > broad-wgs-prod5, 2018-01-16T15:37:16Z, 2018-01-16T17:45:43Z, ggp-1801918915849415035, us-central1-c/n1-standard-16; > broad-wgs-prod5, 2018-01-16T15:37:16Z, 2018-01-16T15:46:25Z, ggp-1347601243842424591, us-east1-c/n1-standard-16; > broad-wgs-prod5, 2018-01-16T15:37:16Z, 2018-01-16T17:14:27Z, ggp-17952768368412969986, us-east1-c/n1-standard-16; > broad-wgs-prod5, 2018-01-16T20:41:42Z, 2018-01-16T22:28:14Z, ggp-17459223747282221022, us-central1-b/n1-standard-2; > broad-wgs-prod5, 2018-01-16T22:37:32Z, 2018-01-16T23:38:28Z, ggp-1817239588482439788, us-east1-c/n1-standard-2; > broad-wgs-prod5, 2018-01-16T23:46:08Z, 2018-01-17T14:57:19Z, ggp-3754421722448645101, us-east1-d/n1-standard-2. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #11 Jan 17, 2018 12:32PM ; > Mike, ; > ; > For comparison - the previous reported workflow also had a Message 14: type pre-emption. Which is what cromwell normally detects as pre-emption. Not sure what is difference between the above pre-emptions and the one below. Info as follows:; > ; > OPSID; > operations/ENOi-PyPLBioyJKO-s3GhY0BIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > ; > broad-wgs-prod5, 2018-01-16T15:37:17Z, 2018-01-16T16:43:22Z, ggp-10163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:11562,Safety,abort,aborted,11562,"---- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:575,Testability,log,logic,575,"Here is the current situation from Google from https://partnerissuetracker.corp.google.com/issues/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1514,Testability,log,logic,1514," showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:1681,Testability,log,logic,1681," ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:9788,Testability,log,logic,9788,", ggp-3754421722448645101, us-east1-d/n1-standard-2. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #11 Jan 17, 2018 12:32PM ; > Mike, ; > ; > For comparison - the previous reported workflow also had a Message 14: type pre-emption. Which is what cromwell normally detects as pre-emption. Not sure what is difference between the above pre-emptions and the one below. Info as follows:; > ; > OPSID; > operations/ENOi-PyPLBioyJKO-s3GhY0BIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > ; > broad-wgs-prod5, 2018-01-16T15:37:17Z, 2018-01-16T16:43:22Z, ggp-10163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty su",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:3139,Usability,clear,clear,3139,"say6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EMPehaqLLBiS7p7OzdzYu5wBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl. > ------------------------------- ; > kcibul@broadinstitute.org <kcibul@broadinstitute.org> #2 Jan 8, 2018 03:52PM ; > This is important to understand so Cromwell can do the right thing. It ; > hasn't been clear in the past why we sometimes get 13s on these preemptible ; > jobs ; > ; > Kristian Cibulskis ; > Director of Platform Engineering, Data Sciences Platform ; > Broad Institute of MIT and Harvard ; > kcibul@broadinstitute.org ; > ; > ; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:9642,Usability,simpl,simply,9642,", ggp-1817239588482439788, us-east1-c/n1-standard-2; > broad-wgs-prod5, 2018-01-16T23:46:08Z, 2018-01-17T14:57:19Z, ggp-3754421722448645101, us-east1-d/n1-standard-2. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #11 Jan 17, 2018 12:32PM ; > Mike, ; > ; > For comparison - the previous reported workflow also had a Message 14: type pre-emption. Which is what cromwell normally detects as pre-emption. Not sure what is difference between the above pre-emptions and the one below. Info as follows:; > ; > OPSID; > operations/ENOi-PyPLBioyJKO-s3GhY0BIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > ; > broad-wgs-prod5, 2018-01-16T15:37:17Z, 2018-01-16T16:43:22Z, ggp-10163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've bee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/issues/3157:10929,Usability,simpl,simply,10929,"VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receiv",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157
https://github.com/broadinstitute/cromwell/pull/3158:5,Deployability,configurat,configuration,5,"In a configuration like. ```wdl; workflow ifs_in_scatters {; call hello. scatter (n in range(5)) {; if (true) {; call goodbye { input: i = hello.out }; }; }; }; ```; When the conditional graph is created, all nodes outside of the scatter get ""wrapped"" in an `OuterGraphInputNode` that gets passed into the inner conditional graph so that nodes in the inner graph can reference nodes outside of the scatter.; The issue is that those OGINs are created with `preserveScatterIndex = true` even though the node they're pointing to is outside of the scatter. This was preventing the `ExecutionStore` from detecting the `i = hello.out` expression as being runnable because it was looking for a `hello.out` node in `Done` state at index `n`, which doesn't exist since `call hello` is outside the scatter.; This PR changes that to use the `preserveIndexForOuterLookups ` value of the conditional / scatter instead, which in this case will be `false`, because the scatter node does set `preserveScatterIndex = false` to build its inner graph (in this case the if).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3158
https://github.com/broadinstitute/cromwell/pull/3158:243,Integrability,wrap,wrapped,243,"In a configuration like. ```wdl; workflow ifs_in_scatters {; call hello. scatter (n in range(5)) {; if (true) {; call goodbye { input: i = hello.out }; }; }; }; ```; When the conditional graph is created, all nodes outside of the scatter get ""wrapped"" in an `OuterGraphInputNode` that gets passed into the inner conditional graph so that nodes in the inner graph can reference nodes outside of the scatter.; The issue is that those OGINs are created with `preserveScatterIndex = true` even though the node they're pointing to is outside of the scatter. This was preventing the `ExecutionStore` from detecting the `i = hello.out` expression as being runnable because it was looking for a `hello.out` node in `Done` state at index `n`, which doesn't exist since `call hello` is outside the scatter.; This PR changes that to use the `preserveIndexForOuterLookups ` value of the conditional / scatter instead, which in this case will be `false`, because the scatter node does set `preserveScatterIndex = false` to build its inner graph (in this case the if).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3158
https://github.com/broadinstitute/cromwell/pull/3158:5,Modifiability,config,configuration,5,"In a configuration like. ```wdl; workflow ifs_in_scatters {; call hello. scatter (n in range(5)) {; if (true) {; call goodbye { input: i = hello.out }; }; }; }; ```; When the conditional graph is created, all nodes outside of the scatter get ""wrapped"" in an `OuterGraphInputNode` that gets passed into the inner conditional graph so that nodes in the inner graph can reference nodes outside of the scatter.; The issue is that those OGINs are created with `preserveScatterIndex = true` even though the node they're pointing to is outside of the scatter. This was preventing the `ExecutionStore` from detecting the `i = hello.out` expression as being runnable because it was looking for a `hello.out` node in `Done` state at index `n`, which doesn't exist since `call hello` is outside the scatter.; This PR changes that to use the `preserveIndexForOuterLookups ` value of the conditional / scatter instead, which in this case will be `false`, because the scatter node does set `preserveScatterIndex = false` to build its inner graph (in this case the if).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3158
https://github.com/broadinstitute/cromwell/pull/3158:599,Safety,detect,detecting,599,"In a configuration like. ```wdl; workflow ifs_in_scatters {; call hello. scatter (n in range(5)) {; if (true) {; call goodbye { input: i = hello.out }; }; }; }; ```; When the conditional graph is created, all nodes outside of the scatter get ""wrapped"" in an `OuterGraphInputNode` that gets passed into the inner conditional graph so that nodes in the inner graph can reference nodes outside of the scatter.; The issue is that those OGINs are created with `preserveScatterIndex = true` even though the node they're pointing to is outside of the scatter. This was preventing the `ExecutionStore` from detecting the `i = hello.out` expression as being runnable because it was looking for a `hello.out` node in `Done` state at index `n`, which doesn't exist since `call hello` is outside the scatter.; This PR changes that to use the `preserveIndexForOuterLookups ` value of the conditional / scatter instead, which in this case will be `false`, because the scatter node does set `preserveScatterIndex = false` to build its inner graph (in this case the if).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3158
https://github.com/broadinstitute/cromwell/issues/3160:75,Safety,avoid,avoid,75,Centaur tests poorly assess the format of output files in the metadata. To avoid regressions it would be preferable to have better coverage of this.; The main issue comes from the fact that files path are dynamic and hard to validate with the static test definitions centaur has.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3160
https://github.com/broadinstitute/cromwell/issues/3160:225,Security,validat,validate,225,Centaur tests poorly assess the format of output files in the metadata. To avoid regressions it would be preferable to have better coverage of this.; The main issue comes from the fact that files path are dynamic and hard to validate with the static test definitions centaur has.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3160
https://github.com/broadinstitute/cromwell/issues/3160:8,Testability,test,tests,8,Centaur tests poorly assess the format of output files in the metadata. To avoid regressions it would be preferable to have better coverage of this.; The main issue comes from the fact that files path are dynamic and hard to validate with the static test definitions centaur has.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3160
https://github.com/broadinstitute/cromwell/issues/3160:250,Testability,test,test,250,Centaur tests poorly assess the format of output files in the metadata. To avoid regressions it would be preferable to have better coverage of this.; The main issue comes from the fact that files path are dynamic and hard to validate with the static test definitions centaur has.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3160
https://github.com/broadinstitute/cromwell/issues/3161:485,Availability,failure,failure,485,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:673,Availability,failure,failures,673,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:873,Availability,robust,robust,873,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:1411,Availability,error,error,1411,"flow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If the task says ""preemptible: 5"" and the workflow says ""failed_task_retries: 3"", then Cromwell will retry that task up to 8 times. The first 3 retries will use a preemptible machine -- regardless of the reason for the error. Additional retries beyond that will not use a preemptible machine. This approach would let failed_task_retries act as a floor for the entire workflow that guarantees all tasks, preemptible or not, will retry at least failed_task_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:2216,Availability,error,error,2216,"pe of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If the task says ""preemptible: 5"" and the workflow says ""failed_task_retries: 3"", then Cromwell will retry that task up to 8 times. The first 3 retries will use a preemptible machine -- regardless of the reason for the error. Additional retries beyond that will not use a preemptible machine. This approach would let failed_task_retries act as a floor for the entire workflow that guarantees all tasks, preemptible or not, will retry at least failed_task_retries times upon failure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:2471,Availability,failure,failure,2471,"pe of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If the task says ""preemptible: 5"" and the workflow says ""failed_task_retries: 3"", then Cromwell will retry that task up to 8 times. The first 3 retries will use a preemptible machine -- regardless of the reason for the error. Additional retries beyond that will not use a preemptible machine. This approach would let failed_task_retries act as a floor for the entire workflow that guarantees all tasks, preemptible or not, will retry at least failed_task_retries times upon failure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:60,Deployability,Pipeline,Pipelines,60,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:606,Energy Efficiency,Green,Green,606,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:898,Energy Efficiency,reduce,reduce,898,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:1169,Energy Efficiency,efficient,efficient,1169,"by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If the task says ""preemptible: 5"" and the workflow says ""failed_task_retries: 3"", then Cromwell will retry that task up to 8 times. The first 3 ret",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:1417,Integrability,message,messages,1417,"flow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If the task says ""preemptible: 5"" and the workflow says ""failed_task_retries: 3"", then Cromwell will retry that task up to 8 times. The first 3 retries will use a preemptible machine -- regardless of the reason for the error. Additional retries beyond that will not use a preemptible machine. This approach would let failed_task_retries act as a floor for the entire workflow that guarantees all tasks, preemptible or not, will retry at least failed_task_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:120,Modifiability,config,configurable,120,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:333,Modifiability,config,configured,333,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:581,Testability,test,tested,581,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/issues/3161:1368,Usability,simpl,simpler,1368,"ine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If the task says ""preemptible: 5"" and the workflow says ""failed_task_retries: 3"", then Cromwell will retry that task up to 8 times. The first 3 retries will use a preemptible machine -- regardless of the reason for the error. Additional retries beyond that will not use a preemptible machine. This approach would let faile",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161
https://github.com/broadinstitute/cromwell/pull/3163:75,Energy Efficiency,green,green,75,This was a clean cherry pick so i'm just going to wait for the tests to go green,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3163
https://github.com/broadinstitute/cromwell/pull/3163:63,Testability,test,tests,63,This was a clean cherry pick so i'm just going to wait for the tests to go green,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3163
https://github.com/broadinstitute/cromwell/pull/3171:105,Testability,test,tests,105,I removed the default gcs prefix and default docker conf thinking that they only existed for conformance tests but maybe we want to keep them for other purposes ?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3171
https://github.com/broadinstitute/cromwell/issues/3173:226,Deployability,pipeline,pipelines,226,"NOTE: This ticket refinement will require work with Google/Verily. Currently at the start of a call a PAPI-client must register a path to be delocalized via [`gsutil`](https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines#pipelineparameter). The PAPI `gsutil cp` does not use [`-r`](https://cloud.google.com/storage/docs/gsutil/commands/cp#description), so it does not work for nested directories. For POSIX-globs the cromwell ~~JES~~ PAPI backend is able to delocalize by creating a new folder containing matches of each of the outputs. We could try to flatten a directory in the same way to delocalize all the files nested inside the top level directory, but ideally we would maintain the paths to the directories. For example if a WDL command were to create a structure like:; ```; out/normal/sample.bam; out/tumor/sample.bam; ```. Even using our globs with `Array[File] bams = glob(""out/*/sample.bam"")` would cause problems right now with the way we flatten globs into a single directory. Cromwell creates a directory (sort-of) named `out/cromwell-glob-1/`, and then hard links in each of the `sample.bam` files. This won't work with the aforementioned glob as >1 file match the same name and will collide in the links. A/C: Delocalize globs and nested-directories with paths-in-the-cloud similar to the path-generated-by-the-command. A test should be created to ensure that a collision like the above delocalizes correctly. This will like require updates to the PAPI API, possibly even an implementation of a custom (de-)localizer in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3173
https://github.com/broadinstitute/cromwell/issues/3173:236,Deployability,pipeline,pipelineparameter,236,"NOTE: This ticket refinement will require work with Google/Verily. Currently at the start of a call a PAPI-client must register a path to be delocalized via [`gsutil`](https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines#pipelineparameter). The PAPI `gsutil cp` does not use [`-r`](https://cloud.google.com/storage/docs/gsutil/commands/cp#description), so it does not work for nested directories. For POSIX-globs the cromwell ~~JES~~ PAPI backend is able to delocalize by creating a new folder containing matches of each of the outputs. We could try to flatten a directory in the same way to delocalize all the files nested inside the top level directory, but ideally we would maintain the paths to the directories. For example if a WDL command were to create a structure like:; ```; out/normal/sample.bam; out/tumor/sample.bam; ```. Even using our globs with `Array[File] bams = glob(""out/*/sample.bam"")` would cause problems right now with the way we flatten globs into a single directory. Cromwell creates a directory (sort-of) named `out/cromwell-glob-1/`, and then hard links in each of the `sample.bam` files. This won't work with the aforementioned glob as >1 file match the same name and will collide in the links. A/C: Delocalize globs and nested-directories with paths-in-the-cloud similar to the path-generated-by-the-command. A test should be created to ensure that a collision like the above delocalizes correctly. This will like require updates to the PAPI API, possibly even an implementation of a custom (de-)localizer in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3173
https://github.com/broadinstitute/cromwell/issues/3173:1466,Deployability,update,updates,1466,"NOTE: This ticket refinement will require work with Google/Verily. Currently at the start of a call a PAPI-client must register a path to be delocalized via [`gsutil`](https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines#pipelineparameter). The PAPI `gsutil cp` does not use [`-r`](https://cloud.google.com/storage/docs/gsutil/commands/cp#description), so it does not work for nested directories. For POSIX-globs the cromwell ~~JES~~ PAPI backend is able to delocalize by creating a new folder containing matches of each of the outputs. We could try to flatten a directory in the same way to delocalize all the files nested inside the top level directory, but ideally we would maintain the paths to the directories. For example if a WDL command were to create a structure like:; ```; out/normal/sample.bam; out/tumor/sample.bam; ```. Even using our globs with `Array[File] bams = glob(""out/*/sample.bam"")` would cause problems right now with the way we flatten globs into a single directory. Cromwell creates a directory (sort-of) named `out/cromwell-glob-1/`, and then hard links in each of the `sample.bam` files. This won't work with the aforementioned glob as >1 file match the same name and will collide in the links. A/C: Delocalize globs and nested-directories with paths-in-the-cloud similar to the path-generated-by-the-command. A test should be created to ensure that a collision like the above delocalizes correctly. This will like require updates to the PAPI API, possibly even an implementation of a custom (de-)localizer in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3173
https://github.com/broadinstitute/cromwell/issues/3173:1355,Testability,test,test,1355,"NOTE: This ticket refinement will require work with Google/Verily. Currently at the start of a call a PAPI-client must register a path to be delocalized via [`gsutil`](https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines#pipelineparameter). The PAPI `gsutil cp` does not use [`-r`](https://cloud.google.com/storage/docs/gsutil/commands/cp#description), so it does not work for nested directories. For POSIX-globs the cromwell ~~JES~~ PAPI backend is able to delocalize by creating a new folder containing matches of each of the outputs. We could try to flatten a directory in the same way to delocalize all the files nested inside the top level directory, but ideally we would maintain the paths to the directories. For example if a WDL command were to create a structure like:; ```; out/normal/sample.bam; out/tumor/sample.bam; ```. Even using our globs with `Array[File] bams = glob(""out/*/sample.bam"")` would cause problems right now with the way we flatten globs into a single directory. Cromwell creates a directory (sort-of) named `out/cromwell-glob-1/`, and then hard links in each of the `sample.bam` files. This won't work with the aforementioned glob as >1 file match the same name and will collide in the links. A/C: Delocalize globs and nested-directories with paths-in-the-cloud similar to the path-generated-by-the-command. A test should be created to ensure that a collision like the above delocalizes correctly. This will like require updates to the PAPI API, possibly even an implementation of a custom (de-)localizer in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3173
https://github.com/broadinstitute/cromwell/issues/3176:24,Availability,error,errors,24,"I get cyclic dependency errors in places where I do not see a reason for them. I enclose the wdl and input.json; [cyclic_error.zip](https://github.com/broadinstitute/cromwell/files/1654348/cyclic_error.zip). Here is also the error log:; ```; Workflow input processing failed; WorkflowFailure(This workflow contains a cyclic dependency on quality_de_novo_with_download.copy_initial_quality_reports,List())WorkflowFailure(wdl.Scope.childGraphNodesSorted(Scope.scala:51),List())WorkflowFailure(wdl.Scope.childGraphNodesSorted$(Scope.scala:42),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140),List())WorkflowFailure(wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27),List())WorkflowFailure(wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:97),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176
https://github.com/broadinstitute/cromwell/issues/3176:225,Availability,error,error,225,"I get cyclic dependency errors in places where I do not see a reason for them. I enclose the wdl and input.json; [cyclic_error.zip](https://github.com/broadinstitute/cromwell/files/1654348/cyclic_error.zip). Here is also the error log:; ```; Workflow input processing failed; WorkflowFailure(This workflow contains a cyclic dependency on quality_de_novo_with_download.copy_initial_quality_reports,List())WorkflowFailure(wdl.Scope.childGraphNodesSorted(Scope.scala:51),List())WorkflowFailure(wdl.Scope.childGraphNodesSorted$(Scope.scala:42),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140),List())WorkflowFailure(wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27),List())WorkflowFailure(wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:97),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176
https://github.com/broadinstitute/cromwell/issues/3176:13,Integrability,depend,dependency,13,"I get cyclic dependency errors in places where I do not see a reason for them. I enclose the wdl and input.json; [cyclic_error.zip](https://github.com/broadinstitute/cromwell/files/1654348/cyclic_error.zip). Here is also the error log:; ```; Workflow input processing failed; WorkflowFailure(This workflow contains a cyclic dependency on quality_de_novo_with_download.copy_initial_quality_reports,List())WorkflowFailure(wdl.Scope.childGraphNodesSorted(Scope.scala:51),List())WorkflowFailure(wdl.Scope.childGraphNodesSorted$(Scope.scala:42),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140),List())WorkflowFailure(wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27),List())WorkflowFailure(wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:97),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176
https://github.com/broadinstitute/cromwell/issues/3176:324,Integrability,depend,dependency,324,"I get cyclic dependency errors in places where I do not see a reason for them. I enclose the wdl and input.json; [cyclic_error.zip](https://github.com/broadinstitute/cromwell/files/1654348/cyclic_error.zip). Here is also the error log:; ```; Workflow input processing failed; WorkflowFailure(This workflow contains a cyclic dependency on quality_de_novo_with_download.copy_initial_quality_reports,List())WorkflowFailure(wdl.Scope.childGraphNodesSorted(Scope.scala:51),List())WorkflowFailure(wdl.Scope.childGraphNodesSorted$(Scope.scala:42),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140),List())WorkflowFailure(wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27),List())WorkflowFailure(wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:97),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176
https://github.com/broadinstitute/cromwell/issues/3176:2389,Performance,concurren,concurrent,2389,"r.scala:493),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157),List())WorkflowFailure(scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304),List())WorkflowFailure(scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37),List())WorkflowFailure(scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60),List())WorkflowFailure(akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91),List())WorkflowFailure(scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12),List())WorkflowFailure(scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91),List())WorkflowFailure(akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40),List())WorkflowFailure(akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43),List())WorkflowFailure(akka.dispatch.forkjoin.ForkJo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176
https://github.com/broadinstitute/cromwell/issues/3176:2473,Performance,concurren,concurrent,2473,"())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157),List())WorkflowFailure(scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304),List())WorkflowFailure(scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37),List())WorkflowFailure(scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60),List())WorkflowFailure(akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91),List())WorkflowFailure(scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12),List())WorkflowFailure(scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91),List())WorkflowFailure(akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40),List())WorkflowFailure(akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43),List())WorkflowFailure(akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260),List())WorkflowFailure(akka.dispatch.forkjoin.F",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176
https://github.com/broadinstitute/cromwell/issues/3176:2569,Performance,concurren,concurrent,2569,"torActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157),List())WorkflowFailure(scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304),List())WorkflowFailure(scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37),List())WorkflowFailure(scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60),List())WorkflowFailure(akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91),List())WorkflowFailure(scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12),List())WorkflowFailure(scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91),List())WorkflowFailure(akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40),List())WorkflowFailure(akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43),List())WorkflowFailure(akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260),List())WorkflowFailure(akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339),List())WorkflowFailure(akka.dispatch.forkj",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176
https://github.com/broadinstitute/cromwell/issues/3176:2966,Performance,concurren,concurrent,2966,"flowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157),List())WorkflowFailure(scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304),List())WorkflowFailure(scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37),List())WorkflowFailure(scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60),List())WorkflowFailure(akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91),List())WorkflowFailure(scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12),List())WorkflowFailure(scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91),List())WorkflowFailure(akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40),List())WorkflowFailure(akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43),List())WorkflowFailure(akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260),List())WorkflowFailure(akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339),List())WorkflowFailure(akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979),List())WorkflowFailure(akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107),List()); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176
https://github.com/broadinstitute/cromwell/issues/3176:1337,Security,validat,validateWdlNamespace,1337,"nload.copy_initial_quality_reports,List())WorkflowFailure(wdl.Scope.childGraphNodesSorted(Scope.scala:51),List())WorkflowFailure(wdl.Scope.childGraphNodesSorted$(Scope.scala:42),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140),List())WorkflowFailure(wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27),List())WorkflowFailure(wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:97),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157),Li",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176
https://github.com/broadinstitute/cromwell/issues/3176:1592,Security,validat,validateWdlNamespace,1592,"rkflow.scala:63),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140),List())WorkflowFailure(wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27),List())WorkflowFailure(wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:97),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157),List())WorkflowFailure(scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304),List())WorkflowFailure(scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37),List())WorkflowFailure(scala.concurrent.impl.CallbackRunnable.run(Promise.sca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176
https://github.com/broadinstitute/cromwell/issues/3176:1838,Security,validat,validateWdlNamespace,1838,"low.scala:52),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27),List())WorkflowFailure(wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:97),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157),List())WorkflowFailure(scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304),List())WorkflowFailure(scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37),List())WorkflowFailure(scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60),List())WorkflowFailure(akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55),List())WorkflowFailure(akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91),List())WorkflowFailure(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176
https://github.com/broadinstitute/cromwell/issues/3176:231,Testability,log,log,231,"I get cyclic dependency errors in places where I do not see a reason for them. I enclose the wdl and input.json; [cyclic_error.zip](https://github.com/broadinstitute/cromwell/files/1654348/cyclic_error.zip). Here is also the error log:; ```; Workflow input processing failed; WorkflowFailure(This workflow contains a cyclic dependency on quality_de_novo_with_download.copy_initial_quality_reports,List())WorkflowFailure(wdl.Scope.childGraphNodesSorted(Scope.scala:51),List())WorkflowFailure(wdl.Scope.childGraphNodesSorted$(Scope.scala:42),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140),List())WorkflowFailure(wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27),List())WorkflowFailure(wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:97),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176
https://github.com/broadinstitute/cromwell/pull/3178:236,Performance,cache,cache,236,"Previously, if a docker image was named ""docker.io/<repo>/<imageName>:<tag>"", there was an issue such that Cromwell failed to pull the docker hash. The job was allowed to run but the results were never saved and so one could never call-cache to jobs with that kind of docker syntax. . This change is meant to make ""docker.io"" an accept hostname for the DockerHubFlow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3178
https://github.com/broadinstitute/cromwell/pull/3178:142,Security,hash,hash,142,"Previously, if a docker image was named ""docker.io/<repo>/<imageName>:<tag>"", there was an issue such that Cromwell failed to pull the docker hash. The job was allowed to run but the results were never saved and so one could never call-cache to jobs with that kind of docker syntax. . This change is meant to make ""docker.io"" an accept hostname for the DockerHubFlow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3178
https://github.com/broadinstitute/cromwell/pull/3180:341,Availability,error,errors,341,## What it does do; Gives you the field that does not parse. (This is better than the previous `CanBuildFrom` brick wall.). ## What it does not do; * Tell you the offending value; * Distinguish between CWL's when multiple are submitted in one file. This is because `id` field is optional and made random by SALAD preprocessing. Example of 2 errors combined:; ```; DecodingFailure at .inputs[1].inputBinding.position: Int; DecodingFailure at .stdout: DecodingFailure at .stdout: DecodingFailure at .stdout: String; ```; So not amazing (don't know why `at .stdout` is repeated thrice) but better.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3180
https://github.com/broadinstitute/cromwell/pull/3181:222,Integrability,depend,dependencies,222,"Fixes #3176. The `case co: CallOutput` looks so deliberately different but I can't work out why. Maybe there's a test case that will now fail to show why it was like that originally. I almost made this PR just not look up dependencies for CallOutputs because... they're never used for anything and can cause breakages. Yet another case of ""why on earth is this a `GraphNode`""... 🤷‍♂️",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3181
https://github.com/broadinstitute/cromwell/pull/3181:113,Testability,test,test,113,"Fixes #3176. The `case co: CallOutput` looks so deliberately different but I can't work out why. Maybe there's a test case that will now fail to show why it was like that originally. I almost made this PR just not look up dependencies for CallOutputs because... they're never used for anything and can cause breakages. Yet another case of ""why on earth is this a `GraphNode`""... 🤷‍♂️",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3181
https://github.com/broadinstitute/cromwell/issues/3185:47,Availability,error,errors,47,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185
https://github.com/broadinstitute/cromwell/issues/3185:950,Availability,alive,alive,950,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185
https://github.com/broadinstitute/cromwell/issues/3185:421,Deployability,configurat,configuration,421,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185
https://github.com/broadinstitute/cromwell/issues/3185:330,Modifiability,config,configure,330,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185
https://github.com/broadinstitute/cromwell/issues/3185:421,Modifiability,config,configuration,421,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185
https://github.com/broadinstitute/cromwell/issues/3185:597,Modifiability,config,config,597,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185
https://github.com/broadinstitute/cromwell/issues/3185:604,Modifiability,Config,ConfigBackendLifecycleActorFactory,604,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185
https://github.com/broadinstitute/cromwell/issues/3185:641,Modifiability,config,config,641,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185
https://github.com/broadinstitute/cromwell/issues/3185:651,Performance,concurren,concurrent-job-limit,651,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185
https://github.com/broadinstitute/cromwell/issues/3186:680,Testability,log,logs,680,"While running a CWL workflow, cromwell writes files to disk so that [cwltool](https://github.com/broadinstitute/cromwell/issues/2718) may salad them. While the top-level CWL is read back into memory (& db??) the top-level CWL often refers to other relative files on disk. These relative files are required to remain for the duration of the workflow. After the workflow completes the files should be deleted. Otherwise after a large number of workflows the local disk will fill up. Right now, the salad files are being written to a random path. For the purposes of this ticket the files could be written to a directory based on the stable `workflowId` (a `UUID`). The per-workflow logs do something similar, and are deleted at the end of the workflow in a way that the cwl files should be also.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3186
https://github.com/broadinstitute/cromwell/pull/3187:264,Modifiability,config,config,264,"Decode cwl into a temporary sub-directory so that unzipping doesn't collide under top level `/tmp`; BUG/TODO: cannot delete the cwl sub-directory to clean up files until after the workflow finishes.; Fixed passing cromwell inputs via `-i`.; Passing centaur google config via new environment variables listed in reference.conf instead of on command line.; Limit conformance tests to -Xmx1g per java process by default, and 2g for the cromwells running the conformance tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3187
https://github.com/broadinstitute/cromwell/pull/3187:291,Modifiability,variab,variables,291,"Decode cwl into a temporary sub-directory so that unzipping doesn't collide under top level `/tmp`; BUG/TODO: cannot delete the cwl sub-directory to clean up files until after the workflow finishes.; Fixed passing cromwell inputs via `-i`.; Passing centaur google config via new environment variables listed in reference.conf instead of on command line.; Limit conformance tests to -Xmx1g per java process by default, and 2g for the cromwells running the conformance tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3187
https://github.com/broadinstitute/cromwell/pull/3187:373,Testability,test,tests,373,"Decode cwl into a temporary sub-directory so that unzipping doesn't collide under top level `/tmp`; BUG/TODO: cannot delete the cwl sub-directory to clean up files until after the workflow finishes.; Fixed passing cromwell inputs via `-i`.; Passing centaur google config via new environment variables listed in reference.conf instead of on command line.; Limit conformance tests to -Xmx1g per java process by default, and 2g for the cromwells running the conformance tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3187
https://github.com/broadinstitute/cromwell/pull/3187:467,Testability,test,tests,467,"Decode cwl into a temporary sub-directory so that unzipping doesn't collide under top level `/tmp`; BUG/TODO: cannot delete the cwl sub-directory to clean up files until after the workflow finishes.; Fixed passing cromwell inputs via `-i`.; Passing centaur google config via new environment variables listed in reference.conf instead of on command line.; Limit conformance tests to -Xmx1g per java process by default, and 2g for the cromwells running the conformance tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3187
https://github.com/broadinstitute/cromwell/issues/3190:252,Availability,echo,echo,252,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:367,Availability,failure,failures,367,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:399,Availability,failure,failures,399,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1011,Availability,echo,echo,1011,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1251,Availability,echo,echo,1251,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1407,Deployability,pipeline,pipelines,1407,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1535,Deployability,pipeline,pipelines,1535,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:448,Integrability,message,message,448,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:74,Testability,Test,Test,74,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:90,Testability,test,testString,90,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:126,Testability,test,testString,126,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:147,Testability,test,testTask,147,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:167,Testability,test,testString,167,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:180,Testability,test,testString,180,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:209,Testability,test,testTask,209,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:229,Testability,test,testString,229,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:324,Testability,Test,Test,324,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:329,Testability,test,testString,329,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:343,Testability,test,test,343,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:554,Testability,test,testString,554,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:643,Testability,test,testString,643,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:786,Testability,Test,Test,786,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:802,Testability,test,testString,802,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:834,Testability,test,testStringDefined,834,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:862,Testability,test,testString,862,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:879,Testability,test,testStringDefined,879,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:906,Testability,test,testTask,906,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:926,Testability,test,testString,926,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:939,Testability,test,testString,939,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:968,Testability,test,testTask,968,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:988,Testability,test,testString,988,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1088,Testability,Test,Test,1088,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1104,Testability,test,testString,1104,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1128,Testability,test,testString,1128,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1149,Testability,test,testTask,1149,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1169,Testability,test,testString,1169,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1182,Testability,test,testString,1182,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1208,Testability,test,testTask,1208,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1228,Testability,test,testString,1228,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1586,Testability,test,tests,1586,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1478,Usability,Simpl,SimpleInput,1478,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/issues/3190:1612,Usability,Simpl,SimpleInput,1612,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190
https://github.com/broadinstitute/cromwell/pull/3192:14,Modifiability,config,config,14,The commented config in cromwell.examples.conf is correct so this just looks like a cut & paste issue in the docs.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3192
https://github.com/broadinstitute/cromwell/pull/3194:0,Deployability,Hotfix,Hotfix,0,Hotfix closes #3190,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3194
https://github.com/broadinstitute/cromwell/issues/3195:34,Deployability,configurat,configuration,34,Get languages to come in from the configuration file rather than be statically known.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3195
https://github.com/broadinstitute/cromwell/issues/3195:34,Modifiability,config,configuration,34,Get languages to come in from the configuration file rather than be statically known.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3195
https://github.com/broadinstitute/cromwell/issues/3196:522,Performance,perform,performed,522,"In a conformance test and the wild we've seen expressions such as:. `$(self.nameroot).idx6$(self.nameext)` seen [here](https://github.com/common-workflow-language/common-workflow-language/blob/3b747ab973fd05b663182fd1b166bcd114d7d569/v1.0/v1.0/search.cwl#L40).; `$(runtime.outdir)/output.vcf.gz` seen in MuTect2.; ; The spec has a couple discrepancies: ; * String interpolation (a.k.a. ""Template literals"") is a feature in ECMAScript 6, whereas the spec designates 5.1 as the supported version.; * String interpolation is performed using braces, and not parens. In other words **it appears as though this is not valid ECMAScript**.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3196
https://github.com/broadinstitute/cromwell/issues/3196:17,Testability,test,test,17,"In a conformance test and the wild we've seen expressions such as:. `$(self.nameroot).idx6$(self.nameext)` seen [here](https://github.com/common-workflow-language/common-workflow-language/blob/3b747ab973fd05b663182fd1b166bcd114d7d569/v1.0/v1.0/search.cwl#L40).; `$(runtime.outdir)/output.vcf.gz` seen in MuTect2.; ; The spec has a couple discrepancies: ; * String interpolation (a.k.a. ""Template literals"") is a feature in ECMAScript 6, whereas the spec designates 5.1 as the supported version.; * String interpolation is performed using braces, and not parens. In other words **it appears as though this is not valid ECMAScript**.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3196
https://github.com/broadinstitute/cromwell/pull/3198:133,Testability,log,logic,133,"Changes are smaller than they appear, much of this is restructuring to do command instantiation and then flatMapping the rest of the logic using a half-built `InstantiatedCommand`. Slinging around a half-built `InstantiatedCommand` is IMHO a bit gross but preexisting. This leverages the preprocessing and value mapping of inputs that takes place in command instantiation and captures those versions of the inputs. This enables the stdin redirection in particular to get the actual name of the standard input file as seen within the container.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3198
https://github.com/broadinstitute/cromwell/pull/3200:257,Modifiability,extend,extended,257,"This prepares the next PR that will implement support for `ExpressionTool`; The first 2 commits are pure renames.; The 3rd one breaks `CommandLineTool` apart:; Put `InputParameter` and `OutputParameter` in their own file, and make them trait so they can be extended by specific CWL objects (`WorkflowInputParameter`, `CommandInputParameter`, `ExpressionInputParameter`); Some logic in `CommandLineTool` is abstracted into a `Tool` trait (that will then be extended by `ExpressionTool`); TaskDefinition is also being turned into a trait so we can have `CommandTaskDefinition` and (in the next PR) `ExpressionTaskDefinition`. The general idea is to have `ExpressionTool` behave much like a `CommandLineTool`, including in WOM, except the actual work to be done is an expression to be evaluated by the engine (at least for now ?) instead of a command line run by the backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3200
https://github.com/broadinstitute/cromwell/pull/3200:456,Modifiability,extend,extended,456,"This prepares the next PR that will implement support for `ExpressionTool`; The first 2 commits are pure renames.; The 3rd one breaks `CommandLineTool` apart:; Put `InputParameter` and `OutputParameter` in their own file, and make them trait so they can be extended by specific CWL objects (`WorkflowInputParameter`, `CommandInputParameter`, `ExpressionInputParameter`); Some logic in `CommandLineTool` is abstracted into a `Tool` trait (that will then be extended by `ExpressionTool`); TaskDefinition is also being turned into a trait so we can have `CommandTaskDefinition` and (in the next PR) `ExpressionTaskDefinition`. The general idea is to have `ExpressionTool` behave much like a `CommandLineTool`, including in WOM, except the actual work to be done is an expression to be evaluated by the engine (at least for now ?) instead of a command line run by the backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3200
https://github.com/broadinstitute/cromwell/pull/3200:376,Testability,log,logic,376,"This prepares the next PR that will implement support for `ExpressionTool`; The first 2 commits are pure renames.; The 3rd one breaks `CommandLineTool` apart:; Put `InputParameter` and `OutputParameter` in their own file, and make them trait so they can be extended by specific CWL objects (`WorkflowInputParameter`, `CommandInputParameter`, `ExpressionInputParameter`); Some logic in `CommandLineTool` is abstracted into a `Tool` trait (that will then be extended by `ExpressionTool`); TaskDefinition is also being turned into a trait so we can have `CommandTaskDefinition` and (in the next PR) `ExpressionTaskDefinition`. The general idea is to have `ExpressionTool` behave much like a `CommandLineTool`, including in WOM, except the actual work to be done is an expression to be evaluated by the engine (at least for now ?) instead of a command line run by the backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3200
https://github.com/broadinstitute/cromwell/issues/3201:59,Availability,error,error,59,"Or maybe provide a flag to switch the default?. I get this error when running the hello world example from docs on nixos:. ```; cromwell.core.CromwellFatalException: java.io.IOException: Cannot run program ""/bin/bash"": error=2, No such file or directory; ```; Is it just to change this line?. https://github.com/broadinstitute/cromwell/blob/1b1a56372659b9cb7a168bb1fa2a2296103e1256/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/BackgroundAsyncJobExecutionActor.scala#L22. I could possibly just make my own version building from source then .. It is due to the way nixos is built. A similar case is referenced here :. https://github.com/RcppCore/RcppArmadillo/issues/15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3201
https://github.com/broadinstitute/cromwell/issues/3201:219,Availability,error,error,219,"Or maybe provide a flag to switch the default?. I get this error when running the hello world example from docs on nixos:. ```; cromwell.core.CromwellFatalException: java.io.IOException: Cannot run program ""/bin/bash"": error=2, No such file or directory; ```; Is it just to change this line?. https://github.com/broadinstitute/cromwell/blob/1b1a56372659b9cb7a168bb1fa2a2296103e1256/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/BackgroundAsyncJobExecutionActor.scala#L22. I could possibly just make my own version building from source then .. It is due to the way nixos is built. A similar case is referenced here :. https://github.com/RcppCore/RcppArmadillo/issues/15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3201
https://github.com/broadinstitute/cromwell/pull/3203:743,Testability,test,tests,743,Enables `ExpressionTool`s to be run in the same way `CommandLineTool`s are run (as standalone tools or as a workflow step). They follow exactly the same transformation into WOM and then to Cromwell as `CommandLineTool`s. The only major difference being that instead of a command line to be run it's an expression.; There's a new type of `CallNode`: `ExpressionCallNode` to represent calls that encapsulate an expression instead of a command line.; When the call becomes runnable Cromwell will run the `evaluate` method on the node using its engine `IoFunctions` and the result will be aded to the value store immediately. The node will not make it to any backend.; Happy to whiteboard / tech talk / walkthrough this if needed. Only 2 of the 6 tests enabled on local work on PAPI:; - One should fix itself once the PAPI input redirection PR is merged.; - The 3 others I plan to address in a 3rd PR,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3203
https://github.com/broadinstitute/cromwell/pull/3204:23,Availability,down,down,23,"This requirement boils down to declaring multiple sources of an input and providing them to a `valueFrom` expression to determine the actual value of the input. We should pass another valueFrom-related test (\#63) when ExpressionTool support is merged, which is imminent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3204
https://github.com/broadinstitute/cromwell/pull/3204:202,Testability,test,test,202,"This requirement boils down to declaring multiple sources of an input and providing them to a `valueFrom` expression to determine the actual value of the input. We should pass another valueFrom-related test (\#63) when ExpressionTool support is merged, which is imminent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3204
https://github.com/broadinstitute/cromwell/issues/3209:1154,Availability,error,error,1154,"version: `""cromwell"": ""30-f58c191-SNAP""`. I'm trying to get metadata for some workflows and it seems that for workflows whose subworkflows were running when cromwell was restarted are unable to retrieve their metadata. `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=true` . returns; ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```; even if i try compressed payload or any other kind of trick I could think of. . If I try grabbing the metadata without expanding the subworkflows. `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=false`. returns the metadata just fine almost instantly. If I try to get the metadata of the subworkflow(s) directly it works as well. These workflows also have interesting responses to `includeKeys` parameter. When trying to get only the key `calls` from the workflow that was timing out (in hopes to make it not time out by requesting less data) . `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=true&includeKey=calls`. returns; ```; {; ""status"": ""error"",; ""message"": ""Received unexpected response while waiting for sub workflow metadata.""; }; ```; note `calls` is a key that normally doesn't return anything so normally you would expect to get. `/api/workflows/v1/905e2b4c-908d-4e93-a99c-ad20f6e4c41a/metadata?expandSubWorkflows=true&includeKey=calls`; ```; {}; ```; if you try to filter down the metadata to a key that does exist in the metadata like. `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=true&includeKey=call`. returns; ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```; for the workflow that was restarted mid run (like it did when we were asking for the full metadata); and returns the `calls` metadata successfully for the workflow that always returned the metadata.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3209
https://github.com/broadinstitute/cromwell/issues/3209:1495,Availability,down,down,1495,"version: `""cromwell"": ""30-f58c191-SNAP""`. I'm trying to get metadata for some workflows and it seems that for workflows whose subworkflows were running when cromwell was restarted are unable to retrieve their metadata. `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=true` . returns; ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```; even if i try compressed payload or any other kind of trick I could think of. . If I try grabbing the metadata without expanding the subworkflows. `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=false`. returns the metadata just fine almost instantly. If I try to get the metadata of the subworkflow(s) directly it works as well. These workflows also have interesting responses to `includeKeys` parameter. When trying to get only the key `calls` from the workflow that was timing out (in hopes to make it not time out by requesting less data) . `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=true&includeKey=calls`. returns; ```; {; ""status"": ""error"",; ""message"": ""Received unexpected response while waiting for sub workflow metadata.""; }; ```; note `calls` is a key that normally doesn't return anything so normally you would expect to get. `/api/workflows/v1/905e2b4c-908d-4e93-a99c-ad20f6e4c41a/metadata?expandSubWorkflows=true&includeKey=calls`; ```; {}; ```; if you try to filter down the metadata to a key that does exist in the metadata like. `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=true&includeKey=call`. returns; ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```; for the workflow that was restarted mid run (like it did when we were asking for the full metadata); and returns the `calls` metadata successfully for the workflow that always returned the metadata.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3209
https://github.com/broadinstitute/cromwell/issues/3209:1164,Integrability,message,message,1164,"version: `""cromwell"": ""30-f58c191-SNAP""`. I'm trying to get metadata for some workflows and it seems that for workflows whose subworkflows were running when cromwell was restarted are unable to retrieve their metadata. `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=true` . returns; ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```; even if i try compressed payload or any other kind of trick I could think of. . If I try grabbing the metadata without expanding the subworkflows. `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=false`. returns the metadata just fine almost instantly. If I try to get the metadata of the subworkflow(s) directly it works as well. These workflows also have interesting responses to `includeKeys` parameter. When trying to get only the key `calls` from the workflow that was timing out (in hopes to make it not time out by requesting less data) . `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=true&includeKey=calls`. returns; ```; {; ""status"": ""error"",; ""message"": ""Received unexpected response while waiting for sub workflow metadata.""; }; ```; note `calls` is a key that normally doesn't return anything so normally you would expect to get. `/api/workflows/v1/905e2b4c-908d-4e93-a99c-ad20f6e4c41a/metadata?expandSubWorkflows=true&includeKey=calls`; ```; {}; ```; if you try to filter down the metadata to a key that does exist in the metadata like. `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=true&includeKey=call`. returns; ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```; for the workflow that was restarted mid run (like it did when we were asking for the full metadata); and returns the `calls` metadata successfully for the workflow that always returned the metadata.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3209
https://github.com/broadinstitute/cromwell/issues/3210:0,Deployability,Upgrade,Upgrade,0,Upgrade swagger version & then pull in changes from Cromwell. This can't be a direct copy as the CromIAM API is a superset of Cromwell.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3210
https://github.com/broadinstitute/cromwell/pull/3211:252,Security,access,access,252,"Closes #3195 . - Mostly cut and paste from `MaterializeWorkflowDescriptorActor` to the two new language projects.; - `MaterializeWorkflowDescriptorActor` now gets its language support from one of the plugged in languages and `engine` is not allowed to access language classes directly (except that it still can via `databaseMigration` but pretend you don't know that); - Doesn't do anything to split WDL into two versions. That can come later...!; - If no version is specified, we just pick the first version from the list. I suspect we'll actually want some kind of a ""decider"" function based on the file contents but walking before running...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3211
https://github.com/broadinstitute/cromwell/pull/3212:133,Availability,heartbeat,heartbeat,133,"updated Single sample wdl; added haplotypcaller, jointdiscovery, and data preprocessing wdls. All wdls using gatk4; increased travis heartbeat to 180, ; increased max time for travis to 3 hours",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3212
https://github.com/broadinstitute/cromwell/pull/3212:0,Deployability,update,updated,0,"updated Single sample wdl; added haplotypcaller, jointdiscovery, and data preprocessing wdls. All wdls using gatk4; increased travis heartbeat to 180, ; increased max time for travis to 3 hours",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3212
https://github.com/broadinstitute/cromwell/pull/3213:0,Energy Efficiency,Green,Greening,0,Greening develop for newly added conformance tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3213
https://github.com/broadinstitute/cromwell/pull/3213:45,Testability,test,tests,45,Greening develop for newly added conformance tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3213
https://github.com/broadinstitute/cromwell/issues/3217:212,Availability,down,down,212,CromIAM is already more than an IAM service and that will continue. This has already caused some confusion. Perhaps `caas`? . If moved to monorepo I'm thinking a `workbench` subproject would be ideal as we start down this path. . either way these changes will likely need some interaction w/ devops for hte docker magic,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3217
https://github.com/broadinstitute/cromwell/pull/3219:2,Deployability,Upgrade,Upgrade,2,- Upgrade Pair & Mutect2 Workflows to GATK-4.0.1.0; - Unignore Mutect2; ; * MERGE AFTER GERMLINE CHANGES *,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3219
https://github.com/broadinstitute/cromwell/issues/3222:35,Security,access,access,35,"right now we authz that a user has access to read/manipulate a workflow, but anyone can submit. how do we whitelist this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3222
https://github.com/broadinstitute/cromwell/pull/3223:42,Availability,down,down,42,Possibly the first in a series as I trace down two other classes of failures.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3223
https://github.com/broadinstitute/cromwell/pull/3223:68,Availability,failure,failures,68,Possibly the first in a series as I trace down two other classes of failures.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3223
https://github.com/broadinstitute/cromwell/issues/3224:81,Availability,error,error,81,"This will always mean that Cromwell doesn't believe the workflow exists, yet the error message is that there's no collction for the workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3224
https://github.com/broadinstitute/cromwell/issues/3224:87,Integrability,message,message,87,"This will always mean that Cromwell doesn't believe the workflow exists, yet the error message is that there's no collction for the workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3224
https://github.com/broadinstitute/cromwell/issues/3225:154,Availability,error,errors,154,"We track collection via a label injection. If a workflow dies early on, eg MWDA, the label doesn't appear to be persisted and thus the user can't look up errors for their WF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3225
https://github.com/broadinstitute/cromwell/issues/3225:32,Integrability,inject,injection,32,"We track collection via a label injection. If a workflow dies early on, eg MWDA, the label doesn't appear to be persisted and thus the user can't look up errors for their WF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3225
https://github.com/broadinstitute/cromwell/issues/3225:32,Security,inject,injection,32,"We track collection via a label injection. If a workflow dies early on, eg MWDA, the label doesn't appear to be persisted and thus the user can't look up errors for their WF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3225
https://github.com/broadinstitute/cromwell/issues/3226:181,Availability,failure,failure,181,"If a workflow fails, I would like cromwell to tell me somewhere really obvious, like perhaps the last thing it prints to stdout:. * The first thing that failed.; * Whether it was a failure that occurred while cromwell was running the wdl, or whether it occurred during the execution of a task.; * If a cromwell error, which line of which wdl it happened on.; * If a task, which task and the stderr file. I think a lot of this already exists, but I'm suggesting for it to be super-simple and in one place. . Here's an example of the desired output:. ```; Lots of output. . .; . . .; Your workflow failed while executing task HelloWorld, See cromwell-executions/Hello/c44566iifgg57/call-HelloWorld/execution/stderr for details.; ```. Or. ```; Cromwell failed while executing line 346 of HelloWorld.wdl. The index 6 is out of bounds for the array popular_salutations.; ```. This would be extremely helpful whenever a non-expert runs a workflow, for example our mutect2 wdl. Currently I debug with a mix of home-brewed greps through the cromwell metadata, fishing through cromwell-executions, and running the darn thing myself. It would be really great just to tell them to look at the last line of stdout. A few points:; * The first error is all you need because you can iterate and fix bugs one at a time.; * It's crucial to let the user know very explicitly if this is a cromwell error or a within-task error.; * Stack traces from running the tool could be useful and acceptable, but cromwell stack traces with all that stuff about akka and spark would be overwhelming.; * Rigor isn't important here. For example, the order of execution is not prescribed to the point that the first task to fail will be the same every time, but for this it doesn't matter. Just reporting the first error on the wall clock is sufficient.; * It doesn't matter which workflows, subworkflows etc fails. Just the task.; * Putting this on the last line of stdout has the added virtue of avoiding hassle in a screen session.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226
https://github.com/broadinstitute/cromwell/issues/3226:311,Availability,error,error,311,"If a workflow fails, I would like cromwell to tell me somewhere really obvious, like perhaps the last thing it prints to stdout:. * The first thing that failed.; * Whether it was a failure that occurred while cromwell was running the wdl, or whether it occurred during the execution of a task.; * If a cromwell error, which line of which wdl it happened on.; * If a task, which task and the stderr file. I think a lot of this already exists, but I'm suggesting for it to be super-simple and in one place. . Here's an example of the desired output:. ```; Lots of output. . .; . . .; Your workflow failed while executing task HelloWorld, See cromwell-executions/Hello/c44566iifgg57/call-HelloWorld/execution/stderr for details.; ```. Or. ```; Cromwell failed while executing line 346 of HelloWorld.wdl. The index 6 is out of bounds for the array popular_salutations.; ```. This would be extremely helpful whenever a non-expert runs a workflow, for example our mutect2 wdl. Currently I debug with a mix of home-brewed greps through the cromwell metadata, fishing through cromwell-executions, and running the darn thing myself. It would be really great just to tell them to look at the last line of stdout. A few points:; * The first error is all you need because you can iterate and fix bugs one at a time.; * It's crucial to let the user know very explicitly if this is a cromwell error or a within-task error.; * Stack traces from running the tool could be useful and acceptable, but cromwell stack traces with all that stuff about akka and spark would be overwhelming.; * Rigor isn't important here. For example, the order of execution is not prescribed to the point that the first task to fail will be the same every time, but for this it doesn't matter. Just reporting the first error on the wall clock is sufficient.; * It doesn't matter which workflows, subworkflows etc fails. Just the task.; * Putting this on the last line of stdout has the added virtue of avoiding hassle in a screen session.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226
https://github.com/broadinstitute/cromwell/issues/3226:1230,Availability,error,error,1230,"If a workflow fails, I would like cromwell to tell me somewhere really obvious, like perhaps the last thing it prints to stdout:. * The first thing that failed.; * Whether it was a failure that occurred while cromwell was running the wdl, or whether it occurred during the execution of a task.; * If a cromwell error, which line of which wdl it happened on.; * If a task, which task and the stderr file. I think a lot of this already exists, but I'm suggesting for it to be super-simple and in one place. . Here's an example of the desired output:. ```; Lots of output. . .; . . .; Your workflow failed while executing task HelloWorld, See cromwell-executions/Hello/c44566iifgg57/call-HelloWorld/execution/stderr for details.; ```. Or. ```; Cromwell failed while executing line 346 of HelloWorld.wdl. The index 6 is out of bounds for the array popular_salutations.; ```. This would be extremely helpful whenever a non-expert runs a workflow, for example our mutect2 wdl. Currently I debug with a mix of home-brewed greps through the cromwell metadata, fishing through cromwell-executions, and running the darn thing myself. It would be really great just to tell them to look at the last line of stdout. A few points:; * The first error is all you need because you can iterate and fix bugs one at a time.; * It's crucial to let the user know very explicitly if this is a cromwell error or a within-task error.; * Stack traces from running the tool could be useful and acceptable, but cromwell stack traces with all that stuff about akka and spark would be overwhelming.; * Rigor isn't important here. For example, the order of execution is not prescribed to the point that the first task to fail will be the same every time, but for this it doesn't matter. Just reporting the first error on the wall clock is sufficient.; * It doesn't matter which workflows, subworkflows etc fails. Just the task.; * Putting this on the last line of stdout has the added virtue of avoiding hassle in a screen session.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226
https://github.com/broadinstitute/cromwell/issues/3226:1379,Availability,error,error,1379,"If a workflow fails, I would like cromwell to tell me somewhere really obvious, like perhaps the last thing it prints to stdout:. * The first thing that failed.; * Whether it was a failure that occurred while cromwell was running the wdl, or whether it occurred during the execution of a task.; * If a cromwell error, which line of which wdl it happened on.; * If a task, which task and the stderr file. I think a lot of this already exists, but I'm suggesting for it to be super-simple and in one place. . Here's an example of the desired output:. ```; Lots of output. . .; . . .; Your workflow failed while executing task HelloWorld, See cromwell-executions/Hello/c44566iifgg57/call-HelloWorld/execution/stderr for details.; ```. Or. ```; Cromwell failed while executing line 346 of HelloWorld.wdl. The index 6 is out of bounds for the array popular_salutations.; ```. This would be extremely helpful whenever a non-expert runs a workflow, for example our mutect2 wdl. Currently I debug with a mix of home-brewed greps through the cromwell metadata, fishing through cromwell-executions, and running the darn thing myself. It would be really great just to tell them to look at the last line of stdout. A few points:; * The first error is all you need because you can iterate and fix bugs one at a time.; * It's crucial to let the user know very explicitly if this is a cromwell error or a within-task error.; * Stack traces from running the tool could be useful and acceptable, but cromwell stack traces with all that stuff about akka and spark would be overwhelming.; * Rigor isn't important here. For example, the order of execution is not prescribed to the point that the first task to fail will be the same every time, but for this it doesn't matter. Just reporting the first error on the wall clock is sufficient.; * It doesn't matter which workflows, subworkflows etc fails. Just the task.; * Putting this on the last line of stdout has the added virtue of avoiding hassle in a screen session.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226
https://github.com/broadinstitute/cromwell/issues/3226:1402,Availability,error,error,1402,"If a workflow fails, I would like cromwell to tell me somewhere really obvious, like perhaps the last thing it prints to stdout:. * The first thing that failed.; * Whether it was a failure that occurred while cromwell was running the wdl, or whether it occurred during the execution of a task.; * If a cromwell error, which line of which wdl it happened on.; * If a task, which task and the stderr file. I think a lot of this already exists, but I'm suggesting for it to be super-simple and in one place. . Here's an example of the desired output:. ```; Lots of output. . .; . . .; Your workflow failed while executing task HelloWorld, See cromwell-executions/Hello/c44566iifgg57/call-HelloWorld/execution/stderr for details.; ```. Or. ```; Cromwell failed while executing line 346 of HelloWorld.wdl. The index 6 is out of bounds for the array popular_salutations.; ```. This would be extremely helpful whenever a non-expert runs a workflow, for example our mutect2 wdl. Currently I debug with a mix of home-brewed greps through the cromwell metadata, fishing through cromwell-executions, and running the darn thing myself. It would be really great just to tell them to look at the last line of stdout. A few points:; * The first error is all you need because you can iterate and fix bugs one at a time.; * It's crucial to let the user know very explicitly if this is a cromwell error or a within-task error.; * Stack traces from running the tool could be useful and acceptable, but cromwell stack traces with all that stuff about akka and spark would be overwhelming.; * Rigor isn't important here. For example, the order of execution is not prescribed to the point that the first task to fail will be the same every time, but for this it doesn't matter. Just reporting the first error on the wall clock is sufficient.; * It doesn't matter which workflows, subworkflows etc fails. Just the task.; * Putting this on the last line of stdout has the added virtue of avoiding hassle in a screen session.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226
https://github.com/broadinstitute/cromwell/issues/3226:1781,Availability,error,error,1781,"If a workflow fails, I would like cromwell to tell me somewhere really obvious, like perhaps the last thing it prints to stdout:. * The first thing that failed.; * Whether it was a failure that occurred while cromwell was running the wdl, or whether it occurred during the execution of a task.; * If a cromwell error, which line of which wdl it happened on.; * If a task, which task and the stderr file. I think a lot of this already exists, but I'm suggesting for it to be super-simple and in one place. . Here's an example of the desired output:. ```; Lots of output. . .; . . .; Your workflow failed while executing task HelloWorld, See cromwell-executions/Hello/c44566iifgg57/call-HelloWorld/execution/stderr for details.; ```. Or. ```; Cromwell failed while executing line 346 of HelloWorld.wdl. The index 6 is out of bounds for the array popular_salutations.; ```. This would be extremely helpful whenever a non-expert runs a workflow, for example our mutect2 wdl. Currently I debug with a mix of home-brewed greps through the cromwell metadata, fishing through cromwell-executions, and running the darn thing myself. It would be really great just to tell them to look at the last line of stdout. A few points:; * The first error is all you need because you can iterate and fix bugs one at a time.; * It's crucial to let the user know very explicitly if this is a cromwell error or a within-task error.; * Stack traces from running the tool could be useful and acceptable, but cromwell stack traces with all that stuff about akka and spark would be overwhelming.; * Rigor isn't important here. For example, the order of execution is not prescribed to the point that the first task to fail will be the same every time, but for this it doesn't matter. Just reporting the first error on the wall clock is sufficient.; * It doesn't matter which workflows, subworkflows etc fails. Just the task.; * Putting this on the last line of stdout has the added virtue of avoiding hassle in a screen session.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226
https://github.com/broadinstitute/cromwell/issues/3226:1964,Safety,avoid,avoiding,1964,"If a workflow fails, I would like cromwell to tell me somewhere really obvious, like perhaps the last thing it prints to stdout:. * The first thing that failed.; * Whether it was a failure that occurred while cromwell was running the wdl, or whether it occurred during the execution of a task.; * If a cromwell error, which line of which wdl it happened on.; * If a task, which task and the stderr file. I think a lot of this already exists, but I'm suggesting for it to be super-simple and in one place. . Here's an example of the desired output:. ```; Lots of output. . .; . . .; Your workflow failed while executing task HelloWorld, See cromwell-executions/Hello/c44566iifgg57/call-HelloWorld/execution/stderr for details.; ```. Or. ```; Cromwell failed while executing line 346 of HelloWorld.wdl. The index 6 is out of bounds for the array popular_salutations.; ```. This would be extremely helpful whenever a non-expert runs a workflow, for example our mutect2 wdl. Currently I debug with a mix of home-brewed greps through the cromwell metadata, fishing through cromwell-executions, and running the darn thing myself. It would be really great just to tell them to look at the last line of stdout. A few points:; * The first error is all you need because you can iterate and fix bugs one at a time.; * It's crucial to let the user know very explicitly if this is a cromwell error or a within-task error.; * Stack traces from running the tool could be useful and acceptable, but cromwell stack traces with all that stuff about akka and spark would be overwhelming.; * Rigor isn't important here. For example, the order of execution is not prescribed to the point that the first task to fail will be the same every time, but for this it doesn't matter. Just reporting the first error on the wall clock is sufficient.; * It doesn't matter which workflows, subworkflows etc fails. Just the task.; * Putting this on the last line of stdout has the added virtue of avoiding hassle in a screen session.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226
https://github.com/broadinstitute/cromwell/issues/3226:480,Usability,simpl,simple,480,"If a workflow fails, I would like cromwell to tell me somewhere really obvious, like perhaps the last thing it prints to stdout:. * The first thing that failed.; * Whether it was a failure that occurred while cromwell was running the wdl, or whether it occurred during the execution of a task.; * If a cromwell error, which line of which wdl it happened on.; * If a task, which task and the stderr file. I think a lot of this already exists, but I'm suggesting for it to be super-simple and in one place. . Here's an example of the desired output:. ```; Lots of output. . .; . . .; Your workflow failed while executing task HelloWorld, See cromwell-executions/Hello/c44566iifgg57/call-HelloWorld/execution/stderr for details.; ```. Or. ```; Cromwell failed while executing line 346 of HelloWorld.wdl. The index 6 is out of bounds for the array popular_salutations.; ```. This would be extremely helpful whenever a non-expert runs a workflow, for example our mutect2 wdl. Currently I debug with a mix of home-brewed greps through the cromwell metadata, fishing through cromwell-executions, and running the darn thing myself. It would be really great just to tell them to look at the last line of stdout. A few points:; * The first error is all you need because you can iterate and fix bugs one at a time.; * It's crucial to let the user know very explicitly if this is a cromwell error or a within-task error.; * Stack traces from running the tool could be useful and acceptable, but cromwell stack traces with all that stuff about akka and spark would be overwhelming.; * Rigor isn't important here. For example, the order of execution is not prescribed to the point that the first task to fail will be the same every time, but for this it doesn't matter. Just reporting the first error on the wall clock is sufficient.; * It doesn't matter which workflows, subworkflows etc fails. Just the task.; * Putting this on the last line of stdout has the added virtue of avoiding hassle in a screen session.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226
https://github.com/broadinstitute/cromwell/issues/3227:90,Integrability,depend,depends,90,"I believe this is possible, at first examination it looks like code-wise label publishing depends on a successful namespace generation, but it also looks like the value it's depending on is not used. . Assuming that statement holds, publish labels to metadata as early in the process of starting a workflow as possible",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3227
https://github.com/broadinstitute/cromwell/issues/3227:174,Integrability,depend,depending,174,"I believe this is possible, at first examination it looks like code-wise label publishing depends on a successful namespace generation, but it also looks like the value it's depending on is not used. . Assuming that statement holds, publish labels to metadata as early in the process of starting a workflow as possible",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3227
https://github.com/broadinstitute/cromwell/pull/3229:334,Availability,failure,failures,334,"Closes #3214. Top level view:; - Totally split `draft2` from a fresh copy of WDL code in `draft3`.; - They now inhabit separate and unconnected sbt projects.; - Any existing code references to `wdl.` are now references to `wdl.draft2.`; - Please review my `build.sbt` file, the rest is just moving, shuffling and fixing up intellij's failures to refactor packages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3229
https://github.com/broadinstitute/cromwell/pull/3229:346,Modifiability,refactor,refactor,346,"Closes #3214. Top level view:; - Totally split `draft2` from a fresh copy of WDL code in `draft3`.; - They now inhabit separate and unconnected sbt projects.; - Any existing code references to `wdl.` are now references to `wdl.draft2.`; - Please review my `build.sbt` file, the rest is just moving, shuffling and fixing up intellij's failures to refactor packages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3229
https://github.com/broadinstitute/cromwell/issues/3230:485,Security,validat,validates,485,"Given the following call to a sub-workflow in a larger workflow:; ```; call CNVOncotator.CNVOncotatorWorkflow as CNVOncotatorWorkflow {; input:; called_file = CallCopyRatioSegmentsTumor.called_copy_ratio_segments,; additional_args = additional_args,; oncotator_docker = oncotator_docker,; mem_gb_for_oncotator = mem_gb_for_oncotator,; preemptible_attempts = preemptible_attempts; }; }; ```. Even though `additional_args` and `mem_gb_for_oncotator` are not defined, this workflow still validates.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3230
https://github.com/broadinstitute/cromwell/issues/3233:43,Integrability,inject,injected,43,- Remove the bit where supplied labels are injected into Google. ; - Remove the Google-related restrictions on label structure. These should just be allowed to be anything*; - Provide a new workflow option `google-labels` which **do** get injected into Google and enforces Google's restrictions; - Communicate that this changed to FC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3233
https://github.com/broadinstitute/cromwell/issues/3233:239,Integrability,inject,injected,239,- Remove the bit where supplied labels are injected into Google. ; - Remove the Google-related restrictions on label structure. These should just be allowed to be anything*; - Provide a new workflow option `google-labels` which **do** get injected into Google and enforces Google's restrictions; - Communicate that this changed to FC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3233
https://github.com/broadinstitute/cromwell/issues/3233:43,Security,inject,injected,43,- Remove the bit where supplied labels are injected into Google. ; - Remove the Google-related restrictions on label structure. These should just be allowed to be anything*; - Provide a new workflow option `google-labels` which **do** get injected into Google and enforces Google's restrictions; - Communicate that this changed to FC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3233
https://github.com/broadinstitute/cromwell/issues/3233:239,Security,inject,injected,239,- Remove the bit where supplied labels are injected into Google. ; - Remove the Google-related restrictions on label structure. These should just be allowed to be anything*; - Provide a new workflow option `google-labels` which **do** get injected into Google and enforces Google's restrictions; - Communicate that this changed to FC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3233
https://github.com/broadinstitute/cromwell/pull/3234:215,Integrability,inject,inject,215,"Takes the proof of concept as far as:. - A core WDL package which is the base for draft 2 and draft 3.; - Answers ""can I separate the common classes out of WdlParser?"" I believe I can.; - Sees what it looks like to inject a new piece of functionality into the middle of WdlStandardLibrary to replace the `size` function (`size` was awkwardly implemented in full in `IoFunctions` which needed to be changed, so it was in some ways a bad example but in others it was a good one since it highlights the kinds of hurdles this draft2/draft3 splitting is going to involve in general).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3234
https://github.com/broadinstitute/cromwell/pull/3234:215,Security,inject,inject,215,"Takes the proof of concept as far as:. - A core WDL package which is the base for draft 2 and draft 3.; - Answers ""can I separate the common classes out of WdlParser?"" I believe I can.; - Sees what it looks like to inject a new piece of functionality into the middle of WdlStandardLibrary to replace the `size` function (`size` was awkwardly implemented in full in `IoFunctions` which needed to be changed, so it was in some ways a bad example but in others it was a good one since it highlights the kinds of hurdles this draft2/draft3 splitting is going to involve in general).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3234
https://github.com/broadinstitute/cromwell/pull/3235:20,Testability,assert,assertions,20,"Correct the lack of assertions in an existing centaur test. If you've been looking for a nobrainer PR to rubber stamp, I have good news.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3235
https://github.com/broadinstitute/cromwell/pull/3235:54,Testability,test,test,54,"Correct the lack of assertions in an existing centaur test. If you've been looking for a nobrainer PR to rubber stamp, I have good news.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3235
https://github.com/broadinstitute/cromwell/pull/3236:69,Testability,test,tests,69,"Don't let the number of lines scare you, they're mostly json used in tests to assert that the pre-processed CWL looks right.; The first commit is only the pre-processing code + its tests.; The second actually uses it.; The 3rd and 4th are about abstracting code in the CromwellEndpoint and the CommandLineParser to accommodate for a ""submit"" command that can submit to a cromwell server + a few adjustments.; The submit thing could be isolated in its own sub-project I guess since it doesn't need Cromwell engine but just the api client.; I might do that in a part 1.1. But there will be a part 2 needed because the input files (as the ""files referenced in the input json"") also need to be massaged somehow, because they're currently assumed to be relative to where cromwell is being run although they're actually relative to the input json.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3236
https://github.com/broadinstitute/cromwell/pull/3236:78,Testability,assert,assert,78,"Don't let the number of lines scare you, they're mostly json used in tests to assert that the pre-processed CWL looks right.; The first commit is only the pre-processing code + its tests.; The second actually uses it.; The 3rd and 4th are about abstracting code in the CromwellEndpoint and the CommandLineParser to accommodate for a ""submit"" command that can submit to a cromwell server + a few adjustments.; The submit thing could be isolated in its own sub-project I guess since it doesn't need Cromwell engine but just the api client.; I might do that in a part 1.1. But there will be a part 2 needed because the input files (as the ""files referenced in the input json"") also need to be massaged somehow, because they're currently assumed to be relative to where cromwell is being run although they're actually relative to the input json.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3236
https://github.com/broadinstitute/cromwell/pull/3236:181,Testability,test,tests,181,"Don't let the number of lines scare you, they're mostly json used in tests to assert that the pre-processed CWL looks right.; The first commit is only the pre-processing code + its tests.; The second actually uses it.; The 3rd and 4th are about abstracting code in the CromwellEndpoint and the CommandLineParser to accommodate for a ""submit"" command that can submit to a cromwell server + a few adjustments.; The submit thing could be isolated in its own sub-project I guess since it doesn't need Cromwell engine but just the api client.; I might do that in a part 1.1. But there will be a part 2 needed because the input files (as the ""files referenced in the input json"") also need to be massaged somehow, because they're currently assumed to be relative to where cromwell is being run although they're actually relative to the input json.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3236
https://github.com/broadinstitute/cromwell/pull/3239:64,Integrability,interface,interface,64,"A bit of a shuffle of classes to tidy up the pluggable language interface. In particular see `build.sbt` where this PR inverts the previous ""`languageFactory` depends on `engine`"" dependency. Good for a hundred reasons but also opens the door to statically imported `ToExecutable` methods from language factories if we wanted to.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3239
https://github.com/broadinstitute/cromwell/pull/3239:159,Integrability,depend,depends,159,"A bit of a shuffle of classes to tidy up the pluggable language interface. In particular see `build.sbt` where this PR inverts the previous ""`languageFactory` depends on `engine`"" dependency. Good for a hundred reasons but also opens the door to statically imported `ToExecutable` methods from language factories if we wanted to.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3239
https://github.com/broadinstitute/cromwell/pull/3239:180,Integrability,depend,dependency,180,"A bit of a shuffle of classes to tidy up the pluggable language interface. In particular see `build.sbt` where this PR inverts the previous ""`languageFactory` depends on `engine`"" dependency. Good for a hundred reasons but also opens the door to statically imported `ToExecutable` methods from language factories if we wanted to.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3239
https://github.com/broadinstitute/cromwell/issues/3241:647,Availability,error,error,647,"So given the following directory structure : ; ```; base; ├── imported_workflow; │   ├── wdl_tasks; │   │   ├── taska.wdl; │   │   └── taskb.wdl; │   └── workflow_b.wdl; ├── task_wdls; │   ├── task1.wdl; │   └── task2.wdl; └── workflow.wdl; ```. In `workflow.wdl` I have; ```WDL; import ""task_wdls/task1.wdl"" as task1; import ""task_wdls/task2.wdl"" as task2; import ""imported_workflow/workflow_b.wdl"" as workflow_b; ```. In `imported_workflow/worflow_b.wdl` I have; ```WDL; import ""wdl_tasks/taska.wdl"" as taska; import ""wdl_tasks/taskb.wdl"" as taskb; ```. If I run cromwell from the directory `base` and run `workflow.wdl`. I will stumble upon an error. It will try to import `wdl_tasks/taska.wdl` from the base directory instead of from the `imported_workflow` directory where `workflow_b.wdl` is located. ```; Failed to import workflow wdl_tasks/taska.wdl.:; File not found /home/ruben/test/base/wdl_tasks/taska.wdl; ```. This has also been discussed at: https://gatkforums.broadinstitute.org/wdl/discussion/11330/how-to-import-workflows-that-also-have-imports-themselves. Evaluating import statements for the directory where cromwell is executed is ""bad"" because; * Where you run cromwell matters. Even if using the same workflows. In terms of reproducibility this is not very nice.; * You cannot see from the WDL file what the base importing directory was meant to be. This follows from where cromwell is run, and this is not specified in the file itself.; * Because of this context dependence, sub workflows are not modular building blocks which can be moved around freely between workflows. This severely handicaps the usefulness of WDL. Import statements should be evaluated from the files which they are in because:; * It makes sub-workflows modular building blocks that can be moved around; * It makes it easier to review and understand the import statements.; * It follows the principle of least surprise.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241
https://github.com/broadinstitute/cromwell/issues/3241:1487,Integrability,depend,dependence,1487,"So given the following directory structure : ; ```; base; ├── imported_workflow; │   ├── wdl_tasks; │   │   ├── taska.wdl; │   │   └── taskb.wdl; │   └── workflow_b.wdl; ├── task_wdls; │   ├── task1.wdl; │   └── task2.wdl; └── workflow.wdl; ```. In `workflow.wdl` I have; ```WDL; import ""task_wdls/task1.wdl"" as task1; import ""task_wdls/task2.wdl"" as task2; import ""imported_workflow/workflow_b.wdl"" as workflow_b; ```. In `imported_workflow/worflow_b.wdl` I have; ```WDL; import ""wdl_tasks/taska.wdl"" as taska; import ""wdl_tasks/taskb.wdl"" as taskb; ```. If I run cromwell from the directory `base` and run `workflow.wdl`. I will stumble upon an error. It will try to import `wdl_tasks/taska.wdl` from the base directory instead of from the `imported_workflow` directory where `workflow_b.wdl` is located. ```; Failed to import workflow wdl_tasks/taska.wdl.:; File not found /home/ruben/test/base/wdl_tasks/taska.wdl; ```. This has also been discussed at: https://gatkforums.broadinstitute.org/wdl/discussion/11330/how-to-import-workflows-that-also-have-imports-themselves. Evaluating import statements for the directory where cromwell is executed is ""bad"" because; * Where you run cromwell matters. Even if using the same workflows. In terms of reproducibility this is not very nice.; * You cannot see from the WDL file what the base importing directory was meant to be. This follows from where cromwell is run, and this is not specified in the file itself.; * Because of this context dependence, sub workflows are not modular building blocks which can be moved around freely between workflows. This severely handicaps the usefulness of WDL. Import statements should be evaluated from the files which they are in because:; * It makes sub-workflows modular building blocks that can be moved around; * It makes it easier to review and understand the import statements.; * It follows the principle of least surprise.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241
https://github.com/broadinstitute/cromwell/issues/3241:888,Testability,test,test,888,"So given the following directory structure : ; ```; base; ├── imported_workflow; │   ├── wdl_tasks; │   │   ├── taska.wdl; │   │   └── taskb.wdl; │   └── workflow_b.wdl; ├── task_wdls; │   ├── task1.wdl; │   └── task2.wdl; └── workflow.wdl; ```. In `workflow.wdl` I have; ```WDL; import ""task_wdls/task1.wdl"" as task1; import ""task_wdls/task2.wdl"" as task2; import ""imported_workflow/workflow_b.wdl"" as workflow_b; ```. In `imported_workflow/worflow_b.wdl` I have; ```WDL; import ""wdl_tasks/taska.wdl"" as taska; import ""wdl_tasks/taskb.wdl"" as taskb; ```. If I run cromwell from the directory `base` and run `workflow.wdl`. I will stumble upon an error. It will try to import `wdl_tasks/taska.wdl` from the base directory instead of from the `imported_workflow` directory where `workflow_b.wdl` is located. ```; Failed to import workflow wdl_tasks/taska.wdl.:; File not found /home/ruben/test/base/wdl_tasks/taska.wdl; ```. This has also been discussed at: https://gatkforums.broadinstitute.org/wdl/discussion/11330/how-to-import-workflows-that-also-have-imports-themselves. Evaluating import statements for the directory where cromwell is executed is ""bad"" because; * Where you run cromwell matters. Even if using the same workflows. In terms of reproducibility this is not very nice.; * You cannot see from the WDL file what the base importing directory was meant to be. This follows from where cromwell is run, and this is not specified in the file itself.; * Because of this context dependence, sub workflows are not modular building blocks which can be moved around freely between workflows. This severely handicaps the usefulness of WDL. Import statements should be evaluated from the files which they are in because:; * It makes sub-workflows modular building blocks that can be moved around; * It makes it easier to review and understand the import statements.; * It follows the principle of least surprise.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241
https://github.com/broadinstitute/cromwell/issues/3242:373,Availability,robust,robust,373,"Concoct an SQL table schema which can handle efficiently looking up any information currently provided by the Cromwell endpoints which read from the metadata service. This might get hairy with query and/or metadata, but see what can be done. This schema will be realized in a CloudSQL database. NB: Google PubSub does not guarantee ordering of events. We should already be robust to this via the CRDT structures and such, but take that into account w/ this table structure. I could imagine there being situations where e.g. “timestamp from the payload of last event I processed which led to this state” might also be good to track or something like that. If you find yourself looking for ideas on how to handle ordering issues, [this Google doc](https://cloud.google.com/pubsub/docs/ordering) provides some examples for all of the situations we have",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3242
https://github.com/broadinstitute/cromwell/issues/3242:45,Energy Efficiency,efficient,efficiently,45,"Concoct an SQL table schema which can handle efficiently looking up any information currently provided by the Cromwell endpoints which read from the metadata service. This might get hairy with query and/or metadata, but see what can be done. This schema will be realized in a CloudSQL database. NB: Google PubSub does not guarantee ordering of events. We should already be robust to this via the CRDT structures and such, but take that into account w/ this table structure. I could imagine there being situations where e.g. “timestamp from the payload of last event I processed which led to this state” might also be good to track or something like that. If you find yourself looking for ideas on how to handle ordering issues, [this Google doc](https://cloud.google.com/pubsub/docs/ordering) provides some examples for all of the situations we have",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3242
https://github.com/broadinstitute/cromwell/issues/3243:485,Availability,down,downstream,485,"Starting to spider out the proofing of concept for a google-y metadata system into something which is actually storing events as well as providing a not horribly inefficient read access for the current set of metadata-y endpoints. A high level description: Stream metadata events out of Cromwell via Google PubSub, and store them in two locations. The first is a permanent event store which will be storing these events in an immutable fashion, which will allow us to be flexible with downstream presentation w/o information loss. The second will be a set of SQL tables which have been designed to provide efficient results for all of the standard Cromwell metadata endpoints such as metadata, status, outputs, etc. For Broad folks, more information is available [here](https://docs.google.com/document/d/1F5WsEAKvYx6njdF-yJZ4LHvT39KcErCdBpCOySq-NoQ/edit)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3243
https://github.com/broadinstitute/cromwell/issues/3243:753,Availability,avail,available,753,"Starting to spider out the proofing of concept for a google-y metadata system into something which is actually storing events as well as providing a not horribly inefficient read access for the current set of metadata-y endpoints. A high level description: Stream metadata events out of Cromwell via Google PubSub, and store them in two locations. The first is a permanent event store which will be storing these events in an immutable fashion, which will allow us to be flexible with downstream presentation w/o information loss. The second will be a set of SQL tables which have been designed to provide efficient results for all of the standard Cromwell metadata endpoints such as metadata, status, outputs, etc. For Broad folks, more information is available [here](https://docs.google.com/document/d/1F5WsEAKvYx6njdF-yJZ4LHvT39KcErCdBpCOySq-NoQ/edit)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3243
https://github.com/broadinstitute/cromwell/issues/3243:606,Energy Efficiency,efficient,efficient,606,"Starting to spider out the proofing of concept for a google-y metadata system into something which is actually storing events as well as providing a not horribly inefficient read access for the current set of metadata-y endpoints. A high level description: Stream metadata events out of Cromwell via Google PubSub, and store them in two locations. The first is a permanent event store which will be storing these events in an immutable fashion, which will allow us to be flexible with downstream presentation w/o information loss. The second will be a set of SQL tables which have been designed to provide efficient results for all of the standard Cromwell metadata endpoints such as metadata, status, outputs, etc. For Broad folks, more information is available [here](https://docs.google.com/document/d/1F5WsEAKvYx6njdF-yJZ4LHvT39KcErCdBpCOySq-NoQ/edit)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3243
https://github.com/broadinstitute/cromwell/issues/3243:471,Modifiability,flexible,flexible,471,"Starting to spider out the proofing of concept for a google-y metadata system into something which is actually storing events as well as providing a not horribly inefficient read access for the current set of metadata-y endpoints. A high level description: Stream metadata events out of Cromwell via Google PubSub, and store them in two locations. The first is a permanent event store which will be storing these events in an immutable fashion, which will allow us to be flexible with downstream presentation w/o information loss. The second will be a set of SQL tables which have been designed to provide efficient results for all of the standard Cromwell metadata endpoints such as metadata, status, outputs, etc. For Broad folks, more information is available [here](https://docs.google.com/document/d/1F5WsEAKvYx6njdF-yJZ4LHvT39KcErCdBpCOySq-NoQ/edit)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3243
https://github.com/broadinstitute/cromwell/issues/3243:179,Security,access,access,179,"Starting to spider out the proofing of concept for a google-y metadata system into something which is actually storing events as well as providing a not horribly inefficient read access for the current set of metadata-y endpoints. A high level description: Stream metadata events out of Cromwell via Google PubSub, and store them in two locations. The first is a permanent event store which will be storing these events in an immutable fashion, which will allow us to be flexible with downstream presentation w/o information loss. The second will be a set of SQL tables which have been designed to provide efficient results for all of the standard Cromwell metadata endpoints such as metadata, status, outputs, etc. For Broad folks, more information is available [here](https://docs.google.com/document/d/1F5WsEAKvYx6njdF-yJZ4LHvT39KcErCdBpCOySq-NoQ/edit)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3243
https://github.com/broadinstitute/cromwell/issues/3244:425,Availability,error,errors,425,"Create a standalone application which will:. - Create/update schema as per #3242 ; - Subscribe to a PubSub topic; - Consume events in the format emitted by the metadata implementation at #3098 ; - For each metadatum, performs any necessary upserts into CloudSQL. This system should pull events off of the message queue exactly as fast as it is writing them to the database, i.e. it shouldn't be backing up and causing Slick* errors but it should also not be dawdling either. * I'm not at all opposed to using something other than Slick. If you want to go this route, let's talk.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3244
https://github.com/broadinstitute/cromwell/issues/3244:54,Deployability,update,update,54,"Create a standalone application which will:. - Create/update schema as per #3242 ; - Subscribe to a PubSub topic; - Consume events in the format emitted by the metadata implementation at #3098 ; - For each metadatum, performs any necessary upserts into CloudSQL. This system should pull events off of the message queue exactly as fast as it is writing them to the database, i.e. it shouldn't be backing up and causing Slick* errors but it should also not be dawdling either. * I'm not at all opposed to using something other than Slick. If you want to go this route, let's talk.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3244
https://github.com/broadinstitute/cromwell/issues/3244:305,Integrability,message,message,305,"Create a standalone application which will:. - Create/update schema as per #3242 ; - Subscribe to a PubSub topic; - Consume events in the format emitted by the metadata implementation at #3098 ; - For each metadatum, performs any necessary upserts into CloudSQL. This system should pull events off of the message queue exactly as fast as it is writing them to the database, i.e. it shouldn't be backing up and causing Slick* errors but it should also not be dawdling either. * I'm not at all opposed to using something other than Slick. If you want to go this route, let's talk.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3244
https://github.com/broadinstitute/cromwell/issues/3244:560,Integrability,rout,route,560,"Create a standalone application which will:. - Create/update schema as per #3242 ; - Subscribe to a PubSub topic; - Consume events in the format emitted by the metadata implementation at #3098 ; - For each metadatum, performs any necessary upserts into CloudSQL. This system should pull events off of the message queue exactly as fast as it is writing them to the database, i.e. it shouldn't be backing up and causing Slick* errors but it should also not be dawdling either. * I'm not at all opposed to using something other than Slick. If you want to go this route, let's talk.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3244
https://github.com/broadinstitute/cromwell/issues/3244:217,Performance,perform,performs,217,"Create a standalone application which will:. - Create/update schema as per #3242 ; - Subscribe to a PubSub topic; - Consume events in the format emitted by the metadata implementation at #3098 ; - For each metadatum, performs any necessary upserts into CloudSQL. This system should pull events off of the message queue exactly as fast as it is writing them to the database, i.e. it shouldn't be backing up and causing Slick* errors but it should also not be dawdling either. * I'm not at all opposed to using something other than Slick. If you want to go this route, let's talk.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3244
https://github.com/broadinstitute/cromwell/issues/3244:313,Performance,queue,queue,313,"Create a standalone application which will:. - Create/update schema as per #3242 ; - Subscribe to a PubSub topic; - Consume events in the format emitted by the metadata implementation at #3098 ; - For each metadatum, performs any necessary upserts into CloudSQL. This system should pull events off of the message queue exactly as fast as it is writing them to the database, i.e. it shouldn't be backing up and causing Slick* errors but it should also not be dawdling either. * I'm not at all opposed to using something other than Slick. If you want to go this route, let's talk.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3244
https://github.com/broadinstitute/cromwell/issues/3246:243,Energy Efficiency,efficient,efficient,243,"For now at least, the implementation isn’t that big of a deal as long as it’s pushbutton GCP stuff. The goal is to store the JSON events coming out of PubSub as-is in a fashion that we can access them in the future as necessary. We don’t need efficient querying of these events but we do need the ability to easily get all the events associated with a workflow. Cloud Datastore seems like it’d work (for instance, kind being workflow ID and entity being the metadatum) but I don’t really know. We could also just do something like store these in a google bucket. Let’s not get too complicated here, the idea is to find something simple and verify that it’s feasible over time. Once this is squared away, enhance the application from #3244 to also dump the events **as-is** into this event store w/o modification.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3246
https://github.com/broadinstitute/cromwell/issues/3246:704,Modifiability,enhance,enhance,704,"For now at least, the implementation isn’t that big of a deal as long as it’s pushbutton GCP stuff. The goal is to store the JSON events coming out of PubSub as-is in a fashion that we can access them in the future as necessary. We don’t need efficient querying of these events but we do need the ability to easily get all the events associated with a workflow. Cloud Datastore seems like it’d work (for instance, kind being workflow ID and entity being the metadatum) but I don’t really know. We could also just do something like store these in a google bucket. Let’s not get too complicated here, the idea is to find something simple and verify that it’s feasible over time. Once this is squared away, enhance the application from #3244 to also dump the events **as-is** into this event store w/o modification.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3246
https://github.com/broadinstitute/cromwell/issues/3246:189,Security,access,access,189,"For now at least, the implementation isn’t that big of a deal as long as it’s pushbutton GCP stuff. The goal is to store the JSON events coming out of PubSub as-is in a fashion that we can access them in the future as necessary. We don’t need efficient querying of these events but we do need the ability to easily get all the events associated with a workflow. Cloud Datastore seems like it’d work (for instance, kind being workflow ID and entity being the metadatum) but I don’t really know. We could also just do something like store these in a google bucket. Let’s not get too complicated here, the idea is to find something simple and verify that it’s feasible over time. Once this is squared away, enhance the application from #3244 to also dump the events **as-is** into this event store w/o modification.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3246
https://github.com/broadinstitute/cromwell/issues/3246:629,Usability,simpl,simple,629,"For now at least, the implementation isn’t that big of a deal as long as it’s pushbutton GCP stuff. The goal is to store the JSON events coming out of PubSub as-is in a fashion that we can access them in the future as necessary. We don’t need efficient querying of these events but we do need the ability to easily get all the events associated with a workflow. Cloud Datastore seems like it’d work (for instance, kind being workflow ID and entity being the metadatum) but I don’t really know. We could also just do something like store these in a google bucket. Let’s not get too complicated here, the idea is to find something simple and verify that it’s feasible over time. Once this is squared away, enhance the application from #3244 to also dump the events **as-is** into this event store w/o modification.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3246
https://github.com/broadinstitute/cromwell/pull/3250:80,Testability,test,tests,80,This is to keep the rug from moving out from under us as far as CWL conformance tests is concerned.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3250
https://github.com/broadinstitute/cromwell/issues/3251:266,Deployability,update,update,266,"http://cromwell.readthedocs.io/en/develop/backends/Google/. The section on ""Application Default Credentials"" doesn't mention that google deprecated setting app default creds by default. Search google, get new commands that users must use to set the credentials, and update docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3251
https://github.com/broadinstitute/cromwell/pull/3252:898,Integrability,wrap,wrapped,898,"Step Inputs have the task of tying wf inputs to their `run` command. This is done via the `source` field of a `WorkflowStepInput`, and there may be multiple, hence the name of this requirement. This logic falls into two buckets:. # Type. Determing the type of the element being passed to the input is a function of:; * what type the `run` is expecting; * whether the variable appears in a scatter ; * whether there is an expression in the `valueFrom` field; * How to ""merge"" when multiple sources are declared.; * the ""default"" (On which I am punting here). # Expression . The way the expression presents the inputs to the run command is determined by that `LinkMergeMethod` field i.e. how the run step expects the value to appear. . # Assumptions. The Spec says to use `merge_nested` as default, but also says:. > If ""merge_nested"" is specified with a single link, the value from the link must be wrapped in a single-item list. Thus it seemed that all values should be wrapped in a list. this is wrong and I'm going to ask on gitter what the deal is. # What I've punted on. the `default` field.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3252
https://github.com/broadinstitute/cromwell/pull/3252:970,Integrability,wrap,wrapped,970,"Step Inputs have the task of tying wf inputs to their `run` command. This is done via the `source` field of a `WorkflowStepInput`, and there may be multiple, hence the name of this requirement. This logic falls into two buckets:. # Type. Determing the type of the element being passed to the input is a function of:; * what type the `run` is expecting; * whether the variable appears in a scatter ; * whether there is an expression in the `valueFrom` field; * How to ""merge"" when multiple sources are declared.; * the ""default"" (On which I am punting here). # Expression . The way the expression presents the inputs to the run command is determined by that `LinkMergeMethod` field i.e. how the run step expects the value to appear. . # Assumptions. The Spec says to use `merge_nested` as default, but also says:. > If ""merge_nested"" is specified with a single link, the value from the link must be wrapped in a single-item list. Thus it seemed that all values should be wrapped in a list. this is wrong and I'm going to ask on gitter what the deal is. # What I've punted on. the `default` field.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3252
https://github.com/broadinstitute/cromwell/pull/3252:367,Modifiability,variab,variable,367,"Step Inputs have the task of tying wf inputs to their `run` command. This is done via the `source` field of a `WorkflowStepInput`, and there may be multiple, hence the name of this requirement. This logic falls into two buckets:. # Type. Determing the type of the element being passed to the input is a function of:; * what type the `run` is expecting; * whether the variable appears in a scatter ; * whether there is an expression in the `valueFrom` field; * How to ""merge"" when multiple sources are declared.; * the ""default"" (On which I am punting here). # Expression . The way the expression presents the inputs to the run command is determined by that `LinkMergeMethod` field i.e. how the run step expects the value to appear. . # Assumptions. The Spec says to use `merge_nested` as default, but also says:. > If ""merge_nested"" is specified with a single link, the value from the link must be wrapped in a single-item list. Thus it seemed that all values should be wrapped in a list. this is wrong and I'm going to ask on gitter what the deal is. # What I've punted on. the `default` field.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3252
https://github.com/broadinstitute/cromwell/pull/3252:199,Testability,log,logic,199,"Step Inputs have the task of tying wf inputs to their `run` command. This is done via the `source` field of a `WorkflowStepInput`, and there may be multiple, hence the name of this requirement. This logic falls into two buckets:. # Type. Determing the type of the element being passed to the input is a function of:; * what type the `run` is expecting; * whether the variable appears in a scatter ; * whether there is an expression in the `valueFrom` field; * How to ""merge"" when multiple sources are declared.; * the ""default"" (On which I am punting here). # Expression . The way the expression presents the inputs to the run command is determined by that `LinkMergeMethod` field i.e. how the run step expects the value to appear. . # Assumptions. The Spec says to use `merge_nested` as default, but also says:. > If ""merge_nested"" is specified with a single link, the value from the link must be wrapped in a single-item list. Thus it seemed that all values should be wrapped in a list. this is wrong and I'm going to ask on gitter what the deal is. # What I've punted on. the `default` field.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3252
https://github.com/broadinstitute/cromwell/issues/3253:39,Testability,test,test,39,"GDC blocker, also blocking conformance test 85.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3253
https://github.com/broadinstitute/cromwell/issues/3254:120,Deployability,install,install,120,"Currently, the formula in hombrew-core (https://github.com/Homebrew/homebrew-core/blob/master/Formula/cromwell.rb) does install only cromwell, but not womtool. It will be nice to install both at the same time, or have an option to do so. Another possibility is to add a new formula for it informing that it exists...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3254
https://github.com/broadinstitute/cromwell/issues/3254:179,Deployability,install,install,179,"Currently, the formula in hombrew-core (https://github.com/Homebrew/homebrew-core/blob/master/Formula/cromwell.rb) does install only cromwell, but not womtool. It will be nice to install both at the same time, or have an option to do so. Another possibility is to add a new formula for it informing that it exists...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3254
https://github.com/broadinstitute/cromwell/pull/3255:0,Integrability,Depend,Depends,0,Depends on https://github.com/broadinstitute/cromwell/pull/3252,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3255
https://github.com/broadinstitute/cromwell/issues/3256:144,Modifiability,extend,extends,144,This only makes sense for output situations and many (if not most) expressions are used to determine inputs. Could possibly create a trait that extends WomExpression and holds this def,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3256
https://github.com/broadinstitute/cromwell/issues/3259:97,Availability,error,errors,97,"With Cromwell 30.2:. Attempting to abort a workflow mostly running on SGE triggers lots of these errors:. 2018-02-09 11:52:46,500 cromwell-system-akka.dispatchers.engine-dispatcher-96 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Queued and running SGE jobs continue as if nothing happened.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259
https://github.com/broadinstitute/cromwell/issues/3259:274,Performance,Queue,Queued,274,"With Cromwell 30.2:. Attempting to abort a workflow mostly running on SGE triggers lots of these errors:. 2018-02-09 11:52:46,500 cromwell-system-akka.dispatchers.engine-dispatcher-96 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Queued and running SGE jobs continue as if nothing happened.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259
https://github.com/broadinstitute/cromwell/issues/3259:35,Safety,abort,abort,35,"With Cromwell 30.2:. Attempting to abort a workflow mostly running on SGE triggers lots of these errors:. 2018-02-09 11:52:46,500 cromwell-system-akka.dispatchers.engine-dispatcher-96 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Queued and running SGE jobs continue as if nothing happened.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259
https://github.com/broadinstitute/cromwell/issues/3262:15,Deployability,configurat,configuration,15,"Currently, the configuration file should be passed as `java -Dconfig.file=/path/to/your.conf cromwell.jar`; neverhteless, if installed with the formula from homebrew-core, the wrapper script does not allow that configuration:. ```; #!/bin/bash; exec java -jar /usr/local/Cellar/cromwell/30.2/libexec/cromwell-30.2.jar ""$@""; ```. Could it be possible to add a way to provide the config file in this case? Something like an environmental variable can be useful (e.g., `export CROMWELL_CONFIG_FILE=/path/to/your.conf` will pick up directly this in every run).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3262
https://github.com/broadinstitute/cromwell/issues/3262:125,Deployability,install,installed,125,"Currently, the configuration file should be passed as `java -Dconfig.file=/path/to/your.conf cromwell.jar`; neverhteless, if installed with the formula from homebrew-core, the wrapper script does not allow that configuration:. ```; #!/bin/bash; exec java -jar /usr/local/Cellar/cromwell/30.2/libexec/cromwell-30.2.jar ""$@""; ```. Could it be possible to add a way to provide the config file in this case? Something like an environmental variable can be useful (e.g., `export CROMWELL_CONFIG_FILE=/path/to/your.conf` will pick up directly this in every run).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3262
https://github.com/broadinstitute/cromwell/issues/3262:211,Deployability,configurat,configuration,211,"Currently, the configuration file should be passed as `java -Dconfig.file=/path/to/your.conf cromwell.jar`; neverhteless, if installed with the formula from homebrew-core, the wrapper script does not allow that configuration:. ```; #!/bin/bash; exec java -jar /usr/local/Cellar/cromwell/30.2/libexec/cromwell-30.2.jar ""$@""; ```. Could it be possible to add a way to provide the config file in this case? Something like an environmental variable can be useful (e.g., `export CROMWELL_CONFIG_FILE=/path/to/your.conf` will pick up directly this in every run).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3262
https://github.com/broadinstitute/cromwell/issues/3262:176,Integrability,wrap,wrapper,176,"Currently, the configuration file should be passed as `java -Dconfig.file=/path/to/your.conf cromwell.jar`; neverhteless, if installed with the formula from homebrew-core, the wrapper script does not allow that configuration:. ```; #!/bin/bash; exec java -jar /usr/local/Cellar/cromwell/30.2/libexec/cromwell-30.2.jar ""$@""; ```. Could it be possible to add a way to provide the config file in this case? Something like an environmental variable can be useful (e.g., `export CROMWELL_CONFIG_FILE=/path/to/your.conf` will pick up directly this in every run).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3262
https://github.com/broadinstitute/cromwell/issues/3262:15,Modifiability,config,configuration,15,"Currently, the configuration file should be passed as `java -Dconfig.file=/path/to/your.conf cromwell.jar`; neverhteless, if installed with the formula from homebrew-core, the wrapper script does not allow that configuration:. ```; #!/bin/bash; exec java -jar /usr/local/Cellar/cromwell/30.2/libexec/cromwell-30.2.jar ""$@""; ```. Could it be possible to add a way to provide the config file in this case? Something like an environmental variable can be useful (e.g., `export CROMWELL_CONFIG_FILE=/path/to/your.conf` will pick up directly this in every run).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3262
https://github.com/broadinstitute/cromwell/issues/3262:211,Modifiability,config,configuration,211,"Currently, the configuration file should be passed as `java -Dconfig.file=/path/to/your.conf cromwell.jar`; neverhteless, if installed with the formula from homebrew-core, the wrapper script does not allow that configuration:. ```; #!/bin/bash; exec java -jar /usr/local/Cellar/cromwell/30.2/libexec/cromwell-30.2.jar ""$@""; ```. Could it be possible to add a way to provide the config file in this case? Something like an environmental variable can be useful (e.g., `export CROMWELL_CONFIG_FILE=/path/to/your.conf` will pick up directly this in every run).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3262
https://github.com/broadinstitute/cromwell/issues/3262:378,Modifiability,config,config,378,"Currently, the configuration file should be passed as `java -Dconfig.file=/path/to/your.conf cromwell.jar`; neverhteless, if installed with the formula from homebrew-core, the wrapper script does not allow that configuration:. ```; #!/bin/bash; exec java -jar /usr/local/Cellar/cromwell/30.2/libexec/cromwell-30.2.jar ""$@""; ```. Could it be possible to add a way to provide the config file in this case? Something like an environmental variable can be useful (e.g., `export CROMWELL_CONFIG_FILE=/path/to/your.conf` will pick up directly this in every run).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3262
https://github.com/broadinstitute/cromwell/issues/3262:436,Modifiability,variab,variable,436,"Currently, the configuration file should be passed as `java -Dconfig.file=/path/to/your.conf cromwell.jar`; neverhteless, if installed with the formula from homebrew-core, the wrapper script does not allow that configuration:. ```; #!/bin/bash; exec java -jar /usr/local/Cellar/cromwell/30.2/libexec/cromwell-30.2.jar ""$@""; ```. Could it be possible to add a way to provide the config file in this case? Something like an environmental variable can be useful (e.g., `export CROMWELL_CONFIG_FILE=/path/to/your.conf` will pick up directly this in every run).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3262
https://github.com/broadinstitute/cromwell/pull/3263:109,Testability,log,logging,109,This allows to do write directly the output of `womtool womgraph` into a valid `.dot` file without the extra logging from cwl pre-processing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3263
https://github.com/broadinstitute/cromwell/pull/3267:17,Usability,simpl,simple,17,"- Demonstrates a simple (empty workflow) being processed by `FromAtoB[File, Executable]` (i.e passing all the way through from file system to WOM); - Next up, enough wiring to let us submit the `empty_workflow.wdl` to the rest API and get the void result from it. - [x] **Note: needs rebase onto develop after merge of #3265**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3267
https://github.com/broadinstitute/cromwell/issues/3269:276,Testability,test,test,276,"Hello! . Firstly, I'm thank you so much for useful program. (cromwell). ![2018-02-13 10 44 32](https://user-images.githubusercontent.com/4966343/36129525-eed876ea-10aa-11e8-8956-2ccf81ae18c7.png). Command line is..; $ java -jar /home/brandon/cromwell/cromwell-30.2.jar run -i test.json processing-for-variant-discovery-gatk4.wdl. It does not processing anymore in this status. . How can I handle this problem?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3269
https://github.com/broadinstitute/cromwell/issues/3270:11,Availability,error,errors,11,"Last [CRON errors](https://travis-ci.org/broadinstitute/cromwell/jobs/340487274):. ```scala; 2018-02-12 13:50:46,018 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 4c983def-4f9c-4e62-ae38-4543da9922de failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_dict' not specified; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_dict' not specified; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:203); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3270
https://github.com/broadinstitute/cromwell/issues/3270:171,Availability,ERROR,ERROR,171,"Last [CRON errors](https://travis-ci.org/broadinstitute/cromwell/jobs/340487274):. ```scala; 2018-02-12 13:50:46,018 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 4c983def-4f9c-4e62-ae38-4543da9922de failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_dict' not specified; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_dict' not specified; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:203); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3270
https://github.com/broadinstitute/cromwell/issues/3271:11,Availability,error,errors,11,"Last [CRON errors](https://travis-ci.org/broadinstitute/cromwell/jobs/340487274):. ```scala; java.lang.Exception: Task Mutect2.oncotate_m2:NA:1 failed. Job exit code 1. Check gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/oncotate_m2-stderr.log for more information. PAPI error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/HCC1143.maf.annotated -> gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/HCC1143.maf.annotated gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated, command failed: CommandException: No URLs matched: /mnt/local-disk/HCC1143.maf.annotated\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3271
https://github.com/broadinstitute/cromwell/issues/3271:337,Availability,error,error,337,"Last [CRON errors](https://travis-ci.org/broadinstitute/cromwell/jobs/340487274):. ```scala; java.lang.Exception: Task Mutect2.oncotate_m2:NA:1 failed. Job exit code 1. Check gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/oncotate_m2-stderr.log for more information. PAPI error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/HCC1143.maf.annotated -> gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/HCC1143.maf.annotated gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated, command failed: CommandException: No URLs matched: /mnt/local-disk/HCC1143.maf.annotated\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3271
https://github.com/broadinstitute/cromwell/issues/3271:351,Integrability,Message,Message,351,"Last [CRON errors](https://travis-ci.org/broadinstitute/cromwell/jobs/340487274):. ```scala; java.lang.Exception: Task Mutect2.oncotate_m2:NA:1 failed. Job exit code 1. Check gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/oncotate_m2-stderr.log for more information. PAPI error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/HCC1143.maf.annotated -> gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/HCC1143.maf.annotated gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated, command failed: CommandException: No URLs matched: /mnt/local-disk/HCC1143.maf.annotated\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3271
https://github.com/broadinstitute/cromwell/issues/3271:306,Testability,log,log,306,"Last [CRON errors](https://travis-ci.org/broadinstitute/cromwell/jobs/340487274):. ```scala; java.lang.Exception: Task Mutect2.oncotate_m2:NA:1 failed. Job exit code 1. Check gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/oncotate_m2-stderr.log for more information. PAPI error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/HCC1143.maf.annotated -> gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/HCC1143.maf.annotated gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated, command failed: CommandException: No URLs matched: /mnt/local-disk/HCC1143.maf.annotated\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3271
https://github.com/broadinstitute/cromwell/issues/3271:640,Testability,log,log,640,"Last [CRON errors](https://travis-ci.org/broadinstitute/cromwell/jobs/340487274):. ```scala; java.lang.Exception: Task Mutect2.oncotate_m2:NA:1 failed. Job exit code 1. Check gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/oncotate_m2-stderr.log for more information. PAPI error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/HCC1143.maf.annotated -> gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/HCC1143.maf.annotated gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated, command failed: CommandException: No URLs matched: /mnt/local-disk/HCC1143.maf.annotated\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3271
https://github.com/broadinstitute/cromwell/issues/3271:664,Testability,log,log,664,"Last [CRON errors](https://travis-ci.org/broadinstitute/cromwell/jobs/340487274):. ```scala; java.lang.Exception: Task Mutect2.oncotate_m2:NA:1 failed. Job exit code 1. Check gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/oncotate_m2-stderr.log for more information. PAPI error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/HCC1143.maf.annotated -> gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/HCC1143.maf.annotated gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated, command failed: CommandException: No URLs matched: /mnt/local-disk/HCC1143.maf.annotated\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3271
https://github.com/broadinstitute/cromwell/pull/3274:158,Deployability,update,updates,158,Implements the grammar changes for https://github.com/openwdl/wdl/pull/185 and https://github.com/openwdl/wdl/pull/163 for draft-3. Based on the grammar file updates in https://github.com/cjllanwarne/wdl/tree/cjl_draft3_grammar_changes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3274
https://github.com/broadinstitute/cromwell/issues/3284:73,Availability,error,error,73,"I have a large WDL file. At some point, I write the below WDL and get an error:. ```; Array[File] mybams. if (run_my_bams) {; scatter (i in range(length(mybams))) {; ......; }; }; ```; ```; Unable to build WOM node for Scatter '$scatter_0': Unable to build WOM node for If '$if_1': Two or more nodes have the same FullyQualifiedName: ^.mybams""; ```. To fix, I randomly tried the below and it worked:; ```; Array[File] mybams; Int num_mybams = length(mybams). if (run_my_bams) {; scatter (i in range(num_mybams)) {; ......; }; }; ```. Given that this is equivalent WDL, there should not have been an error message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3284
https://github.com/broadinstitute/cromwell/issues/3284:599,Availability,error,error,599,"I have a large WDL file. At some point, I write the below WDL and get an error:. ```; Array[File] mybams. if (run_my_bams) {; scatter (i in range(length(mybams))) {; ......; }; }; ```; ```; Unable to build WOM node for Scatter '$scatter_0': Unable to build WOM node for If '$if_1': Two or more nodes have the same FullyQualifiedName: ^.mybams""; ```. To fix, I randomly tried the below and it worked:; ```; Array[File] mybams; Int num_mybams = length(mybams). if (run_my_bams) {; scatter (i in range(num_mybams)) {; ......; }; }; ```. Given that this is equivalent WDL, there should not have been an error message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3284
https://github.com/broadinstitute/cromwell/issues/3284:605,Integrability,message,message,605,"I have a large WDL file. At some point, I write the below WDL and get an error:. ```; Array[File] mybams. if (run_my_bams) {; scatter (i in range(length(mybams))) {; ......; }; }; ```; ```; Unable to build WOM node for Scatter '$scatter_0': Unable to build WOM node for If '$if_1': Two or more nodes have the same FullyQualifiedName: ^.mybams""; ```. To fix, I randomly tried the below and it worked:; ```; Array[File] mybams; Int num_mybams = length(mybams). if (run_my_bams) {; scatter (i in range(num_mybams)) {; ......; }; }; ```. Given that this is equivalent WDL, there should not have been an error message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3284
https://github.com/broadinstitute/cromwell/issues/3287:68,Security,authenticat,authentication,68,There is a jes-specific docker compose file that includes hooks for authentication. We probably need the same thing for the AWS backend.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3287
https://github.com/broadinstitute/cromwell/issues/3288:89,Deployability,Pipeline,Pipelines,89,Looking at the database migration scripts there is a JES_JOB table for overseeing Google Pipelines work. Do we need a) an AWS_JOB or b) should this be more generic to accommodate all CSPs?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3288
https://github.com/broadinstitute/cromwell/pull/3294:331,Integrability,message,messages,331,"Turns out the batch size for write metadata was ignored (we were looking at the wrong config path); Also turns out it doesn't matter that much when load is very high.; That is mostly because when we flush the event queue, we flush all of it, not just `dbBatchSize` of it.; This added to the fact that the flushes are controlled by messages we sent to `self` that can get lost in an ocean of metadata event messages, we can end up flushing queues of several million events at once, in one transaction, which can take quite some time for slick to do. The more time it takes, the more events we accumulate before the next flush, so the next flush takes even more time etc... ; I'll have a follow up PR to re-factor this a bit, with stats to be able to compare. TODO:; - [x] Add some tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3294
https://github.com/broadinstitute/cromwell/pull/3294:406,Integrability,message,messages,406,"Turns out the batch size for write metadata was ignored (we were looking at the wrong config path); Also turns out it doesn't matter that much when load is very high.; That is mostly because when we flush the event queue, we flush all of it, not just `dbBatchSize` of it.; This added to the fact that the flushes are controlled by messages we sent to `self` that can get lost in an ocean of metadata event messages, we can end up flushing queues of several million events at once, in one transaction, which can take quite some time for slick to do. The more time it takes, the more events we accumulate before the next flush, so the next flush takes even more time etc... ; I'll have a follow up PR to re-factor this a bit, with stats to be able to compare. TODO:; - [x] Add some tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3294
https://github.com/broadinstitute/cromwell/pull/3294:86,Modifiability,config,config,86,"Turns out the batch size for write metadata was ignored (we were looking at the wrong config path); Also turns out it doesn't matter that much when load is very high.; That is mostly because when we flush the event queue, we flush all of it, not just `dbBatchSize` of it.; This added to the fact that the flushes are controlled by messages we sent to `self` that can get lost in an ocean of metadata event messages, we can end up flushing queues of several million events at once, in one transaction, which can take quite some time for slick to do. The more time it takes, the more events we accumulate before the next flush, so the next flush takes even more time etc... ; I'll have a follow up PR to re-factor this a bit, with stats to be able to compare. TODO:; - [x] Add some tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3294
https://github.com/broadinstitute/cromwell/pull/3294:148,Performance,load,load,148,"Turns out the batch size for write metadata was ignored (we were looking at the wrong config path); Also turns out it doesn't matter that much when load is very high.; That is mostly because when we flush the event queue, we flush all of it, not just `dbBatchSize` of it.; This added to the fact that the flushes are controlled by messages we sent to `self` that can get lost in an ocean of metadata event messages, we can end up flushing queues of several million events at once, in one transaction, which can take quite some time for slick to do. The more time it takes, the more events we accumulate before the next flush, so the next flush takes even more time etc... ; I'll have a follow up PR to re-factor this a bit, with stats to be able to compare. TODO:; - [x] Add some tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3294
https://github.com/broadinstitute/cromwell/pull/3294:215,Performance,queue,queue,215,"Turns out the batch size for write metadata was ignored (we were looking at the wrong config path); Also turns out it doesn't matter that much when load is very high.; That is mostly because when we flush the event queue, we flush all of it, not just `dbBatchSize` of it.; This added to the fact that the flushes are controlled by messages we sent to `self` that can get lost in an ocean of metadata event messages, we can end up flushing queues of several million events at once, in one transaction, which can take quite some time for slick to do. The more time it takes, the more events we accumulate before the next flush, so the next flush takes even more time etc... ; I'll have a follow up PR to re-factor this a bit, with stats to be able to compare. TODO:; - [x] Add some tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3294
https://github.com/broadinstitute/cromwell/pull/3294:439,Performance,queue,queues,439,"Turns out the batch size for write metadata was ignored (we were looking at the wrong config path); Also turns out it doesn't matter that much when load is very high.; That is mostly because when we flush the event queue, we flush all of it, not just `dbBatchSize` of it.; This added to the fact that the flushes are controlled by messages we sent to `self` that can get lost in an ocean of metadata event messages, we can end up flushing queues of several million events at once, in one transaction, which can take quite some time for slick to do. The more time it takes, the more events we accumulate before the next flush, so the next flush takes even more time etc... ; I'll have a follow up PR to re-factor this a bit, with stats to be able to compare. TODO:; - [x] Add some tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3294
https://github.com/broadinstitute/cromwell/pull/3294:780,Testability,test,tests,780,"Turns out the batch size for write metadata was ignored (we were looking at the wrong config path); Also turns out it doesn't matter that much when load is very high.; That is mostly because when we flush the event queue, we flush all of it, not just `dbBatchSize` of it.; This added to the fact that the flushes are controlled by messages we sent to `self` that can get lost in an ocean of metadata event messages, we can end up flushing queues of several million events at once, in one transaction, which can take quite some time for slick to do. The more time it takes, the more events we accumulate before the next flush, so the next flush takes even more time etc... ; I'll have a follow up PR to re-factor this a bit, with stats to be able to compare. TODO:; - [x] Add some tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3294
https://github.com/broadinstitute/cromwell/pull/3299:0,Testability,Log,Logic,0,Logic to convert draft-3 AstNodes into wdlom expression elements,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3299
https://github.com/broadinstitute/cromwell/pull/3303:124,Safety,safe,safe,124,"I'm not sure what this was originally intended for?. It was always set to `List.empty` and never read from, so I think it's safe to remove it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3303
https://github.com/broadinstitute/cromwell/issues/3305:20,Modifiability,variab,variables,20,"I cannot mutate any variables outside of the loop. I cannot even rewrite an array by its index if array is declared outside of while loop. Why the hell do we need loops then?; ```wdl; Array[Int] array = [""one"", ""two"", ""three""]; Int i = 0; Int len = length(array); while(i < len){; i = i +1 #does not work; array[i] = ""other value"" #does not work; #some other stuff; }; ```. I also tried to do the same with scatters, same problems (I cannot change variables outside of the loop). That means that both while and scatter loops are half-functional in Cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305
https://github.com/broadinstitute/cromwell/issues/3305:65,Modifiability,rewrite,rewrite,65,"I cannot mutate any variables outside of the loop. I cannot even rewrite an array by its index if array is declared outside of while loop. Why the hell do we need loops then?; ```wdl; Array[Int] array = [""one"", ""two"", ""three""]; Int i = 0; Int len = length(array); while(i < len){; i = i +1 #does not work; array[i] = ""other value"" #does not work; #some other stuff; }; ```. I also tried to do the same with scatters, same problems (I cannot change variables outside of the loop). That means that both while and scatter loops are half-functional in Cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305
https://github.com/broadinstitute/cromwell/issues/3305:448,Modifiability,variab,variables,448,"I cannot mutate any variables outside of the loop. I cannot even rewrite an array by its index if array is declared outside of while loop. Why the hell do we need loops then?; ```wdl; Array[Int] array = [""one"", ""two"", ""three""]; Int i = 0; Int len = length(array); while(i < len){; i = i +1 #does not work; array[i] = ""other value"" #does not work; #some other stuff; }; ```. I also tried to do the same with scatters, same problems (I cannot change variables outside of the loop). That means that both while and scatter loops are half-functional in Cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305
https://github.com/broadinstitute/cromwell/issues/3306:1166,Performance,load,load,1166,"Whenever I try to use read_json it does not work.; For instance, when I make a simple wdl like:; ```wdl; workflow quantification {. File references; Map[String, String] indexes = read_json(references); }; where test.json is:; ```json; {; ""quantification.references"": ""/home/antonkulaga/rna-seq/workflows/rna-seq/quantification/inputs/test.json""; }; ```; ```; it tells; ```; Unable to build WOM node for Declaration 'indexes': Return type of read_json() can't be known statically; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); ```; That means I cannot just load a file and parse it, so how else can I read JSON then?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306
https://github.com/broadinstitute/cromwell/issues/3306:211,Testability,test,test,211,"Whenever I try to use read_json it does not work.; For instance, when I make a simple wdl like:; ```wdl; workflow quantification {. File references; Map[String, String] indexes = read_json(references); }; where test.json is:; ```json; {; ""quantification.references"": ""/home/antonkulaga/rna-seq/workflows/rna-seq/quantification/inputs/test.json""; }; ```; ```; it tells; ```; Unable to build WOM node for Declaration 'indexes': Return type of read_json() can't be known statically; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); ```; That means I cannot just load a file and parse it, so how else can I read JSON then?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306
https://github.com/broadinstitute/cromwell/issues/3306:334,Testability,test,test,334,"Whenever I try to use read_json it does not work.; For instance, when I make a simple wdl like:; ```wdl; workflow quantification {. File references; Map[String, String] indexes = read_json(references); }; where test.json is:; ```json; {; ""quantification.references"": ""/home/antonkulaga/rna-seq/workflows/rna-seq/quantification/inputs/test.json""; }; ```; ```; it tells; ```; Unable to build WOM node for Declaration 'indexes': Return type of read_json() can't be known statically; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); ```; That means I cannot just load a file and parse it, so how else can I read JSON then?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306
https://github.com/broadinstitute/cromwell/issues/3306:79,Usability,simpl,simple,79,"Whenever I try to use read_json it does not work.; For instance, when I make a simple wdl like:; ```wdl; workflow quantification {. File references; Map[String, String] indexes = read_json(references); }; where test.json is:; ```json; {; ""quantification.references"": ""/home/antonkulaga/rna-seq/workflows/rna-seq/quantification/inputs/test.json""; }; ```; ```; it tells; ```; Unable to build WOM node for Declaration 'indexes': Return type of read_json() can't be known statically; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); ```; That means I cannot just load a file and parse it, so how else can I read JSON then?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306
https://github.com/broadinstitute/cromwell/pull/3307:259,Testability,test,tests,259,"See [declaration_chain.wdl](https://github.com/broadinstitute/cromwell/compare/cjl_first_wdlgraph?expand=1#diff-444c3d4a82de93c01c7cc237702c565f) for the motivating WDL for this PR. Allows us to make the correct WOM graph from these Declaration elements, and tests that centaur is able to run the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3307
https://github.com/broadinstitute/cromwell/pull/3312:63,Deployability,Update,Updates,63,Based on changes in https://github.com/cjllanwarne/wdl/pull/5; Updates String literal elements to maybe contain expression placeholders,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3312
https://github.com/broadinstitute/cromwell/pull/3313:158,Modifiability,variab,variable,158,CRON Job: https://travis-ci.org/broadinstitute/cromwell/jobs/344821732. TIL: The Mutect2 workflow contains a task where the WDL author had defined the `HOME` variable upon Docker image creation and used it inside the command block. Cromwell seems to be setting its own `HOME` variable in the exec.sh and thus caused the task to fail.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3313
https://github.com/broadinstitute/cromwell/pull/3313:276,Modifiability,variab,variable,276,CRON Job: https://travis-ci.org/broadinstitute/cromwell/jobs/344821732. TIL: The Mutect2 workflow contains a task where the WDL author had defined the `HOME` variable upon Docker image creation and used it inside the command block. Cromwell seems to be setting its own `HOME` variable in the exec.sh and thus caused the task to fail.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3313
https://github.com/broadinstitute/cromwell/issues/3316:579,Usability,learn,learned,579,"@cjllanwarne, let me know if this description is sufficient. . While running v4.0.1.1 of `cnv_somatic_pair_workflow.wdl` from https://github.com/broadinstitute/gatk/tree/4.0.1.1/scripts/cnv_wdl/somatic, I noticed that runs where I changed values for optional parameters ran with the default parameter values and to completion. I am running Cromwell v30.2 on a Google Compute VM, using `sudo -i` and also `tmux`. Note `cnv_somatic_pair_workflow.wdl` requires two subworkflow WDLs from the same repo. Initially, I erroneously thought that call-caching was the culprit, but later I learned that this is something called ""unforwarded subworkflow inputs not getting reported"". To recapitulate this bug, specify the following optional parameters:; ```; ""CNVSomaticPairWorkflow.CollectCountsTumor.output_format"": ""TSV"",; ""CNVSomaticPairWorkflow.CollectCountsNormal.output_format"": ""TSV"",; ``` . The result of this is that the workflow still produces HDF5 format data, and not TSV data. The original bug is described at https://gatkforums.broadinstitute.org/gatk/discussion/comment/46375#Comment_46375.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3316
https://github.com/broadinstitute/cromwell/issues/3317:44,Testability,test,test,44,"Required for the Wash U CWL and conformance test 93. I originally wrote this to demonstrate that handling of array of file secondary files was broken in Cromwell, but actually it looks like even regular file secondary files are broken on the Docker local backend. 😢. This must use the branch `cwl_secondary_files` as it requires a fix for a shellquoting bug wrt CWL expressions that is only on this branch. The `cwl_secondary_files` conformance test demonstrates these issues in the context of inputs to a CommandLineTool. To test with cwltool:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; cwltool cwl_secondary_files.cwl cwl_secondary_files.json; ```. Note from the cwltool output how each primary input file is put into its own separate staging directory alongside all of its secondary files. 🤔 . For Cromwell this needs to be tested with server mode since run mode blows up. Start up a server:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; java -jar /path/to/cromwell.jar server; ```. Then run the Centaur test:. ```; cd ${CROMWELL_REPO}; sbt ""centaur/it:testOnly * -- -n cwl_secondary_files""; ```. Note that in Cromwell none of the secondary files are staged into the inputs directory, not even for the single file. This Centaur test requires files to be staged in the same directory where the Cromwell server is running which I don't think is something the Travis Centaur setup currently supports. However once this bug is fixed conformance test 93 may be sufficiently equivalent and perhaps this test can just be deleted. Please don't delete the expression shellquoting fix though, we want to keep that. 🙂 It would be nice to find a test other than this that exercises it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3317
https://github.com/broadinstitute/cromwell/issues/3317:445,Testability,test,test,445,"Required for the Wash U CWL and conformance test 93. I originally wrote this to demonstrate that handling of array of file secondary files was broken in Cromwell, but actually it looks like even regular file secondary files are broken on the Docker local backend. 😢. This must use the branch `cwl_secondary_files` as it requires a fix for a shellquoting bug wrt CWL expressions that is only on this branch. The `cwl_secondary_files` conformance test demonstrates these issues in the context of inputs to a CommandLineTool. To test with cwltool:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; cwltool cwl_secondary_files.cwl cwl_secondary_files.json; ```. Note from the cwltool output how each primary input file is put into its own separate staging directory alongside all of its secondary files. 🤔 . For Cromwell this needs to be tested with server mode since run mode blows up. Start up a server:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; java -jar /path/to/cromwell.jar server; ```. Then run the Centaur test:. ```; cd ${CROMWELL_REPO}; sbt ""centaur/it:testOnly * -- -n cwl_secondary_files""; ```. Note that in Cromwell none of the secondary files are staged into the inputs directory, not even for the single file. This Centaur test requires files to be staged in the same directory where the Cromwell server is running which I don't think is something the Travis Centaur setup currently supports. However once this bug is fixed conformance test 93 may be sufficiently equivalent and perhaps this test can just be deleted. Please don't delete the expression shellquoting fix though, we want to keep that. 🙂 It would be nice to find a test other than this that exercises it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3317
https://github.com/broadinstitute/cromwell/issues/3317:526,Testability,test,test,526,"Required for the Wash U CWL and conformance test 93. I originally wrote this to demonstrate that handling of array of file secondary files was broken in Cromwell, but actually it looks like even regular file secondary files are broken on the Docker local backend. 😢. This must use the branch `cwl_secondary_files` as it requires a fix for a shellquoting bug wrt CWL expressions that is only on this branch. The `cwl_secondary_files` conformance test demonstrates these issues in the context of inputs to a CommandLineTool. To test with cwltool:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; cwltool cwl_secondary_files.cwl cwl_secondary_files.json; ```. Note from the cwltool output how each primary input file is put into its own separate staging directory alongside all of its secondary files. 🤔 . For Cromwell this needs to be tested with server mode since run mode blows up. Start up a server:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; java -jar /path/to/cromwell.jar server; ```. Then run the Centaur test:. ```; cd ${CROMWELL_REPO}; sbt ""centaur/it:testOnly * -- -n cwl_secondary_files""; ```. Note that in Cromwell none of the secondary files are staged into the inputs directory, not even for the single file. This Centaur test requires files to be staged in the same directory where the Cromwell server is running which I don't think is something the Travis Centaur setup currently supports. However once this bug is fixed conformance test 93 may be sufficiently equivalent and perhaps this test can just be deleted. Please don't delete the expression shellquoting fix though, we want to keep that. 🙂 It would be nice to find a test other than this that exercises it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3317
https://github.com/broadinstitute/cromwell/issues/3317:876,Testability,test,tested,876,"Required for the Wash U CWL and conformance test 93. I originally wrote this to demonstrate that handling of array of file secondary files was broken in Cromwell, but actually it looks like even regular file secondary files are broken on the Docker local backend. 😢. This must use the branch `cwl_secondary_files` as it requires a fix for a shellquoting bug wrt CWL expressions that is only on this branch. The `cwl_secondary_files` conformance test demonstrates these issues in the context of inputs to a CommandLineTool. To test with cwltool:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; cwltool cwl_secondary_files.cwl cwl_secondary_files.json; ```. Note from the cwltool output how each primary input file is put into its own separate staging directory alongside all of its secondary files. 🤔 . For Cromwell this needs to be tested with server mode since run mode blows up. Start up a server:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; java -jar /path/to/cromwell.jar server; ```. Then run the Centaur test:. ```; cd ${CROMWELL_REPO}; sbt ""centaur/it:testOnly * -- -n cwl_secondary_files""; ```. Note that in Cromwell none of the secondary files are staged into the inputs directory, not even for the single file. This Centaur test requires files to be staged in the same directory where the Cromwell server is running which I don't think is something the Travis Centaur setup currently supports. However once this bug is fixed conformance test 93 may be sufficiently equivalent and perhaps this test can just be deleted. Please don't delete the expression shellquoting fix though, we want to keep that. 🙂 It would be nice to find a test other than this that exercises it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3317
https://github.com/broadinstitute/cromwell/issues/3317:1102,Testability,test,test,1102,"Required for the Wash U CWL and conformance test 93. I originally wrote this to demonstrate that handling of array of file secondary files was broken in Cromwell, but actually it looks like even regular file secondary files are broken on the Docker local backend. 😢. This must use the branch `cwl_secondary_files` as it requires a fix for a shellquoting bug wrt CWL expressions that is only on this branch. The `cwl_secondary_files` conformance test demonstrates these issues in the context of inputs to a CommandLineTool. To test with cwltool:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; cwltool cwl_secondary_files.cwl cwl_secondary_files.json; ```. Note from the cwltool output how each primary input file is put into its own separate staging directory alongside all of its secondary files. 🤔 . For Cromwell this needs to be tested with server mode since run mode blows up. Start up a server:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; java -jar /path/to/cromwell.jar server; ```. Then run the Centaur test:. ```; cd ${CROMWELL_REPO}; sbt ""centaur/it:testOnly * -- -n cwl_secondary_files""; ```. Note that in Cromwell none of the secondary files are staged into the inputs directory, not even for the single file. This Centaur test requires files to be staged in the same directory where the Cromwell server is running which I don't think is something the Travis Centaur setup currently supports. However once this bug is fixed conformance test 93 may be sufficiently equivalent and perhaps this test can just be deleted. Please don't delete the expression shellquoting fix though, we want to keep that. 🙂 It would be nice to find a test other than this that exercises it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3317
https://github.com/broadinstitute/cromwell/issues/3317:1151,Testability,test,testOnly,1151,"Required for the Wash U CWL and conformance test 93. I originally wrote this to demonstrate that handling of array of file secondary files was broken in Cromwell, but actually it looks like even regular file secondary files are broken on the Docker local backend. 😢. This must use the branch `cwl_secondary_files` as it requires a fix for a shellquoting bug wrt CWL expressions that is only on this branch. The `cwl_secondary_files` conformance test demonstrates these issues in the context of inputs to a CommandLineTool. To test with cwltool:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; cwltool cwl_secondary_files.cwl cwl_secondary_files.json; ```. Note from the cwltool output how each primary input file is put into its own separate staging directory alongside all of its secondary files. 🤔 . For Cromwell this needs to be tested with server mode since run mode blows up. Start up a server:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; java -jar /path/to/cromwell.jar server; ```. Then run the Centaur test:. ```; cd ${CROMWELL_REPO}; sbt ""centaur/it:testOnly * -- -n cwl_secondary_files""; ```. Note that in Cromwell none of the secondary files are staged into the inputs directory, not even for the single file. This Centaur test requires files to be staged in the same directory where the Cromwell server is running which I don't think is something the Travis Centaur setup currently supports. However once this bug is fixed conformance test 93 may be sufficiently equivalent and perhaps this test can just be deleted. Please don't delete the expression shellquoting fix though, we want to keep that. 🙂 It would be nice to find a test other than this that exercises it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3317
https://github.com/broadinstitute/cromwell/issues/3317:1326,Testability,test,test,1326,"Required for the Wash U CWL and conformance test 93. I originally wrote this to demonstrate that handling of array of file secondary files was broken in Cromwell, but actually it looks like even regular file secondary files are broken on the Docker local backend. 😢. This must use the branch `cwl_secondary_files` as it requires a fix for a shellquoting bug wrt CWL expressions that is only on this branch. The `cwl_secondary_files` conformance test demonstrates these issues in the context of inputs to a CommandLineTool. To test with cwltool:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; cwltool cwl_secondary_files.cwl cwl_secondary_files.json; ```. Note from the cwltool output how each primary input file is put into its own separate staging directory alongside all of its secondary files. 🤔 . For Cromwell this needs to be tested with server mode since run mode blows up. Start up a server:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; java -jar /path/to/cromwell.jar server; ```. Then run the Centaur test:. ```; cd ${CROMWELL_REPO}; sbt ""centaur/it:testOnly * -- -n cwl_secondary_files""; ```. Note that in Cromwell none of the secondary files are staged into the inputs directory, not even for the single file. This Centaur test requires files to be staged in the same directory where the Cromwell server is running which I don't think is something the Travis Centaur setup currently supports. However once this bug is fixed conformance test 93 may be sufficiently equivalent and perhaps this test can just be deleted. Please don't delete the expression shellquoting fix though, we want to keep that. 🙂 It would be nice to find a test other than this that exercises it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3317
https://github.com/broadinstitute/cromwell/issues/3317:1539,Testability,test,test,1539,"Required for the Wash U CWL and conformance test 93. I originally wrote this to demonstrate that handling of array of file secondary files was broken in Cromwell, but actually it looks like even regular file secondary files are broken on the Docker local backend. 😢. This must use the branch `cwl_secondary_files` as it requires a fix for a shellquoting bug wrt CWL expressions that is only on this branch. The `cwl_secondary_files` conformance test demonstrates these issues in the context of inputs to a CommandLineTool. To test with cwltool:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; cwltool cwl_secondary_files.cwl cwl_secondary_files.json; ```. Note from the cwltool output how each primary input file is put into its own separate staging directory alongside all of its secondary files. 🤔 . For Cromwell this needs to be tested with server mode since run mode blows up. Start up a server:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; java -jar /path/to/cromwell.jar server; ```. Then run the Centaur test:. ```; cd ${CROMWELL_REPO}; sbt ""centaur/it:testOnly * -- -n cwl_secondary_files""; ```. Note that in Cromwell none of the secondary files are staged into the inputs directory, not even for the single file. This Centaur test requires files to be staged in the same directory where the Cromwell server is running which I don't think is something the Travis Centaur setup currently supports. However once this bug is fixed conformance test 93 may be sufficiently equivalent and perhaps this test can just be deleted. Please don't delete the expression shellquoting fix though, we want to keep that. 🙂 It would be nice to find a test other than this that exercises it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3317
https://github.com/broadinstitute/cromwell/issues/3317:1595,Testability,test,test,1595,"Required for the Wash U CWL and conformance test 93. I originally wrote this to demonstrate that handling of array of file secondary files was broken in Cromwell, but actually it looks like even regular file secondary files are broken on the Docker local backend. 😢. This must use the branch `cwl_secondary_files` as it requires a fix for a shellquoting bug wrt CWL expressions that is only on this branch. The `cwl_secondary_files` conformance test demonstrates these issues in the context of inputs to a CommandLineTool. To test with cwltool:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; cwltool cwl_secondary_files.cwl cwl_secondary_files.json; ```. Note from the cwltool output how each primary input file is put into its own separate staging directory alongside all of its secondary files. 🤔 . For Cromwell this needs to be tested with server mode since run mode blows up. Start up a server:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; java -jar /path/to/cromwell.jar server; ```. Then run the Centaur test:. ```; cd ${CROMWELL_REPO}; sbt ""centaur/it:testOnly * -- -n cwl_secondary_files""; ```. Note that in Cromwell none of the secondary files are staged into the inputs directory, not even for the single file. This Centaur test requires files to be staged in the same directory where the Cromwell server is running which I don't think is something the Travis Centaur setup currently supports. However once this bug is fixed conformance test 93 may be sufficiently equivalent and perhaps this test can just be deleted. Please don't delete the expression shellquoting fix though, we want to keep that. 🙂 It would be nice to find a test other than this that exercises it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3317
https://github.com/broadinstitute/cromwell/issues/3317:1732,Testability,test,test,1732,"Required for the Wash U CWL and conformance test 93. I originally wrote this to demonstrate that handling of array of file secondary files was broken in Cromwell, but actually it looks like even regular file secondary files are broken on the Docker local backend. 😢. This must use the branch `cwl_secondary_files` as it requires a fix for a shellquoting bug wrt CWL expressions that is only on this branch. The `cwl_secondary_files` conformance test demonstrates these issues in the context of inputs to a CommandLineTool. To test with cwltool:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; cwltool cwl_secondary_files.cwl cwl_secondary_files.json; ```. Note from the cwltool output how each primary input file is put into its own separate staging directory alongside all of its secondary files. 🤔 . For Cromwell this needs to be tested with server mode since run mode blows up. Start up a server:. ```; cd ${CROMWELL_REPO}/centaur/src/main/resources/standardTestCases/cwl_secondary_files; java -jar /path/to/cromwell.jar server; ```. Then run the Centaur test:. ```; cd ${CROMWELL_REPO}; sbt ""centaur/it:testOnly * -- -n cwl_secondary_files""; ```. Note that in Cromwell none of the secondary files are staged into the inputs directory, not even for the single file. This Centaur test requires files to be staged in the same directory where the Cromwell server is running which I don't think is something the Travis Centaur setup currently supports. However once this bug is fixed conformance test 93 may be sufficiently equivalent and perhaps this test can just be deleted. Please don't delete the expression shellquoting fix though, we want to keep that. 🙂 It would be nice to find a test other than this that exercises it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3317
https://github.com/broadinstitute/cromwell/issues/3318:407,Availability,error,error,407,"This seems weird but Cromwell shouldn't crash. In Jeff's Wash U testing instance, `docker attach` to the container and then:. ```; java -jar /root/cromwell/target/scala-2.12/cromwell-*-SNAP.jar run \; /root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl \; --inputs /root/mgi/inputs_downsample.yml -t CWL; ```; Pretty soon there's this exception:. ```; [2018-02-25 11:18:35,97] [error] WorkflowManagerActor Workflow d35a3d0d-f48d-429f-9120-31cf67bd3e55 failed (during ExecutingWorkflowState): actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; akka.actor.InvalidActorNameException: actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; 	at akka.actor.dungeon.ChildrenContainer$NormalChildrenContainer.reserve(ChildrenContainer.scala:129); 	at akka.actor.dungeon.Children.reserveChild(Children.scala:134); 	at akka.actor.dungeon.Children.reserveChild$(Children.scala:132); 	at akka.actor.ActorCell.reserveChild(ActorCell.scala:370); 	at akka.actor.dungeon.Children.makeChild(Children.scala:272); 	at akka.actor.dungeon.Children.actorOf(Children.scala:44); 	at akka.actor.dungeon.Children.actorOf$(Children.scala:43); 	at akka.actor.ActorCell.actorOf(ActorCell.scala:370); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.startEJEA(WorkflowExecutionActor.scala:546); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun; ```. There's some odd looking CWL here in `/root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl` but conformance test 75 does something very similar and we're passing that so the ""one element array of arrays"" thing may be a red herring. ```; steps:; downsample:; scatter: [bam]; scatterMethod: dotproduct; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3318
https://github.com/broadinstitute/cromwell/issues/3318:594,Availability,down,downsample,594,"This seems weird but Cromwell shouldn't crash. In Jeff's Wash U testing instance, `docker attach` to the container and then:. ```; java -jar /root/cromwell/target/scala-2.12/cromwell-*-SNAP.jar run \; /root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl \; --inputs /root/mgi/inputs_downsample.yml -t CWL; ```; Pretty soon there's this exception:. ```; [2018-02-25 11:18:35,97] [error] WorkflowManagerActor Workflow d35a3d0d-f48d-429f-9120-31cf67bd3e55 failed (during ExecutingWorkflowState): actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; akka.actor.InvalidActorNameException: actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; 	at akka.actor.dungeon.ChildrenContainer$NormalChildrenContainer.reserve(ChildrenContainer.scala:129); 	at akka.actor.dungeon.Children.reserveChild(Children.scala:134); 	at akka.actor.dungeon.Children.reserveChild$(Children.scala:132); 	at akka.actor.ActorCell.reserveChild(ActorCell.scala:370); 	at akka.actor.dungeon.Children.makeChild(Children.scala:272); 	at akka.actor.dungeon.Children.actorOf(Children.scala:44); 	at akka.actor.dungeon.Children.actorOf$(Children.scala:43); 	at akka.actor.ActorCell.actorOf(ActorCell.scala:370); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.startEJEA(WorkflowExecutionActor.scala:546); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun; ```. There's some odd looking CWL here in `/root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl` but conformance test 75 does something very similar and we're passing that so the ""one element array of arrays"" thing may be a red herring. ```; steps:; downsample:; scatter: [bam]; scatterMethod: dotproduct; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3318
https://github.com/broadinstitute/cromwell/issues/3318:737,Availability,down,downsample,737,"This seems weird but Cromwell shouldn't crash. In Jeff's Wash U testing instance, `docker attach` to the container and then:. ```; java -jar /root/cromwell/target/scala-2.12/cromwell-*-SNAP.jar run \; /root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl \; --inputs /root/mgi/inputs_downsample.yml -t CWL; ```; Pretty soon there's this exception:. ```; [2018-02-25 11:18:35,97] [error] WorkflowManagerActor Workflow d35a3d0d-f48d-429f-9120-31cf67bd3e55 failed (during ExecutingWorkflowState): actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; akka.actor.InvalidActorNameException: actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; 	at akka.actor.dungeon.ChildrenContainer$NormalChildrenContainer.reserve(ChildrenContainer.scala:129); 	at akka.actor.dungeon.Children.reserveChild(Children.scala:134); 	at akka.actor.dungeon.Children.reserveChild$(Children.scala:132); 	at akka.actor.ActorCell.reserveChild(ActorCell.scala:370); 	at akka.actor.dungeon.Children.makeChild(Children.scala:272); 	at akka.actor.dungeon.Children.actorOf(Children.scala:44); 	at akka.actor.dungeon.Children.actorOf$(Children.scala:43); 	at akka.actor.ActorCell.actorOf(ActorCell.scala:370); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.startEJEA(WorkflowExecutionActor.scala:546); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun; ```. There's some odd looking CWL here in `/root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl` but conformance test 75 does something very similar and we're passing that so the ""one element array of arrays"" thing may be a red herring. ```; steps:; downsample:; scatter: [bam]; scatterMethod: dotproduct; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3318
https://github.com/broadinstitute/cromwell/issues/3318:1781,Availability,down,downsample,1781,"This seems weird but Cromwell shouldn't crash. In Jeff's Wash U testing instance, `docker attach` to the container and then:. ```; java -jar /root/cromwell/target/scala-2.12/cromwell-*-SNAP.jar run \; /root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl \; --inputs /root/mgi/inputs_downsample.yml -t CWL; ```; Pretty soon there's this exception:. ```; [2018-02-25 11:18:35,97] [error] WorkflowManagerActor Workflow d35a3d0d-f48d-429f-9120-31cf67bd3e55 failed (during ExecutingWorkflowState): actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; akka.actor.InvalidActorNameException: actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; 	at akka.actor.dungeon.ChildrenContainer$NormalChildrenContainer.reserve(ChildrenContainer.scala:129); 	at akka.actor.dungeon.Children.reserveChild(Children.scala:134); 	at akka.actor.dungeon.Children.reserveChild$(Children.scala:132); 	at akka.actor.ActorCell.reserveChild(ActorCell.scala:370); 	at akka.actor.dungeon.Children.makeChild(Children.scala:272); 	at akka.actor.dungeon.Children.actorOf(Children.scala:44); 	at akka.actor.dungeon.Children.actorOf$(Children.scala:43); 	at akka.actor.ActorCell.actorOf(ActorCell.scala:370); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.startEJEA(WorkflowExecutionActor.scala:546); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun; ```. There's some odd looking CWL here in `/root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl` but conformance test 75 does something very similar and we're passing that so the ""one element array of arrays"" thing may be a red herring. ```; steps:; downsample:; scatter: [bam]; scatterMethod: dotproduct; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3318
https://github.com/broadinstitute/cromwell/issues/3318:64,Testability,test,testing,64,"This seems weird but Cromwell shouldn't crash. In Jeff's Wash U testing instance, `docker attach` to the container and then:. ```; java -jar /root/cromwell/target/scala-2.12/cromwell-*-SNAP.jar run \; /root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl \; --inputs /root/mgi/inputs_downsample.yml -t CWL; ```; Pretty soon there's this exception:. ```; [2018-02-25 11:18:35,97] [error] WorkflowManagerActor Workflow d35a3d0d-f48d-429f-9120-31cf67bd3e55 failed (during ExecutingWorkflowState): actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; akka.actor.InvalidActorNameException: actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; 	at akka.actor.dungeon.ChildrenContainer$NormalChildrenContainer.reserve(ChildrenContainer.scala:129); 	at akka.actor.dungeon.Children.reserveChild(Children.scala:134); 	at akka.actor.dungeon.Children.reserveChild$(Children.scala:132); 	at akka.actor.ActorCell.reserveChild(ActorCell.scala:370); 	at akka.actor.dungeon.Children.makeChild(Children.scala:272); 	at akka.actor.dungeon.Children.actorOf(Children.scala:44); 	at akka.actor.dungeon.Children.actorOf$(Children.scala:43); 	at akka.actor.ActorCell.actorOf(ActorCell.scala:370); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.startEJEA(WorkflowExecutionActor.scala:546); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun; ```. There's some odd looking CWL here in `/root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl` but conformance test 75 does something very similar and we're passing that so the ""one element array of arrays"" thing may be a red herring. ```; steps:; downsample:; scatter: [bam]; scatterMethod: dotproduct; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3318
https://github.com/broadinstitute/cromwell/issues/3318:1644,Testability,test,test,1644,"This seems weird but Cromwell shouldn't crash. In Jeff's Wash U testing instance, `docker attach` to the container and then:. ```; java -jar /root/cromwell/target/scala-2.12/cromwell-*-SNAP.jar run \; /root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl \; --inputs /root/mgi/inputs_downsample.yml -t CWL; ```; Pretty soon there's this exception:. ```; [2018-02-25 11:18:35,97] [error] WorkflowManagerActor Workflow d35a3d0d-f48d-429f-9120-31cf67bd3e55 failed (during ExecutingWorkflowState): actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; akka.actor.InvalidActorNameException: actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; 	at akka.actor.dungeon.ChildrenContainer$NormalChildrenContainer.reserve(ChildrenContainer.scala:129); 	at akka.actor.dungeon.Children.reserveChild(Children.scala:134); 	at akka.actor.dungeon.Children.reserveChild$(Children.scala:132); 	at akka.actor.ActorCell.reserveChild(ActorCell.scala:370); 	at akka.actor.dungeon.Children.makeChild(Children.scala:272); 	at akka.actor.dungeon.Children.actorOf(Children.scala:44); 	at akka.actor.dungeon.Children.actorOf$(Children.scala:43); 	at akka.actor.ActorCell.actorOf(ActorCell.scala:370); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.startEJEA(WorkflowExecutionActor.scala:546); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun; ```. There's some odd looking CWL here in `/root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl` but conformance test 75 does something very similar and we're passing that so the ""one element array of arrays"" thing may be a red herring. ```; steps:; downsample:; scatter: [bam]; scatterMethod: dotproduct; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3318
https://github.com/broadinstitute/cromwell/pull/3320:237,Integrability,depend,dependency,237,Most of this PR is noisy and does not change logic. This is caused by moving all JS stuff to CWL project and consolidated JsEncoder/Decoder into CwlJsEncoder/Decoder. I have a couple small outstanding things to clean up:. * remove Rhino dependency from WOM?; * figure out why encodeString needs to be used to form Map w/ String values. this may be a nashorn artifact.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3320
https://github.com/broadinstitute/cromwell/pull/3320:45,Testability,log,logic,45,Most of this PR is noisy and does not change logic. This is caused by moving all JS stuff to CWL project and consolidated JsEncoder/Decoder into CwlJsEncoder/Decoder. I have a couple small outstanding things to clean up:. * remove Rhino dependency from WOM?; * figure out why encodeString needs to be used to form Map w/ String values. this may be a nashorn artifact.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3320
https://github.com/broadinstitute/cromwell/pull/3321:58,Availability,reliab,reliably,58,Attempts to unflakify the abort tests by making them fail reliably if they fail > 20% of the time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3321
https://github.com/broadinstitute/cromwell/pull/3321:26,Safety,abort,abort,26,Attempts to unflakify the abort tests by making them fail reliably if they fail > 20% of the time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3321
https://github.com/broadinstitute/cromwell/pull/3321:32,Testability,test,tests,32,Attempts to unflakify the abort tests by making them fail reliably if they fail > 20% of the time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3321
https://github.com/broadinstitute/cromwell/pull/3323:226,Security,Validat,ValidatedWomNamespace,226,"As side quests, this included:. * No longer requiring inputs in order to build a `WomBundle` (what `wom graph` really wants, and what imports should really produce).; * A second step to go from `WomBundle` to `WomExecutable`/`ValidatedWomNamespace`; * Fixing draft 3 input processing in the `LanguageFactory`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3323
https://github.com/broadinstitute/cromwell/pull/3324:60,Deployability,update,updates,60,"ftfy the metadata sort by deleting it. Batching the summary updates didn't work out since the metadata summary ""last updated id"" scheme assumes rows are processed in batches where each batch has IDs greater than those of the preceding batch. AFAICT that wouldn't be possible without sorting all rows with IDs greater than the last processed row. Instead this just takes all committed rows with IDs greater than the last processed row and assumes that rows are being committed with monotonically increasing IDs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3324
https://github.com/broadinstitute/cromwell/pull/3324:117,Deployability,update,updated,117,"ftfy the metadata sort by deleting it. Batching the summary updates didn't work out since the metadata summary ""last updated id"" scheme assumes rows are processed in batches where each batch has IDs greater than those of the preceding batch. AFAICT that wouldn't be possible without sorting all rows with IDs greater than the last processed row. Instead this just takes all committed rows with IDs greater than the last processed row and assumes that rows are being committed with monotonically increasing IDs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3324
https://github.com/broadinstitute/cromwell/issues/3325:5,Availability,down,downloaded,5,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325
https://github.com/broadinstitute/cromwell/issues/3325:627,Performance,queue,queue,627,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325
https://github.com/broadinstitute/cromwell/issues/3325:666,Performance,queue,queue,666,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325
https://github.com/broadinstitute/cromwell/issues/3325:744,Performance,queue,queued,744,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325
https://github.com/broadinstitute/cromwell/issues/3325:197,Safety,Abort,Abort,197,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325
https://github.com/broadinstitute/cromwell/issues/3325:428,Safety,Abort,Aborting,428,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325
https://github.com/broadinstitute/cromwell/issues/3326:236,Availability,echo,echo,236,"This currently only throws an exception when it tries to call the task. Instead, it should not be able to generate a validated WOM graph in the first place:; ```wdl; workflow oops {; call oopsie; }. task oopsie {; String str; command { echo ${str} }; runtime { docker: docker_image }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3326
https://github.com/broadinstitute/cromwell/issues/3326:117,Security,validat,validated,117,"This currently only throws an exception when it tries to call the task. Instead, it should not be able to generate a validated WOM graph in the first place:; ```wdl; workflow oops {; call oopsie; }. task oopsie {; String str; command { echo ${str} }; runtime { docker: docker_image }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3326
https://github.com/broadinstitute/cromwell/pull/3327:199,Integrability,rout,router,199,"Throttles the JobStore read queries and KV queries.; This imposes a hard limit that might be too low: not more than 1 query at a time. It might be too low in which case we can simply put it behind a router to increase the number of concurrent queries.; The important part is that we control how many of those requests are being done compared to ""just send them to slick as fast as you possibly can"".; Because it uses the same queue mechanism as batching it also makes it easy to turn the queue size into a metric that we can use to control the system's load. TODO:. - [x] Assess impact on scale",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3327
https://github.com/broadinstitute/cromwell/pull/3327:0,Performance,Throttle,Throttles,0,"Throttles the JobStore read queries and KV queries.; This imposes a hard limit that might be too low: not more than 1 query at a time. It might be too low in which case we can simply put it behind a router to increase the number of concurrent queries.; The important part is that we control how many of those requests are being done compared to ""just send them to slick as fast as you possibly can"".; Because it uses the same queue mechanism as batching it also makes it easy to turn the queue size into a metric that we can use to control the system's load. TODO:. - [x] Assess impact on scale",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3327
https://github.com/broadinstitute/cromwell/pull/3327:232,Performance,concurren,concurrent,232,"Throttles the JobStore read queries and KV queries.; This imposes a hard limit that might be too low: not more than 1 query at a time. It might be too low in which case we can simply put it behind a router to increase the number of concurrent queries.; The important part is that we control how many of those requests are being done compared to ""just send them to slick as fast as you possibly can"".; Because it uses the same queue mechanism as batching it also makes it easy to turn the queue size into a metric that we can use to control the system's load. TODO:. - [x] Assess impact on scale",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3327
https://github.com/broadinstitute/cromwell/pull/3327:426,Performance,queue,queue,426,"Throttles the JobStore read queries and KV queries.; This imposes a hard limit that might be too low: not more than 1 query at a time. It might be too low in which case we can simply put it behind a router to increase the number of concurrent queries.; The important part is that we control how many of those requests are being done compared to ""just send them to slick as fast as you possibly can"".; Because it uses the same queue mechanism as batching it also makes it easy to turn the queue size into a metric that we can use to control the system's load. TODO:. - [x] Assess impact on scale",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3327
https://github.com/broadinstitute/cromwell/pull/3327:488,Performance,queue,queue,488,"Throttles the JobStore read queries and KV queries.; This imposes a hard limit that might be too low: not more than 1 query at a time. It might be too low in which case we can simply put it behind a router to increase the number of concurrent queries.; The important part is that we control how many of those requests are being done compared to ""just send them to slick as fast as you possibly can"".; Because it uses the same queue mechanism as batching it also makes it easy to turn the queue size into a metric that we can use to control the system's load. TODO:. - [x] Assess impact on scale",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3327
https://github.com/broadinstitute/cromwell/pull/3327:553,Performance,load,load,553,"Throttles the JobStore read queries and KV queries.; This imposes a hard limit that might be too low: not more than 1 query at a time. It might be too low in which case we can simply put it behind a router to increase the number of concurrent queries.; The important part is that we control how many of those requests are being done compared to ""just send them to slick as fast as you possibly can"".; Because it uses the same queue mechanism as batching it also makes it easy to turn the queue size into a metric that we can use to control the system's load. TODO:. - [x] Assess impact on scale",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3327
https://github.com/broadinstitute/cromwell/pull/3327:176,Usability,simpl,simply,176,"Throttles the JobStore read queries and KV queries.; This imposes a hard limit that might be too low: not more than 1 query at a time. It might be too low in which case we can simply put it behind a router to increase the number of concurrent queries.; The important part is that we control how many of those requests are being done compared to ""just send them to slick as fast as you possibly can"".; Because it uses the same queue mechanism as batching it also makes it easy to turn the queue size into a metric that we can use to control the system's load. TODO:. - [x] Assess impact on scale",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3327
https://github.com/broadinstitute/cromwell/issues/3328:416,Deployability,update,updated,416,It seems the Swagger definition [here](https://github.com/broadinstitute/cromwell/blob/9de965affe5ee89f55b7e6213bf6093954b9a29f/engine/src/main/resources/swagger/cromwell.yaml#L323) does not reflect all of the statuses:; https://github.com/broadinstitute/cromwell/blob/bbfc747d5737b66ffd422d53d567cdbc38c62cba/core/src/main/scala/cromwell/core/ExecutionStatus.scala#L5 . It would be great if the YAML file could get updated.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3328
https://github.com/broadinstitute/cromwell/pull/3331:11,Testability,test,test,11,Motivating test case: https://github.com/broadinstitute/cromwell/pull/3331/files#diff-41a60e0223c67aab34606c0453d71b9f. - [x] Requires rebase onto develop after https://github.com/broadinstitute/cromwell/pull/3330 merges,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3331
https://github.com/broadinstitute/cromwell/issues/3333:553,Availability,down,downstream,553,"We are using cromwell server to run workflows in a shared HPC environment. As it is a shared environment and we are dealing with sensitive data, we have purposely set cromwell to use umask 0007 so that others can't access our files. However, cromwell seems to override the umask and gives rwx world permissions on all directories it creates. . As far as I can tell, the override is programmed here:; cromwell/core/src/main/scala/cromwell/core/path/EvenBetterPathMethods.scala. Beyond it being non-ideal from a security perspective, this ends up causing downstream ""access denied"" problems for us when cromwell creates directories downstream of a linux access control list (ACL). It seems like what is happening in this case is (1) the directory is created via cromwell (2) via the OS, the ACL is applied to the folder, giving user & group privileges (3) the world privileges are modified via cromwell, which disrupts the ACL, thus the user and group lose all privileges (4) access denied error when cromwell tries to use the folder",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333
https://github.com/broadinstitute/cromwell/issues/3333:630,Availability,down,downstream,630,"We are using cromwell server to run workflows in a shared HPC environment. As it is a shared environment and we are dealing with sensitive data, we have purposely set cromwell to use umask 0007 so that others can't access our files. However, cromwell seems to override the umask and gives rwx world permissions on all directories it creates. . As far as I can tell, the override is programmed here:; cromwell/core/src/main/scala/cromwell/core/path/EvenBetterPathMethods.scala. Beyond it being non-ideal from a security perspective, this ends up causing downstream ""access denied"" problems for us when cromwell creates directories downstream of a linux access control list (ACL). It seems like what is happening in this case is (1) the directory is created via cromwell (2) via the OS, the ACL is applied to the folder, giving user & group privileges (3) the world privileges are modified via cromwell, which disrupts the ACL, thus the user and group lose all privileges (4) access denied error when cromwell tries to use the folder",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333
https://github.com/broadinstitute/cromwell/issues/3333:988,Availability,error,error,988,"We are using cromwell server to run workflows in a shared HPC environment. As it is a shared environment and we are dealing with sensitive data, we have purposely set cromwell to use umask 0007 so that others can't access our files. However, cromwell seems to override the umask and gives rwx world permissions on all directories it creates. . As far as I can tell, the override is programmed here:; cromwell/core/src/main/scala/cromwell/core/path/EvenBetterPathMethods.scala. Beyond it being non-ideal from a security perspective, this ends up causing downstream ""access denied"" problems for us when cromwell creates directories downstream of a linux access control list (ACL). It seems like what is happening in this case is (1) the directory is created via cromwell (2) via the OS, the ACL is applied to the folder, giving user & group privileges (3) the world privileges are modified via cromwell, which disrupts the ACL, thus the user and group lose all privileges (4) access denied error when cromwell tries to use the folder",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333
https://github.com/broadinstitute/cromwell/issues/3333:215,Security,access,access,215,"We are using cromwell server to run workflows in a shared HPC environment. As it is a shared environment and we are dealing with sensitive data, we have purposely set cromwell to use umask 0007 so that others can't access our files. However, cromwell seems to override the umask and gives rwx world permissions on all directories it creates. . As far as I can tell, the override is programmed here:; cromwell/core/src/main/scala/cromwell/core/path/EvenBetterPathMethods.scala. Beyond it being non-ideal from a security perspective, this ends up causing downstream ""access denied"" problems for us when cromwell creates directories downstream of a linux access control list (ACL). It seems like what is happening in this case is (1) the directory is created via cromwell (2) via the OS, the ACL is applied to the folder, giving user & group privileges (3) the world privileges are modified via cromwell, which disrupts the ACL, thus the user and group lose all privileges (4) access denied error when cromwell tries to use the folder",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333
https://github.com/broadinstitute/cromwell/issues/3333:510,Security,secur,security,510,"We are using cromwell server to run workflows in a shared HPC environment. As it is a shared environment and we are dealing with sensitive data, we have purposely set cromwell to use umask 0007 so that others can't access our files. However, cromwell seems to override the umask and gives rwx world permissions on all directories it creates. . As far as I can tell, the override is programmed here:; cromwell/core/src/main/scala/cromwell/core/path/EvenBetterPathMethods.scala. Beyond it being non-ideal from a security perspective, this ends up causing downstream ""access denied"" problems for us when cromwell creates directories downstream of a linux access control list (ACL). It seems like what is happening in this case is (1) the directory is created via cromwell (2) via the OS, the ACL is applied to the folder, giving user & group privileges (3) the world privileges are modified via cromwell, which disrupts the ACL, thus the user and group lose all privileges (4) access denied error when cromwell tries to use the folder",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333
https://github.com/broadinstitute/cromwell/issues/3333:565,Security,access,access,565,"We are using cromwell server to run workflows in a shared HPC environment. As it is a shared environment and we are dealing with sensitive data, we have purposely set cromwell to use umask 0007 so that others can't access our files. However, cromwell seems to override the umask and gives rwx world permissions on all directories it creates. . As far as I can tell, the override is programmed here:; cromwell/core/src/main/scala/cromwell/core/path/EvenBetterPathMethods.scala. Beyond it being non-ideal from a security perspective, this ends up causing downstream ""access denied"" problems for us when cromwell creates directories downstream of a linux access control list (ACL). It seems like what is happening in this case is (1) the directory is created via cromwell (2) via the OS, the ACL is applied to the folder, giving user & group privileges (3) the world privileges are modified via cromwell, which disrupts the ACL, thus the user and group lose all privileges (4) access denied error when cromwell tries to use the folder",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333
https://github.com/broadinstitute/cromwell/issues/3333:652,Security,access,access,652,"We are using cromwell server to run workflows in a shared HPC environment. As it is a shared environment and we are dealing with sensitive data, we have purposely set cromwell to use umask 0007 so that others can't access our files. However, cromwell seems to override the umask and gives rwx world permissions on all directories it creates. . As far as I can tell, the override is programmed here:; cromwell/core/src/main/scala/cromwell/core/path/EvenBetterPathMethods.scala. Beyond it being non-ideal from a security perspective, this ends up causing downstream ""access denied"" problems for us when cromwell creates directories downstream of a linux access control list (ACL). It seems like what is happening in this case is (1) the directory is created via cromwell (2) via the OS, the ACL is applied to the folder, giving user & group privileges (3) the world privileges are modified via cromwell, which disrupts the ACL, thus the user and group lose all privileges (4) access denied error when cromwell tries to use the folder",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333
https://github.com/broadinstitute/cromwell/issues/3333:974,Security,access,access,974,"We are using cromwell server to run workflows in a shared HPC environment. As it is a shared environment and we are dealing with sensitive data, we have purposely set cromwell to use umask 0007 so that others can't access our files. However, cromwell seems to override the umask and gives rwx world permissions on all directories it creates. . As far as I can tell, the override is programmed here:; cromwell/core/src/main/scala/cromwell/core/path/EvenBetterPathMethods.scala. Beyond it being non-ideal from a security perspective, this ends up causing downstream ""access denied"" problems for us when cromwell creates directories downstream of a linux access control list (ACL). It seems like what is happening in this case is (1) the directory is created via cromwell (2) via the OS, the ACL is applied to the folder, giving user & group privileges (3) the world privileges are modified via cromwell, which disrupts the ACL, thus the user and group lose all privileges (4) access denied error when cromwell tries to use the folder",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333
https://github.com/broadinstitute/cromwell/pull/3335:192,Performance,queue,queue,192,This is not wired to actually do anything yet but with https://github.com/broadinstitute/cromwell/pull/3332 it'll make it possible to start controlling the rate of job start based on metadata queue size,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3335
https://github.com/broadinstitute/cromwell/issues/3339:42,Performance,load,load,42,Modify Cromwell so as to play nicely in a load balanced environment where the individual Cromwell instances are sharing the same database,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3339
https://github.com/broadinstitute/cromwell/issues/3340:149,Modifiability,config,configurable,149,"In the workflow store, also record the identity of the Cromwell who has been assigned a workflow. . Without thinking about it deeply, having it be a configurable name seems the best path. Another option would be to use the hostname. Both have issues - config could be misconfigured, but could be running multiple cromwells on the same host. Another thought would be a UUID generated by the Cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340
https://github.com/broadinstitute/cromwell/issues/3340:252,Modifiability,config,config,252,"In the workflow store, also record the identity of the Cromwell who has been assigned a workflow. . Without thinking about it deeply, having it be a configurable name seems the best path. Another option would be to use the hostname. Both have issues - config could be misconfigured, but could be running multiple cromwells on the same host. Another thought would be a UUID generated by the Cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340
https://github.com/broadinstitute/cromwell/issues/3341:146,Availability,heartbeat,heartbeat,146,"In order to discern if a workflow is **really** running instead of having been picked up to be run but abandoned, running workflows should send a heartbeat ping to the workflow store. After some configurable time limit, if a workflow has not sent in a heartbeat, move it to a state (see below) such that it'll again be pick-upable. I think it'd be good to have a new state (e.g. `LostContact`) vs resetting it to `Submitted`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3341
https://github.com/broadinstitute/cromwell/issues/3341:156,Availability,ping,ping,156,"In order to discern if a workflow is **really** running instead of having been picked up to be run but abandoned, running workflows should send a heartbeat ping to the workflow store. After some configurable time limit, if a workflow has not sent in a heartbeat, move it to a state (see below) such that it'll again be pick-upable. I think it'd be good to have a new state (e.g. `LostContact`) vs resetting it to `Submitted`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3341
https://github.com/broadinstitute/cromwell/issues/3341:252,Availability,heartbeat,heartbeat,252,"In order to discern if a workflow is **really** running instead of having been picked up to be run but abandoned, running workflows should send a heartbeat ping to the workflow store. After some configurable time limit, if a workflow has not sent in a heartbeat, move it to a state (see below) such that it'll again be pick-upable. I think it'd be good to have a new state (e.g. `LostContact`) vs resetting it to `Submitted`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3341
https://github.com/broadinstitute/cromwell/issues/3341:195,Modifiability,config,configurable,195,"In order to discern if a workflow is **really** running instead of having been picked up to be run but abandoned, running workflows should send a heartbeat ping to the workflow store. After some configurable time limit, if a workflow has not sent in a heartbeat, move it to a state (see below) such that it'll again be pick-upable. I think it'd be good to have a new state (e.g. `LostContact`) vs resetting it to `Submitted`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3341
https://github.com/broadinstitute/cromwell/issues/3342:30,Deployability,update,update,30,"For any table where there are update queries made (not insert) where the rows represent extra-workflow information, make sure that the table is locked when those updates are happening. Examples of such a table would be the workflow store or call cache stuff. . A counterexample would be the metadata table, which is insert-only",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3342
https://github.com/broadinstitute/cromwell/issues/3342:162,Deployability,update,updates,162,"For any table where there are update queries made (not insert) where the rows represent extra-workflow information, make sure that the table is locked when those updates are happening. Examples of such a table would be the workflow store or call cache stuff. . A counterexample would be the metadata table, which is insert-only",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3342
https://github.com/broadinstitute/cromwell/issues/3342:246,Performance,cache,cache,246,"For any table where there are update queries made (not insert) where the rows represent extra-workflow information, make sure that the table is locked when those updates are happening. Examples of such a table would be the workflow store or call cache stuff. . A counterexample would be the metadata table, which is insert-only",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3342
https://github.com/broadinstitute/cromwell/issues/3343:121,Deployability,update,update,121,"Ideally directly written to GCS (see https://github.com/broadinstitute/gatk/issues/4478), but we could also periodically update. In a very imperfect world where all simple solutions are not feasible we can explore using something built for this purpose (many solutions exist, we're just trying to be quick & dirty here)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3343
https://github.com/broadinstitute/cromwell/issues/3343:165,Usability,simpl,simple,165,"Ideally directly written to GCS (see https://github.com/broadinstitute/gatk/issues/4478), but we could also periodically update. In a very imperfect world where all simple solutions are not feasible we can explore using something built for this purpose (many solutions exist, we're just trying to be quick & dirty here)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3343
https://github.com/broadinstitute/cromwell/issues/3344:513,Deployability,update,updates,513,"Currently when a user issues a request to abort a workflow, the process is purely in-memory. This means that there's no way to manually trigger an abort other than the web endpoint. Instead, do something similar to the workflow store. Record the request in a table, and have cromwell monitor that table looking for workflows it is running, and when it sees such a thing use that to trigger the abort. . It is **not** a bad state if there's a workflow in the table which Cromwell doesn't know about. Make sure any updates to this table are [locked](https://github.com/broadinstitute/cromwell/issues/3342)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344
https://github.com/broadinstitute/cromwell/issues/3344:284,Energy Efficiency,monitor,monitor,284,"Currently when a user issues a request to abort a workflow, the process is purely in-memory. This means that there's no way to manually trigger an abort other than the web endpoint. Instead, do something similar to the workflow store. Record the request in a table, and have cromwell monitor that table looking for workflows it is running, and when it sees such a thing use that to trigger the abort. . It is **not** a bad state if there's a workflow in the table which Cromwell doesn't know about. Make sure any updates to this table are [locked](https://github.com/broadinstitute/cromwell/issues/3342)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344
https://github.com/broadinstitute/cromwell/issues/3344:42,Safety,abort,abort,42,"Currently when a user issues a request to abort a workflow, the process is purely in-memory. This means that there's no way to manually trigger an abort other than the web endpoint. Instead, do something similar to the workflow store. Record the request in a table, and have cromwell monitor that table looking for workflows it is running, and when it sees such a thing use that to trigger the abort. . It is **not** a bad state if there's a workflow in the table which Cromwell doesn't know about. Make sure any updates to this table are [locked](https://github.com/broadinstitute/cromwell/issues/3342)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344
https://github.com/broadinstitute/cromwell/issues/3344:147,Safety,abort,abort,147,"Currently when a user issues a request to abort a workflow, the process is purely in-memory. This means that there's no way to manually trigger an abort other than the web endpoint. Instead, do something similar to the workflow store. Record the request in a table, and have cromwell monitor that table looking for workflows it is running, and when it sees such a thing use that to trigger the abort. . It is **not** a bad state if there's a workflow in the table which Cromwell doesn't know about. Make sure any updates to this table are [locked](https://github.com/broadinstitute/cromwell/issues/3342)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344
https://github.com/broadinstitute/cromwell/issues/3344:394,Safety,abort,abort,394,"Currently when a user issues a request to abort a workflow, the process is purely in-memory. This means that there's no way to manually trigger an abort other than the web endpoint. Instead, do something similar to the workflow store. Record the request in a table, and have cromwell monitor that table looking for workflows it is running, and when it sees such a thing use that to trigger the abort. . It is **not** a bad state if there's a workflow in the table which Cromwell doesn't know about. Make sure any updates to this table are [locked](https://github.com/broadinstitute/cromwell/issues/3342)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344
https://github.com/broadinstitute/cromwell/issues/3346:55,Availability,ERROR,ERROR,55,"cromwell-system-akka.dispatchers.service-dispatcher-19 ERROR - Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '""SUMMARY_STATUS_ENTRY"" where (""SUMMARY_TABLE_NAME"" = 'WORKFLOW_METADATA_SUMMARY_' at line 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3346
https://github.com/broadinstitute/cromwell/issues/3346:63,Availability,Error,Error,63,"cromwell-system-akka.dispatchers.service-dispatcher-19 ERROR - Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '""SUMMARY_STATUS_ENTRY"" where (""SUMMARY_TABLE_NAME"" = 'WORKFLOW_METADATA_SUMMARY_' at line 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3346
https://github.com/broadinstitute/cromwell/issues/3346:162,Availability,error,error,162,"cromwell-system-akka.dispatchers.service-dispatcher-19 ERROR - Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '""SUMMARY_STATUS_ENTRY"" where (""SUMMARY_TABLE_NAME"" = 'WORKFLOW_METADATA_SUMMARY_' at line 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3346
https://github.com/broadinstitute/cromwell/issues/3347:34,Deployability,update,updates,34,"Currently when Cromwell starts it updates all `Running` workflows to `RestartableRunning` and `Aborting` to `RestartableAborting` so that it knows which workflows were already running and need to be restarted.; If multiple Cromwells are started against the same DB, they can't all keep changing all workflow statuses as they start.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3347
https://github.com/broadinstitute/cromwell/issues/3347:95,Safety,Abort,Aborting,95,"Currently when Cromwell starts it updates all `Running` workflows to `RestartableRunning` and `Aborting` to `RestartableAborting` so that it knows which workflows were already running and need to be restarted.; If multiple Cromwells are started against the same DB, they can't all keep changing all workflow statuses as they start.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3347
https://github.com/broadinstitute/cromwell/issues/3348:1326,Availability,failure,failures,1326,"ge](https://user-images.githubusercontent.com/9449764/36913061-ae9bb686-1e16-11e8-9d8e-0f30eebc8dd6.png). ### What is it; This may be more information than you need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this view would require many pings to the cromwell query endpoint with different queries to get back all of the numbers and results. . ### Risks I know of; I don't think this is blocked by the labels endpoint update in #3233, but wanted to mention it in case it is a risk. ### ACs; - The Job Manager Dashboard can be quickly fille",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348
https://github.com/broadinstitute/cromwell/issues/3348:1340,Availability,down,down,1340,"ge](https://user-images.githubusercontent.com/9449764/36913061-ae9bb686-1e16-11e8-9d8e-0f30eebc8dd6.png). ### What is it; This may be more information than you need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this view would require many pings to the cromwell query endpoint with different queries to get back all of the numbers and results. . ### Risks I know of; I don't think this is blocked by the labels endpoint update in #3233, but wanted to mention it in case it is a risk. ### ACs; - The Job Manager Dashboard can be quickly fille",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348
https://github.com/broadinstitute/cromwell/issues/3348:2026,Availability,ping,pings,2026,"need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this view would require many pings to the cromwell query endpoint with different queries to get back all of the numbers and results. . ### Risks I know of; I don't think this is blocked by the labels endpoint update in #3233, but wanted to mention it in case it is a risk. ### ACs; - The Job Manager Dashboard can be quickly filled in; - The user can choose which labels they want to get this summary information on (i.e. it's not only a fixed set of labels that are supported by Cromwell)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348
https://github.com/broadinstitute/cromwell/issues/3348:2206,Deployability,update,update,2206,"need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this view would require many pings to the cromwell query endpoint with different queries to get back all of the numbers and results. . ### Risks I know of; I don't think this is blocked by the labels endpoint update in #3233, but wanted to mention it in case it is a risk. ### ACs; - The Job Manager Dashboard can be quickly filled in; - The user can choose which labels they want to get this summary information on (i.e. it's not only a fixed set of labels that are supported by Cromwell)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348
https://github.com/broadinstitute/cromwell/issues/3348:2136,Safety,Risk,Risks,2136,"need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this view would require many pings to the cromwell query endpoint with different queries to get back all of the numbers and results. . ### Risks I know of; I don't think this is blocked by the labels endpoint update in #3233, but wanted to mention it in case it is a risk. ### ACs; - The Job Manager Dashboard can be quickly filled in; - The user can choose which labels they want to get this summary information on (i.e. it's not only a fixed set of labels that are supported by Cromwell)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348
https://github.com/broadinstitute/cromwell/issues/3348:2264,Safety,risk,risk,2264,"need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this view would require many pings to the cromwell query endpoint with different queries to get back all of the numbers and results. . ### Risks I know of; I don't think this is blocked by the labels endpoint update in #3233, but wanted to mention it in case it is a risk. ### ACs; - The Job Manager Dashboard can be quickly filled in; - The user can choose which labels they want to get this summary information on (i.e. it's not only a fixed set of labels that are supported by Cromwell)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348
https://github.com/broadinstitute/cromwell/issues/3348:1004,Security,access,access,1004,"### Motivation; The Job Manager UI is hoping to support a dashboard view, pictured below. We've heard from multiple users that this view would be very helpful to them. In addition, this capability is of especially high priority for our partners in this, Verily. Verily also needs to make changes to dsub to support this. ![image](https://user-images.githubusercontent.com/9449764/36913061-ae9bb686-1e16-11e8-9d8e-0f30eebc8dd6.png). ### What is it; This may be more information than you need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348
https://github.com/broadinstitute/cromwell/issues/3349:660,Deployability,release,release,660,"Whenever I try to have and if statements inside scatter blocks I get:; ```; Workflow input processing failed; WorkflowFailure(Unable to build WOM node for Scatter '$scatter_1': Unable to build WOM node for If '$if_1': Type evaluation cannot determine type from expression: WomMaybeEmptyArrayType(WomStringType) == WomMaybeEmptyArrayType(WomStringType),List())WorkflowFailure(Unable to build WOM node for Scatter '$scatter_1': Unable to build WOM node for If '$if_1': Type evaluation cannot determine type from expression: WomMaybeEmptyArrayType(WomStringType) == WomMaybeEmptyArrayType(WomStringType),List()); ```; that happens in both latest trunk and latest release; A simple example that crashes is:; ```; # Example input: [ ""A"", ""B"", ""C"", ""D"" ]; Array[String] withUnwantedHead. # In our example: [ 0, 1, 2, 3 ] ; Array[Int] indices = range(length(withUnwantedHead)). # In our example: [ (0, ""A""), (1, ""B""), (2, ""C""), (3, ""D"") ] ; Array[Pair[Int, String]] zippedwithIndices = zip(indices, withUnwantedHead). scatter(p in zippedwithIndices) {; if (p.left != 0) {; String notFirst = p.right; }; }; ```; see also discussions at:; https://gatkforums.broadinstitute.org/wdl/discussion/9893/getting-tail-of-the-array?#",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3349
https://github.com/broadinstitute/cromwell/issues/3349:671,Usability,simpl,simple,671,"Whenever I try to have and if statements inside scatter blocks I get:; ```; Workflow input processing failed; WorkflowFailure(Unable to build WOM node for Scatter '$scatter_1': Unable to build WOM node for If '$if_1': Type evaluation cannot determine type from expression: WomMaybeEmptyArrayType(WomStringType) == WomMaybeEmptyArrayType(WomStringType),List())WorkflowFailure(Unable to build WOM node for Scatter '$scatter_1': Unable to build WOM node for If '$if_1': Type evaluation cannot determine type from expression: WomMaybeEmptyArrayType(WomStringType) == WomMaybeEmptyArrayType(WomStringType),List()); ```; that happens in both latest trunk and latest release; A simple example that crashes is:; ```; # Example input: [ ""A"", ""B"", ""C"", ""D"" ]; Array[String] withUnwantedHead. # In our example: [ 0, 1, 2, 3 ] ; Array[Int] indices = range(length(withUnwantedHead)). # In our example: [ (0, ""A""), (1, ""B""), (2, ""C""), (3, ""D"") ] ; Array[Pair[Int, String]] zippedwithIndices = zip(indices, withUnwantedHead). scatter(p in zippedwithIndices) {; if (p.left != 0) {; String notFirst = p.right; }; }; ```; see also discussions at:; https://gatkforums.broadinstitute.org/wdl/discussion/9893/getting-tail-of-the-array?#",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3349
https://github.com/broadinstitute/cromwell/pull/3352:28,Availability,error,error,28,Fixes broken builds:; ```; [error] (wdlDraft3LanguageFactory/*:dockerPush) Failed to push; [error] (wdlDraft2LanguageFactory/*:dockerPush) Failed to push; [error] (cwlV1_0LanguageFactory/*:dockerPush) Failed to push; [error] (languageFactoryCore/*:dockerPush) Failed to push; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3352
https://github.com/broadinstitute/cromwell/pull/3352:92,Availability,error,error,92,Fixes broken builds:; ```; [error] (wdlDraft3LanguageFactory/*:dockerPush) Failed to push; [error] (wdlDraft2LanguageFactory/*:dockerPush) Failed to push; [error] (cwlV1_0LanguageFactory/*:dockerPush) Failed to push; [error] (languageFactoryCore/*:dockerPush) Failed to push; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3352
https://github.com/broadinstitute/cromwell/pull/3352:156,Availability,error,error,156,Fixes broken builds:; ```; [error] (wdlDraft3LanguageFactory/*:dockerPush) Failed to push; [error] (wdlDraft2LanguageFactory/*:dockerPush) Failed to push; [error] (cwlV1_0LanguageFactory/*:dockerPush) Failed to push; [error] (languageFactoryCore/*:dockerPush) Failed to push; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3352
https://github.com/broadinstitute/cromwell/pull/3352:218,Availability,error,error,218,Fixes broken builds:; ```; [error] (wdlDraft3LanguageFactory/*:dockerPush) Failed to push; [error] (wdlDraft2LanguageFactory/*:dockerPush) Failed to push; [error] (cwlV1_0LanguageFactory/*:dockerPush) Failed to push; [error] (languageFactoryCore/*:dockerPush) Failed to push; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3352
https://github.com/broadinstitute/cromwell/issues/3353:136,Performance,load,load,136,"Placeholder for now as it's unclear what needs to happen until we get closer in. However the gist is this: If one is going from current load balancing is unpossible cromwell to a fleet of cromwell mk2s, how do we keep that from being a disaster?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3353
https://github.com/broadinstitute/cromwell/pull/3354:129,Testability,test,test,129,"Here is the the initial code for adding JSON-like objects to meta sections. In sbt project `wdlTransformsDraft3`, there is a new test case for the new constructions. What is the next step?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3354
https://github.com/broadinstitute/cromwell/pull/3356:273,Energy Efficiency,Reduce,Reduce,273,"This might be a bit controversial so feel free to seagull. _What it does_: When looking for new nodes to run in a workflow, if there are more than 1000 call nodes in ""Queued"" state, don't start new call nodes (still execute other ones like ExpressionNodes etc...).; _Pro_: Reduce load by not starting too many jobs that won't be able to run for the moment anyway since there's already 1000+ queued (waiting for a token); _Con_: Jobs stay in `NotStarted` state longer (this status is sent to metadata and is visible by users), even if they could technically be started as far as dependencies are concerned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356
https://github.com/broadinstitute/cromwell/pull/3356:578,Integrability,depend,dependencies,578,"This might be a bit controversial so feel free to seagull. _What it does_: When looking for new nodes to run in a workflow, if there are more than 1000 call nodes in ""Queued"" state, don't start new call nodes (still execute other ones like ExpressionNodes etc...).; _Pro_: Reduce load by not starting too many jobs that won't be able to run for the moment anyway since there's already 1000+ queued (waiting for a token); _Con_: Jobs stay in `NotStarted` state longer (this status is sent to metadata and is visible by users), even if they could technically be started as far as dependencies are concerned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356
https://github.com/broadinstitute/cromwell/pull/3356:167,Performance,Queue,Queued,167,"This might be a bit controversial so feel free to seagull. _What it does_: When looking for new nodes to run in a workflow, if there are more than 1000 call nodes in ""Queued"" state, don't start new call nodes (still execute other ones like ExpressionNodes etc...).; _Pro_: Reduce load by not starting too many jobs that won't be able to run for the moment anyway since there's already 1000+ queued (waiting for a token); _Con_: Jobs stay in `NotStarted` state longer (this status is sent to metadata and is visible by users), even if they could technically be started as far as dependencies are concerned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356
https://github.com/broadinstitute/cromwell/pull/3356:280,Performance,load,load,280,"This might be a bit controversial so feel free to seagull. _What it does_: When looking for new nodes to run in a workflow, if there are more than 1000 call nodes in ""Queued"" state, don't start new call nodes (still execute other ones like ExpressionNodes etc...).; _Pro_: Reduce load by not starting too many jobs that won't be able to run for the moment anyway since there's already 1000+ queued (waiting for a token); _Con_: Jobs stay in `NotStarted` state longer (this status is sent to metadata and is visible by users), even if they could technically be started as far as dependencies are concerned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356
https://github.com/broadinstitute/cromwell/pull/3356:391,Performance,queue,queued,391,"This might be a bit controversial so feel free to seagull. _What it does_: When looking for new nodes to run in a workflow, if there are more than 1000 call nodes in ""Queued"" state, don't start new call nodes (still execute other ones like ExpressionNodes etc...).; _Pro_: Reduce load by not starting too many jobs that won't be able to run for the moment anyway since there's already 1000+ queued (waiting for a token); _Con_: Jobs stay in `NotStarted` state longer (this status is sent to metadata and is visible by users), even if they could technically be started as far as dependencies are concerned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356
https://github.com/broadinstitute/cromwell/pull/3357:382,Performance,throughput,throughput,382,"- Send abort requests through the `JesAPIQueryManager`. This is wanted because currently each JABJEA aborts on its own which has undesired consequences, like flooding the backend thread pool with blocking requests.; - Lift the ""1 second"" maximum limit of the `JesPollingActor` by switching to milliseconds (new limit is 1 millisecond); - Add a second `JesPollingActor` to help with throughput of PAPI requests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3357
https://github.com/broadinstitute/cromwell/pull/3357:7,Safety,abort,abort,7,"- Send abort requests through the `JesAPIQueryManager`. This is wanted because currently each JABJEA aborts on its own which has undesired consequences, like flooding the backend thread pool with blocking requests.; - Lift the ""1 second"" maximum limit of the `JesPollingActor` by switching to milliseconds (new limit is 1 millisecond); - Add a second `JesPollingActor` to help with throughput of PAPI requests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3357
https://github.com/broadinstitute/cromwell/pull/3357:101,Safety,abort,aborts,101,"- Send abort requests through the `JesAPIQueryManager`. This is wanted because currently each JABJEA aborts on its own which has undesired consequences, like flooding the backend thread pool with blocking requests.; - Lift the ""1 second"" maximum limit of the `JesPollingActor` by switching to milliseconds (new limit is 1 millisecond); - Add a second `JesPollingActor` to help with throughput of PAPI requests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3357
https://github.com/broadinstitute/cromwell/issues/3359:274,Performance,load,loadContents,274,"Example docs:. > If neither location nor path is provided, contents must be non-null. The implementation must assign a unique identifier for the location field. When the file is staged as input to CommandLineTool, the value of contents must be written to a file. ; > ; > If loadContents of inputBinding or outputBinding is true and location is valid, the implementation must read up to the first 64 KiB of text from the file and place it in the ""contents"" field.; and. Link to specs:; - http://www.commonwl.org/v1.0/CommandLineTool.html#File; - http://www.commonwl.org/v1.0/Workflow.html#File. Example conformance test(s):. - <span>#</span>83 [v1.0/cat3-tool.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/cat3-tool.cwl) [v1.0/file-literal.yml](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/file-literal.yml); - <span>#</span>113 [v1.0/cat3-nodocker.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/cat3-nodocker.cwl) [v1.0/file-literal.yml](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/file-literal.yml)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3359
https://github.com/broadinstitute/cromwell/issues/3359:614,Testability,test,test,614,"Example docs:. > If neither location nor path is provided, contents must be non-null. The implementation must assign a unique identifier for the location field. When the file is staged as input to CommandLineTool, the value of contents must be written to a file. ; > ; > If loadContents of inputBinding or outputBinding is true and location is valid, the implementation must read up to the first 64 KiB of text from the file and place it in the ""contents"" field.; and. Link to specs:; - http://www.commonwl.org/v1.0/CommandLineTool.html#File; - http://www.commonwl.org/v1.0/Workflow.html#File. Example conformance test(s):. - <span>#</span>83 [v1.0/cat3-tool.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/cat3-tool.cwl) [v1.0/file-literal.yml](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/file-literal.yml); - <span>#</span>113 [v1.0/cat3-nodocker.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/cat3-nodocker.cwl) [v1.0/file-literal.yml](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/file-literal.yml)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3359
https://github.com/broadinstitute/cromwell/issues/3360:379,Testability,test,test,379,"Example docs:. > Directory objects in CommandLineTool output must provide either a location URI or a path property in the context of the tool execution runtime (local to the compute node, or within the executing container). Link to specs:; - http://www.commonwl.org/v1.0/CommandLineTool.html#Directory; - http://www.commonwl.org/v1.0/Workflow.html#Direcotry. Example conformance test(s):. - <span>#</span>110 [v1.0/writable-dir.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/writable-dir.cwl) [v1.0/empty.json](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/empty.json); - <span>#</span>111 [v1.0/writable-dir-docker.cwl](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/writable-dir-docker.cwl) [v1.0/empty.json](https://github.com/common-workflow-language/common-workflow-language/blob/af9e073634dc6aec4092c55e1c081f335affa54a/v1.0/v1.0/empty.json)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3360
https://github.com/broadinstitute/cromwell/pull/3366:1629,Availability,robust,robustness,1629,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:1676,Deployability,Configurat,Configuration,1676,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:567,Energy Efficiency,monitor,monitored,567,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:1360,Integrability,message,messages,1360,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:1676,Modifiability,Config,Configuration,1676,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:1747,Modifiability,config,configurable,1747,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:1792,Modifiability,config,configure,1792,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:60,Performance,load,load,60,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:152,Performance,load,load,152,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:213,Performance,load,load,213,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:261,Performance,load,load,261,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:326,Performance,load,load,326,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:646,Performance,load,load,646,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:662,Performance,load,load,662,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:710,Performance,load,load,710,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:735,Performance,load,load,735,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:823,Performance,load,load,823,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:892,Performance,load,load,892,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:926,Performance,load,load,926,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:1021,Performance,queue,queue,1021,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:1070,Performance,load,load,1070,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:1119,Performance,queue,queue,1119,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:1156,Performance,queue,queue,1156,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:1423,Performance,load,load,1423,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:1497,Performance,load,load,1497,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:1176,Security,access,accessible,1176,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/pull/3366:245,Usability,simpl,simple,245,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366
https://github.com/broadinstitute/cromwell/issues/3367:202,Availability,error,error,202,"I have, really weird behavior for File? I expect that if file does not exist I should get null for File? (as there are no wld functions that actually check for file existance), however instead I get an error:; ```; Workflow failed; WorkflowFailure(,List(WorkflowFailure(Could not process output, file not found: /pipelines/scripts/cromwell-executions/quantification/87596324-9f68-4419-b3ce-6ff47fe381b4/call-prepare_samples/execution/not_exist,List()))); ```; ```wdl; task prepare_samples {; File samples; File references; File samples_folder. command {; /scripts/run.sc --samples ${samples} --references ${references} --cache ${samples_folder}; }. runtime {; docker: ""quay.io/comp-bio-aging/prepare-samples@sha256:9aaa223ff520634bb0357500ffb90aa80315729e0870ebbc7da4a4b31c382a2c""; }. output {; File? invalid = ""invalid.tsv""; File? novel = ""novel.tsv""; #File? cached = ""cached.tsv""; File? cached = ""not_exist"" #I expect it to be null if the file does not exist; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3367
https://github.com/broadinstitute/cromwell/issues/3367:313,Deployability,pipeline,pipelines,313,"I have, really weird behavior for File? I expect that if file does not exist I should get null for File? (as there are no wld functions that actually check for file existance), however instead I get an error:; ```; Workflow failed; WorkflowFailure(,List(WorkflowFailure(Could not process output, file not found: /pipelines/scripts/cromwell-executions/quantification/87596324-9f68-4419-b3ce-6ff47fe381b4/call-prepare_samples/execution/not_exist,List()))); ```; ```wdl; task prepare_samples {; File samples; File references; File samples_folder. command {; /scripts/run.sc --samples ${samples} --references ${references} --cache ${samples_folder}; }. runtime {; docker: ""quay.io/comp-bio-aging/prepare-samples@sha256:9aaa223ff520634bb0357500ffb90aa80315729e0870ebbc7da4a4b31c382a2c""; }. output {; File? invalid = ""invalid.tsv""; File? novel = ""novel.tsv""; #File? cached = ""cached.tsv""; File? cached = ""not_exist"" #I expect it to be null if the file does not exist; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3367
https://github.com/broadinstitute/cromwell/issues/3367:621,Performance,cache,cache,621,"I have, really weird behavior for File? I expect that if file does not exist I should get null for File? (as there are no wld functions that actually check for file existance), however instead I get an error:; ```; Workflow failed; WorkflowFailure(,List(WorkflowFailure(Could not process output, file not found: /pipelines/scripts/cromwell-executions/quantification/87596324-9f68-4419-b3ce-6ff47fe381b4/call-prepare_samples/execution/not_exist,List()))); ```; ```wdl; task prepare_samples {; File samples; File references; File samples_folder. command {; /scripts/run.sc --samples ${samples} --references ${references} --cache ${samples_folder}; }. runtime {; docker: ""quay.io/comp-bio-aging/prepare-samples@sha256:9aaa223ff520634bb0357500ffb90aa80315729e0870ebbc7da4a4b31c382a2c""; }. output {; File? invalid = ""invalid.tsv""; File? novel = ""novel.tsv""; #File? cached = ""cached.tsv""; File? cached = ""not_exist"" #I expect it to be null if the file does not exist; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3367
https://github.com/broadinstitute/cromwell/issues/3367:860,Performance,cache,cached,860,"I have, really weird behavior for File? I expect that if file does not exist I should get null for File? (as there are no wld functions that actually check for file existance), however instead I get an error:; ```; Workflow failed; WorkflowFailure(,List(WorkflowFailure(Could not process output, file not found: /pipelines/scripts/cromwell-executions/quantification/87596324-9f68-4419-b3ce-6ff47fe381b4/call-prepare_samples/execution/not_exist,List()))); ```; ```wdl; task prepare_samples {; File samples; File references; File samples_folder. command {; /scripts/run.sc --samples ${samples} --references ${references} --cache ${samples_folder}; }. runtime {; docker: ""quay.io/comp-bio-aging/prepare-samples@sha256:9aaa223ff520634bb0357500ffb90aa80315729e0870ebbc7da4a4b31c382a2c""; }. output {; File? invalid = ""invalid.tsv""; File? novel = ""novel.tsv""; #File? cached = ""cached.tsv""; File? cached = ""not_exist"" #I expect it to be null if the file does not exist; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3367
https://github.com/broadinstitute/cromwell/issues/3367:870,Performance,cache,cached,870,"I have, really weird behavior for File? I expect that if file does not exist I should get null for File? (as there are no wld functions that actually check for file existance), however instead I get an error:; ```; Workflow failed; WorkflowFailure(,List(WorkflowFailure(Could not process output, file not found: /pipelines/scripts/cromwell-executions/quantification/87596324-9f68-4419-b3ce-6ff47fe381b4/call-prepare_samples/execution/not_exist,List()))); ```; ```wdl; task prepare_samples {; File samples; File references; File samples_folder. command {; /scripts/run.sc --samples ${samples} --references ${references} --cache ${samples_folder}; }. runtime {; docker: ""quay.io/comp-bio-aging/prepare-samples@sha256:9aaa223ff520634bb0357500ffb90aa80315729e0870ebbc7da4a4b31c382a2c""; }. output {; File? invalid = ""invalid.tsv""; File? novel = ""novel.tsv""; #File? cached = ""cached.tsv""; File? cached = ""not_exist"" #I expect it to be null if the file does not exist; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3367
https://github.com/broadinstitute/cromwell/issues/3367:889,Performance,cache,cached,889,"I have, really weird behavior for File? I expect that if file does not exist I should get null for File? (as there are no wld functions that actually check for file existance), however instead I get an error:; ```; Workflow failed; WorkflowFailure(,List(WorkflowFailure(Could not process output, file not found: /pipelines/scripts/cromwell-executions/quantification/87596324-9f68-4419-b3ce-6ff47fe381b4/call-prepare_samples/execution/not_exist,List()))); ```; ```wdl; task prepare_samples {; File samples; File references; File samples_folder. command {; /scripts/run.sc --samples ${samples} --references ${references} --cache ${samples_folder}; }. runtime {; docker: ""quay.io/comp-bio-aging/prepare-samples@sha256:9aaa223ff520634bb0357500ffb90aa80315729e0870ebbc7da4a4b31c382a2c""; }. output {; File? invalid = ""invalid.tsv""; File? novel = ""novel.tsv""; #File? cached = ""cached.tsv""; File? cached = ""not_exist"" #I expect it to be null if the file does not exist; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3367
https://github.com/broadinstitute/cromwell/issues/3368:252,Performance,cache,cached,252,"I had to make a workaround for the workflow where the tool can sometimes return an empty file tsv file (as there was a bug in cromwell where read_tsv(empty_file) did not work. ; ```; Array[Array[String]] cached_samples = if(read_string(prepare_samples.cached)=="""") then [[]] else read_tsv(prepare_samples.cached); ```. However, with latest WOM model this workaround stopped working and now I get:; ```; WorkflowFailure(Evaluating row[0] + ""_cached"" failed: Failed to find index Success(WomInteger(0)) on array: Success([]) 0,List()); ```; I wonder why does it try to go to the 0 index of an empty array?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3368
https://github.com/broadinstitute/cromwell/issues/3368:305,Performance,cache,cached,305,"I had to make a workaround for the workflow where the tool can sometimes return an empty file tsv file (as there was a bug in cromwell where read_tsv(empty_file) did not work. ; ```; Array[Array[String]] cached_samples = if(read_string(prepare_samples.cached)=="""") then [[]] else read_tsv(prepare_samples.cached); ```. However, with latest WOM model this workaround stopped working and now I get:; ```; WorkflowFailure(Evaluating row[0] + ""_cached"" failed: Failed to find index Success(WomInteger(0)) on array: Success([]) 0,List()); ```; I wonder why does it try to go to the 0 index of an empty array?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3368
https://github.com/broadinstitute/cromwell/pull/3369:78,Deployability,pipeline,pipelines,78,"Made GoogleAuthMode scopes optional via a param/conf, still defaulting to the pipelines api scopes.; Removed unused scopes and data-dirs from google auth modes that do not uses them.; Moved access token TTL refresher from GcrAbstractFlow to GoogleAuthMode.; Fixed queryPostRoute logging method as ""GET"" instead of ""POST"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3369
https://github.com/broadinstitute/cromwell/pull/3369:190,Security,access,access,190,"Made GoogleAuthMode scopes optional via a param/conf, still defaulting to the pipelines api scopes.; Removed unused scopes and data-dirs from google auth modes that do not uses them.; Moved access token TTL refresher from GcrAbstractFlow to GoogleAuthMode.; Fixed queryPostRoute logging method as ""GET"" instead of ""POST"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3369
https://github.com/broadinstitute/cromwell/pull/3369:279,Testability,log,logging,279,"Made GoogleAuthMode scopes optional via a param/conf, still defaulting to the pipelines api scopes.; Removed unused scopes and data-dirs from google auth modes that do not uses them.; Moved access token TTL refresher from GcrAbstractFlow to GoogleAuthMode.; Fixed queryPostRoute logging method as ""GET"" instead of ""POST"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3369
https://github.com/broadinstitute/cromwell/issues/3370:186,Availability,error,error,186,"When trying to run a workflow locally with docker containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:361,Availability,failure,failure,361,"When trying to run a workflow locally with docker containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:457,Availability,error,errors,457,"When trying to run a workflow locally with docker containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:518,Availability,error,error,518,"When trying to run a workflow locally with docker containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:621,Availability,error,error,621,"When trying to run a workflow locally with docker containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:632,Availability,error,error,632,"When trying to run a workflow locally with docker containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:1592,Availability,error,error,1592," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:1901,Availability,failure,failures,1901," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:1993,Availability,failure,failures,1993," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:2157,Availability,error,error,2157," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:164,Deployability,pipeline,pipeline,164,"When trying to run a workflow locally with docker containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:937,Deployability,configurat,configuration,937,"When trying to run a workflow locally with docker containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:1428,Deployability,install,installed,1428," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:1771,Deployability,pipeline,pipeline,1771," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:101,Integrability,depend,depends,101,"When trying to run a workflow locally with docker containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:1438,Integrability,wrap,wrapper,1438," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:937,Modifiability,config,configuration,937,"When trying to run a workflow locally with docker containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:1129,Modifiability,config,config,1129,"line) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:1168,Performance,concurren,concurrent-job-limit,1168,"line) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:1985,Safety,timeout,timeout,1985," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:472,Testability,log,logs,472,"When trying to run a workflow locally with docker containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:1051,Testability,log,logs,1051,"r containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/issues/3370:1083,Testability,log,log-temporary,1083,"always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370
https://github.com/broadinstitute/cromwell/pull/3373:94,Testability,test,tests,94,"The bug I mentioned this morning came from a change in the EJEA (it didn't get caught by unit tests because they're not being run, but that's another story..); They're still not being run in this PR so this is on hold until we fix this (@kshakir is on it); In the meantime I re-factored a bit the ""Running"" state of the EJEA to be able to understand what's going on and added more tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3373
https://github.com/broadinstitute/cromwell/pull/3373:381,Testability,test,tests,381,"The bug I mentioned this morning came from a change in the EJEA (it didn't get caught by unit tests because they're not being run, but that's another story..); They're still not being run in this PR so this is on hold until we fix this (@kshakir is on it); In the meantime I re-factored a bit the ""Running"" state of the EJEA to be able to understand what's going on and added more tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3373
https://github.com/broadinstitute/cromwell/issues/3378:756,Performance,load,load,756,"The JobStore currently remembers Successful jobs (and their outputs) and Failed jobs.; It would be very useful to also remember when jobs are started (from a backend perspective) and aborted.; Indeed, not knowing if a job has already been aborted or even started forces us to create a backend actor for every single job of a restarting workflow to reconnect to it, and if this workflow was aborting, abort the backend job. Again.; Or at least that's the issue, we don't know if it's ""again"" or not because we don't know if the job had already been aborted before restart, or even been started, so we try anyway.; This behavior while guaranteeing that we don't leave any job unaborted even after restart is suboptimal and generates unnecessary requests and load on the system.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3378
https://github.com/broadinstitute/cromwell/issues/3378:183,Safety,abort,aborted,183,"The JobStore currently remembers Successful jobs (and their outputs) and Failed jobs.; It would be very useful to also remember when jobs are started (from a backend perspective) and aborted.; Indeed, not knowing if a job has already been aborted or even started forces us to create a backend actor for every single job of a restarting workflow to reconnect to it, and if this workflow was aborting, abort the backend job. Again.; Or at least that's the issue, we don't know if it's ""again"" or not because we don't know if the job had already been aborted before restart, or even been started, so we try anyway.; This behavior while guaranteeing that we don't leave any job unaborted even after restart is suboptimal and generates unnecessary requests and load on the system.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3378
https://github.com/broadinstitute/cromwell/issues/3378:239,Safety,abort,aborted,239,"The JobStore currently remembers Successful jobs (and their outputs) and Failed jobs.; It would be very useful to also remember when jobs are started (from a backend perspective) and aborted.; Indeed, not knowing if a job has already been aborted or even started forces us to create a backend actor for every single job of a restarting workflow to reconnect to it, and if this workflow was aborting, abort the backend job. Again.; Or at least that's the issue, we don't know if it's ""again"" or not because we don't know if the job had already been aborted before restart, or even been started, so we try anyway.; This behavior while guaranteeing that we don't leave any job unaborted even after restart is suboptimal and generates unnecessary requests and load on the system.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3378
https://github.com/broadinstitute/cromwell/issues/3378:390,Safety,abort,aborting,390,"The JobStore currently remembers Successful jobs (and their outputs) and Failed jobs.; It would be very useful to also remember when jobs are started (from a backend perspective) and aborted.; Indeed, not knowing if a job has already been aborted or even started forces us to create a backend actor for every single job of a restarting workflow to reconnect to it, and if this workflow was aborting, abort the backend job. Again.; Or at least that's the issue, we don't know if it's ""again"" or not because we don't know if the job had already been aborted before restart, or even been started, so we try anyway.; This behavior while guaranteeing that we don't leave any job unaborted even after restart is suboptimal and generates unnecessary requests and load on the system.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3378
https://github.com/broadinstitute/cromwell/issues/3378:400,Safety,abort,abort,400,"The JobStore currently remembers Successful jobs (and their outputs) and Failed jobs.; It would be very useful to also remember when jobs are started (from a backend perspective) and aborted.; Indeed, not knowing if a job has already been aborted or even started forces us to create a backend actor for every single job of a restarting workflow to reconnect to it, and if this workflow was aborting, abort the backend job. Again.; Or at least that's the issue, we don't know if it's ""again"" or not because we don't know if the job had already been aborted before restart, or even been started, so we try anyway.; This behavior while guaranteeing that we don't leave any job unaborted even after restart is suboptimal and generates unnecessary requests and load on the system.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3378
https://github.com/broadinstitute/cromwell/issues/3378:548,Safety,abort,aborted,548,"The JobStore currently remembers Successful jobs (and their outputs) and Failed jobs.; It would be very useful to also remember when jobs are started (from a backend perspective) and aborted.; Indeed, not knowing if a job has already been aborted or even started forces us to create a backend actor for every single job of a restarting workflow to reconnect to it, and if this workflow was aborting, abort the backend job. Again.; Or at least that's the issue, we don't know if it's ""again"" or not because we don't know if the job had already been aborted before restart, or even been started, so we try anyway.; This behavior while guaranteeing that we don't leave any job unaborted even after restart is suboptimal and generates unnecessary requests and load on the system.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3378
https://github.com/broadinstitute/cromwell/issues/3381:442,Availability,echo,echo,442,"Using wom-tool-30.2, the following wdl validates using `java -jar womtool-30.2.jar validate test.wdl` and shows no inputs with `java -jar womtool-30.2.jar inputs test.wdl`. I think this should fail validation because the variable `to_echo` is used without being declared. . ```; workflow test_validation{; # missing variable; #String to_echo; 	call example{ input:; 		to_echo = to_echo; 	}; }. task example{; 	String to_echo; 	; 	command{; 		echo ""${to_echo}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3381
https://github.com/broadinstitute/cromwell/issues/3381:221,Modifiability,variab,variable,221,"Using wom-tool-30.2, the following wdl validates using `java -jar womtool-30.2.jar validate test.wdl` and shows no inputs with `java -jar womtool-30.2.jar inputs test.wdl`. I think this should fail validation because the variable `to_echo` is used without being declared. . ```; workflow test_validation{; # missing variable; #String to_echo; 	call example{ input:; 		to_echo = to_echo; 	}; }. task example{; 	String to_echo; 	; 	command{; 		echo ""${to_echo}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3381
https://github.com/broadinstitute/cromwell/issues/3381:316,Modifiability,variab,variable,316,"Using wom-tool-30.2, the following wdl validates using `java -jar womtool-30.2.jar validate test.wdl` and shows no inputs with `java -jar womtool-30.2.jar inputs test.wdl`. I think this should fail validation because the variable `to_echo` is used without being declared. . ```; workflow test_validation{; # missing variable; #String to_echo; 	call example{ input:; 		to_echo = to_echo; 	}; }. task example{; 	String to_echo; 	; 	command{; 		echo ""${to_echo}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3381
https://github.com/broadinstitute/cromwell/issues/3381:39,Security,validat,validates,39,"Using wom-tool-30.2, the following wdl validates using `java -jar womtool-30.2.jar validate test.wdl` and shows no inputs with `java -jar womtool-30.2.jar inputs test.wdl`. I think this should fail validation because the variable `to_echo` is used without being declared. . ```; workflow test_validation{; # missing variable; #String to_echo; 	call example{ input:; 		to_echo = to_echo; 	}; }. task example{; 	String to_echo; 	; 	command{; 		echo ""${to_echo}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3381
https://github.com/broadinstitute/cromwell/issues/3381:83,Security,validat,validate,83,"Using wom-tool-30.2, the following wdl validates using `java -jar womtool-30.2.jar validate test.wdl` and shows no inputs with `java -jar womtool-30.2.jar inputs test.wdl`. I think this should fail validation because the variable `to_echo` is used without being declared. . ```; workflow test_validation{; # missing variable; #String to_echo; 	call example{ input:; 		to_echo = to_echo; 	}; }. task example{; 	String to_echo; 	; 	command{; 		echo ""${to_echo}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3381
https://github.com/broadinstitute/cromwell/issues/3381:198,Security,validat,validation,198,"Using wom-tool-30.2, the following wdl validates using `java -jar womtool-30.2.jar validate test.wdl` and shows no inputs with `java -jar womtool-30.2.jar inputs test.wdl`. I think this should fail validation because the variable `to_echo` is used without being declared. . ```; workflow test_validation{; # missing variable; #String to_echo; 	call example{ input:; 		to_echo = to_echo; 	}; }. task example{; 	String to_echo; 	; 	command{; 		echo ""${to_echo}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3381
https://github.com/broadinstitute/cromwell/issues/3381:92,Testability,test,test,92,"Using wom-tool-30.2, the following wdl validates using `java -jar womtool-30.2.jar validate test.wdl` and shows no inputs with `java -jar womtool-30.2.jar inputs test.wdl`. I think this should fail validation because the variable `to_echo` is used without being declared. . ```; workflow test_validation{; # missing variable; #String to_echo; 	call example{ input:; 		to_echo = to_echo; 	}; }. task example{; 	String to_echo; 	; 	command{; 		echo ""${to_echo}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3381
https://github.com/broadinstitute/cromwell/issues/3381:162,Testability,test,test,162,"Using wom-tool-30.2, the following wdl validates using `java -jar womtool-30.2.jar validate test.wdl` and shows no inputs with `java -jar womtool-30.2.jar inputs test.wdl`. I think this should fail validation because the variable `to_echo` is used without being declared. . ```; workflow test_validation{; # missing variable; #String to_echo; 	call example{ input:; 		to_echo = to_echo; 	}; }. task example{; 	String to_echo; 	; 	command{; 		echo ""${to_echo}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3381
https://github.com/broadinstitute/cromwell/issues/3382:79,Usability,simpl,simply,79,CWL expresses memory in MB whereas Cromwell expects Bytes. For integer/long we simply multiply but for strings and expressions it's not so straightforward.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3382
https://github.com/broadinstitute/cromwell/issues/3383:451,Availability,ERROR,ERROR,451,"What I started to get from time to time is that some jobs keep running forever while no files are created in its executions folder (incl. technical files like script and script.submit). If happens (althoug not deterministically, I cannot get 100% reproducibility) in the latest versions (like cromwell-31-d84be2e-SNAP.jar) of cromwell when I have folders as File inputs for both workflow and tasks.; When I looked into cromwell logs I noticed:; ```; [ERROR] [03/09/2018 00:55:25.108] [cromwell-system-akka.dispatchers.engine-dispatcher-27] [akka://cromwell-system/user/cromwell-service/WorkflowManager; Mar 09 00:55:25 web start-cromwell.sh[110916]: java.io.IOException: Is a directory; Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileDispatcherImpl.read0(Native Method); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.IOUtil.read(IOUtil.java:197); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.code",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:5430,Availability,echo,echo,5430,"eive$(Actor.scala:512); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.ActorCell.invoke(ActorCell.scala:496); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.Mailbox.run(Mailbox.scala:224); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.Mailbox.exec(Mailbox.scala:234); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; the minimal workflow that caused this (although not all 100% of the time) is:; ```; workflow quantification_broken {. File batch; File references; File samples_folder #folder provided as File, before now it worked perfectly. Boolean keep_sra = true; Int threads. call prepare_samples {; input: samples = batch, references = references, samples_folder = samples_folder; }; }. task prepare_samples {; File samples; File references; File samples_folder. command {; echo ""whatever command you want""; }. output {; File invalid = ""invalid.tsv""; File novel = ""novel.tsv""; File cached = ""cached.tsv""; }; }; ```; I understand that there is plan to introduce Directory type in the future. But it may take long ago and it is better not to break current workflows where people have to use File for Directory inputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:1648,Deployability,update,updateDigest,1648,Exception: Is a directory; Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileDispatcherImpl.read0(Native Method); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.IOUtil.read(IOUtil.java:197); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2128,Modifiability,config,config,2128,well.sh[110916]: at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(Config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2168,Modifiability,Config,ConfigHashingStrategy,2168,elImpl.read(FileChannelImpl.java:159); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2624,Modifiability,config,config,2624,c.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2653,Modifiability,Config,ConfigHashingStrategy,2653,DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2762,Modifiability,config,config,2762,.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive(Actor.scala:514); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.arou,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2769,Modifiability,Config,ConfigHashingStrategy,2769,gest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive(Actor.scala:514); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive$(Act,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2815,Modifiability,Config,ConfigHashingStrategy,2815,00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive(Actor.scala:514); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive$(Actor.scala:512); Mar 09 00:55:25 web,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2924,Modifiability,config,config,2924, Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive(Actor.scala:514); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive$(Actor.scala:512); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(Standa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2931,Modifiability,Config,ConfigHashingStrategy,2931,5 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive(Actor.scala:514); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive$(Actor.scala:512); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingA,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2961,Modifiability,Config,ConfigHashingStrategy,2961,0916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive(Actor.scala:514); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive$(Actor.scala:512); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); Mar 09 00:,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:3070,Modifiability,config,config,3070,tart-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive(Actor.scala:514); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive$(Actor.scala:512); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); Mar 09 0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:3077,Modifiability,Config,ConfigBackendFileHashingActor,3077,0916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive(Actor.scala:514); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive$(Actor.scala:512); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); Mar 09 00:55:25 web start-,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:3126,Modifiability,Config,ConfigBackendFileHashingActor,3126,fig.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive(Actor.scala:514); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor.aroundReceive$(Actor.scala:512); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:5538,Performance,cache,cached,5538,"eive$(Actor.scala:512); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.ActorCell.invoke(ActorCell.scala:496); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.Mailbox.run(Mailbox.scala:224); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.Mailbox.exec(Mailbox.scala:234); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; the minimal workflow that caused this (although not all 100% of the time) is:; ```; workflow quantification_broken {. File batch; File references; File samples_folder #folder provided as File, before now it worked perfectly. Boolean keep_sra = true; Int threads. call prepare_samples {; input: samples = batch, references = references, samples_folder = samples_folder; }; }. task prepare_samples {; File samples; File references; File samples_folder. command {; echo ""whatever command you want""; }. output {; File invalid = ""invalid.tsv""; File novel = ""novel.tsv""; File cached = ""cached.tsv""; }; }; ```; I understand that there is plan to introduce Directory type in the future. But it may take long ago and it is better not to break current workflows where people have to use File for Directory inputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:5548,Performance,cache,cached,5548,"eive$(Actor.scala:512); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.ActorCell.invoke(ActorCell.scala:496); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.Mailbox.run(Mailbox.scala:224); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.Mailbox.exec(Mailbox.scala:234); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; the minimal workflow that caused this (although not all 100% of the time) is:; ```; workflow quantification_broken {. File batch; File references; File samples_folder #folder provided as File, before now it worked perfectly. Boolean keep_sra = true; Int threads. call prepare_samples {; input: samples = batch, references = references, samples_folder = samples_folder; }; }. task prepare_samples {; File samples; File references; File samples_folder. command {; echo ""whatever command you want""; }. output {; File invalid = ""invalid.tsv""; File novel = ""novel.tsv""; File cached = ""cached.tsv""; }; }; ```; I understand that there is plan to introduce Directory type in the future. But it may take long ago and it is better not to break current workflows where people have to use File for Directory inputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2135,Security,Hash,HashFileStrategy,2135,16]: at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileH,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2161,Security,hash,hash,2161,elImpl.read(FileChannelImpl.java:159); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2631,Security,Hash,HashFileStrategy,2631,estUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:2648,Security,hash,hash,2648,DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:52); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:73); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.Actor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3383:428,Testability,log,logs,428,"What I started to get from time to time is that some jobs keep running forever while no files are created in its executions folder (incl. technical files like script and script.submit). If happens (althoug not deterministically, I cannot get 100% reproducibility) in the latest versions (like cromwell-31-d84be2e-SNAP.jar) of cromwell when I have folders as File inputs for both workflow and tasks.; When I looked into cromwell logs I noticed:; ```; [ERROR] [03/09/2018 00:55:25.108] [cromwell-system-akka.dispatchers.engine-dispatcher-27] [akka://cromwell-system/user/cromwell-service/WorkflowManager; Mar 09 00:55:25 web start-cromwell.sh[110916]: java.io.IOException: Is a directory; Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileDispatcherImpl.read0(Native Method); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.IOUtil.read(IOUtil.java:197); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.code",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383
https://github.com/broadinstitute/cromwell/issues/3384:475,Availability,error,error,475,"Reported by @asmirnov239. Cromwell 30.2 doesn't seem to short-circuit for logical operators so that the following fails to evaluate when `optional_int` is undefined:. ```wdl ; Integer? optional_int; if (defined(optional_int) && select_first([optional_int]) == 2 ) {; #expresssions to do if optional_int is defined and equal to 2; }; ```. expected behavior: if `optional_int` is undefined, if statement evaluates to false. ; actual behavior: cromwell fails and stops with the error: . >Evaluating defined(gc_low_high_filter_params) && select_first([gc_low_high_filter_params]) == 2 failed: select_first failed. All provided values were empty. While it hasn't been tested, I presume that cromwell doesn't short-circut for the `||` operator...but it should!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3384
https://github.com/broadinstitute/cromwell/issues/3384:74,Testability,log,logical,74,"Reported by @asmirnov239. Cromwell 30.2 doesn't seem to short-circuit for logical operators so that the following fails to evaluate when `optional_int` is undefined:. ```wdl ; Integer? optional_int; if (defined(optional_int) && select_first([optional_int]) == 2 ) {; #expresssions to do if optional_int is defined and equal to 2; }; ```. expected behavior: if `optional_int` is undefined, if statement evaluates to false. ; actual behavior: cromwell fails and stops with the error: . >Evaluating defined(gc_low_high_filter_params) && select_first([gc_low_high_filter_params]) == 2 failed: select_first failed. All provided values were empty. While it hasn't been tested, I presume that cromwell doesn't short-circut for the `||` operator...but it should!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3384
https://github.com/broadinstitute/cromwell/issues/3384:663,Testability,test,tested,663,"Reported by @asmirnov239. Cromwell 30.2 doesn't seem to short-circuit for logical operators so that the following fails to evaluate when `optional_int` is undefined:. ```wdl ; Integer? optional_int; if (defined(optional_int) && select_first([optional_int]) == 2 ) {; #expresssions to do if optional_int is defined and equal to 2; }; ```. expected behavior: if `optional_int` is undefined, if statement evaluates to false. ; actual behavior: cromwell fails and stops with the error: . >Evaluating defined(gc_low_high_filter_params) && select_first([gc_low_high_filter_params]) == 2 failed: select_first failed. All provided values were empty. While it hasn't been tested, I presume that cromwell doesn't short-circut for the `||` operator...but it should!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3384
https://github.com/broadinstitute/cromwell/pull/3385:148,Modifiability,variab,variables,148,Using a reflection-modified RhinoSandbox to do stricter JS eval.; Fixed other IntelliJ warnings while disabling the one for multiple uses of shadow variables.; Concatenating an OSGI properties file that was causing assembly conflicts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3385
https://github.com/broadinstitute/cromwell/issues/3387:69,Availability,error,error,69,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:1995,Availability,error,error,1995,"ove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.Abst",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:5649,Availability,error,error,5649,.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActio,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:5655,Availability,Error,Error,5655,ch(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionCompo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:5661,Availability,error,error,5661,929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsert,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:10076,Availability,error,error,10076,om.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:11144,Availability,error,error,11144,"onfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-03-09 15:52:58,29] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:14798,Availability,error,error,14798,.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActio,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:14804,Availability,Error,Error,14804,ch(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionCompo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:14810,Availability,error,error,14810,929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsert,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:19076,Availability,error,error,19076,om.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:339,Deployability,configurat,configuration,339,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9041,Deployability,update,updateCache,9041,"InsertAction.run(JdbcActionComponent.scala:527). 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18041,Deployability,update,updateCache,18041,"onComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:2624,Energy Efficiency,adapt,adapted,2624,"""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:4592,Energy Efficiency,adapt,adapted,4592,pl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:5711,Energy Efficiency,allocate,allocateLobForResult,5711,a:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:5772,Energy Efficiency,allocate,allocateResultLob,5772,tor.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$Mu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:6264,Energy Efficiency,allocate,allocateLobForResult,6264,nd$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(I,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:6325,Energy Efficiency,allocate,allocateResultLob,6325,a.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:6826,Energy Efficiency,adapt,adapted,6826,reparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1954/1995472759.apply(Unknown Source); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSessi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:11773,Energy Efficiency,adapt,adapted,11773,".exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-03-09 15:52:58,29] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:13741,Energy Efficiency,adapt,adapted,13741,pl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:14860,Energy Efficiency,allocate,allocateLobForResult,14860,a:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:14921,Energy Efficiency,allocate,allocateResultLob,14921,tor.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$Mu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:15413,Energy Efficiency,allocate,allocateLobForResult,15413,nd$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71). 	at scala.collection.IterableLike.foreach$(I,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:15474,Energy Efficiency,allocate,allocateResultLob,15474,a.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71). 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:15975,Energy Efficiency,adapt,adapted,15975,reparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71). 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1954/1995472759.apply(Unknown Source); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSessi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:158,Integrability,wrap,wrapper,158,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:339,Modifiability,config,configuration,339,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:731,Modifiability,config,config,731,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:2624,Modifiability,adapt,adapted,2624,"""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:4592,Modifiability,adapt,adapted,4592,pl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:6826,Modifiability,adapt,adapted,6826,reparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1954/1995472759.apply(Unknown Source); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSessi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:11773,Modifiability,adapt,adapted,11773,".exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-03-09 15:52:58,29] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:13741,Modifiability,adapt,adapted,13741,pl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:15975,Modifiability,adapt,adapted,15975,reparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71). 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1954/1995472759.apply(Unknown Source); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSessi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:770,Performance,concurren,concurrent-job-limit,770,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:5353,Performance,concurren,concurrent,5353,	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:5438,Performance,concurren,concurrent,5438,usDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:5849,Performance,perform,performPreExecute,5849,ala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:6402,Performance,perform,performPreExecute,6402, 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:8887,Performance,concurren,concurrent,8887,"dbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527). 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:8936,Performance,concurren,concurrent,8936,"preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527). 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAnd",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:14502,Performance,concurren,concurrent,14502,	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:14587,Performance,concurren,concurrent,14587,usDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:14998,Performance,perform,performPreExecute,14998,ala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:15551,Performance,perform,performPreExecute,15551, 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71). 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:17887,Performance,concurren,concurrent,17887,"onDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:17936,Performance,concurren,concurrent,17936,"7); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAnd",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:1209,Safety,timeout,timeout,1209,"low-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:8898,Safety,Timeout,TimeoutException,8898,"t$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527). 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:17898,Safety,Timeout,TimeoutException,17898,"ement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:448,Testability,log,logs,448,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:480,Testability,log,log-temporary,480,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:528,Testability,log,log-dir,528,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:607,Testability,log,logs,607,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:1926,Testability,log,log,1926,"ot allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.Iter",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9354,Testability,log,logback,9354,"0); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9438,Testability,log,logback,9438,"	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Act",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9518,Testability,log,logback,9518,"Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9584,Testability,log,logback,9584,"ctIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9653,Testability,log,logback,9653,"rn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9759,Testability,log,logback,9759," send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9775,Testability,Log,Logger,9775,"[2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9804,Testability,Log,Logger,9804,"48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9833,Testability,log,logback,9833,"ailed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9849,Testability,Log,Logger,9849,the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(F,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9870,Testability,Log,Logger,9870,vailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.ja,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9899,Testability,log,logback,9899,utException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9915,Testability,Log,Logger,9915,ll; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$Wor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9949,Testability,Log,Logger,9949,current.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9978,Testability,log,logback,9978,; 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkj,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:9994,Testability,Log,Logger,9994,ntry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:10024,Testability,Log,Logger,10024,entBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:10053,Testability,log,logback,10053,tBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkj,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:10069,Testability,Log,Logger,10069,91); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWork,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:10082,Testability,Log,Logger,10082,om.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18354,Testability,log,logback,18354,"c.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18438,Testability,log,logback,18438," slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Act",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18518,Testability,log,logback,18518,"bio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18584,Testability,log,logback,18584,"); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18653,Testability,log,logback,18653,"la.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18759,Testability,log,logback,18759,"r.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18775,Testability,Log,Logger,18775,"[2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18804,Testability,Log,Logger,18804,"36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18833,Testability,log,logback,18833,"ailed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18849,Testability,Log,Logger,18849,the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(F,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18870,Testability,Log,Logger,18870,vailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.ja,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18899,Testability,log,logback,18899,utException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18915,Testability,Log,Logger,18915,ll; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$Wor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18949,Testability,Log,Logger,18949,current.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18978,Testability,log,logback,18978,; 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkj,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:18994,Testability,Log,Logger,18994,ntry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:19024,Testability,Log,Logger,19024,entBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:19053,Testability,log,logback,19053,tBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkj,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:19069,Testability,Log,Logger,19069,91); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWork,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:19082,Testability,Log,Logger,19082,om.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:3686,Usability,Simpl,SimpleJdbcProfileAction,3686,rator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:3780,Usability,Simpl,SimpleJdbcProfileAction,3780, 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:8137,Usability,Simpl,SimpleJdbcProfileAction,8137,"terator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1954/1995472759.apply(Unknown Source); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527). 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:8231,Usability,Simpl,SimpleJdbcProfileAction,8231,"cala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1954/1995472759.apply(Unknown Source); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527). 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:12835,Usability,Simpl,SimpleJdbcProfileAction,12835,rator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:12929,Usability,Simpl,SimpleJdbcProfileAction,12929, 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:17286,Usability,Simpl,SimpleJdbcProfileAction,17286,"terator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71). 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1954/1995472759.apply(Unknown Source); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3387:17380,Usability,Simpl,SimpleJdbcProfileAction,17380,"cala:71). 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1954/1995472759.apply(Unknown Source); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.build",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387
https://github.com/broadinstitute/cromwell/issues/3391:11,Availability,error,error,11,"Here's the error message:; ```; ""Unknown status"" did not start with ""Timed out"" (HealthMonitorServiceActorSpec.scala:76; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3391
https://github.com/broadinstitute/cromwell/issues/3391:17,Integrability,message,message,17,"Here's the error message:; ```; ""Unknown status"" did not start with ""Timed out"" (HealthMonitorServiceActorSpec.scala:76; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3391
https://github.com/broadinstitute/cromwell/issues/3392:135,Availability,error,error,135,"The 'should' string is:; ```; should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED ***; ```. The error message is:; ```; Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3392
https://github.com/broadinstitute/cromwell/issues/3392:141,Integrability,message,message,141,"The 'should' string is:; ```; should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED ***; ```. The error message is:; ```; Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3392
https://github.com/broadinstitute/cromwell/issues/3392:37,Safety,abort,abort,37,"The 'should' string is:; ```; should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED ***; ```. The error message is:; ```; Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3392
https://github.com/broadinstitute/cromwell/issues/3392:86,Safety,abort,abort,86,"The 'should' string is:; ```; should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED ***; ```. The error message is:; ```; Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3392
https://github.com/broadinstitute/cromwell/issues/3392:203,Safety,abort,aborted,203,"The 'should' string is:; ```; should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED ***; ```. The error message is:; ```; Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3392
https://github.com/broadinstitute/cromwell/issues/3392:258,Safety,Abort,Aborted,258,"The 'should' string is:; ```; should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED ***; ```. The error message is:; ```; Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3392
https://github.com/broadinstitute/cromwell/pull/3399:142,Availability,heartbeat,heartbeats,142,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3399
https://github.com/broadinstitute/cromwell/pull/3399:238,Availability,heartbeat,heartbeat,238,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3399
https://github.com/broadinstitute/cromwell/pull/3399:778,Deployability,update,update,778,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3399
https://github.com/broadinstitute/cromwell/pull/3399:30,Modifiability,config,config,30,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3399
https://github.com/broadinstitute/cromwell/pull/3399:116,Usability,Clear,Clears,116,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3399
https://github.com/broadinstitute/cromwell/pull/3401:161,Availability,heartbeat,heartbeats,161,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a randomly generated UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like the following when using the MySQL profile: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3401
https://github.com/broadinstitute/cromwell/pull/3401:257,Availability,heartbeat,heartbeat,257,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a randomly generated UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like the following when using the MySQL profile: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3401
https://github.com/broadinstitute/cromwell/pull/3401:840,Deployability,update,update,840,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a randomly generated UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like the following when using the MySQL profile: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3401
https://github.com/broadinstitute/cromwell/pull/3401:30,Modifiability,config,config,30,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a randomly generated UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like the following when using the MySQL profile: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3401
https://github.com/broadinstitute/cromwell/pull/3401:135,Usability,Clear,Clears,135,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a randomly generated UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like the following when using the MySQL profile: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3401
https://github.com/broadinstitute/cromwell/pull/3403:52,Testability,test,test,52,- [x] Rebase onto develop after #3395 . * A centaur test to cover intermingling nested `if`s and `scatter`s in draft 3.; * Required the addition of a few new evaluators for the range engine function,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3403
https://github.com/broadinstitute/cromwell/pull/3404:179,Deployability,update,updated,179,While testing C31 perf I noticed the statsd service can get very slow which translates in metrics being largely out of dates.; I traced this to an issue with how gauges are being updated.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3404
https://github.com/broadinstitute/cromwell/pull/3404:6,Testability,test,testing,6,While testing C31 perf I noticed the statsd service can get very slow which translates in metrics being largely out of dates.; I traced this to an issue with how gauges are being updated.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3404
https://github.com/broadinstitute/cromwell/issues/3410:249,Modifiability,Enhance,EnhancedRhinoSandbox,249,Re-armor the Rhino in a way that plays nice with real world CWLs like Wash U rnaseq and doesn't do this:. ```; Caused by: java.lang.IllegalStateException; at org.mozilla.javascript.ContextFactory.initGlobal(ContextFactory.java:177); at cwl.internal.EnhancedRhinoSandbox.assertContextFactory(EnhancedRhinoSandbox.scala:94); at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:42); at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:71); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3410
https://github.com/broadinstitute/cromwell/issues/3410:291,Modifiability,Enhance,EnhancedRhinoSandbox,291,Re-armor the Rhino in a way that plays nice with real world CWLs like Wash U rnaseq and doesn't do this:. ```; Caused by: java.lang.IllegalStateException; at org.mozilla.javascript.ContextFactory.initGlobal(ContextFactory.java:177); at cwl.internal.EnhancedRhinoSandbox.assertContextFactory(EnhancedRhinoSandbox.scala:94); at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:42); at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:71); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3410
https://github.com/broadinstitute/cromwell/issues/3410:339,Modifiability,Enhance,EnhancedRhinoSandbox,339,Re-armor the Rhino in a way that plays nice with real world CWLs like Wash U rnaseq and doesn't do this:. ```; Caused by: java.lang.IllegalStateException; at org.mozilla.javascript.ContextFactory.initGlobal(ContextFactory.java:177); at cwl.internal.EnhancedRhinoSandbox.assertContextFactory(EnhancedRhinoSandbox.scala:94); at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:42); at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:71); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3410
https://github.com/broadinstitute/cromwell/issues/3410:365,Modifiability,Enhance,EnhancedRhinoSandbox,365,Re-armor the Rhino in a way that plays nice with real world CWLs like Wash U rnaseq and doesn't do this:. ```; Caused by: java.lang.IllegalStateException; at org.mozilla.javascript.ContextFactory.initGlobal(ContextFactory.java:177); at cwl.internal.EnhancedRhinoSandbox.assertContextFactory(EnhancedRhinoSandbox.scala:94); at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:42); at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:71); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3410
https://github.com/broadinstitute/cromwell/issues/3410:270,Testability,assert,assertContextFactory,270,Re-armor the Rhino in a way that plays nice with real world CWLs like Wash U rnaseq and doesn't do this:. ```; Caused by: java.lang.IllegalStateException; at org.mozilla.javascript.ContextFactory.initGlobal(ContextFactory.java:177); at cwl.internal.EnhancedRhinoSandbox.assertContextFactory(EnhancedRhinoSandbox.scala:94); at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:42); at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:71); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3410
https://github.com/broadinstitute/cromwell/pull/3413:28,Testability,test,test,28,- Ports an existing draft 2 test of taskless engine functions into draft 3.; - Also adds implementation or placeholders for all of the type evaluation and value evaluation for engine functions (so apologies about the high line count!). - [x] Rebase on develop after #3403 . Red thumb requirement triggered by the addition of a `WomTypeCoercer` typeclass. I added it so that I didn't have to keep casting results but I think this could be even useful-er if CWL and WDL coercions start to (or need to) diverge.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3413
https://github.com/broadinstitute/cromwell/pull/3414:23,Deployability,patch,patch,23,"Bump to [~3.2.1~ 3.2.2 patch version](http://slick.lightbend.com/news/2017/07/20/slick-3.2.1-released.html) that may help with this problem:. ```; Exception in thread ""db-10602"" java.lang.IllegalArgumentException: requirement failed: count cannot be decreased; at scala.Predef$.require(Predef.scala:277); at slick.util.ManagedArrayBlockingQueue.$anonfun$decreaseInUseCount$1(ManagedArrayBlockingQueue.scala:53); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at slick.util.ManagedArrayBlockingQueue.slick$util$ManagedArrayBlockingQueue$$locked(ManagedArrayBlockingQueue.scala:217); at slick.util.ManagedArrayBlockingQueue.decreaseInUseCount(ManagedArrayBlockingQueue.scala:52); at slick.util.AsyncExecutor$$anon$2$$anon$1.afterExecute(AsyncExecutor.scala:87); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```. Most of the changes in this PR are for the default length of strings in Slick going from 255 to 16777216 in this patch release (!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3414
https://github.com/broadinstitute/cromwell/pull/3414:93,Deployability,release,released,93,"Bump to [~3.2.1~ 3.2.2 patch version](http://slick.lightbend.com/news/2017/07/20/slick-3.2.1-released.html) that may help with this problem:. ```; Exception in thread ""db-10602"" java.lang.IllegalArgumentException: requirement failed: count cannot be decreased; at scala.Predef$.require(Predef.scala:277); at slick.util.ManagedArrayBlockingQueue.$anonfun$decreaseInUseCount$1(ManagedArrayBlockingQueue.scala:53); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at slick.util.ManagedArrayBlockingQueue.slick$util$ManagedArrayBlockingQueue$$locked(ManagedArrayBlockingQueue.scala:217); at slick.util.ManagedArrayBlockingQueue.decreaseInUseCount(ManagedArrayBlockingQueue.scala:52); at slick.util.AsyncExecutor$$anon$2$$anon$1.afterExecute(AsyncExecutor.scala:87); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```. Most of the changes in this PR are for the default length of strings in Slick going from 255 to 16777216 in this patch release (!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3414
https://github.com/broadinstitute/cromwell/pull/3414:1116,Deployability,patch,patch,1116,"Bump to [~3.2.1~ 3.2.2 patch version](http://slick.lightbend.com/news/2017/07/20/slick-3.2.1-released.html) that may help with this problem:. ```; Exception in thread ""db-10602"" java.lang.IllegalArgumentException: requirement failed: count cannot be decreased; at scala.Predef$.require(Predef.scala:277); at slick.util.ManagedArrayBlockingQueue.$anonfun$decreaseInUseCount$1(ManagedArrayBlockingQueue.scala:53); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at slick.util.ManagedArrayBlockingQueue.slick$util$ManagedArrayBlockingQueue$$locked(ManagedArrayBlockingQueue.scala:217); at slick.util.ManagedArrayBlockingQueue.decreaseInUseCount(ManagedArrayBlockingQueue.scala:52); at slick.util.AsyncExecutor$$anon$2$$anon$1.afterExecute(AsyncExecutor.scala:87); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```. Most of the changes in this PR are for the default length of strings in Slick going from 255 to 16777216 in this patch release (!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3414
https://github.com/broadinstitute/cromwell/pull/3414:1122,Deployability,release,release,1122,"Bump to [~3.2.1~ 3.2.2 patch version](http://slick.lightbend.com/news/2017/07/20/slick-3.2.1-released.html) that may help with this problem:. ```; Exception in thread ""db-10602"" java.lang.IllegalArgumentException: requirement failed: count cannot be decreased; at scala.Predef$.require(Predef.scala:277); at slick.util.ManagedArrayBlockingQueue.$anonfun$decreaseInUseCount$1(ManagedArrayBlockingQueue.scala:53); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at slick.util.ManagedArrayBlockingQueue.slick$util$ManagedArrayBlockingQueue$$locked(ManagedArrayBlockingQueue.scala:217); at slick.util.ManagedArrayBlockingQueue.decreaseInUseCount(ManagedArrayBlockingQueue.scala:52); at slick.util.AsyncExecutor$$anon$2$$anon$1.afterExecute(AsyncExecutor.scala:87); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```. Most of the changes in this PR are for the default length of strings in Slick going from 255 to 16777216 in this patch release (!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3414
https://github.com/broadinstitute/cromwell/pull/3414:801,Performance,concurren,concurrent,801,"Bump to [~3.2.1~ 3.2.2 patch version](http://slick.lightbend.com/news/2017/07/20/slick-3.2.1-released.html) that may help with this problem:. ```; Exception in thread ""db-10602"" java.lang.IllegalArgumentException: requirement failed: count cannot be decreased; at scala.Predef$.require(Predef.scala:277); at slick.util.ManagedArrayBlockingQueue.$anonfun$decreaseInUseCount$1(ManagedArrayBlockingQueue.scala:53); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at slick.util.ManagedArrayBlockingQueue.slick$util$ManagedArrayBlockingQueue$$locked(ManagedArrayBlockingQueue.scala:217); at slick.util.ManagedArrayBlockingQueue.decreaseInUseCount(ManagedArrayBlockingQueue.scala:52); at slick.util.AsyncExecutor$$anon$2$$anon$1.afterExecute(AsyncExecutor.scala:87); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```. Most of the changes in this PR are for the default length of strings in Slick going from 255 to 16777216 in this patch release (!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3414
https://github.com/broadinstitute/cromwell/pull/3414:885,Performance,concurren,concurrent,885,"Bump to [~3.2.1~ 3.2.2 patch version](http://slick.lightbend.com/news/2017/07/20/slick-3.2.1-released.html) that may help with this problem:. ```; Exception in thread ""db-10602"" java.lang.IllegalArgumentException: requirement failed: count cannot be decreased; at scala.Predef$.require(Predef.scala:277); at slick.util.ManagedArrayBlockingQueue.$anonfun$decreaseInUseCount$1(ManagedArrayBlockingQueue.scala:53); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at slick.util.ManagedArrayBlockingQueue.slick$util$ManagedArrayBlockingQueue$$locked(ManagedArrayBlockingQueue.scala:217); at slick.util.ManagedArrayBlockingQueue.decreaseInUseCount(ManagedArrayBlockingQueue.scala:52); at slick.util.AsyncExecutor$$anon$2$$anon$1.afterExecute(AsyncExecutor.scala:87); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```. Most of the changes in this PR are for the default length of strings in Slick going from 255 to 16777216 in this patch release (!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3414
https://github.com/broadinstitute/cromwell/issues/3415:164,Deployability,pipeline,pipeline,164,"I am experimenting by running some workflows with a MySQL database to avoid problems like #3387, but after a successful run and several days without re-running the pipeline (or a similar one) I would like to clean up the database to free some space. I would appreciate if this is included in the cromwell documentation...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3415
https://github.com/broadinstitute/cromwell/issues/3415:70,Safety,avoid,avoid,70,"I am experimenting by running some workflows with a MySQL database to avoid problems like #3387, but after a successful run and several days without re-running the pipeline (or a similar one) I would like to clean up the database to free some space. I would appreciate if this is included in the cromwell documentation...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3415
https://github.com/broadinstitute/cromwell/issues/3417:38,Availability,failure,failures,38,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417
https://github.com/broadinstitute/cromwell/issues/3417:259,Availability,failure,failure,259,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417
https://github.com/broadinstitute/cromwell/issues/3417:376,Availability,failure,failures,376,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417
https://github.com/broadinstitute/cromwell/issues/3417:753,Availability,failure,failure,753,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417
https://github.com/broadinstitute/cromwell/issues/3417:275,Deployability,pipeline,pipeline,275,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417
https://github.com/broadinstitute/cromwell/issues/3417:353,Deployability,pipeline,pipeline,353,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417
https://github.com/broadinstitute/cromwell/issues/3417:484,Deployability,configurat,configuration,484,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417
https://github.com/broadinstitute/cromwell/issues/3417:728,Deployability,pipeline,pipeline,728,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417
https://github.com/broadinstitute/cromwell/issues/3417:484,Modifiability,config,configuration,484,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417
https://github.com/broadinstitute/cromwell/issues/3417:409,Safety,avoid,avoid,409,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417
https://github.com/broadinstitute/cromwell/issues/3417:461,Safety,avoid,avoided,461,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417
https://github.com/broadinstitute/cromwell/issues/3417:703,Safety,avoid,avoid,703,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417
https://github.com/broadinstitute/cromwell/pull/3419:69,Testability,test,tests,69,The reasons for doing this are chiefly:. * We now have centaur which tests much of this for us; * Doing a giant diff of this vs centaur is untenable; * It doesn't test JES; * We plan to reproduce the valuable single workflow runner aspect of this in a travis build (coming soon); * It is flaky as hell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3419
https://github.com/broadinstitute/cromwell/pull/3419:163,Testability,test,test,163,The reasons for doing this are chiefly:. * We now have centaur which tests much of this for us; * Doing a giant diff of this vs centaur is untenable; * It doesn't test JES; * We plan to reproduce the valuable single workflow runner aspect of this in a travis build (coming soon); * It is flaky as hell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3419
https://github.com/broadinstitute/cromwell/issues/3420:21,Performance,cache,cache,21,"If we bump the ""call cache"" check from job-run time up to call-run time, we might be able to enable cool things like:. * We already ran this subworkflow!; * We don't need to store intermediate files for subworkflow tasks!; * We can call cache expression tools!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3420
https://github.com/broadinstitute/cromwell/issues/3420:237,Performance,cache,cache,237,"If we bump the ""call cache"" check from job-run time up to call-run time, we might be able to enable cool things like:. * We already ran this subworkflow!; * We don't need to store intermediate files for subworkflow tasks!; * We can call cache expression tools!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3420
https://github.com/broadinstitute/cromwell/issues/3421:403,Availability,echo,echo,403,"I noticed that cromwell generates exec.sh setting HOME presumably incorrectly.; This is from GATK4's somatic CNV workflow executed by cromwell 31 with JES backend. Thanks!. ```; #!/bin/bash; tmpDir=$(; set -e; cd /cromwell_root; tmpDir=""$(mkdir -p ""/cromwell_root/infant-all-cnv-out/cromwell-execution/CNVSomaticPairWorkflow/aa10c071-5c42-4f6a-a4e0-3ba96cc54283/call-ModelSegmentsTumor/tmp.3efcbfd1"" && echo ""/cromwell_root/infant-all-cnv-out/cromwell-execution/CNVSomaticPairWorkflow/aa10c071-5c42-4f6a-a4e0-3ba96cc54283/call-ModelSegmentsTumor/tmp.3efcbfd1"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME""; (; cd /cromwell_root. ); ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3421
https://github.com/broadinstitute/cromwell/issues/3421:563,Availability,echo,echo,563,"I noticed that cromwell generates exec.sh setting HOME presumably incorrectly.; This is from GATK4's somatic CNV workflow executed by cromwell 31 with JES backend. Thanks!. ```; #!/bin/bash; tmpDir=$(; set -e; cd /cromwell_root; tmpDir=""$(mkdir -p ""/cromwell_root/infant-all-cnv-out/cromwell-execution/CNVSomaticPairWorkflow/aa10c071-5c42-4f6a-a4e0-3ba96cc54283/call-ModelSegmentsTumor/tmp.3efcbfd1"" && echo ""/cromwell_root/infant-all-cnv-out/cromwell-execution/CNVSomaticPairWorkflow/aa10c071-5c42-4f6a-a4e0-3ba96cc54283/call-ModelSegmentsTumor/tmp.3efcbfd1"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME""; (; cd /cromwell_root. ); ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3421
https://github.com/broadinstitute/cromwell/pull/3424:29,Integrability,message,message,29,"This `RequestJesPollingWork` message needs to be handled in priority, otherwise if the JES API Manager get flooded with requests, it will take a long time to get to it so we can do work, hence reducing the throughput of working actually being done, hence making the queue even larger etc..",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3424
https://github.com/broadinstitute/cromwell/pull/3424:206,Performance,throughput,throughput,206,"This `RequestJesPollingWork` message needs to be handled in priority, otherwise if the JES API Manager get flooded with requests, it will take a long time to get to it so we can do work, hence reducing the throughput of working actually being done, hence making the queue even larger etc..",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3424
https://github.com/broadinstitute/cromwell/pull/3424:266,Performance,queue,queue,266,"This `RequestJesPollingWork` message needs to be handled in priority, otherwise if the JES API Manager get flooded with requests, it will take a long time to get to it so we can do work, hence reducing the throughput of working actually being done, hence making the queue even larger etc..",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3424
https://github.com/broadinstitute/cromwell/issues/3426:111,Testability,test,test,111,This meta issue will serve to track Implementation of AwsBatchBackendLifecycleActorFactory. The following (non-test) classes are required:. - [x] AwsBatchBackendLifecycleActorFactory; - [x] AwsBatchInitializationActor; - [x] AwsBatchAsyncBackendJobExecutionActor; - [x] AwsBatchFinalizationActor; - [x] AwsBatchAsyncBackendJobExecutionActor; - [x] AwsBatchConfiguration; - [x] AwsBatchInitializationActorParams; - [x] AwsBatchFinalizationActorParams; - [x] AwsBatchBackendCacheHitCopyingActor; - [x] AwsBatchBackendSingletonActor; - [x] AwsBatchBackendFileHashingActor; - [x] AwsBatchBackendInitializationData,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3426
https://github.com/broadinstitute/cromwell/issues/3427:46,Integrability,depend,dependency,46,Placeholder for S3 filesystem which will be a dependency of the backend core support #3426,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3427
https://github.com/broadinstitute/cromwell/pull/3430:56,Availability,checkpoint,checkpoint,56,"Still driving on the Wash U workflow, this is a pregull checkpoint with some ~hacks~ work I did for blockers:. We round trip inputs YAML through Circe which uses its own ""Big"" number types that stringify with scientific notation. Our existing WomInteger / WomLong code didn't deal with that so the changes here add some flexibility. CWL explicitly allows filled optional to non-optional assignments with warnings, with an error at runtime if it turns out the optional wasn't actually filled. I expect some discussion on this 🙂",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3430
https://github.com/broadinstitute/cromwell/pull/3430:422,Availability,error,error,422,"Still driving on the Wash U workflow, this is a pregull checkpoint with some ~hacks~ work I did for blockers:. We round trip inputs YAML through Circe which uses its own ""Big"" number types that stringify with scientific notation. Our existing WomInteger / WomLong code didn't deal with that so the changes here add some flexibility. CWL explicitly allows filled optional to non-optional assignments with warnings, with an error at runtime if it turns out the optional wasn't actually filled. I expect some discussion on this 🙂",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3430
https://github.com/broadinstitute/cromwell/pull/3431:58,Integrability,interface,interface,58,"Involved a little bit of shuffling of the LanguageFactory interface, and there's a lot of noise caused by renaming the `ImportResolver` from draft 2 (so that I could use the name myself for a more general Kleisli version). There's an open question of how exactly to define where to look for `""structs.wdl""` for example (right now it's the language but maybe it should be the engine?). - [x] Rebase on develop after #3413",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3431
https://github.com/broadinstitute/cromwell/pull/3434:256,Security,access,accessed,256,"Closes #3297. If you unzip a zip file containing a directory, don't assume the user wants to reference things from within that directory. Especially don't ignore the fact that there might be *other* directories in the zip file as well, which now cannot be accessed at all",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3434
https://github.com/broadinstitute/cromwell/pull/3440:30,Testability,test,test,30,Now newly possible:; ```; sbt test; ```. (previously); ```; sbt -mem 4096 test; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3440
https://github.com/broadinstitute/cromwell/pull/3440:74,Testability,test,test,74,Now newly possible:; ```; sbt test; ```. (previously); ```; sbt -mem 4096 test; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3440
https://github.com/broadinstitute/cromwell/issues/3441:318,Availability,avail,available,318,"Currently the attributes of `WorkflowInputParameter` (secondaryFiles, doc, format etc...) are being ignored and only the type / name are used to create an `ExternalGraphInputNode` that will then get its value from the input yaml file.; Those attributes might contain information that we want to use and should be made available somehow for processing once the value is assigned to the input.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3441
https://github.com/broadinstitute/cromwell/pull/3442:62,Modifiability,plugin,plugins,62,doc for flags:. > enable caching of classloaders for compiler plugins and macro definitions. This can lead to significant performance improvements,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3442
https://github.com/broadinstitute/cromwell/pull/3442:122,Performance,perform,performance,122,doc for flags:. > enable caching of classloaders for compiler plugins and macro definitions. This can lead to significant performance improvements,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3442
https://github.com/broadinstitute/cromwell/pull/3444:93,Integrability,depend,depend,93,"- [x] Rebase on develop after #3439. Implements the ability to override inputs, even if they depend on upstream nodes. Another trick on the conversion into WOM. In this case we simplify/expand this:; ```wdl; input {; Int b = a; }; Int a = 55; ```. Into this:; ```wdl; input {; Int __b; }; Int a = 55; Int b = select_first([_b, a]); ```. But preserve the fact that the input being looked for in the input file is still just `b`. Also brings in a fix from the hermes grammar to fix the string regex (it wasn't allowing `(` or `)`)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3444
https://github.com/broadinstitute/cromwell/pull/3444:177,Usability,simpl,simplify,177,"- [x] Rebase on develop after #3439. Implements the ability to override inputs, even if they depend on upstream nodes. Another trick on the conversion into WOM. In this case we simplify/expand this:; ```wdl; input {; Int b = a; }; Int a = 55; ```. Into this:; ```wdl; input {; Int __b; }; Int a = 55; Int b = select_first([_b, a]); ```. But preserve the fact that the input being looked for in the input file is still just `b`. Also brings in a fix from the hermes grammar to fix the string regex (it wasn't allowing `(` or `)`)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3444
https://github.com/broadinstitute/cromwell/issues/3449:7,Deployability,upgrade,upgrade,7,"Please upgrade Cromwell Pipelines API Backend to version 2. This will be necessary for our portal to launch workflows against GCS ""requester pays"" buckets with appropriate billing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3449
https://github.com/broadinstitute/cromwell/issues/3449:24,Deployability,Pipeline,Pipelines,24,"Please upgrade Cromwell Pipelines API Backend to version 2. This will be necessary for our portal to launch workflows against GCS ""requester pays"" buckets with appropriate billing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3449
https://github.com/broadinstitute/cromwell/issues/3454:539,Modifiability,rewrite,rewriter,539,"If output of task A is read by task B, and both tasks support streaming, then they can be run in parallel, rather than sequentially, using a named pipe to stream A's output to B's input (or a socket if run on different machines). Streamable file params would be designated via param_metadata, as in dxWDL: https://github.com/dnanexus/dxWDL/blob/master/doc/ExpertOptions.md#extensions; Then chains of tasks could effectively become one task, reducing the overall workflow runtime.; This could also be implemented by a standalone WDL-to-WDL rewriter, which would identify chains of mergeable tasks in a workflow and auto-generate a new task that runs the original tasks in parallel.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454
https://github.com/broadinstitute/cromwell/issues/3459:285,Availability,ping,ping,285,"Currently when a user uses a subworkflow id for anything which takes a workflow id it will fail. This is because CromIAM queries SAM first and SAM only knows about root workflow IDs. . It seems like the right thing to do here will be to get the root workflow ID from Cromwell and then ping Sam, but feel free to do something edifferent if it makes sense to do so",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3459
https://github.com/broadinstitute/cromwell/issues/3461:38,Energy Efficiency,schedul,scheduler,38,"Hi,. Are there any plans to support a scheduler that can submit jobs to a mesos cluster (e.g. Chronos or whatever?). Thanks; M",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461
https://github.com/broadinstitute/cromwell/pull/3467:319,Availability,failure,failures,319,* Re-pin conformance test hash to the current HEAD of master.; * Allow workflow inputs to be recycled back as outputs (part 1/2 fixing conformance 20).; * Allow arrays to be coerced to Anys (part 2/2 fixing conformance 20).; * Fix prefix handling with empty arrays (fix conformance 125).; * Adjust expected conformance failures for all of the above.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3467
https://github.com/broadinstitute/cromwell/pull/3467:26,Security,hash,hash,26,* Re-pin conformance test hash to the current HEAD of master.; * Allow workflow inputs to be recycled back as outputs (part 1/2 fixing conformance 20).; * Allow arrays to be coerced to Anys (part 2/2 fixing conformance 20).; * Fix prefix handling with empty arrays (fix conformance 125).; * Adjust expected conformance failures for all of the above.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3467
https://github.com/broadinstitute/cromwell/pull/3467:21,Testability,test,test,21,* Re-pin conformance test hash to the current HEAD of master.; * Allow workflow inputs to be recycled back as outputs (part 1/2 fixing conformance 20).; * Allow arrays to be coerced to Anys (part 2/2 fixing conformance 20).; * Fix prefix handling with empty arrays (fix conformance 125).; * Adjust expected conformance failures for all of the above.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3467
https://github.com/broadinstitute/cromwell/pull/3468:362,Safety,avoid,avoid,362,"- Cleans up the `IoFunctionSet` a little and add `PathFunctionSet` in it to deal with path manipulation that doesn't involve I/O; - When secondary files are actually secondary directories, list their content and return a `WomMaybeListedDirectory` instead of a `WomSingleFile` or `WomMaybePopulatedFile`; - Make paths absolute as much and as early as possible to avoid ambiguity and having to guess later where they are relative to; - Adjust the `OutputManipulator` to deal with secondary listed directories",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3468
https://github.com/broadinstitute/cromwell/pull/3470:69,Availability,robust,robustify,69,"~~Pulling this back to a pregull, I want to think about and possibly robustify simpleton key handling.~~ Have at it!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3470
https://github.com/broadinstitute/cromwell/pull/3470:79,Usability,simpl,simpleton,79,"~~Pulling this back to a pregull, I want to think about and possibly robustify simpleton key handling.~~ Have at it!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3470
https://github.com/broadinstitute/cromwell/pull/3471:11,Deployability,integrat,integration,11,Your FOSSA integration was successful! Attached in this PR is a badge and license report to track scan status in your README. Below are docs for integrating FOSSA license checks into your CI:. - [CircleCI](http://fossa.io/docs/integrating-tools/circleci/); - [TravisCI](http://fossa.io/docs/integrating-tools/travisci/); - [Jenkins](https://github.com/fossas/fossa-jenkins-plugin); - [Other](https://github.com/fossas/license-cli),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3471
https://github.com/broadinstitute/cromwell/pull/3471:145,Deployability,integrat,integrating,145,Your FOSSA integration was successful! Attached in this PR is a badge and license report to track scan status in your README. Below are docs for integrating FOSSA license checks into your CI:. - [CircleCI](http://fossa.io/docs/integrating-tools/circleci/); - [TravisCI](http://fossa.io/docs/integrating-tools/travisci/); - [Jenkins](https://github.com/fossas/fossa-jenkins-plugin); - [Other](https://github.com/fossas/license-cli),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3471
https://github.com/broadinstitute/cromwell/pull/3471:227,Deployability,integrat,integrating-tools,227,Your FOSSA integration was successful! Attached in this PR is a badge and license report to track scan status in your README. Below are docs for integrating FOSSA license checks into your CI:. - [CircleCI](http://fossa.io/docs/integrating-tools/circleci/); - [TravisCI](http://fossa.io/docs/integrating-tools/travisci/); - [Jenkins](https://github.com/fossas/fossa-jenkins-plugin); - [Other](https://github.com/fossas/license-cli),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3471
https://github.com/broadinstitute/cromwell/pull/3471:291,Deployability,integrat,integrating-tools,291,Your FOSSA integration was successful! Attached in this PR is a badge and license report to track scan status in your README. Below are docs for integrating FOSSA license checks into your CI:. - [CircleCI](http://fossa.io/docs/integrating-tools/circleci/); - [TravisCI](http://fossa.io/docs/integrating-tools/travisci/); - [Jenkins](https://github.com/fossas/fossa-jenkins-plugin); - [Other](https://github.com/fossas/license-cli),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3471
https://github.com/broadinstitute/cromwell/pull/3471:11,Integrability,integrat,integration,11,Your FOSSA integration was successful! Attached in this PR is a badge and license report to track scan status in your README. Below are docs for integrating FOSSA license checks into your CI:. - [CircleCI](http://fossa.io/docs/integrating-tools/circleci/); - [TravisCI](http://fossa.io/docs/integrating-tools/travisci/); - [Jenkins](https://github.com/fossas/fossa-jenkins-plugin); - [Other](https://github.com/fossas/license-cli),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3471
https://github.com/broadinstitute/cromwell/pull/3471:145,Integrability,integrat,integrating,145,Your FOSSA integration was successful! Attached in this PR is a badge and license report to track scan status in your README. Below are docs for integrating FOSSA license checks into your CI:. - [CircleCI](http://fossa.io/docs/integrating-tools/circleci/); - [TravisCI](http://fossa.io/docs/integrating-tools/travisci/); - [Jenkins](https://github.com/fossas/fossa-jenkins-plugin); - [Other](https://github.com/fossas/license-cli),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3471
https://github.com/broadinstitute/cromwell/pull/3471:227,Integrability,integrat,integrating-tools,227,Your FOSSA integration was successful! Attached in this PR is a badge and license report to track scan status in your README. Below are docs for integrating FOSSA license checks into your CI:. - [CircleCI](http://fossa.io/docs/integrating-tools/circleci/); - [TravisCI](http://fossa.io/docs/integrating-tools/travisci/); - [Jenkins](https://github.com/fossas/fossa-jenkins-plugin); - [Other](https://github.com/fossas/license-cli),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3471
https://github.com/broadinstitute/cromwell/pull/3471:291,Integrability,integrat,integrating-tools,291,Your FOSSA integration was successful! Attached in this PR is a badge and license report to track scan status in your README. Below are docs for integrating FOSSA license checks into your CI:. - [CircleCI](http://fossa.io/docs/integrating-tools/circleci/); - [TravisCI](http://fossa.io/docs/integrating-tools/travisci/); - [Jenkins](https://github.com/fossas/fossa-jenkins-plugin); - [Other](https://github.com/fossas/license-cli),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3471
https://github.com/broadinstitute/cromwell/pull/3471:373,Modifiability,plugin,plugin,373,Your FOSSA integration was successful! Attached in this PR is a badge and license report to track scan status in your README. Below are docs for integrating FOSSA license checks into your CI:. - [CircleCI](http://fossa.io/docs/integrating-tools/circleci/); - [TravisCI](http://fossa.io/docs/integrating-tools/travisci/); - [Jenkins](https://github.com/fossas/fossa-jenkins-plugin); - [Other](https://github.com/fossas/license-cli),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3471
https://github.com/broadinstitute/cromwell/pull/3473:17,Security,validat,validate,17,* Makes Cromwell validate that inputs provided were actually wanted by the workflow.; * Adds a validation option to womtool that allows users to validate their inputs json without needing to submit to Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3473
https://github.com/broadinstitute/cromwell/pull/3473:95,Security,validat,validation,95,* Makes Cromwell validate that inputs provided were actually wanted by the workflow.; * Adds a validation option to womtool that allows users to validate their inputs json without needing to submit to Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3473
https://github.com/broadinstitute/cromwell/pull/3473:145,Security,validat,validate,145,* Makes Cromwell validate that inputs provided were actually wanted by the workflow.; * Adds a validation option to womtool that allows users to validate their inputs json without needing to submit to Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3473
https://github.com/broadinstitute/cromwell/pull/3474:46,Testability,test,test,46,"# To be discussed. I would like to delete the test for ""A task which contains a parameter "" from Mapworkflowspec:. * it was constructed by making assumptions about the path that are no longer the case.; * It is basic and almost certainly covered in centaur tests; * It was written in the late Cretaceous period",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3474
https://github.com/broadinstitute/cromwell/pull/3474:257,Testability,test,tests,257,"# To be discussed. I would like to delete the test for ""A task which contains a parameter "" from Mapworkflowspec:. * it was constructed by making assumptions about the path that are no longer the case.; * It is basic and almost certainly covered in centaur tests; * It was written in the late Cretaceous period",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3474
https://github.com/broadinstitute/cromwell/pull/3476:126,Performance,cache,cache,126,"See https://github.com/broadinstitute/cromwell/issues/3074 for a description of the problem. This re-uses the values from the cache instead of re-running the job.; The call cache capoeira test was failing in centaur intermittently because a side effect of the issue was that the hashes were not published to metadata, which was failing the test.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3476
https://github.com/broadinstitute/cromwell/pull/3476:173,Performance,cache,cache,173,"See https://github.com/broadinstitute/cromwell/issues/3074 for a description of the problem. This re-uses the values from the cache instead of re-running the job.; The call cache capoeira test was failing in centaur intermittently because a side effect of the issue was that the hashes were not published to metadata, which was failing the test.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3476
https://github.com/broadinstitute/cromwell/pull/3476:279,Security,hash,hashes,279,"See https://github.com/broadinstitute/cromwell/issues/3074 for a description of the problem. This re-uses the values from the cache instead of re-running the job.; The call cache capoeira test was failing in centaur intermittently because a side effect of the issue was that the hashes were not published to metadata, which was failing the test.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3476
https://github.com/broadinstitute/cromwell/pull/3476:188,Testability,test,test,188,"See https://github.com/broadinstitute/cromwell/issues/3074 for a description of the problem. This re-uses the values from the cache instead of re-running the job.; The call cache capoeira test was failing in centaur intermittently because a side effect of the issue was that the hashes were not published to metadata, which was failing the test.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3476
https://github.com/broadinstitute/cromwell/pull/3476:340,Testability,test,test,340,"See https://github.com/broadinstitute/cromwell/issues/3074 for a description of the problem. This re-uses the values from the cache instead of re-running the job.; The call cache capoeira test was failing in centaur intermittently because a side effect of the issue was that the hashes were not published to metadata, which was failing the test.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3476
https://github.com/broadinstitute/cromwell/issues/3477:1035,Availability,echo,echo,1035,"l workflows. I realize I might be using Cromwell for things it was not supposed to do, but since it is so very slow I thought I might ask anyway. Just so you understand my application, I'm not running bioinformatics pipelines: I'm rather interested in using WDL instead of Makefiles to do various scientific calculations. It's often a bunch of relatively simple Python or R scripts that I want to connect in a chain, and then sweep some parameters, etc. While developing and debugging the workflows (the majority of the time) I have a need for running and re-running single tasks or small sub-workflows many times. I was hoping I could use Cromwell to do this, but at present it just is too slow. Let's say I run a tiny workflow like the [Hello world example](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/#step-2-writing-your-first-workflow-description) from Cromwell's docs:; ```wdl ; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. I'm on Ubuntu 16.04, I start a Cromwell server and wait for it to spin up. Then. ```bash; $ time /usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar cromwell-31.jar submit wf.wdl ; [2018-04-04 15:37:15,92] [info] Slf4jLogger started; [2018-04-04 15:37:17,06] [info] Workflow ab6ec0d5-20d2-4113-a58a-0b0e15097476 submitted to http://localhost:8000. real	0m2.536s; user	0m5.316s; sys	0m0.292s; ```. Just submitting the job takes 2.5 seconds wall clock time. Then watching the server do the job takes another 21 seconds:. ```bash; 2018-04-04 15:37:17,001 cromwell-system-akka.dispatchers.api-dispatcher-117 INFO - WDL (Unspecified version) workflow ab6ec0d5, so essentially the job is equivalent to-20d2-4113-a58a-0b0e15097476 submitted. [various log outputs]. 2018-04-04 15:37:38,440 cromwell-system-akka.dispatchers.engine-dispatcher-6 INFO - WorkflowExecutionActor-ab6ec0d5-20d2-4113-a58a-0b0e15097476 [UUID(ab6ec0d5)]: Workflow myWorkflow complete. ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477
https://github.com/broadinstitute/cromwell/issues/3477:277,Deployability,pipeline,pipelines,277,"I'm having problems using Cromwell for fast iteration on small workflows. I realize I might be using Cromwell for things it was not supposed to do, but since it is so very slow I thought I might ask anyway. Just so you understand my application, I'm not running bioinformatics pipelines: I'm rather interested in using WDL instead of Makefiles to do various scientific calculations. It's often a bunch of relatively simple Python or R scripts that I want to connect in a chain, and then sweep some parameters, etc. While developing and debugging the workflows (the majority of the time) I have a need for running and re-running single tasks or small sub-workflows many times. I was hoping I could use Cromwell to do this, but at present it just is too slow. Let's say I run a tiny workflow like the [Hello world example](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/#step-2-writing-your-first-workflow-description) from Cromwell's docs:; ```wdl ; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. I'm on Ubuntu 16.04, I start a Cromwell server and wait for it to spin up. Then. ```bash; $ time /usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar cromwell-31.jar submit wf.wdl ; [2018-04-04 15:37:15,92] [info] Slf4jLogger started; [2018-04-04 15:37:17,06] [info] Workflow ab6ec0d5-20d2-4113-a58a-0b0e15097476 submitted to http://localhost:8000. real	0m2.536s; user	0m5.316s; sys	0m0.292s; ```. Just submitting the job takes 2.5 seconds wall clock time. Then watching the server do the job takes another 21 seconds:. ```bash; 2018-04-04 15:37:17,001 cromwell-system-akka.dispatchers.api-dispatcher-117 INFO - WDL (Unspecified version) workflow ab6ec0d5, so essentially the job is equivalent to-20d2-4113-a58a-0b0e15097476 submitted. [various log outputs]. 2018-04-04 15:37:38,440 cromwell-system-akka.dispatchers.engine-dispatcher-6 INFO - WorkflowExecutionActor-ab6ec0d5-20d2-4113-a58a-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477
https://github.com/broadinstitute/cromwell/issues/3477:1855,Testability,log,log,1855,"ntific calculations. It's often a bunch of relatively simple Python or R scripts that I want to connect in a chain, and then sweep some parameters, etc. While developing and debugging the workflows (the majority of the time) I have a need for running and re-running single tasks or small sub-workflows many times. I was hoping I could use Cromwell to do this, but at present it just is too slow. Let's say I run a tiny workflow like the [Hello world example](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/#step-2-writing-your-first-workflow-description) from Cromwell's docs:; ```wdl ; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. I'm on Ubuntu 16.04, I start a Cromwell server and wait for it to spin up. Then. ```bash; $ time /usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar cromwell-31.jar submit wf.wdl ; [2018-04-04 15:37:15,92] [info] Slf4jLogger started; [2018-04-04 15:37:17,06] [info] Workflow ab6ec0d5-20d2-4113-a58a-0b0e15097476 submitted to http://localhost:8000. real	0m2.536s; user	0m5.316s; sys	0m0.292s; ```. Just submitting the job takes 2.5 seconds wall clock time. Then watching the server do the job takes another 21 seconds:. ```bash; 2018-04-04 15:37:17,001 cromwell-system-akka.dispatchers.api-dispatcher-117 INFO - WDL (Unspecified version) workflow ab6ec0d5, so essentially the job is equivalent to-20d2-4113-a58a-0b0e15097476 submitted. [various log outputs]. 2018-04-04 15:37:38,440 cromwell-system-akka.dispatchers.engine-dispatcher-6 INFO - WorkflowExecutionActor-ab6ec0d5-20d2-4113-a58a-0b0e15097476 [UUID(ab6ec0d5)]: Workflow myWorkflow complete. Final Outputs:; {; ""myWorkflow.myTask.out"": ""hello world""; }; ```. As far as I understand, it's not even doing any of this in a docker environment. Am I doing something wrong, or is it expected that there is such an overhead? Is there anything I can do to speed it up?. Many thanks for the great work.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477
https://github.com/broadinstitute/cromwell/issues/3477:416,Usability,simpl,simple,416,"I'm having problems using Cromwell for fast iteration on small workflows. I realize I might be using Cromwell for things it was not supposed to do, but since it is so very slow I thought I might ask anyway. Just so you understand my application, I'm not running bioinformatics pipelines: I'm rather interested in using WDL instead of Makefiles to do various scientific calculations. It's often a bunch of relatively simple Python or R scripts that I want to connect in a chain, and then sweep some parameters, etc. While developing and debugging the workflows (the majority of the time) I have a need for running and re-running single tasks or small sub-workflows many times. I was hoping I could use Cromwell to do this, but at present it just is too slow. Let's say I run a tiny workflow like the [Hello world example](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/#step-2-writing-your-first-workflow-description) from Cromwell's docs:; ```wdl ; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. I'm on Ubuntu 16.04, I start a Cromwell server and wait for it to spin up. Then. ```bash; $ time /usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar cromwell-31.jar submit wf.wdl ; [2018-04-04 15:37:15,92] [info] Slf4jLogger started; [2018-04-04 15:37:17,06] [info] Workflow ab6ec0d5-20d2-4113-a58a-0b0e15097476 submitted to http://localhost:8000. real	0m2.536s; user	0m5.316s; sys	0m0.292s; ```. Just submitting the job takes 2.5 seconds wall clock time. Then watching the server do the job takes another 21 seconds:. ```bash; 2018-04-04 15:37:17,001 cromwell-system-akka.dispatchers.api-dispatcher-117 INFO - WDL (Unspecified version) workflow ab6ec0d5, so essentially the job is equivalent to-20d2-4113-a58a-0b0e15097476 submitted. [various log outputs]. 2018-04-04 15:37:38,440 cromwell-system-akka.dispatchers.engine-dispatcher-6 INFO - WorkflowExecutionActor-ab6ec0d5-20d2-4113-a58a-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477
https://github.com/broadinstitute/cromwell/issues/3478:425,Availability,echo,echo,425,"Cromwell version: 30-9a7de06. Minimized from one of our WDLs:; ```wdl; workflow Test {; Boolean do; Int n. if (do) {; call Optional; }. scatter (i in range(n)) {; call Scattered; }. call Gather {; input:; # HERE: select_first returns String, and Scattered.out is an Array[String]; ins = if defined(Optional.out) then select_first([Optional.out]) else Scattered.out; }; output {; Gather.out; }; }. task Optional {; command {; echo ""Hey!""; }; output {; String out = read_string(stdout()); }; }. task Scattered {; command {; echo ""Hello!""; }; output {; String out = read_string(stdout()); }; }. task Gather {; Array[String] ins. command {; cat ${write_lines(ins)}; }; output {; String out = read_string(stdout()); }; }; ```. This WDL runs successfully, but in code review I noticed the weird type mismatch between the branches. I asked @cjllanwarne about it and he thought it was an old ""feature"" that had been purged to avoid bugs / confusion. I'd expect something like this to be rejected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3478
https://github.com/broadinstitute/cromwell/issues/3478:522,Availability,echo,echo,522,"Cromwell version: 30-9a7de06. Minimized from one of our WDLs:; ```wdl; workflow Test {; Boolean do; Int n. if (do) {; call Optional; }. scatter (i in range(n)) {; call Scattered; }. call Gather {; input:; # HERE: select_first returns String, and Scattered.out is an Array[String]; ins = if defined(Optional.out) then select_first([Optional.out]) else Scattered.out; }; output {; Gather.out; }; }. task Optional {; command {; echo ""Hey!""; }; output {; String out = read_string(stdout()); }; }. task Scattered {; command {; echo ""Hello!""; }; output {; String out = read_string(stdout()); }; }. task Gather {; Array[String] ins. command {; cat ${write_lines(ins)}; }; output {; String out = read_string(stdout()); }; }; ```. This WDL runs successfully, but in code review I noticed the weird type mismatch between the branches. I asked @cjllanwarne about it and he thought it was an old ""feature"" that had been purged to avoid bugs / confusion. I'd expect something like this to be rejected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3478
https://github.com/broadinstitute/cromwell/issues/3478:918,Safety,avoid,avoid,918,"Cromwell version: 30-9a7de06. Minimized from one of our WDLs:; ```wdl; workflow Test {; Boolean do; Int n. if (do) {; call Optional; }. scatter (i in range(n)) {; call Scattered; }. call Gather {; input:; # HERE: select_first returns String, and Scattered.out is an Array[String]; ins = if defined(Optional.out) then select_first([Optional.out]) else Scattered.out; }; output {; Gather.out; }; }. task Optional {; command {; echo ""Hey!""; }; output {; String out = read_string(stdout()); }; }. task Scattered {; command {; echo ""Hello!""; }; output {; String out = read_string(stdout()); }; }. task Gather {; Array[String] ins. command {; cat ${write_lines(ins)}; }; output {; String out = read_string(stdout()); }; }; ```. This WDL runs successfully, but in code review I noticed the weird type mismatch between the branches. I asked @cjllanwarne about it and he thought it was an old ""feature"" that had been purged to avoid bugs / confusion. I'd expect something like this to be rejected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3478
https://github.com/broadinstitute/cromwell/issues/3478:80,Testability,Test,Test,80,"Cromwell version: 30-9a7de06. Minimized from one of our WDLs:; ```wdl; workflow Test {; Boolean do; Int n. if (do) {; call Optional; }. scatter (i in range(n)) {; call Scattered; }. call Gather {; input:; # HERE: select_first returns String, and Scattered.out is an Array[String]; ins = if defined(Optional.out) then select_first([Optional.out]) else Scattered.out; }; output {; Gather.out; }; }. task Optional {; command {; echo ""Hey!""; }; output {; String out = read_string(stdout()); }; }. task Scattered {; command {; echo ""Hello!""; }; output {; String out = read_string(stdout()); }; }. task Gather {; Array[String] ins. command {; cat ${write_lines(ins)}; }; output {; String out = read_string(stdout()); }; }; ```. This WDL runs successfully, but in code review I noticed the weird type mismatch between the branches. I asked @cjllanwarne about it and he thought it was an old ""feature"" that had been purged to avoid bugs / confusion. I'd expect something like this to be rejected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3478
https://github.com/broadinstitute/cromwell/issues/3480:808,Availability,echo,echo,808,"While trying to write call caching tests. Note the `rCopy` is just a recycled `float` input. ```; {; ""radius"": 1.0; }; ```. ```; cwlVersion: v1.0; class: Workflow; # Workflow-level DockerRequirement; requirements:; DockerRequirement:; dockerPull: ""ubuntu:latest""; inputs:; - id: radius; type: float; outputs:; - id: area; outputSource: ""#one/rSquared""; type: float; steps:; - id: one; in:; - id: radius; source: ""#radius""; out:; - id: rSquared; - id: rCopy; run:; inputs:; - id: radius; type: float; outputs:; - id: rSquared; outputBinding:; glob: stdout.txt; loadContents: true; outputEval: $(parseFloat(self[0].contents)); type: float; - id: rCopy; outputBinding:; outputEval: $(inputs.radius); type: float; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; arguments:; - valueFrom: echo; - valueFrom: ""`expr ""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""*""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""`""; shellQuote: false; stdout: stdout.txt; ```. ```; name: cwl_caching; testFormat: workflowsuccess; backendsMode: ""only""; backends: [Local]; tags: [localdockertest]. files {; wdl: cwl_caching/cwl_caching.cwl; inputs: cwl_caching/cwl_caching.json; }. metadata {; status: Succeeded; }. workflowType: CWL; workflowTypeVersion: v1.0; ```. ```; cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'rCopy': No coercion defined from '1' of type 'Int' to 'Float'.; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:688); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.Bat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3480
https://github.com/broadinstitute/cromwell/issues/3480:560,Performance,load,loadContents,560,"While trying to write call caching tests. Note the `rCopy` is just a recycled `float` input. ```; {; ""radius"": 1.0; }; ```. ```; cwlVersion: v1.0; class: Workflow; # Workflow-level DockerRequirement; requirements:; DockerRequirement:; dockerPull: ""ubuntu:latest""; inputs:; - id: radius; type: float; outputs:; - id: area; outputSource: ""#one/rSquared""; type: float; steps:; - id: one; in:; - id: radius; source: ""#radius""; out:; - id: rSquared; - id: rCopy; run:; inputs:; - id: radius; type: float; outputs:; - id: rSquared; outputBinding:; glob: stdout.txt; loadContents: true; outputEval: $(parseFloat(self[0].contents)); type: float; - id: rCopy; outputBinding:; outputEval: $(inputs.radius); type: float; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; arguments:; - valueFrom: echo; - valueFrom: ""`expr ""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""*""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""`""; shellQuote: false; stdout: stdout.txt; ```. ```; name: cwl_caching; testFormat: workflowsuccess; backendsMode: ""only""; backends: [Local]; tags: [localdockertest]. files {; wdl: cwl_caching/cwl_caching.cwl; inputs: cwl_caching/cwl_caching.json; }. metadata {; status: Succeeded; }. workflowType: CWL; workflowTypeVersion: v1.0; ```. ```; cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'rCopy': No coercion defined from '1' of type 'Int' to 'Float'.; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:688); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.Bat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3480
https://github.com/broadinstitute/cromwell/issues/3480:1721,Performance,concurren,concurrent,1721,"neTool; requirements:; - class: ShellCommandRequirement; arguments:; - valueFrom: echo; - valueFrom: ""`expr ""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""*""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""`""; shellQuote: false; stdout: stdout.txt; ```. ```; name: cwl_caching; testFormat: workflowsuccess; backendsMode: ""only""; backends: [Local]; tags: [localdockertest]. files {; wdl: cwl_caching/cwl_caching.cwl; inputs: cwl_caching/cwl_caching.json; }. metadata {; status: Succeeded; }. workflowType: CWL; workflowTypeVersion: v1.0; ```. ```; cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'rCopy': No coercion defined from '1' of type 'Int' to 'Float'.; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:688); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3480
https://github.com/broadinstitute/cromwell/issues/3480:1783,Performance,concurren,concurrent,1783,"ents:; - valueFrom: echo; - valueFrom: ""`expr ""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""*""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""`""; shellQuote: false; stdout: stdout.txt; ```. ```; name: cwl_caching; testFormat: workflowsuccess; backendsMode: ""only""; backends: [Local]; tags: [localdockertest]. files {; wdl: cwl_caching/cwl_caching.cwl; inputs: cwl_caching/cwl_caching.json; }. metadata {; status: Succeeded; }. workflowType: CWL; workflowTypeVersion: v1.0; ```. ```; cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'rCopy': No coercion defined from '1' of type 'Int' to 'Float'.; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:688); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3480
https://github.com/broadinstitute/cromwell/issues/3480:1850,Performance,concurren,concurrent,1850," - valueFrom: $(inputs.radius); - valueFrom: ""*""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""`""; shellQuote: false; stdout: stdout.txt; ```. ```; name: cwl_caching; testFormat: workflowsuccess; backendsMode: ""only""; backends: [Local]; tags: [localdockertest]. files {; wdl: cwl_caching/cwl_caching.cwl; inputs: cwl_caching/cwl_caching.json; }. metadata {; status: Succeeded; }. workflowType: CWL; workflowTypeVersion: v1.0; ```. ```; cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'rCopy': No coercion defined from '1' of type 'Int' to 'Float'.; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:688); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(Fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3480
https://github.com/broadinstitute/cromwell/issues/3480:1924,Performance,concurren,concurrent,1924,"valueFrom: ""*""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""`""; shellQuote: false; stdout: stdout.txt; ```. ```; name: cwl_caching; testFormat: workflowsuccess; backendsMode: ""only""; backends: [Local]; tags: [localdockertest]. files {; wdl: cwl_caching/cwl_caching.cwl; inputs: cwl_caching/cwl_caching.json; }. metadata {; status: Succeeded; }. workflowType: CWL; workflowTypeVersion: v1.0; ```. ```; cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'rCopy': No coercion defined from '1' of type 'Int' to 'Float'.; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:688); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3480
https://github.com/broadinstitute/cromwell/issues/3480:2249,Performance,concurren,concurrent,2249,"valueFrom: ""*""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""`""; shellQuote: false; stdout: stdout.txt; ```. ```; name: cwl_caching; testFormat: workflowsuccess; backendsMode: ""only""; backends: [Local]; tags: [localdockertest]. files {; wdl: cwl_caching/cwl_caching.cwl; inputs: cwl_caching/cwl_caching.json; }. metadata {; status: Succeeded; }. workflowType: CWL; workflowTypeVersion: v1.0; ```. ```; cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'rCopy': No coercion defined from '1' of type 'Int' to 'Float'.; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:688); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3480
https://github.com/broadinstitute/cromwell/issues/3480:35,Testability,test,tests,35,"While trying to write call caching tests. Note the `rCopy` is just a recycled `float` input. ```; {; ""radius"": 1.0; }; ```. ```; cwlVersion: v1.0; class: Workflow; # Workflow-level DockerRequirement; requirements:; DockerRequirement:; dockerPull: ""ubuntu:latest""; inputs:; - id: radius; type: float; outputs:; - id: area; outputSource: ""#one/rSquared""; type: float; steps:; - id: one; in:; - id: radius; source: ""#radius""; out:; - id: rSquared; - id: rCopy; run:; inputs:; - id: radius; type: float; outputs:; - id: rSquared; outputBinding:; glob: stdout.txt; loadContents: true; outputEval: $(parseFloat(self[0].contents)); type: float; - id: rCopy; outputBinding:; outputEval: $(inputs.radius); type: float; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; arguments:; - valueFrom: echo; - valueFrom: ""`expr ""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""*""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""`""; shellQuote: false; stdout: stdout.txt; ```. ```; name: cwl_caching; testFormat: workflowsuccess; backendsMode: ""only""; backends: [Local]; tags: [localdockertest]. files {; wdl: cwl_caching/cwl_caching.cwl; inputs: cwl_caching/cwl_caching.json; }. metadata {; status: Succeeded; }. workflowType: CWL; workflowTypeVersion: v1.0; ```. ```; cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'rCopy': No coercion defined from '1' of type 'Int' to 'Float'.; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:688); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.Bat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3480
https://github.com/broadinstitute/cromwell/issues/3480:1041,Testability,test,testFormat,1041,"ed `float` input. ```; {; ""radius"": 1.0; }; ```. ```; cwlVersion: v1.0; class: Workflow; # Workflow-level DockerRequirement; requirements:; DockerRequirement:; dockerPull: ""ubuntu:latest""; inputs:; - id: radius; type: float; outputs:; - id: area; outputSource: ""#one/rSquared""; type: float; steps:; - id: one; in:; - id: radius; source: ""#radius""; out:; - id: rSquared; - id: rCopy; run:; inputs:; - id: radius; type: float; outputs:; - id: rSquared; outputBinding:; glob: stdout.txt; loadContents: true; outputEval: $(parseFloat(self[0].contents)); type: float; - id: rCopy; outputBinding:; outputEval: $(inputs.radius); type: float; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; arguments:; - valueFrom: echo; - valueFrom: ""`expr ""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""*""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""`""; shellQuote: false; stdout: stdout.txt; ```. ```; name: cwl_caching; testFormat: workflowsuccess; backendsMode: ""only""; backends: [Local]; tags: [localdockertest]. files {; wdl: cwl_caching/cwl_caching.cwl; inputs: cwl_caching/cwl_caching.json; }. metadata {; status: Succeeded; }. workflowType: CWL; workflowTypeVersion: v1.0; ```. ```; cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'rCopy': No coercion defined from '1' of type 'Int' to 'Float'.; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:688); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at ak",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3480
https://github.com/broadinstitute/cromwell/issues/3483:228,Availability,error,error,228,"I have a job that ended up in a weird state. It's a series of 3 tasks, where each one depends on the output of the previous one. The workflow finished, but the middle job is listed as still running. It's workflow. Seems like an error state. See https://cromwell-v30.dsde-methods.broadinstitute.org/api/workflows/v1/01c7d76f-5b2b-48cd-be08-ce75b923666e/timing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483
https://github.com/broadinstitute/cromwell/issues/3483:86,Integrability,depend,depends,86,"I have a job that ended up in a weird state. It's a series of 3 tasks, where each one depends on the output of the previous one. The workflow finished, but the middle job is listed as still running. It's workflow. Seems like an error state. See https://cromwell-v30.dsde-methods.broadinstitute.org/api/workflows/v1/01c7d76f-5b2b-48cd-be08-ce75b923666e/timing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483
https://github.com/broadinstitute/cromwell/issues/3484:483,Availability,error,errors,483,"Sometimes the timing diagram splits the results onto multiple lines, which then hides the lower part of the diagram because it's twice as long as expected. You end up with a mostly empty page with a small box that has as scroll wheel at the top of it. <img width=""1431"" alt=""screen shot 2018-04-05 at 4 06 59 pm"" src=""https://user-images.githubusercontent.com/4700332/38389168-956414ac-38eb-11e8-8459-dff52946091c.png"">. I noticed that the console for this diagram has the following errors:; ```; timing:20 Unable to add 'MarkDuplicates.SortSam's entry: 'RunningJob' because start-time 'Tue Apr 03 2018 23:54:20 GMT-0400 (EDT)'' is greater than end-time 'Tue Apr 03 2018 23:54:20 GMT-0400 (EDT)'; addDataTableRow @ timing:20; timing:20 Unable to add 'MarkDuplicates.PreSort's entry: 'RunningJob' because start-time 'Tue Apr 03 2018 17:35:05 GMT-0400 (EDT)'' is greater than end-time 'Tue Apr 03 2018 17:35:05 GMT-0400 (EDT)'; ```. I'm not sure if it's related to the display issue or not, but I don't see it on other workflows. ; See diagram here. https://cromwell-v30.dsde-methods.broadinstitute.org/api/workflows/v1/01c7d76f-5b2b-48cd-be08-ce75b923666e/timing. It's the same job from issue #3483 so there might just be something off about the whole job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3484
https://github.com/broadinstitute/cromwell/pull/3485:79,Testability,test,testing,79,I mentioned in standup that I realize and apologize for the utter lack of unit testing here. You'll note some pre-existing FIXMEs which themselves are part of why testing this is hard - they **are** more easily solved now but open up a can of worms and this issue is blocking Mint. I'd like to get this in and then go back and fix those FIXMEs/lack of testing.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3485
https://github.com/broadinstitute/cromwell/pull/3485:163,Testability,test,testing,163,I mentioned in standup that I realize and apologize for the utter lack of unit testing here. You'll note some pre-existing FIXMEs which themselves are part of why testing this is hard - they **are** more easily solved now but open up a can of worms and this issue is blocking Mint. I'd like to get this in and then go back and fix those FIXMEs/lack of testing.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3485
https://github.com/broadinstitute/cromwell/pull/3485:352,Testability,test,testing,352,I mentioned in standup that I realize and apologize for the utter lack of unit testing here. You'll note some pre-existing FIXMEs which themselves are part of why testing this is hard - they **are** more easily solved now but open up a can of worms and this issue is blocking Mint. I'd like to get this in and then go back and fix those FIXMEs/lack of testing.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3485
https://github.com/broadinstitute/cromwell/issues/3487:319,Availability,Error,Error,319,"I was running through the tutorial, https://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/. I typed the inputs.json file with a tab instead of spaces. ```curl -X POST http://localhost:8000/api/workflows/v1 -F workflowSource=@hello.wdl -F workflowInputs=@inputs.json```. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Input file is not valid yaml nor json: while scanning for the next token\nfound character '\\t(TAB)' that cannot start any token. (Do not use \\t(TAB) for indentation)\n in 'reader', line 2, column 1:\n \t\""test.hello.name\"": \""World\""\n ^\n""; }; ```. However per the JSON spec, tabs, or any whitespace, ""can be inserted between any pair of tokens"" https://www.json.org/. Python:; ```; json.loads(""{\n\t\""valid\"":\""json\""\n}""); ```. Sadly I do not know Java very well or I would just check and or fix whichever parser you are using. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3487
https://github.com/broadinstitute/cromwell/issues/3487:308,Integrability,message,message,308,"I was running through the tutorial, https://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/. I typed the inputs.json file with a tab instead of spaces. ```curl -X POST http://localhost:8000/api/workflows/v1 -F workflowSource=@hello.wdl -F workflowInputs=@inputs.json```. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Input file is not valid yaml nor json: while scanning for the next token\nfound character '\\t(TAB)' that cannot start any token. (Do not use \\t(TAB) for indentation)\n in 'reader', line 2, column 1:\n \t\""test.hello.name\"": \""World\""\n ^\n""; }; ```. However per the JSON spec, tabs, or any whitespace, ""can be inserted between any pair of tokens"" https://www.json.org/. Python:; ```; json.loads(""{\n\t\""valid\"":\""json\""\n}""); ```. Sadly I do not know Java very well or I would just check and or fix whichever parser you are using. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3487
https://github.com/broadinstitute/cromwell/issues/3487:720,Performance,load,loads,720,"I was running through the tutorial, https://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/. I typed the inputs.json file with a tab instead of spaces. ```curl -X POST http://localhost:8000/api/workflows/v1 -F workflowSource=@hello.wdl -F workflowInputs=@inputs.json```. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Input file is not valid yaml nor json: while scanning for the next token\nfound character '\\t(TAB)' that cannot start any token. (Do not use \\t(TAB) for indentation)\n in 'reader', line 2, column 1:\n \t\""test.hello.name\"": \""World\""\n ^\n""; }; ```. However per the JSON spec, tabs, or any whitespace, ""can be inserted between any pair of tokens"" https://www.json.org/. Python:; ```; json.loads(""{\n\t\""valid\"":\""json\""\n}""); ```. Sadly I do not know Java very well or I would just check and or fix whichever parser you are using. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3487
https://github.com/broadinstitute/cromwell/issues/3487:536,Testability,test,test,536,"I was running through the tutorial, https://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/. I typed the inputs.json file with a tab instead of spaces. ```curl -X POST http://localhost:8000/api/workflows/v1 -F workflowSource=@hello.wdl -F workflowInputs=@inputs.json```. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Input file is not valid yaml nor json: while scanning for the next token\nfound character '\\t(TAB)' that cannot start any token. (Do not use \\t(TAB) for indentation)\n in 'reader', line 2, column 1:\n \t\""test.hello.name\"": \""World\""\n ^\n""; }; ```. However per the JSON spec, tabs, or any whitespace, ""can be inserted between any pair of tokens"" https://www.json.org/. Python:; ```; json.loads(""{\n\t\""valid\"":\""json\""\n}""); ```. Sadly I do not know Java very well or I would just check and or fix whichever parser you are using. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3487
https://github.com/broadinstitute/cromwell/pull/3488:45,Availability,error,error,45,The HPC tutorial config causes the server to error.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3488
https://github.com/broadinstitute/cromwell/pull/3488:17,Modifiability,config,config,17,The HPC tutorial config causes the server to error.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3488
https://github.com/broadinstitute/cromwell/pull/3489:253,Availability,robust,robust,253,"SGE defaults to csh, and at least in my environment, even with #!/bin/bash bash was not launched in the job. This could well just be a quirk of my environment, and I am happy if you decide to not take this change. However it seems like a more generally robust default for different environments.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3489
https://github.com/broadinstitute/cromwell/pull/3491:29,Deployability,update,updated,29,"Red thumb required because I updated the `WomIdentifier` to have a ""workflow-local"" name (ie the identifier minus the workflow name. I'm not sure how CWL and WDL draft 2 did this, and the whole `WomIdentifier` thing seems inconsistently applied... I'm open to better ideas but it seems to work for now.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3491
https://github.com/broadinstitute/cromwell/pull/3492:163,Availability,redundant,redundant,163,- Prefix default locations in CWL with `gs://...` root for PAPI conformance tests; - Retrieve size of files early to avoid unnecessary I/O (there's still too much redundant I/O but it's a step); - Uses `WomObject` to map back JS objects instead of `WomMap` that needs homogoneous value type (or it ends up being `WomAnyType`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3492
https://github.com/broadinstitute/cromwell/pull/3492:117,Safety,avoid,avoid,117,- Prefix default locations in CWL with `gs://...` root for PAPI conformance tests; - Retrieve size of files early to avoid unnecessary I/O (there's still too much redundant I/O but it's a step); - Uses `WomObject` to map back JS objects instead of `WomMap` that needs homogoneous value type (or it ends up being `WomAnyType`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3492
https://github.com/broadinstitute/cromwell/pull/3492:163,Safety,redund,redundant,163,- Prefix default locations in CWL with `gs://...` root for PAPI conformance tests; - Retrieve size of files early to avoid unnecessary I/O (there's still too much redundant I/O but it's a step); - Uses `WomObject` to map back JS objects instead of `WomMap` that needs homogoneous value type (or it ends up being `WomAnyType`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3492
https://github.com/broadinstitute/cromwell/pull/3492:76,Testability,test,tests,76,- Prefix default locations in CWL with `gs://...` root for PAPI conformance tests; - Retrieve size of files early to avoid unnecessary I/O (there's still too much redundant I/O but it's a step); - Uses `WomObject` to map back JS objects instead of `WomMap` that needs homogoneous value type (or it ends up being `WomAnyType`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3492
https://github.com/broadinstitute/cromwell/pull/3493:12,Availability,heartbeat,heartbeat-scanning,12,"The current heartbeat-scanning code is incorrect and will attempt to relaunch workflows an hour after they start. Fortunately there is an error check that prevents workflows from actually being restarted but this situation is still less than ideal since these workflows cycle back to the top of the runnable queue an hour later (running workflows will have submission times earlier than submitted-but-never-started workflows and the sort is by submission time). Also some semi-alarming warning messages are generated for these failed starts. These changes should fix this problem by actually writing workflow heartbeats for running workflows. The current vitality logic is simply ""is this workflow actor not dead"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3493
https://github.com/broadinstitute/cromwell/pull/3493:138,Availability,error,error,138,"The current heartbeat-scanning code is incorrect and will attempt to relaunch workflows an hour after they start. Fortunately there is an error check that prevents workflows from actually being restarted but this situation is still less than ideal since these workflows cycle back to the top of the runnable queue an hour later (running workflows will have submission times earlier than submitted-but-never-started workflows and the sort is by submission time). Also some semi-alarming warning messages are generated for these failed starts. These changes should fix this problem by actually writing workflow heartbeats for running workflows. The current vitality logic is simply ""is this workflow actor not dead"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3493
https://github.com/broadinstitute/cromwell/pull/3493:609,Availability,heartbeat,heartbeats,609,"The current heartbeat-scanning code is incorrect and will attempt to relaunch workflows an hour after they start. Fortunately there is an error check that prevents workflows from actually being restarted but this situation is still less than ideal since these workflows cycle back to the top of the runnable queue an hour later (running workflows will have submission times earlier than submitted-but-never-started workflows and the sort is by submission time). Also some semi-alarming warning messages are generated for these failed starts. These changes should fix this problem by actually writing workflow heartbeats for running workflows. The current vitality logic is simply ""is this workflow actor not dead"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3493
https://github.com/broadinstitute/cromwell/pull/3493:494,Integrability,message,messages,494,"The current heartbeat-scanning code is incorrect and will attempt to relaunch workflows an hour after they start. Fortunately there is an error check that prevents workflows from actually being restarted but this situation is still less than ideal since these workflows cycle back to the top of the runnable queue an hour later (running workflows will have submission times earlier than submitted-but-never-started workflows and the sort is by submission time). Also some semi-alarming warning messages are generated for these failed starts. These changes should fix this problem by actually writing workflow heartbeats for running workflows. The current vitality logic is simply ""is this workflow actor not dead"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3493
https://github.com/broadinstitute/cromwell/pull/3493:308,Performance,queue,queue,308,"The current heartbeat-scanning code is incorrect and will attempt to relaunch workflows an hour after they start. Fortunately there is an error check that prevents workflows from actually being restarted but this situation is still less than ideal since these workflows cycle back to the top of the runnable queue an hour later (running workflows will have submission times earlier than submitted-but-never-started workflows and the sort is by submission time). Also some semi-alarming warning messages are generated for these failed starts. These changes should fix this problem by actually writing workflow heartbeats for running workflows. The current vitality logic is simply ""is this workflow actor not dead"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3493
https://github.com/broadinstitute/cromwell/pull/3493:664,Testability,log,logic,664,"The current heartbeat-scanning code is incorrect and will attempt to relaunch workflows an hour after they start. Fortunately there is an error check that prevents workflows from actually being restarted but this situation is still less than ideal since these workflows cycle back to the top of the runnable queue an hour later (running workflows will have submission times earlier than submitted-but-never-started workflows and the sort is by submission time). Also some semi-alarming warning messages are generated for these failed starts. These changes should fix this problem by actually writing workflow heartbeats for running workflows. The current vitality logic is simply ""is this workflow actor not dead"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3493
https://github.com/broadinstitute/cromwell/pull/3493:673,Usability,simpl,simply,673,"The current heartbeat-scanning code is incorrect and will attempt to relaunch workflows an hour after they start. Fortunately there is an error check that prevents workflows from actually being restarted but this situation is still less than ideal since these workflows cycle back to the top of the runnable queue an hour later (running workflows will have submission times earlier than submitted-but-never-started workflows and the sort is by submission time). Also some semi-alarming warning messages are generated for these failed starts. These changes should fix this problem by actually writing workflow heartbeats for running workflows. The current vitality logic is simply ""is this workflow actor not dead"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3493
https://github.com/broadinstitute/cromwell/pull/3494:151,Testability,test,tests,151,- Allow CromIAM to specify headers per-request and thus use a single instance of the API CromwellClient; - Make use of the above to provide a few unit tests in CromIAM CromwellClient,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3494
https://github.com/broadinstitute/cromwell/pull/3496:491,Availability,Avail,Available,491,"This started more as a POC that I had in mind but it ended up being a lot less refactoring than I anticipated so I'm making a PR for it.; Following the way we can plug services and languages this allows to plug in filesystems. All you need is a `PathBuilderFactory`.; How to make a `PathBuilderFactory` could still be made simpler but that's a separate issue.; This has the advantage that a filesystem can automatically be added to Cromwell engine or standard backend without code changes.; Available filesystems are defined in the config with their corresponding class, and the engine and backends can pick which ones they want to enable.; It removes some dependency of `engine` over the individual filesystem sub projects but it's not all the way there yet.; Thinking about PAPI2 this possibly opens the door to automatically support new filesystems for (de)localization as well if we were to go down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3496
https://github.com/broadinstitute/cromwell/pull/3496:898,Availability,down,down,898,"This started more as a POC that I had in mind but it ended up being a lot less refactoring than I anticipated so I'm making a PR for it.; Following the way we can plug services and languages this allows to plug in filesystems. All you need is a `PathBuilderFactory`.; How to make a `PathBuilderFactory` could still be made simpler but that's a separate issue.; This has the advantage that a filesystem can automatically be added to Cromwell engine or standard backend without code changes.; Available filesystems are defined in the config with their corresponding class, and the engine and backends can pick which ones they want to enable.; It removes some dependency of `engine` over the individual filesystem sub projects but it's not all the way there yet.; Thinking about PAPI2 this possibly opens the door to automatically support new filesystems for (de)localization as well if we were to go down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3496
https://github.com/broadinstitute/cromwell/pull/3496:657,Integrability,depend,dependency,657,"This started more as a POC that I had in mind but it ended up being a lot less refactoring than I anticipated so I'm making a PR for it.; Following the way we can plug services and languages this allows to plug in filesystems. All you need is a `PathBuilderFactory`.; How to make a `PathBuilderFactory` could still be made simpler but that's a separate issue.; This has the advantage that a filesystem can automatically be added to Cromwell engine or standard backend without code changes.; Available filesystems are defined in the config with their corresponding class, and the engine and backends can pick which ones they want to enable.; It removes some dependency of `engine` over the individual filesystem sub projects but it's not all the way there yet.; Thinking about PAPI2 this possibly opens the door to automatically support new filesystems for (de)localization as well if we were to go down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3496
https://github.com/broadinstitute/cromwell/pull/3496:79,Modifiability,refactor,refactoring,79,"This started more as a POC that I had in mind but it ended up being a lot less refactoring than I anticipated so I'm making a PR for it.; Following the way we can plug services and languages this allows to plug in filesystems. All you need is a `PathBuilderFactory`.; How to make a `PathBuilderFactory` could still be made simpler but that's a separate issue.; This has the advantage that a filesystem can automatically be added to Cromwell engine or standard backend without code changes.; Available filesystems are defined in the config with their corresponding class, and the engine and backends can pick which ones they want to enable.; It removes some dependency of `engine` over the individual filesystem sub projects but it's not all the way there yet.; Thinking about PAPI2 this possibly opens the door to automatically support new filesystems for (de)localization as well if we were to go down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3496
https://github.com/broadinstitute/cromwell/pull/3496:532,Modifiability,config,config,532,"This started more as a POC that I had in mind but it ended up being a lot less refactoring than I anticipated so I'm making a PR for it.; Following the way we can plug services and languages this allows to plug in filesystems. All you need is a `PathBuilderFactory`.; How to make a `PathBuilderFactory` could still be made simpler but that's a separate issue.; This has the advantage that a filesystem can automatically be added to Cromwell engine or standard backend without code changes.; Available filesystems are defined in the config with their corresponding class, and the engine and backends can pick which ones they want to enable.; It removes some dependency of `engine` over the individual filesystem sub projects but it's not all the way there yet.; Thinking about PAPI2 this possibly opens the door to automatically support new filesystems for (de)localization as well if we were to go down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3496
https://github.com/broadinstitute/cromwell/pull/3496:323,Usability,simpl,simpler,323,"This started more as a POC that I had in mind but it ended up being a lot less refactoring than I anticipated so I'm making a PR for it.; Following the way we can plug services and languages this allows to plug in filesystems. All you need is a `PathBuilderFactory`.; How to make a `PathBuilderFactory` could still be made simpler but that's a separate issue.; This has the advantage that a filesystem can automatically be added to Cromwell engine or standard backend without code changes.; Available filesystems are defined in the config with their corresponding class, and the engine and backends can pick which ones they want to enable.; It removes some dependency of `engine` over the individual filesystem sub projects but it's not all the way there yet.; Thinking about PAPI2 this possibly opens the door to automatically support new filesystems for (de)localization as well if we were to go down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3496
https://github.com/broadinstitute/cromwell/issues/3497:111,Availability,error,error,111,"eg:; ```; workflow foo {; call a { input: x=5, y=10 }; }. task a {; input { Int x }; }; ```; This should be an error: `y` is not an input to `a`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3497
https://github.com/broadinstitute/cromwell/issues/3500:862,Deployability,configurat,configuration,862,"Hi,. I just moved to a new cluster (no sudo) and try to run a WDL script with Cromwell-31.; Everything worked fine on my previous cluster (same Cromwell / WDL / Java versions and same script). After creating the wdl script, I validated it and generated a JSON file (with wdl-0.14), no problem. After running `java -jar cromwell-31.jar run my_script.wdl -i my_script.JSON` the workflow stops, outputting `Permission denied` for the program I call in my .wdl script (which is called from the 'cromwell_executions' folder during the process). I changed the permission to 777 for this program located in my `/usr/bin`, but still the same issue. The program, once located in the 'cromwell_executions' folder, loses the execution permission for the owner.; Same issue no matter if I submit a PBS job or run it interactively. Is there anything to mention in a cromwell configuration file or something to tune in the cluster?. Thanks !",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3500
https://github.com/broadinstitute/cromwell/issues/3500:862,Modifiability,config,configuration,862,"Hi,. I just moved to a new cluster (no sudo) and try to run a WDL script with Cromwell-31.; Everything worked fine on my previous cluster (same Cromwell / WDL / Java versions and same script). After creating the wdl script, I validated it and generated a JSON file (with wdl-0.14), no problem. After running `java -jar cromwell-31.jar run my_script.wdl -i my_script.JSON` the workflow stops, outputting `Permission denied` for the program I call in my .wdl script (which is called from the 'cromwell_executions' folder during the process). I changed the permission to 777 for this program located in my `/usr/bin`, but still the same issue. The program, once located in the 'cromwell_executions' folder, loses the execution permission for the owner.; Same issue no matter if I submit a PBS job or run it interactively. Is there anything to mention in a cromwell configuration file or something to tune in the cluster?. Thanks !",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3500
https://github.com/broadinstitute/cromwell/issues/3500:897,Performance,tune,tune,897,"Hi,. I just moved to a new cluster (no sudo) and try to run a WDL script with Cromwell-31.; Everything worked fine on my previous cluster (same Cromwell / WDL / Java versions and same script). After creating the wdl script, I validated it and generated a JSON file (with wdl-0.14), no problem. After running `java -jar cromwell-31.jar run my_script.wdl -i my_script.JSON` the workflow stops, outputting `Permission denied` for the program I call in my .wdl script (which is called from the 'cromwell_executions' folder during the process). I changed the permission to 777 for this program located in my `/usr/bin`, but still the same issue. The program, once located in the 'cromwell_executions' folder, loses the execution permission for the owner.; Same issue no matter if I submit a PBS job or run it interactively. Is there anything to mention in a cromwell configuration file or something to tune in the cluster?. Thanks !",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3500
https://github.com/broadinstitute/cromwell/issues/3500:226,Security,validat,validated,226,"Hi,. I just moved to a new cluster (no sudo) and try to run a WDL script with Cromwell-31.; Everything worked fine on my previous cluster (same Cromwell / WDL / Java versions and same script). After creating the wdl script, I validated it and generated a JSON file (with wdl-0.14), no problem. After running `java -jar cromwell-31.jar run my_script.wdl -i my_script.JSON` the workflow stops, outputting `Permission denied` for the program I call in my .wdl script (which is called from the 'cromwell_executions' folder during the process). I changed the permission to 777 for this program located in my `/usr/bin`, but still the same issue. The program, once located in the 'cromwell_executions' folder, loses the execution permission for the owner.; Same issue no matter if I submit a PBS job or run it interactively. Is there anything to mention in a cromwell configuration file or something to tune in the cluster?. Thanks !",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3500
https://github.com/broadinstitute/cromwell/issues/3501:428,Energy Efficiency,reduce,reduce,428,"The essence of this issue is that:. ...I have a workflow that calls sub workflows; ...My Workflow validates and everything is swell; ...I want to now run this against a Cromwell server, and turns out I need a zip. ; ...I make a dependencies zip, but see that Cromwell doesn't seem to like it!. AC: Have there be an option in Womtool that packages the files referenced in my import statements as a valid dependencies zip to help reduce issues at runtime.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3501
https://github.com/broadinstitute/cromwell/issues/3501:228,Integrability,depend,dependencies,228,"The essence of this issue is that:. ...I have a workflow that calls sub workflows; ...My Workflow validates and everything is swell; ...I want to now run this against a Cromwell server, and turns out I need a zip. ; ...I make a dependencies zip, but see that Cromwell doesn't seem to like it!. AC: Have there be an option in Womtool that packages the files referenced in my import statements as a valid dependencies zip to help reduce issues at runtime.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3501
https://github.com/broadinstitute/cromwell/issues/3501:403,Integrability,depend,dependencies,403,"The essence of this issue is that:. ...I have a workflow that calls sub workflows; ...My Workflow validates and everything is swell; ...I want to now run this against a Cromwell server, and turns out I need a zip. ; ...I make a dependencies zip, but see that Cromwell doesn't seem to like it!. AC: Have there be an option in Womtool that packages the files referenced in my import statements as a valid dependencies zip to help reduce issues at runtime.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3501
https://github.com/broadinstitute/cromwell/issues/3501:98,Security,validat,validates,98,"The essence of this issue is that:. ...I have a workflow that calls sub workflows; ...My Workflow validates and everything is swell; ...I want to now run this against a Cromwell server, and turns out I need a zip. ; ...I make a dependencies zip, but see that Cromwell doesn't seem to like it!. AC: Have there be an option in Womtool that packages the files referenced in my import statements as a valid dependencies zip to help reduce issues at runtime.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3501
https://github.com/broadinstitute/cromwell/issues/3502:0,Deployability,Pipeline,Pipeline,0,"Pipeline developers like me would benefit from being able to parse the wdl language model so I can write tests that aren't covered by the current wdltool validate. . I hear this could be easily accomplished by wdl4s dumping to json, which users could then parse with custom tools. . Desired tests:; 1. Specified inputs exist and are accessible (depends on backend); 2. Docker images exist and can be read/pulled; 3. dependencies.zip contains all of the necessary files to run the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3502
https://github.com/broadinstitute/cromwell/issues/3502:345,Integrability,depend,depends,345,"Pipeline developers like me would benefit from being able to parse the wdl language model so I can write tests that aren't covered by the current wdltool validate. . I hear this could be easily accomplished by wdl4s dumping to json, which users could then parse with custom tools. . Desired tests:; 1. Specified inputs exist and are accessible (depends on backend); 2. Docker images exist and can be read/pulled; 3. dependencies.zip contains all of the necessary files to run the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3502
https://github.com/broadinstitute/cromwell/issues/3502:416,Integrability,depend,dependencies,416,"Pipeline developers like me would benefit from being able to parse the wdl language model so I can write tests that aren't covered by the current wdltool validate. . I hear this could be easily accomplished by wdl4s dumping to json, which users could then parse with custom tools. . Desired tests:; 1. Specified inputs exist and are accessible (depends on backend); 2. Docker images exist and can be read/pulled; 3. dependencies.zip contains all of the necessary files to run the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3502
https://github.com/broadinstitute/cromwell/issues/3502:154,Security,validat,validate,154,"Pipeline developers like me would benefit from being able to parse the wdl language model so I can write tests that aren't covered by the current wdltool validate. . I hear this could be easily accomplished by wdl4s dumping to json, which users could then parse with custom tools. . Desired tests:; 1. Specified inputs exist and are accessible (depends on backend); 2. Docker images exist and can be read/pulled; 3. dependencies.zip contains all of the necessary files to run the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3502
https://github.com/broadinstitute/cromwell/issues/3502:333,Security,access,accessible,333,"Pipeline developers like me would benefit from being able to parse the wdl language model so I can write tests that aren't covered by the current wdltool validate. . I hear this could be easily accomplished by wdl4s dumping to json, which users could then parse with custom tools. . Desired tests:; 1. Specified inputs exist and are accessible (depends on backend); 2. Docker images exist and can be read/pulled; 3. dependencies.zip contains all of the necessary files to run the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3502
https://github.com/broadinstitute/cromwell/issues/3502:105,Testability,test,tests,105,"Pipeline developers like me would benefit from being able to parse the wdl language model so I can write tests that aren't covered by the current wdltool validate. . I hear this could be easily accomplished by wdl4s dumping to json, which users could then parse with custom tools. . Desired tests:; 1. Specified inputs exist and are accessible (depends on backend); 2. Docker images exist and can be read/pulled; 3. dependencies.zip contains all of the necessary files to run the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3502
https://github.com/broadinstitute/cromwell/issues/3502:291,Testability,test,tests,291,"Pipeline developers like me would benefit from being able to parse the wdl language model so I can write tests that aren't covered by the current wdltool validate. . I hear this could be easily accomplished by wdl4s dumping to json, which users could then parse with custom tools. . Desired tests:; 1. Specified inputs exist and are accessible (depends on backend); 2. Docker images exist and can be read/pulled; 3. dependencies.zip contains all of the necessary files to run the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3502
https://github.com/broadinstitute/cromwell/issues/3503:21,Availability,error,errors,21,"Many types of common errors could be checked faster, but instead fail when they get to a task. Move as much checking as possible up-front so that submission fails (or allow user to request a dummy run with a no-op backend wherein all these checks can be made). Things that aren't checked immediately: ; 1. input miss-specification; 2. docker image non-availability; 3. missing dependencies",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503
https://github.com/broadinstitute/cromwell/issues/3503:352,Availability,avail,availability,352,"Many types of common errors could be checked faster, but instead fail when they get to a task. Move as much checking as possible up-front so that submission fails (or allow user to request a dummy run with a no-op backend wherein all these checks can be made). Things that aren't checked immediately: ; 1. input miss-specification; 2. docker image non-availability; 3. missing dependencies",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503
https://github.com/broadinstitute/cromwell/issues/3503:377,Integrability,depend,dependencies,377,"Many types of common errors could be checked faster, but instead fail when they get to a task. Move as much checking as possible up-front so that submission fails (or allow user to request a dummy run with a no-op backend wherein all these checks can be made). Things that aren't checked immediately: ; 1. input miss-specification; 2. docker image non-availability; 3. missing dependencies",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503
https://github.com/broadinstitute/cromwell/pull/3504:54,Testability,test,tests,54,This was bugging me as I write a bunch of CWL Centaur tests but I didn't want to mix this in with substantive changes so here's a PR that's nearly all noise.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3504
https://github.com/broadinstitute/cromwell/pull/3505:617,Availability,down,down,617,"* Wiring in the remaining draft 3 engine functions' value evaluators; * Modified value evaluation to keep a running track of its side-effect file generation; * Also allowed value evaluation to run value mapping if used in placeholder interpolation. Note: red thumb required because there's some changes to WomIdentifier undoing some of the changes in the forkjoin PR and making them again, but correcter this time. . This PR ended up with far wider-ranging changes than I originally expected (even requiring changes in draft 2 and CWL) so if you see something that smells wrong... ⚠️ ❗️ ❗️ ⚠️ . ~~NB: this PR will go down by about 700 lines once the `forkjoin` PR merges~~ DONE!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3505
https://github.com/broadinstitute/cromwell/pull/3505:320,Usability,undo,undoing,320,"* Wiring in the remaining draft 3 engine functions' value evaluators; * Modified value evaluation to keep a running track of its side-effect file generation; * Also allowed value evaluation to run value mapping if used in placeholder interpolation. Note: red thumb required because there's some changes to WomIdentifier undoing some of the changes in the forkjoin PR and making them again, but correcter this time. . This PR ended up with far wider-ranging changes than I originally expected (even requiring changes in draft 2 and CWL) so if you see something that smells wrong... ⚠️ ❗️ ❗️ ⚠️ . ~~NB: this PR will go down by about 700 lines once the `forkjoin` PR merges~~ DONE!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3505
https://github.com/broadinstitute/cromwell/pull/3506:81,Availability,failure,failure,81,"This will add to the instrumentation metric path the http return code in case of failure, giving better insight as to the reason of the failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3506
https://github.com/broadinstitute/cromwell/pull/3506:136,Availability,failure,failures,136,"This will add to the instrumentation metric path the http return code in case of failure, giving better insight as to the reason of the failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3506
https://github.com/broadinstitute/cromwell/issues/3507:22,Performance,optimiz,optimize,22,"Hi,. We would like to optimize our workflows to make use of local SSD on our HPC compute nodes to minimize iow. Is it possible to run subworkflows in ""local"" backend mode, but then all other tasks in the main workflow as individual SGE jobs? For example, we might want to run Fastq through HaplotypeCaller for an individual sample on the same compute node as a sub workflow to make use of the local SSD for intermediate files and then copy final result to shared storage. These sub workflows would be scattered among samples in our batch. Once the scatter (all samples) is completed, then a new subworkflow is called, again to a single compute node ""local"" backend mode which would run a joint genotype through filter/annotation steps with a copy of final results to shared storage (but any intermediates would be to the local SSD). We are considering a single SGE job per sample dispatched to take the sequencing data through data preprocessing and variant discovery, then a new SGE job to do joint genotyping (for all samples in the batch) and annotation, etc. Would something like this be possible/recommended? I also posted this in the WDL forum - didn't know where best to post this question. Thanks much for your time and thoughts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3507
https://github.com/broadinstitute/cromwell/issues/3510:160,Performance,cache,cache,160,From the [GDoc](https://docs.google.com/document/d/1mHbgzS7UlodljgV8JVk3QWHMqXn8kZNu-x8jFql7vX0/edit?usp=sharing):. > Should we consider doing a lookup of call cache outputs pertaining to the user running the workflow first; We don’t have a way of tracking users; Find other outputs with same outputs?; Consider creating a general concept of an account? Analogous to service account but not GCP-specific,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3510
https://github.com/broadinstitute/cromwell/issues/3511:5,Performance,queue,queued,5,Jobs queued up instead of running. Turns out one job was trying to copy files without permissions over and over. [More info](https://docs.google.com/document/d/1mHbgzS7UlodljgV8JVk3QWHMqXn8kZNu-x8jFql7vX0/edit?usp=sharing),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3511
https://github.com/broadinstitute/cromwell/pull/3512:178,Deployability,release,release,178,"This will allow the user to submit a workflow but in a state where it will not automatically be picked up for execution. This new state is called 'On Hold'. A new API end point 'release' is added which will allow the user to send a signal to Cromwell to allow a workflow to be startable, at which point it will be picked up by normal execution schemes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3512
https://github.com/broadinstitute/cromwell/pull/3513:117,Deployability,Pipeline,PipelinesApiBatchHandler,117,"This is almost only moving around / splitting / renaming stuff.; The pieces to review are the `build.sbt` and maybe `PipelinesApiBatchHandler`, `PipelinesApiFactoryInterface`, and `PipelinesApiRequestFactory` and their implementation under `v1alpha2`. Those are the interfaces for which implementation will be different in v1 and v2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3513
https://github.com/broadinstitute/cromwell/pull/3513:145,Deployability,Pipeline,PipelinesApiFactoryInterface,145,"This is almost only moving around / splitting / renaming stuff.; The pieces to review are the `build.sbt` and maybe `PipelinesApiBatchHandler`, `PipelinesApiFactoryInterface`, and `PipelinesApiRequestFactory` and their implementation under `v1alpha2`. Those are the interfaces for which implementation will be different in v1 and v2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3513
https://github.com/broadinstitute/cromwell/pull/3513:181,Deployability,Pipeline,PipelinesApiRequestFactory,181,"This is almost only moving around / splitting / renaming stuff.; The pieces to review are the `build.sbt` and maybe `PipelinesApiBatchHandler`, `PipelinesApiFactoryInterface`, and `PipelinesApiRequestFactory` and their implementation under `v1alpha2`. Those are the interfaces for which implementation will be different in v1 and v2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3513
https://github.com/broadinstitute/cromwell/pull/3513:266,Integrability,interface,interfaces,266,"This is almost only moving around / splitting / renaming stuff.; The pieces to review are the `build.sbt` and maybe `PipelinesApiBatchHandler`, `PipelinesApiFactoryInterface`, and `PipelinesApiRequestFactory` and their implementation under `v1alpha2`. Those are the interfaces for which implementation will be different in v1 and v2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3513
https://github.com/broadinstitute/cromwell/issues/3514:129,Availability,error,error,129,When running `sbt assembly` I get a merge conflict on netty-resolver. Likely this is coming from the AWS SDK dependency. . ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/asynchttpclient/netty-resolver/2.0.35/netty-resolver-2.0.35.jar:io/netty/resolver/NoopAddressResolverGroup.class; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-resolver/4.1.22.Final/netty-resolver-4.1.22.Final.jar:io/netty/resolver/NoopAddressResolverGroup.class; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514
https://github.com/broadinstitute/cromwell/issues/3514:199,Availability,error,error,199,When running `sbt assembly` I get a merge conflict on netty-resolver. Likely this is coming from the AWS SDK dependency. . ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/asynchttpclient/netty-resolver/2.0.35/netty-resolver-2.0.35.jar:io/netty/resolver/NoopAddressResolverGroup.class; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-resolver/4.1.22.Final/netty-resolver-4.1.22.Final.jar:io/netty/resolver/NoopAddressResolverGroup.class; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514
https://github.com/broadinstitute/cromwell/issues/3514:394,Availability,error,error,394,When running `sbt assembly` I get a merge conflict on netty-resolver. Likely this is coming from the AWS SDK dependency. . ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/asynchttpclient/netty-resolver/2.0.35/netty-resolver-2.0.35.jar:io/netty/resolver/NoopAddressResolverGroup.class; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-resolver/4.1.22.Final/netty-resolver-4.1.22.Final.jar:io/netty/resolver/NoopAddressResolverGroup.class; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514
https://github.com/broadinstitute/cromwell/issues/3514:109,Integrability,depend,dependency,109,When running `sbt assembly` I get a merge conflict on netty-resolver. Likely this is coming from the AWS SDK dependency. . ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/asynchttpclient/netty-resolver/2.0.35/netty-resolver-2.0.35.jar:io/netty/resolver/NoopAddressResolverGroup.class; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-resolver/4.1.22.Final/netty-resolver-4.1.22.Final.jar:io/netty/resolver/NoopAddressResolverGroup.class; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514
https://github.com/broadinstitute/cromwell/issues/3514:227,Performance,Cache,Caches,227,When running `sbt assembly` I get a merge conflict on netty-resolver. Likely this is coming from the AWS SDK dependency. . ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/asynchttpclient/netty-resolver/2.0.35/netty-resolver-2.0.35.jar:io/netty/resolver/NoopAddressResolverGroup.class; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-resolver/4.1.22.Final/netty-resolver-4.1.22.Final.jar:io/netty/resolver/NoopAddressResolverGroup.class; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514
https://github.com/broadinstitute/cromwell/issues/3514:422,Performance,Cache,Caches,422,When running `sbt assembly` I get a merge conflict on netty-resolver. Likely this is coming from the AWS SDK dependency. . ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/asynchttpclient/netty-resolver/2.0.35/netty-resolver-2.0.35.jar:io/netty/resolver/NoopAddressResolverGroup.class; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-resolver/4.1.22.Final/netty-resolver-4.1.22.Final.jar:io/netty/resolver/NoopAddressResolverGroup.class; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514
https://github.com/broadinstitute/cromwell/issues/3515:90,Availability,error,error,90,The Java SDK for AWS produces multiple JARs with some path conflicts. For example:; ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/ses/2.0.0-preview-9/ses-2.0.0-preview-9.jar:codegen-resources/service-2.json; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/snowball/2.0.0-preview-9/snowball-2.0.0-preview-9.jar:codegen-resources/service-2.json; # ...; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3515
https://github.com/broadinstitute/cromwell/issues/3515:160,Availability,error,error,160,The Java SDK for AWS produces multiple JARs with some path conflicts. For example:; ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/ses/2.0.0-preview-9/ses-2.0.0-preview-9.jar:codegen-resources/service-2.json; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/snowball/2.0.0-preview-9/snowball-2.0.0-preview-9.jar:codegen-resources/service-2.json; # ...; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3515
https://github.com/broadinstitute/cromwell/issues/3515:338,Availability,error,error,338,The Java SDK for AWS produces multiple JARs with some path conflicts. For example:; ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/ses/2.0.0-preview-9/ses-2.0.0-preview-9.jar:codegen-resources/service-2.json; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/snowball/2.0.0-preview-9/snowball-2.0.0-preview-9.jar:codegen-resources/service-2.json; # ...; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3515
https://github.com/broadinstitute/cromwell/issues/3515:188,Performance,Cache,Caches,188,The Java SDK for AWS produces multiple JARs with some path conflicts. For example:; ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/ses/2.0.0-preview-9/ses-2.0.0-preview-9.jar:codegen-resources/service-2.json; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/snowball/2.0.0-preview-9/snowball-2.0.0-preview-9.jar:codegen-resources/service-2.json; # ...; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3515
https://github.com/broadinstitute/cromwell/issues/3515:366,Performance,Cache,Caches,366,The Java SDK for AWS produces multiple JARs with some path conflicts. For example:; ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/ses/2.0.0-preview-9/ses-2.0.0-preview-9.jar:codegen-resources/service-2.json; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/snowball/2.0.0-preview-9/snowball-2.0.0-preview-9.jar:codegen-resources/service-2.json; # ...; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3515
https://github.com/broadinstitute/cromwell/pull/3516:79,Testability,Test,Test,79,Borne of a requirement on GDC Dnaseq using Enumerations. Represents a spike on Test \#3 which was the only one that tested Enumerations at the time of this writing. A list of features/fixes. * implementation of SchemaDefRequirement. This produced a lot of noise as any time `Myriad` types were used this must be provided for reference.; * nested input bindings sort properly. ; * impl of EnumSchema for input & output; * fix for coproduct of types -> command parts ( previously was using values from fields that matched any field of any coproduct type. this fix regards each type holistically and thus rejects unless all fields match); * Introduced a `RequirementsAndHints` class to handle common operations on `List[Requirement]`. ## ~TODO~. ~I broke conf test \#71 and centaur test dealing w/ secondary files. I expect fixes to be minor.~,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3516
https://github.com/broadinstitute/cromwell/pull/3516:116,Testability,test,tested,116,Borne of a requirement on GDC Dnaseq using Enumerations. Represents a spike on Test \#3 which was the only one that tested Enumerations at the time of this writing. A list of features/fixes. * implementation of SchemaDefRequirement. This produced a lot of noise as any time `Myriad` types were used this must be provided for reference.; * nested input bindings sort properly. ; * impl of EnumSchema for input & output; * fix for coproduct of types -> command parts ( previously was using values from fields that matched any field of any coproduct type. this fix regards each type holistically and thus rejects unless all fields match); * Introduced a `RequirementsAndHints` class to handle common operations on `List[Requirement]`. ## ~TODO~. ~I broke conf test \#71 and centaur test dealing w/ secondary files. I expect fixes to be minor.~,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3516
https://github.com/broadinstitute/cromwell/pull/3516:757,Testability,test,test,757,Borne of a requirement on GDC Dnaseq using Enumerations. Represents a spike on Test \#3 which was the only one that tested Enumerations at the time of this writing. A list of features/fixes. * implementation of SchemaDefRequirement. This produced a lot of noise as any time `Myriad` types were used this must be provided for reference.; * nested input bindings sort properly. ; * impl of EnumSchema for input & output; * fix for coproduct of types -> command parts ( previously was using values from fields that matched any field of any coproduct type. this fix regards each type holistically and thus rejects unless all fields match); * Introduced a `RequirementsAndHints` class to handle common operations on `List[Requirement]`. ## ~TODO~. ~I broke conf test \#71 and centaur test dealing w/ secondary files. I expect fixes to be minor.~,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3516
https://github.com/broadinstitute/cromwell/pull/3516:779,Testability,test,test,779,Borne of a requirement on GDC Dnaseq using Enumerations. Represents a spike on Test \#3 which was the only one that tested Enumerations at the time of this writing. A list of features/fixes. * implementation of SchemaDefRequirement. This produced a lot of noise as any time `Myriad` types were used this must be provided for reference.; * nested input bindings sort properly. ; * impl of EnumSchema for input & output; * fix for coproduct of types -> command parts ( previously was using values from fields that matched any field of any coproduct type. this fix regards each type holistically and thus rejects unless all fields match); * Introduced a `RequirementsAndHints` class to handle common operations on `List[Requirement]`. ## ~TODO~. ~I broke conf test \#71 and centaur test dealing w/ secondary files. I expect fixes to be minor.~,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3516
https://github.com/broadinstitute/cromwell/issues/3517:189,Performance,race condition,race conditions,189,Some endpoints validate the workflow Id by checking in the metadata that there is a workflow with this ID.; This is not great because. 1. It relies on metadata; 2. Opens a small window for race conditions. We also return 500 if the workflow does not exist which doesn't seem like the best suited code for that,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3517
https://github.com/broadinstitute/cromwell/issues/3517:15,Security,validat,validate,15,Some endpoints validate the workflow Id by checking in the metadata that there is a workflow with this ID.; This is not great because. 1. It relies on metadata; 2. Opens a small window for race conditions. We also return 500 if the workflow does not exist which doesn't seem like the best suited code for that,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3517
https://github.com/broadinstitute/cromwell/issues/3518:1071,Availability,avail,available,1071,"thin China. There is no known Alibaba Cloud provided CDN or cache for docker images on BCS. To significantly speed up docker pulls users are able to upload docker images to their own private docker registry hosted within one of their OSS buckets. This uses a plugin contributed to the docker codebase that stores and retrieves docker images via an OSS client. Currently the BCS backend allows users to specify the private OSS registry within the `docker` runtime attribute. For portability, the `docker` runtime attribute should only specify the image, and a separate `dockerRegistry` runtime attribute should optionally specify a private OSS registry. Ideally there should be a way for a user to cache docker images on their own private OSS registry while still using contributed by others WDLs. One particular issue for call caching may be that the docker image hashes are probably registry specific. Cromwell's call caching code requires WDL to specify a hash that may only be available on docker hub, and may not be available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution/tree/v2.6.2/registry/storage/driver/oss; - https://stackoverflow.com/questions/45533005/w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:1111,Availability,avail,available,1111,"thin China. There is no known Alibaba Cloud provided CDN or cache for docker images on BCS. To significantly speed up docker pulls users are able to upload docker images to their own private docker registry hosted within one of their OSS buckets. This uses a plugin contributed to the docker codebase that stores and retrieves docker images via an OSS client. Currently the BCS backend allows users to specify the private OSS registry within the `docker` runtime attribute. For portability, the `docker` runtime attribute should only specify the image, and a separate `dockerRegistry` runtime attribute should optionally specify a private OSS registry. Ideally there should be a way for a user to cache docker images on their own private OSS registry while still using contributed by others WDLs. One particular issue for call caching may be that the docker image hashes are probably registry specific. Cromwell's call caching code requires WDL to specify a hash that may only be available on docker hub, and may not be available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution/tree/v2.6.2/registry/storage/driver/oss; - https://stackoverflow.com/questions/45533005/w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:2116,Integrability,depend,depend-on-registry,2116," available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution/tree/v2.6.2/registry/storage/driver/oss; - https://stackoverflow.com/questions/45533005/why-digests-are-different-depend-on-registry; - http://cromwell.readthedocs.io/en/develop/CallCaching/; - https://github.com/broadinstitute/cromwell/blob/31/docs/backends/BCS.md#user-content-docker. A/C:. - Document if an image copied to OSS has the same sha256 as docker hub; - Pull the `ubuntu:latest` image from docker hub; - Record the sha256 from docker hub; - Obtain the OSS credentials from `secret/dsde/cromwell/common/cromwell-bcs` in vault; - Push the image to a private OSS bucket; - Check if the image contains the same hash in OSS as was in docker hub; - Document the results; - Create tickets as appropriate for:; - Splitting the docker runtime attribute used by OSS into `docker` and `dockerRegistry`; - Cromwell engine hashing of docker images stored in OSS; - (Optional) Require the `docker` runtime attribute to be mandatory for the BCS backend; - The A/C of the above tickets should include:; - Restore centaur docker testing to `testCentaurBcs.sh`; - Add a centaur `call_cache_capoeira_bcs`; - Updatin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:350,Modifiability,plugin,plugin,350,"Docker Hub is incredibly slow when accessed directly from Hangzhou or any other location within China. There is no known Alibaba Cloud provided CDN or cache for docker images on BCS. To significantly speed up docker pulls users are able to upload docker images to their own private docker registry hosted within one of their OSS buckets. This uses a plugin contributed to the docker codebase that stores and retrieves docker images via an OSS client. Currently the BCS backend allows users to specify the private OSS registry within the `docker` runtime attribute. For portability, the `docker` runtime attribute should only specify the image, and a separate `dockerRegistry` runtime attribute should optionally specify a private OSS registry. Ideally there should be a way for a user to cache docker images on their own private OSS registry while still using contributed by others WDLs. One particular issue for call caching may be that the docker image hashes are probably registry specific. Cromwell's call caching code requires WDL to specify a hash that may only be available on docker hub, and may not be available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:569,Modifiability,portab,portability,569,"Docker Hub is incredibly slow when accessed directly from Hangzhou or any other location within China. There is no known Alibaba Cloud provided CDN or cache for docker images on BCS. To significantly speed up docker pulls users are able to upload docker images to their own private docker registry hosted within one of their OSS buckets. This uses a plugin contributed to the docker codebase that stores and retrieves docker images via an OSS client. Currently the BCS backend allows users to specify the private OSS registry within the `docker` runtime attribute. For portability, the `docker` runtime attribute should only specify the image, and a separate `dockerRegistry` runtime attribute should optionally specify a private OSS registry. Ideally there should be a way for a user to cache docker images on their own private OSS registry while still using contributed by others WDLs. One particular issue for call caching may be that the docker image hashes are probably registry specific. Cromwell's call caching code requires WDL to specify a hash that may only be available on docker hub, and may not be available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:151,Performance,cache,cache,151,"Docker Hub is incredibly slow when accessed directly from Hangzhou or any other location within China. There is no known Alibaba Cloud provided CDN or cache for docker images on BCS. To significantly speed up docker pulls users are able to upload docker images to their own private docker registry hosted within one of their OSS buckets. This uses a plugin contributed to the docker codebase that stores and retrieves docker images via an OSS client. Currently the BCS backend allows users to specify the private OSS registry within the `docker` runtime attribute. For portability, the `docker` runtime attribute should only specify the image, and a separate `dockerRegistry` runtime attribute should optionally specify a private OSS registry. Ideally there should be a way for a user to cache docker images on their own private OSS registry while still using contributed by others WDLs. One particular issue for call caching may be that the docker image hashes are probably registry specific. Cromwell's call caching code requires WDL to specify a hash that may only be available on docker hub, and may not be available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:788,Performance,cache,cache,788,"Docker Hub is incredibly slow when accessed directly from Hangzhou or any other location within China. There is no known Alibaba Cloud provided CDN or cache for docker images on BCS. To significantly speed up docker pulls users are able to upload docker images to their own private docker registry hosted within one of their OSS buckets. This uses a plugin contributed to the docker codebase that stores and retrieves docker images via an OSS client. Currently the BCS backend allows users to specify the private OSS registry within the `docker` runtime attribute. For portability, the `docker` runtime attribute should only specify the image, and a separate `dockerRegistry` runtime attribute should optionally specify a private OSS registry. Ideally there should be a way for a user to cache docker images on their own private OSS registry while still using contributed by others WDLs. One particular issue for call caching may be that the docker image hashes are probably registry specific. Cromwell's call caching code requires WDL to specify a hash that may only be available on docker hub, and may not be available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:35,Security,access,accessed,35,"Docker Hub is incredibly slow when accessed directly from Hangzhou or any other location within China. There is no known Alibaba Cloud provided CDN or cache for docker images on BCS. To significantly speed up docker pulls users are able to upload docker images to their own private docker registry hosted within one of their OSS buckets. This uses a plugin contributed to the docker codebase that stores and retrieves docker images via an OSS client. Currently the BCS backend allows users to specify the private OSS registry within the `docker` runtime attribute. For portability, the `docker` runtime attribute should only specify the image, and a separate `dockerRegistry` runtime attribute should optionally specify a private OSS registry. Ideally there should be a way for a user to cache docker images on their own private OSS registry while still using contributed by others WDLs. One particular issue for call caching may be that the docker image hashes are probably registry specific. Cromwell's call caching code requires WDL to specify a hash that may only be available on docker hub, and may not be available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:955,Security,hash,hashes,955,"Docker Hub is incredibly slow when accessed directly from Hangzhou or any other location within China. There is no known Alibaba Cloud provided CDN or cache for docker images on BCS. To significantly speed up docker pulls users are able to upload docker images to their own private docker registry hosted within one of their OSS buckets. This uses a plugin contributed to the docker codebase that stores and retrieves docker images via an OSS client. Currently the BCS backend allows users to specify the private OSS registry within the `docker` runtime attribute. For portability, the `docker` runtime attribute should only specify the image, and a separate `dockerRegistry` runtime attribute should optionally specify a private OSS registry. Ideally there should be a way for a user to cache docker images on their own private OSS registry while still using contributed by others WDLs. One particular issue for call caching may be that the docker image hashes are probably registry specific. Cromwell's call caching code requires WDL to specify a hash that may only be available on docker hub, and may not be available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:1049,Security,hash,hash,1049,"thin China. There is no known Alibaba Cloud provided CDN or cache for docker images on BCS. To significantly speed up docker pulls users are able to upload docker images to their own private docker registry hosted within one of their OSS buckets. This uses a plugin contributed to the docker codebase that stores and retrieves docker images via an OSS client. Currently the BCS backend allows users to specify the private OSS registry within the `docker` runtime attribute. For portability, the `docker` runtime attribute should only specify the image, and a separate `dockerRegistry` runtime attribute should optionally specify a private OSS registry. Ideally there should be a way for a user to cache docker images on their own private OSS registry while still using contributed by others WDLs. One particular issue for call caching may be that the docker image hashes are probably registry specific. Cromwell's call caching code requires WDL to specify a hash that may only be available on docker hub, and may not be available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution/tree/v2.6.2/registry/storage/driver/oss; - https://stackoverflow.com/questions/45533005/w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:2622,Security,hash,hash,2622,"ven if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution/tree/v2.6.2/registry/storage/driver/oss; - https://stackoverflow.com/questions/45533005/why-digests-are-different-depend-on-registry; - http://cromwell.readthedocs.io/en/develop/CallCaching/; - https://github.com/broadinstitute/cromwell/blob/31/docs/backends/BCS.md#user-content-docker. A/C:. - Document if an image copied to OSS has the same sha256 as docker hub; - Pull the `ubuntu:latest` image from docker hub; - Record the sha256 from docker hub; - Obtain the OSS credentials from `secret/dsde/cromwell/common/cromwell-bcs` in vault; - Push the image to a private OSS bucket; - Check if the image contains the same hash in OSS as was in docker hub; - Document the results; - Create tickets as appropriate for:; - Splitting the docker runtime attribute used by OSS into `docker` and `dockerRegistry`; - Cromwell engine hashing of docker images stored in OSS; - (Optional) Require the `docker` runtime attribute to be mandatory for the BCS backend; - The A/C of the above tickets should include:; - Restore centaur docker testing to `testCentaurBcs.sh`; - Add a centaur `call_cache_capoeira_bcs`; - Updating cromwell readme docs on BCS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:2825,Security,hash,hashing,2825,"ven if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution/tree/v2.6.2/registry/storage/driver/oss; - https://stackoverflow.com/questions/45533005/why-digests-are-different-depend-on-registry; - http://cromwell.readthedocs.io/en/develop/CallCaching/; - https://github.com/broadinstitute/cromwell/blob/31/docs/backends/BCS.md#user-content-docker. A/C:. - Document if an image copied to OSS has the same sha256 as docker hub; - Pull the `ubuntu:latest` image from docker hub; - Record the sha256 from docker hub; - Obtain the OSS credentials from `secret/dsde/cromwell/common/cromwell-bcs` in vault; - Push the image to a private OSS bucket; - Check if the image contains the same hash in OSS as was in docker hub; - Document the results; - Create tickets as appropriate for:; - Splitting the docker runtime attribute used by OSS into `docker` and `dockerRegistry`; - Cromwell engine hashing of docker images stored in OSS; - (Optional) Require the `docker` runtime attribute to be mandatory for the BCS backend; - The A/C of the above tickets should include:; - Restore centaur docker testing to `testCentaurBcs.sh`; - Add a centaur `call_cache_capoeira_bcs`; - Updating cromwell readme docs on BCS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:3027,Testability,test,testing,3027,"ven if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution/tree/v2.6.2/registry/storage/driver/oss; - https://stackoverflow.com/questions/45533005/why-digests-are-different-depend-on-registry; - http://cromwell.readthedocs.io/en/develop/CallCaching/; - https://github.com/broadinstitute/cromwell/blob/31/docs/backends/BCS.md#user-content-docker. A/C:. - Document if an image copied to OSS has the same sha256 as docker hub; - Pull the `ubuntu:latest` image from docker hub; - Record the sha256 from docker hub; - Obtain the OSS credentials from `secret/dsde/cromwell/common/cromwell-bcs` in vault; - Push the image to a private OSS bucket; - Check if the image contains the same hash in OSS as was in docker hub; - Document the results; - Create tickets as appropriate for:; - Splitting the docker runtime attribute used by OSS into `docker` and `dockerRegistry`; - Cromwell engine hashing of docker images stored in OSS; - (Optional) Require the `docker` runtime attribute to be mandatory for the BCS backend; - The A/C of the above tickets should include:; - Restore centaur docker testing to `testCentaurBcs.sh`; - Add a centaur `call_cache_capoeira_bcs`; - Updating cromwell readme docs on BCS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3518:3039,Testability,test,testCentaurBcs,3039,"ven if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution/tree/v2.6.2/registry/storage/driver/oss; - https://stackoverflow.com/questions/45533005/why-digests-are-different-depend-on-registry; - http://cromwell.readthedocs.io/en/develop/CallCaching/; - https://github.com/broadinstitute/cromwell/blob/31/docs/backends/BCS.md#user-content-docker. A/C:. - Document if an image copied to OSS has the same sha256 as docker hub; - Pull the `ubuntu:latest` image from docker hub; - Record the sha256 from docker hub; - Obtain the OSS credentials from `secret/dsde/cromwell/common/cromwell-bcs` in vault; - Push the image to a private OSS bucket; - Check if the image contains the same hash in OSS as was in docker hub; - Document the results; - Create tickets as appropriate for:; - Splitting the docker runtime attribute used by OSS into `docker` and `dockerRegistry`; - Cromwell engine hashing of docker images stored in OSS; - (Optional) Require the `docker` runtime attribute to be mandatory for the BCS backend; - The A/C of the above tickets should include:; - Restore centaur docker testing to `testCentaurBcs.sh`; - Add a centaur `call_cache_capoeira_bcs`; - Updating cromwell readme docs on BCS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518
https://github.com/broadinstitute/cromwell/issues/3519:338,Testability,test,testing,338,Globs are currently not supported by the BCS backend. Currently the backend submits one BCS task per BCS job where the task runs the script specified by the user within the WDL command block. Adding glob support may require modifications to the BCS python worker and/or adding an additional BCS task to each BCS job. A/C:; - Restore glob testing to `testCentaurBcs.sh`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3519
https://github.com/broadinstitute/cromwell/issues/3519:350,Testability,test,testCentaurBcs,350,Globs are currently not supported by the BCS backend. Currently the backend submits one BCS task per BCS job where the task runs the script specified by the user within the WDL command block. Adding glob support may require modifications to the BCS python worker and/or adding an additional BCS task to each BCS job. A/C:; - Restore glob testing to `testCentaurBcs.sh`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3519
https://github.com/broadinstitute/cromwell/issues/3520:1516,Deployability,Update,Update,1516,"The code used for localizing and executing the Cromwell exec script is embedded within tar.gz file in the cromwell code base. Today, each user of cromwell must locate the worker.tar.gz from cromwell's source code repository and provide a location of the file as a runtime attribute. If the path provided in the runtime attributes is a local path then cromwell streams the bytes to the OSS `Command.packagePath`. Instead of requiring the user to access cromwell's source code cromwell should move the file into an embedded resource under the directory `src/main/resources/`. The likely package location based on the other classes is `supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/`. Then using `better.files.File(java.lang.Object.getClass.getResource(""worker.tar.gz"").getPath)` the bytes can still be streamed to OSS. - https://github.com/aliyun/aliyun-openapi-java-sdk/blob/77e3f7639db351ced87d13b9a62f5566645f107c/aliyun-java-sdk-batchcompute/src/main/java/com/aliyuncs/batchcompute/pojo/v20151111/Command.java#L44-L47; - https://github.com/broadinstitute/cromwell/blob/31/supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/BcsAsyncBackendJobExecutionActor.scala#L245-L255; - https://github.com/broadinstitute/cromwell/blob/31/supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/worker.tar.gz. A/C:; - A user no longer needs to specify the path to the `worker.tar.gz` as it is an embedded resource; - Remove the `worker.tar.gz` reference from the `bcs_centaur.conf.ctmpl`; - Update cromwell docs on BCS so that specifying the worker path is not mandatory; - (Optional) Allow a configuration value / runtime attribute to still override the worker path for debugging purposes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3520
https://github.com/broadinstitute/cromwell/issues/3520:1618,Deployability,configurat,configuration,1618,"The code used for localizing and executing the Cromwell exec script is embedded within tar.gz file in the cromwell code base. Today, each user of cromwell must locate the worker.tar.gz from cromwell's source code repository and provide a location of the file as a runtime attribute. If the path provided in the runtime attributes is a local path then cromwell streams the bytes to the OSS `Command.packagePath`. Instead of requiring the user to access cromwell's source code cromwell should move the file into an embedded resource under the directory `src/main/resources/`. The likely package location based on the other classes is `supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/`. Then using `better.files.File(java.lang.Object.getClass.getResource(""worker.tar.gz"").getPath)` the bytes can still be streamed to OSS. - https://github.com/aliyun/aliyun-openapi-java-sdk/blob/77e3f7639db351ced87d13b9a62f5566645f107c/aliyun-java-sdk-batchcompute/src/main/java/com/aliyuncs/batchcompute/pojo/v20151111/Command.java#L44-L47; - https://github.com/broadinstitute/cromwell/blob/31/supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/BcsAsyncBackendJobExecutionActor.scala#L245-L255; - https://github.com/broadinstitute/cromwell/blob/31/supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/worker.tar.gz. A/C:; - A user no longer needs to specify the path to the `worker.tar.gz` as it is an embedded resource; - Remove the `worker.tar.gz` reference from the `bcs_centaur.conf.ctmpl`; - Update cromwell docs on BCS so that specifying the worker path is not mandatory; - (Optional) Allow a configuration value / runtime attribute to still override the worker path for debugging purposes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3520
https://github.com/broadinstitute/cromwell/issues/3520:1618,Modifiability,config,configuration,1618,"The code used for localizing and executing the Cromwell exec script is embedded within tar.gz file in the cromwell code base. Today, each user of cromwell must locate the worker.tar.gz from cromwell's source code repository and provide a location of the file as a runtime attribute. If the path provided in the runtime attributes is a local path then cromwell streams the bytes to the OSS `Command.packagePath`. Instead of requiring the user to access cromwell's source code cromwell should move the file into an embedded resource under the directory `src/main/resources/`. The likely package location based on the other classes is `supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/`. Then using `better.files.File(java.lang.Object.getClass.getResource(""worker.tar.gz"").getPath)` the bytes can still be streamed to OSS. - https://github.com/aliyun/aliyun-openapi-java-sdk/blob/77e3f7639db351ced87d13b9a62f5566645f107c/aliyun-java-sdk-batchcompute/src/main/java/com/aliyuncs/batchcompute/pojo/v20151111/Command.java#L44-L47; - https://github.com/broadinstitute/cromwell/blob/31/supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/BcsAsyncBackendJobExecutionActor.scala#L245-L255; - https://github.com/broadinstitute/cromwell/blob/31/supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/worker.tar.gz. A/C:; - A user no longer needs to specify the path to the `worker.tar.gz` as it is an embedded resource; - Remove the `worker.tar.gz` reference from the `bcs_centaur.conf.ctmpl`; - Update cromwell docs on BCS so that specifying the worker path is not mandatory; - (Optional) Allow a configuration value / runtime attribute to still override the worker path for debugging purposes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3520
https://github.com/broadinstitute/cromwell/issues/3520:445,Security,access,access,445,"The code used for localizing and executing the Cromwell exec script is embedded within tar.gz file in the cromwell code base. Today, each user of cromwell must locate the worker.tar.gz from cromwell's source code repository and provide a location of the file as a runtime attribute. If the path provided in the runtime attributes is a local path then cromwell streams the bytes to the OSS `Command.packagePath`. Instead of requiring the user to access cromwell's source code cromwell should move the file into an embedded resource under the directory `src/main/resources/`. The likely package location based on the other classes is `supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/`. Then using `better.files.File(java.lang.Object.getClass.getResource(""worker.tar.gz"").getPath)` the bytes can still be streamed to OSS. - https://github.com/aliyun/aliyun-openapi-java-sdk/blob/77e3f7639db351ced87d13b9a62f5566645f107c/aliyun-java-sdk-batchcompute/src/main/java/com/aliyuncs/batchcompute/pojo/v20151111/Command.java#L44-L47; - https://github.com/broadinstitute/cromwell/blob/31/supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/BcsAsyncBackendJobExecutionActor.scala#L245-L255; - https://github.com/broadinstitute/cromwell/blob/31/supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/worker.tar.gz. A/C:; - A user no longer needs to specify the path to the `worker.tar.gz` as it is an embedded resource; - Remove the `worker.tar.gz` reference from the `bcs_centaur.conf.ctmpl`; - Update cromwell docs on BCS so that specifying the worker path is not mandatory; - (Optional) Allow a configuration value / runtime attribute to still override the worker path for debugging purposes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3520
https://github.com/broadinstitute/cromwell/issues/3521:445,Availability,avail,available,445,The source code used for localizing and executing the Cromwell exec script on BCS is embedded within tar.gz file in the cromwell code base. To make this code easier to update the code should be moved out of the tar and live as individual files within the source code. - https://github.com/broadinstitute/cromwell/blob/31/supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/worker.tar.gz. A/C:; - The source code for the BCS worker is available in the cromwell source tree ; - Running `sbt assembly` still creates a an embedded resource `worker.tar.gz` that the BCS backend can upload to OSS.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3521
https://github.com/broadinstitute/cromwell/issues/3521:168,Deployability,update,update,168,The source code used for localizing and executing the Cromwell exec script on BCS is embedded within tar.gz file in the cromwell code base. To make this code easier to update the code should be moved out of the tar and live as individual files within the source code. - https://github.com/broadinstitute/cromwell/blob/31/supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/worker.tar.gz. A/C:; - The source code for the BCS worker is available in the cromwell source tree ; - Running `sbt assembly` still creates a an embedded resource `worker.tar.gz` that the BCS backend can upload to OSS.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3521
https://github.com/broadinstitute/cromwell/issues/3522:34,Availability,error,error,34,A bad file input should create an error on the BCS backend but is currently succeeding. A/C:; - Restore the test `bad_file_string` in `testCentaurBcs.sh`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3522
https://github.com/broadinstitute/cromwell/issues/3522:108,Testability,test,test,108,A bad file input should create an error on the BCS backend but is currently succeeding. A/C:; - Restore the test `bad_file_string` in `testCentaurBcs.sh`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3522
https://github.com/broadinstitute/cromwell/issues/3522:135,Testability,test,testCentaurBcs,135,A bad file input should create an error on the BCS backend but is currently succeeding. A/C:; - Restore the test `bad_file_string` in `testCentaurBcs.sh`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3522
https://github.com/broadinstitute/cromwell/issues/3523:56,Availability,error,error,56,Delocalizing a file that doesn't exist should create an error on the BCS backend but is currently succeeding. This may or may not require changes to the embedded python worker. A/C:; - Restore the test `bad_file_string` in `testCentaurBcs.sh`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3523
https://github.com/broadinstitute/cromwell/issues/3523:197,Testability,test,test,197,Delocalizing a file that doesn't exist should create an error on the BCS backend but is currently succeeding. This may or may not require changes to the embedded python worker. A/C:; - Restore the test `bad_file_string` in `testCentaurBcs.sh`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3523
https://github.com/broadinstitute/cromwell/issues/3523:224,Testability,test,testCentaurBcs,224,Delocalizing a file that doesn't exist should create an error on the BCS backend but is currently succeeding. This may or may not require changes to the embedded python worker. A/C:; - Restore the test `bad_file_string` in `testCentaurBcs.sh`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3523
https://github.com/broadinstitute/cromwell/issues/3524:278,Testability,test,test,278,When running on the BCS backend a dir called tmp should not be present within the execution directory. The BCS backend currently runs directly on the VM without docker. Thus this issue may or may not be a problem when running on the BCS backend with docker. A/C:; - Restore the test `tmp_dir` in `testCentaurBcs.sh`; - If the BCS backend is allowed to run with & without docker ensure there are centaur tests for both cases,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3524
https://github.com/broadinstitute/cromwell/issues/3524:297,Testability,test,testCentaurBcs,297,When running on the BCS backend a dir called tmp should not be present within the execution directory. The BCS backend currently runs directly on the VM without docker. Thus this issue may or may not be a problem when running on the BCS backend with docker. A/C:; - Restore the test `tmp_dir` in `testCentaurBcs.sh`; - If the BCS backend is allowed to run with & without docker ensure there are centaur tests for both cases,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3524
https://github.com/broadinstitute/cromwell/issues/3524:403,Testability,test,tests,403,When running on the BCS backend a dir called tmp should not be present within the execution directory. The BCS backend currently runs directly on the VM without docker. Thus this issue may or may not be a problem when running on the BCS backend with docker. A/C:; - Restore the test `tmp_dir` in `testCentaurBcs.sh`; - If the BCS backend is allowed to run with & without docker ensure there are centaur tests for both cases,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3524
https://github.com/broadinstitute/cromwell/issues/3526:70,Testability,log,logging,70,"The OSS filesystem is built on the OSS api. In the OSS api v3.1.0 the logging is [very verbose by default](https://github.com/aliyun/aliyun-oss-java-sdk/blob/3.1.0/src/main/java/com/aliyun/oss/model/WebServiceRequest.java#L34-L36). Some quieting in Cromwell was added to the PR #3525, but there was a note that this should be generalized:. > there should be a thing that takes an ossPath and returns a request with logging disabled. Alternatively the logging may be quieter by default in a future version of the OSS API.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3526
https://github.com/broadinstitute/cromwell/issues/3526:415,Testability,log,logging,415,"The OSS filesystem is built on the OSS api. In the OSS api v3.1.0 the logging is [very verbose by default](https://github.com/aliyun/aliyun-oss-java-sdk/blob/3.1.0/src/main/java/com/aliyun/oss/model/WebServiceRequest.java#L34-L36). Some quieting in Cromwell was added to the PR #3525, but there was a note that this should be generalized:. > there should be a thing that takes an ossPath and returns a request with logging disabled. Alternatively the logging may be quieter by default in a future version of the OSS API.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3526
https://github.com/broadinstitute/cromwell/issues/3526:451,Testability,log,logging,451,"The OSS filesystem is built on the OSS api. In the OSS api v3.1.0 the logging is [very verbose by default](https://github.com/aliyun/aliyun-oss-java-sdk/blob/3.1.0/src/main/java/com/aliyun/oss/model/WebServiceRequest.java#L34-L36). Some quieting in Cromwell was added to the PR #3525, but there was a note that this should be generalized:. > there should be a thing that takes an ossPath and returns a request with logging disabled. Alternatively the logging may be quieter by default in a future version of the OSS API.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3526
https://github.com/broadinstitute/cromwell/pull/3530:171,Security,validat,validate,171,"- [x] Needs a rebase on `develop` after #3505 . Wires in and checks imported tasks, workflows, namespaces and aliases; NB: the test WDL appears twice - once in a `womtool validate` test and once as a centaur test",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3530
https://github.com/broadinstitute/cromwell/pull/3530:127,Testability,test,test,127,"- [x] Needs a rebase on `develop` after #3505 . Wires in and checks imported tasks, workflows, namespaces and aliases; NB: the test WDL appears twice - once in a `womtool validate` test and once as a centaur test",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3530
https://github.com/broadinstitute/cromwell/pull/3530:181,Testability,test,test,181,"- [x] Needs a rebase on `develop` after #3505 . Wires in and checks imported tasks, workflows, namespaces and aliases; NB: the test WDL appears twice - once in a `womtool validate` test and once as a centaur test",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3530
https://github.com/broadinstitute/cromwell/pull/3530:208,Testability,test,test,208,"- [x] Needs a rebase on `develop` after #3505 . Wires in and checks imported tasks, workflows, namespaces and aliases; NB: the test WDL appears twice - once in a `womtool validate` test and once as a centaur test",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3530
https://github.com/broadinstitute/cromwell/issues/3533:51,Availability,error,error,51,"I tried to ask this on the broad forums but got an error.""You need the Garden.Community.Manage permission to do that."", ""Class"": ""Gdn_UserException"" . I have been using a . backend.default = SGE. Line in my config file. This has been working until now, with my jobs submitted using my defined SGE backend. Today I changed some other parts of the server, unrelated and numerous. Now SOME jobs are being submitted on the SGE backend, and some on the Local backend. (I did not configure a local backend, but I assume one is built in to cromwell. How is the backend to run on chosen by cromwell? (I know of backend.default, and I beleive there is a wdl.task.runtime.backend parameter also (undocumented from what I can tell)). How can I prevent any task ever running on the local backend? I want this to be a hard error, and not overload my sge login node. . Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533
https://github.com/broadinstitute/cromwell/issues/3533:810,Availability,error,error,810,"I tried to ask this on the broad forums but got an error.""You need the Garden.Community.Manage permission to do that."", ""Class"": ""Gdn_UserException"" . I have been using a . backend.default = SGE. Line in my config file. This has been working until now, with my jobs submitted using my defined SGE backend. Today I changed some other parts of the server, unrelated and numerous. Now SOME jobs are being submitted on the SGE backend, and some on the Local backend. (I did not configure a local backend, but I assume one is built in to cromwell. How is the backend to run on chosen by cromwell? (I know of backend.default, and I beleive there is a wdl.task.runtime.backend parameter also (undocumented from what I can tell)). How can I prevent any task ever running on the local backend? I want this to be a hard error, and not overload my sge login node. . Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533
https://github.com/broadinstitute/cromwell/issues/3533:207,Modifiability,config,config,207,"I tried to ask this on the broad forums but got an error.""You need the Garden.Community.Manage permission to do that."", ""Class"": ""Gdn_UserException"" . I have been using a . backend.default = SGE. Line in my config file. This has been working until now, with my jobs submitted using my defined SGE backend. Today I changed some other parts of the server, unrelated and numerous. Now SOME jobs are being submitted on the SGE backend, and some on the Local backend. (I did not configure a local backend, but I assume one is built in to cromwell. How is the backend to run on chosen by cromwell? (I know of backend.default, and I beleive there is a wdl.task.runtime.backend parameter also (undocumented from what I can tell)). How can I prevent any task ever running on the local backend? I want this to be a hard error, and not overload my sge login node. . Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533
https://github.com/broadinstitute/cromwell/issues/3533:474,Modifiability,config,configure,474,"I tried to ask this on the broad forums but got an error.""You need the Garden.Community.Manage permission to do that."", ""Class"": ""Gdn_UserException"" . I have been using a . backend.default = SGE. Line in my config file. This has been working until now, with my jobs submitted using my defined SGE backend. Today I changed some other parts of the server, unrelated and numerous. Now SOME jobs are being submitted on the SGE backend, and some on the Local backend. (I did not configure a local backend, but I assume one is built in to cromwell. How is the backend to run on chosen by cromwell? (I know of backend.default, and I beleive there is a wdl.task.runtime.backend parameter also (undocumented from what I can tell)). How can I prevent any task ever running on the local backend? I want this to be a hard error, and not overload my sge login node. . Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533
https://github.com/broadinstitute/cromwell/issues/3533:841,Testability,log,login,841,"I tried to ask this on the broad forums but got an error.""You need the Garden.Community.Manage permission to do that."", ""Class"": ""Gdn_UserException"" . I have been using a . backend.default = SGE. Line in my config file. This has been working until now, with my jobs submitted using my defined SGE backend. Today I changed some other parts of the server, unrelated and numerous. Now SOME jobs are being submitted on the SGE backend, and some on the Local backend. (I did not configure a local backend, but I assume one is built in to cromwell. How is the backend to run on chosen by cromwell? (I know of backend.default, and I beleive there is a wdl.task.runtime.backend parameter also (undocumented from what I can tell)). How can I prevent any task ever running on the local backend? I want this to be a hard error, and not overload my sge login node. . Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533
https://github.com/broadinstitute/cromwell/issues/3533:686,Usability,undo,undocumented,686,"I tried to ask this on the broad forums but got an error.""You need the Garden.Community.Manage permission to do that."", ""Class"": ""Gdn_UserException"" . I have been using a . backend.default = SGE. Line in my config file. This has been working until now, with my jobs submitted using my defined SGE backend. Today I changed some other parts of the server, unrelated and numerous. Now SOME jobs are being submitted on the SGE backend, and some on the Local backend. (I did not configure a local backend, but I assume one is built in to cromwell. How is the backend to run on chosen by cromwell? (I know of backend.default, and I beleive there is a wdl.task.runtime.backend parameter also (undocumented from what I can tell)). How can I prevent any task ever running on the local backend? I want this to be a hard error, and not overload my sge login node. . Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533
https://github.com/broadinstitute/cromwell/pull/3540:64,Performance,load,load,64,"- Remove memory control which doesn't work anyway; - Make it so load control in general can be disabled by setting control-frequency to ""Inf""; - Log which components have reported with high load when the system freezes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3540
https://github.com/broadinstitute/cromwell/pull/3540:190,Performance,load,load,190,"- Remove memory control which doesn't work anyway; - Make it so load control in general can be disabled by setting control-frequency to ""Inf""; - Log which components have reported with high load when the system freezes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3540
https://github.com/broadinstitute/cromwell/pull/3540:145,Testability,Log,Log,145,"- Remove memory control which doesn't work anyway; - Make it so load control in general can be disabled by setting control-frequency to ""Inf""; - Log which components have reported with high load when the system freezes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3540
https://github.com/broadinstitute/cromwell/pull/3541:165,Availability,failure,failures,165,"Fully caches Genome in a Bottle chm to run in ~2 hours. Still waiting on results from GIAB Joint because that takes much longer and has to be restarted for sporadic failures, but all the calls so far will cache.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3541
https://github.com/broadinstitute/cromwell/pull/3541:6,Performance,cache,caches,6,"Fully caches Genome in a Bottle chm to run in ~2 hours. Still waiting on results from GIAB Joint because that takes much longer and has to be restarted for sporadic failures, but all the calls so far will cache.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3541
https://github.com/broadinstitute/cromwell/pull/3541:205,Performance,cache,cache,205,"Fully caches Genome in a Bottle chm to run in ~2 hours. Still waiting on results from GIAB Joint because that takes much longer and has to be restarted for sporadic failures, but all the calls so far will cache.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3541
https://github.com/broadinstitute/cromwell/issues/3542:608,Availability,down,down,608,"If a CWL workflow has `DockerRequirement` specified as a hint, there's no way to run that workflow without Docker enabled currently. Provide a workflow option such that in these circumstances the workflow will be run *without* Docker. This workflow option should only be recognized by the SFS backends (local + HPC). @ruchim @cjllanwarne we might want to consider expanding this more generally to also include cases where `DockerRequirement` is a requirement and for WDL (where there is no hint), IOW a general docker on/off switch for people who know the consequences of their actions. If we did want to go down that path I'd make it a separate option from this one",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3542
https://github.com/broadinstitute/cromwell/issues/3544:320,Testability,test,test,320,"For some of our workflows, there are wrong `startTime` and `endTime` timestamps for ""RunningJob"" in executionEvents. For example, we got the below time, which doesn't make sense:; ```; {; startTime: ""2018-04-06T13:58:15.329Z"",; description: ""RunningJob"",; endTime: ""2018-04-06T13:37:28Z""; },; ```; https://cromwell.mint-test.broadinstitute.org/api/workflows/v1/fde178bd-dae1-4686-af6b-3a0af806dc48/metadata. Cromwell version is **30-16f3632**. A fact worth mentioning is that Cromwell crashed around the time ""2018-04-06T13:58:15.329Z"". Besides, we are not sure if this problem is ""flipped timestamps"" or just wrong timestamps, we have assumed it is the former case, but not sure about it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3544
https://github.com/broadinstitute/cromwell/issues/3546:408,Deployability,update,updated,408,"To reproduce:. 1. create a new label (has to be new) with a value by passing `{""key-1"":""value-1""}` to /api/workflows/{version}/{id}/labels; 2. set that label to an empty string `{""key-1"":""""}`; 3. set that label to something other than an empty string `{""key-1"":""value-2""}`; 4. set that label to an empty string `{""key-1"":""""}` again. The first time the label is set to """", it works; the second time, it isn't updated (although the response from /api/workflows/{version}/{id}/labels makes it look like it was)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3546
https://github.com/broadinstitute/cromwell/pull/3551:6,Testability,test,tests,6,These tests assert ~ 50 different nuances of JS evaluation.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3551
https://github.com/broadinstitute/cromwell/pull/3551:12,Testability,assert,assert,12,These tests assert ~ 50 different nuances of JS evaluation.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3551
https://github.com/broadinstitute/cromwell/issues/3554:144,Availability,down,down,144,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554
https://github.com/broadinstitute/cromwell/issues/3554:230,Availability,avail,available,230,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554
https://github.com/broadinstitute/cromwell/issues/3554:420,Availability,failure,failures,420,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554
https://github.com/broadinstitute/cromwell/issues/3554:574,Availability,avail,available,574,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554
https://github.com/broadinstitute/cromwell/issues/3554:693,Availability,failure,failures,693,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554
https://github.com/broadinstitute/cromwell/issues/3554:376,Deployability,integrat,integration,376,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554
https://github.com/broadinstitute/cromwell/issues/3554:118,Energy Efficiency,allocate,allocated,118,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554
https://github.com/broadinstitute/cromwell/issues/3554:376,Integrability,integrat,integration,376,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554
https://github.com/broadinstitute/cromwell/issues/3554:429,Safety,timeout,timeouts,429,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554
https://github.com/broadinstitute/cromwell/issues/3554:161,Testability,test,tests,161,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554
https://github.com/broadinstitute/cromwell/issues/3554:388,Testability,test,tests,388,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554
https://github.com/broadinstitute/cromwell/issues/3555:364,Availability,avail,available,364,"Current known issues (feel free to add/remove/edit):; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. Before running the nightly cron test another script should auto sweep old jobs so they don't have to be manually cleaned up with `bcs dj`. Some mix of bcs/bash/jq/sed/awk/python should work and are all available on Travis.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3555
https://github.com/broadinstitute/cromwell/issues/3555:194,Testability,test,test,194,"Current known issues (feel free to add/remove/edit):; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. Before running the nightly cron test another script should auto sweep old jobs so they don't have to be manually cleaned up with `bcs dj`. Some mix of bcs/bash/jq/sed/awk/python should work and are all available on Travis.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3555
https://github.com/broadinstitute/cromwell/pull/3566:63,Security,hash,hash,63,Mix one line of super obvious fix with 45 lines of conformance hash repinning noise.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3566
https://github.com/broadinstitute/cromwell/issues/3569:26,Security,validat,validates,26,A/C:; - centaur test that validates that inputs schemas are used for validation when the input specifies a format; - centaur test that validates that inputs schemas are ignored for validation when the input doesn't specify a format,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3569
https://github.com/broadinstitute/cromwell/issues/3569:69,Security,validat,validation,69,A/C:; - centaur test that validates that inputs schemas are used for validation when the input specifies a format; - centaur test that validates that inputs schemas are ignored for validation when the input doesn't specify a format,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3569
https://github.com/broadinstitute/cromwell/issues/3569:135,Security,validat,validates,135,A/C:; - centaur test that validates that inputs schemas are used for validation when the input specifies a format; - centaur test that validates that inputs schemas are ignored for validation when the input doesn't specify a format,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3569
https://github.com/broadinstitute/cromwell/issues/3569:181,Security,validat,validation,181,A/C:; - centaur test that validates that inputs schemas are used for validation when the input specifies a format; - centaur test that validates that inputs schemas are ignored for validation when the input doesn't specify a format,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3569
https://github.com/broadinstitute/cromwell/issues/3569:16,Testability,test,test,16,A/C:; - centaur test that validates that inputs schemas are used for validation when the input specifies a format; - centaur test that validates that inputs schemas are ignored for validation when the input doesn't specify a format,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3569
https://github.com/broadinstitute/cromwell/issues/3569:125,Testability,test,test,125,A/C:; - centaur test that validates that inputs schemas are used for validation when the input specifies a format; - centaur test that validates that inputs schemas are ignored for validation when the input doesn't specify a format,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3569
https://github.com/broadinstitute/cromwell/issues/3571:14,Safety,abort,abort,14,"[31m- should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED *** (1 minute, 52 seconds)[0m; [31m java.lang.Exception: Invalid metadata response:[0m; [31m -Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""[0m; [31m at centaur.test.Operations$$anon$13.checkDiff$1(Test.scala:330)[0m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3571
https://github.com/broadinstitute/cromwell/issues/3571:63,Safety,abort,abort,63,"[31m- should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED *** (1 minute, 52 seconds)[0m; [31m java.lang.Exception: Invalid metadata response:[0m; [31m -Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""[0m; [31m at centaur.test.Operations$$anon$13.checkDiff$1(Test.scala:330)[0m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3571
https://github.com/broadinstitute/cromwell/issues/3571:240,Safety,abort,aborted,240,"[31m- should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED *** (1 minute, 52 seconds)[0m; [31m java.lang.Exception: Invalid metadata response:[0m; [31m -Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""[0m; [31m at centaur.test.Operations$$anon$13.checkDiff$1(Test.scala:330)[0m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3571
https://github.com/broadinstitute/cromwell/issues/3571:295,Safety,Abort,Aborted,295,"[31m- should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED *** (1 minute, 52 seconds)[0m; [31m java.lang.Exception: Invalid metadata response:[0m; [31m -Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""[0m; [31m at centaur.test.Operations$$anon$13.checkDiff$1(Test.scala:330)[0m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3571
https://github.com/broadinstitute/cromwell/issues/3571:326,Testability,test,test,326,"[31m- should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED *** (1 minute, 52 seconds)[0m; [31m java.lang.Exception: Invalid metadata response:[0m; [31m -Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""[0m; [31m at centaur.test.Operations$$anon$13.checkDiff$1(Test.scala:330)[0m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3571
https://github.com/broadinstitute/cromwell/issues/3571:363,Testability,Test,Test,363,"[31m- should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED *** (1 minute, 52 seconds)[0m; [31m java.lang.Exception: Invalid metadata response:[0m; [31m -Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""[0m; [31m at centaur.test.Operations$$anon$13.checkDiff$1(Test.scala:330)[0m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3571
https://github.com/broadinstitute/cromwell/pull/3572:6,Security,validat,validation,6,Tests validation of placeholder attributes and adds handling of the `true=` and `false=` attributes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3572
https://github.com/broadinstitute/cromwell/pull/3572:0,Testability,Test,Tests,0,Tests validation of placeholder attributes and adds handling of the `true=` and `false=` attributes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3572
https://github.com/broadinstitute/cromwell/pull/3573:0,Deployability,Update,Updates,0,Updates suggested from #3562 but w/ autogeneration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3573
https://github.com/broadinstitute/cromwell/issues/3574:243,Security,access,access,243,"The current [AWS 101 docs](https://github.com/broadinstitute/cromwell/blob/aws_backend/docs/tutorials/AwsBatch101.md) lists an optional section for specifying a private Docker registry. . ```; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; ```. This should be taken out, since it is handled at the level of a custom AMI for AWS Batch and is not something that you can specify at runtime or within Cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3574
https://github.com/broadinstitute/cromwell/issues/3575:343,Availability,toler,tolerate,343,Known issues: . - A `WomMaybeListedDirectory` used as input to an IWDR expression does not have its `listing` attribute populated as expected.; - Even if this directory's `listing` attribute is force-populated the files are not copied to the IWDR.; - Even if the files are force-copied to the IWDR the test runs a `find` command that does not tolerate the presence of detritus.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3575
https://github.com/broadinstitute/cromwell/issues/3575:302,Testability,test,test,302,Known issues: . - A `WomMaybeListedDirectory` used as input to an IWDR expression does not have its `listing` attribute populated as expected.; - Even if this directory's `listing` attribute is force-populated the files are not copied to the IWDR.; - Even if the files are force-copied to the IWDR the test runs a `find` command that does not tolerate the presence of detritus.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3575
https://github.com/broadinstitute/cromwell/issues/3579:140,Modifiability,Config,Config,140,"I have a [CWL workflow](https://gist.github.com/ruchim/df038401cc68383b75310422ece8e088) with a docker requirement. When I run it against a Config backend that doesn't have docker support ([backend config](https://gist.github.com/ruchim/4228058d39e6306f0a3e12cd92e5123c)), I expect the workflow to fail -- yet it simply succeeds by running the submit command without using docker. AC: When a workflow requires docker and a backend doesn't support docker, fail the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3579
https://github.com/broadinstitute/cromwell/issues/3579:198,Modifiability,config,config,198,"I have a [CWL workflow](https://gist.github.com/ruchim/df038401cc68383b75310422ece8e088) with a docker requirement. When I run it against a Config backend that doesn't have docker support ([backend config](https://gist.github.com/ruchim/4228058d39e6306f0a3e12cd92e5123c)), I expect the workflow to fail -- yet it simply succeeds by running the submit command without using docker. AC: When a workflow requires docker and a backend doesn't support docker, fail the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3579
https://github.com/broadinstitute/cromwell/issues/3579:313,Usability,simpl,simply,313,"I have a [CWL workflow](https://gist.github.com/ruchim/df038401cc68383b75310422ece8e088) with a docker requirement. When I run it against a Config backend that doesn't have docker support ([backend config](https://gist.github.com/ruchim/4228058d39e6306f0a3e12cd92e5123c)), I expect the workflow to fail -- yet it simply succeeds by running the submit command without using docker. AC: When a workflow requires docker and a backend doesn't support docker, fail the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3579
https://github.com/broadinstitute/cromwell/pull/3580:224,Availability,Error,ErrorOr,224,"This compiler flags allows the compiler to take types of kind `F[_,_]` and infers `F[_]` by parameterizing the right most argument and fixing the other ones. This allows us to omit type args when traversing to `Either` and `ErrorOr`, because scala can now see them as shape `F[_]` instead of ""you gave me a type w/ 2 type args and I was expecting one"". **I did a find/replace on all our traverses, this may be controversial**, chime in if you disagree. [Longer explanation](https://gist.github.com/djspiewak/7a81a395c461fd3a09a6941d4cd040f2)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3580
https://github.com/broadinstitute/cromwell/pull/3580:92,Modifiability,parameteriz,parameterizing,92,"This compiler flags allows the compiler to take types of kind `F[_,_]` and infers `F[_]` by parameterizing the right most argument and fixing the other ones. This allows us to omit type args when traversing to `Either` and `ErrorOr`, because scala can now see them as shape `F[_]` instead of ""you gave me a type w/ 2 type args and I was expecting one"". **I did a find/replace on all our traverses, this may be controversial**, chime in if you disagree. [Longer explanation](https://gist.github.com/djspiewak/7a81a395c461fd3a09a6941d4cd040f2)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3580
https://github.com/broadinstitute/cromwell/pull/3583:102,Availability,resilien,resilience,102,What's not there yet:; - Private dockerhub; - ~~GPU support~~. Everything else should work ~~once the resilience fix for dockerhub is deployed in PAPI.~~. 95% of the relevant code of this PR is in the `cromwell.backend.google.pipelines.v2alpha1` package. The 5% left is due to refactoring to better isolate what is V1 logic and V2 logic.; The rest is noise due to renaming JES stuff to PAPI stuff.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3583
https://github.com/broadinstitute/cromwell/pull/3583:134,Deployability,deploy,deployed,134,What's not there yet:; - Private dockerhub; - ~~GPU support~~. Everything else should work ~~once the resilience fix for dockerhub is deployed in PAPI.~~. 95% of the relevant code of this PR is in the `cromwell.backend.google.pipelines.v2alpha1` package. The 5% left is due to refactoring to better isolate what is V1 logic and V2 logic.; The rest is noise due to renaming JES stuff to PAPI stuff.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3583
https://github.com/broadinstitute/cromwell/pull/3583:226,Deployability,pipeline,pipelines,226,What's not there yet:; - Private dockerhub; - ~~GPU support~~. Everything else should work ~~once the resilience fix for dockerhub is deployed in PAPI.~~. 95% of the relevant code of this PR is in the `cromwell.backend.google.pipelines.v2alpha1` package. The 5% left is due to refactoring to better isolate what is V1 logic and V2 logic.; The rest is noise due to renaming JES stuff to PAPI stuff.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3583
https://github.com/broadinstitute/cromwell/pull/3583:277,Modifiability,refactor,refactoring,277,What's not there yet:; - Private dockerhub; - ~~GPU support~~. Everything else should work ~~once the resilience fix for dockerhub is deployed in PAPI.~~. 95% of the relevant code of this PR is in the `cromwell.backend.google.pipelines.v2alpha1` package. The 5% left is due to refactoring to better isolate what is V1 logic and V2 logic.; The rest is noise due to renaming JES stuff to PAPI stuff.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3583
https://github.com/broadinstitute/cromwell/pull/3583:318,Testability,log,logic,318,What's not there yet:; - Private dockerhub; - ~~GPU support~~. Everything else should work ~~once the resilience fix for dockerhub is deployed in PAPI.~~. 95% of the relevant code of this PR is in the `cromwell.backend.google.pipelines.v2alpha1` package. The 5% left is due to refactoring to better isolate what is V1 logic and V2 logic.; The rest is noise due to renaming JES stuff to PAPI stuff.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3583
https://github.com/broadinstitute/cromwell/pull/3583:331,Testability,log,logic,331,What's not there yet:; - Private dockerhub; - ~~GPU support~~. Everything else should work ~~once the resilience fix for dockerhub is deployed in PAPI.~~. 95% of the relevant code of this PR is in the `cromwell.backend.google.pipelines.v2alpha1` package. The 5% left is due to refactoring to better isolate what is V1 logic and V2 logic.; The rest is noise due to renaming JES stuff to PAPI stuff.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3583
https://github.com/broadinstitute/cromwell/issues/3584:6709,Availability,error,error,6709,"d.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:11469,Availability,down,download,11469,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:11499,Availability,error,error,11499,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:451,Deployability,pipeline,pipeline,451,"Hi all;; I'm testing out a CWL run (https://github.com/bcbio/bcbio_validation_workflows/tree/master/somatic-giab-mix) with a SLURM backend using file based caching:; ```; [2018-05-02 13:10:20,09] [info] Running with database db.url = jdbc:hsqldb:file:/projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc; ```; and running into a hash exception at a consistent spot in the pipeline:; ```; [2018-05-02 15:16:51,49] [info] WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f [[38;5;2mbc4644da[0m]: Starting batch_for_variantcall; [2018-05-02 15:16:52,47] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:16:55,18] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: [38;5;5m'bcbio_nextgen.py' 'runfn' 'batch_for_variantcall' 'cwl' 'sentinel_runtime=cores,1,ram,3839.9999999999995' 'sentinel_parallel=multi-batch' 'sentinel_outputs=batch_rec:resources;description;reference__fasta__base;config__algorithm__variantcaller;reference__snpeff__GRCh37_75;config__algorithm__coverage_interval;genome_resources__variation__train_hapmap;genome_resources__variation__encode_blacklist;metadata__batch;genome_resources__variation__lcr;metadata__phenotype;vrn_file;reference__twobit;config__algorithm__validate;config__algorithm__validate_regions;genome_build;genome_resources__aliases__human;config__algorithm__tools_off;genome_resources__variation__dbsnp;genome_resources__variation__polyx;genome_resources__variation__cosmic;reference__genome_context;analysis;config__algorithm__tools_on;config__algorithm__effects;config__algorithm__variant_regions;genome_resources__aliases__ensembl;config__algorithm__exclude_regions;reference__rtg;genome_resources__variation__train_indels;genome",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:9733,Deployability,configurat,configuration,9733,"or/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:11020,Deployability,configurat,configuration,11020,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:4051,Integrability,wrap,wrap,4051,"ces__aliases__human:var,genome_resources__aliases__snpeff:var,reference__snpeff__GRCh37_75:var,resources:var,description:var'[0m; [2018-05-02 15:16:55,18] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: executing: sbatch -J cromwell_bc4644da_batch_for_variantcall -D /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall -o /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/stdout -e /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/stderr -t 1-00:00 -p core --cpus-per-task=1 --mem=4026 --wrap ""/usr/bin/env bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:8566,Integrability,Message,Message,8566,"ve$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$SingleFileHashRequest] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:9858,Integrability,Message,Message,9858,"2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:4951,Modifiability,config,config,4951,"0f/call-batch_for_variantcall/execution/stderr -t 1-00:00 -p core --cpus-per-task=1 --mem=4026 --wrap ""/usr/bin/env bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:4958,Modifiability,Config,ConfigHashingStrategy,4958,"or_variantcall/execution/stderr -t 1-00:00 -p core --cpus-per-task=1 --mem=4026 --wrap ""/usr/bin/env bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:5004,Modifiability,Config,ConfigHashingStrategy,5004," 1-00:00 -p core --cpus-per-task=1 --mem=4026 --wrap ""/usr/bin/env bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:5067,Modifiability,config,config,5067,"bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:5074,Modifiability,Config,ConfigHashingStrategy,5074,"ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:5104,Modifiability,Config,ConfigHashingStrategy,5104,"idation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.Acto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:5167,Modifiability,config,config,5167,"xecutions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:5174,Modifiability,Config,ConfigBackendFileHashingActor,5174,"tic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Ma",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:5223,Modifiability,Config,ConfigBackendFileHashingActor,5223,"1-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:6864,Modifiability,config,config,6864,"actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:6871,Modifiability,Config,ConfigHashingStrategy,6871,"oundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:6917,Modifiability,Config,ConfigHashingStrategy,6917," cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:6980,Modifiability,config,config,6980,"undReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:6987,Modifiability,Config,ConfigHashingStrategy,6987,"dardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:7017,Modifiability,Config,ConfigHashingStrategy,7017,":59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.Acto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:7080,Modifiability,config,config,7080,"la:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:7087,Modifiability,Config,ConfigBackendFileHashingActor,7087,"ctor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Ma",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:7136,Modifiability,Config,ConfigBackendFileHashingActor,7136,"96); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:9733,Modifiability,config,configuration,9733,"or/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:11020,Modifiability,config,configuration,11020,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:408,Security,hash,hash,408,"Hi all;; I'm testing out a CWL run (https://github.com/bcbio/bcbio_validation_workflows/tree/master/somatic-giab-mix) with a SLURM backend using file based caching:; ```; [2018-05-02 13:10:20,09] [info] Running with database db.url = jdbc:hsqldb:file:/projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc; ```; and running into a hash exception at a consistent spot in the pipeline:; ```; [2018-05-02 15:16:51,49] [info] WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f [[38;5;2mbc4644da[0m]: Starting batch_for_variantcall; [2018-05-02 15:16:52,47] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:16:55,18] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: [38;5;5m'bcbio_nextgen.py' 'runfn' 'batch_for_variantcall' 'cwl' 'sentinel_runtime=cores,1,ram,3839.9999999999995' 'sentinel_parallel=multi-batch' 'sentinel_outputs=batch_rec:resources;description;reference__fasta__base;config__algorithm__variantcaller;reference__snpeff__GRCh37_75;config__algorithm__coverage_interval;genome_resources__variation__train_hapmap;genome_resources__variation__encode_blacklist;metadata__batch;genome_resources__variation__lcr;metadata__phenotype;vrn_file;reference__twobit;config__algorithm__validate;config__algorithm__validate_regions;genome_build;genome_resources__aliases__human;config__algorithm__tools_off;genome_resources__variation__dbsnp;genome_resources__variation__polyx;genome_resources__variation__cosmic;reference__genome_context;analysis;config__algorithm__tools_on;config__algorithm__effects;config__algorithm__variant_regions;genome_resources__aliases__ensembl;config__algorithm__exclude_regions;reference__rtg;genome_resources__variation__train_indels;genome",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:4830,Security,hash,hash,4830,"5-9791-9011a2fae80f/call-batch_for_variantcall -o /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/stdout -e /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/stderr -t 1-00:00 -p core --cpus-per-task=1 --mem=4026 --wrap ""/usr/bin/env bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrEl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:4879,Security,hash,hash,4879,"ns/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/stderr -t 1-00:00 -p core --cpus-per-task=1 --mem=4026 --wrap ""/usr/bin/env bash /projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/cromwell-executions/main-somatic-giab-mix.cwl/bc4644da-87f9-4765-9791-9011a2fae80f/call-batch_for_variantcall/execution/script""; [2018-05-02 15:16:57,63] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: job id: 134053; [2018-05-02 15:16:57,66] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-05-02 15:17:05,03] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-05-02 15:22:54,62] [[38;5;1merror[0m] Failed to hash null; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceiv",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:6704,Security,Hash,Hash,6704,"d.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:6792,Security,hash,hash,6792,"	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceiv",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:13,Testability,test,testing,13,"Hi all;; I'm testing out a CWL run (https://github.com/bcbio/bcbio_validation_workflows/tree/master/somatic-giab-mix) with a SLURM backend using file based caching:; ```; [2018-05-02 13:10:20,09] [info] Running with database db.url = jdbc:hsqldb:file:/projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc; ```; and running into a hash exception at a consistent spot in the pipeline:; ```; [2018-05-02 15:16:51,49] [info] WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f [[38;5;2mbc4644da[0m]: Starting batch_for_variantcall; [2018-05-02 15:16:52,47] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:16:55,18] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: [38;5;5m'bcbio_nextgen.py' 'runfn' 'batch_for_variantcall' 'cwl' 'sentinel_runtime=cores,1,ram,3839.9999999999995' 'sentinel_parallel=multi-batch' 'sentinel_outputs=batch_rec:resources;description;reference__fasta__base;config__algorithm__variantcaller;reference__snpeff__GRCh37_75;config__algorithm__coverage_interval;genome_resources__variation__train_hapmap;genome_resources__variation__encode_blacklist;metadata__batch;genome_resources__variation__lcr;metadata__phenotype;vrn_file;reference__twobit;config__algorithm__validate;config__algorithm__validate_regions;genome_build;genome_resources__aliases__human;config__algorithm__tools_off;genome_resources__variation__dbsnp;genome_resources__variation__polyx;genome_resources__variation__cosmic;reference__genome_context;analysis;config__algorithm__tools_on;config__algorithm__effects;config__algorithm__variant_regions;genome_resources__aliases__ensembl;config__algorithm__exclude_regions;reference__rtg;genome_resources__variation__train_indels;genome",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:9690,Testability,log,logging,9690,"or/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:9762,Testability,log,log-dead-letters,9762,"765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:9790,Testability,log,log-dead-letters-during-shutdown,9790,"xecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescript",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:10977,Testability,log,logging,10977,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:11049,Testability,log,log-dead-letters,11049,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:11077,Testability,log,log-dead-letters-during-shutdown,11077,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:11334,Testability,test,test,11334,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:11346,Testability,test,test,11346,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3584:11371,Testability,test,tests,11371,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584
https://github.com/broadinstitute/cromwell/issues/3588:321,Availability,echo,echo,321,"When using the type `string[]` along with the key `prefix`, instead of the prefix being added at the head of string array, the prefix is being applied to each element, along with the head of the list of elements. Example CWL:; ```; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```. Example Inputs:; ```; bonus: [""first"", ""second""]; ```. Actual Response:; ```; [2018-05-02 15:19:13,62] [info] BackgroundConfigAsyncJobExecutionActor [93064d21test.cwl:NA:1]: '/bin/echo' '--bonus' '--bonus' 'first' '--bonus' 'second'; ```. Expected Response: http://www.commonwl.org/user_guide/09-array-inputs/; ```; '/bin/echo' '--bonus' 'first' 'second'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3588
https://github.com/broadinstitute/cromwell/issues/3588:634,Availability,echo,echo,634,"When using the type `string[]` along with the key `prefix`, instead of the prefix being added at the head of string array, the prefix is being applied to each element, along with the head of the list of elements. Example CWL:; ```; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```. Example Inputs:; ```; bonus: [""first"", ""second""]; ```. Actual Response:; ```; [2018-05-02 15:19:13,62] [info] BackgroundConfigAsyncJobExecutionActor [93064d21test.cwl:NA:1]: '/bin/echo' '--bonus' '--bonus' 'first' '--bonus' 'second'; ```. Expected Response: http://www.commonwl.org/user_guide/09-array-inputs/; ```; '/bin/echo' '--bonus' 'first' 'second'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3588
https://github.com/broadinstitute/cromwell/issues/3588:776,Availability,echo,echo,776,"When using the type `string[]` along with the key `prefix`, instead of the prefix being added at the head of string array, the prefix is being applied to each element, along with the head of the list of elements. Example CWL:; ```; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```. Example Inputs:; ```; bonus: [""first"", ""second""]; ```. Actual Response:; ```; [2018-05-02 15:19:13,62] [info] BackgroundConfigAsyncJobExecutionActor [93064d21test.cwl:NA:1]: '/bin/echo' '--bonus' '--bonus' 'first' '--bonus' 'second'; ```. Expected Response: http://www.commonwl.org/user_guide/09-array-inputs/; ```; '/bin/echo' '--bonus' 'first' 'second'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3588
https://github.com/broadinstitute/cromwell/issues/3589:403,Availability,echo,echo,403,"When running [this workflow step](https://github.com/genome/cancer-genomics-workflow/blob/c5eabd41d5cba375bec79eaffed97c80ab24babb/umi_alignment/extract_umis.cwl#L22), the constructed command-line was different than expected for the `--read-structure` parameter. This short CWL demonstrates:. `test.cwl`:; ```cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```; `test.yml`:; ```yml; bonus: [""first"", ""second""]; ```. Running with `cwltool test.cwl test.yml` yields:; `hello.txt`:; ```; --bonus first second; ```; Running with `/usr/bin/java -jar cromwell-32-0c557ab-SNAP.jar run -t cwl -i test.yml test.cwl` yields:; `cromwell-executions/test.cwl/6817868a-76e6-48b6-8702-4fe456b23277/call-test.cwl/execution/hello.txt`:; ```; --bonus --bonus first --bonus second; ```. The result from `cwltool` is what I would expect based on the `filesA` example in [this user guide](http://www.commonwl.org/user_guide/09-array-inputs/).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3589
https://github.com/broadinstitute/cromwell/issues/3589:294,Testability,test,test,294,"When running [this workflow step](https://github.com/genome/cancer-genomics-workflow/blob/c5eabd41d5cba375bec79eaffed97c80ab24babb/umi_alignment/extract_umis.cwl#L22), the constructed command-line was different than expected for the `--read-structure` parameter. This short CWL demonstrates:. `test.cwl`:; ```cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```; `test.yml`:; ```yml; bonus: [""first"", ""second""]; ```. Running with `cwltool test.cwl test.yml` yields:; `hello.txt`:; ```; --bonus first second; ```; Running with `/usr/bin/java -jar cromwell-32-0c557ab-SNAP.jar run -t cwl -i test.yml test.cwl` yields:; `cromwell-executions/test.cwl/6817868a-76e6-48b6-8702-4fe456b23277/call-test.cwl/execution/hello.txt`:; ```; --bonus --bonus first --bonus second; ```. The result from `cwltool` is what I would expect based on the `filesA` example in [this user guide](http://www.commonwl.org/user_guide/09-array-inputs/).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3589
https://github.com/broadinstitute/cromwell/issues/3589:537,Testability,test,test,537,"When running [this workflow step](https://github.com/genome/cancer-genomics-workflow/blob/c5eabd41d5cba375bec79eaffed97c80ab24babb/umi_alignment/extract_umis.cwl#L22), the constructed command-line was different than expected for the `--read-structure` parameter. This short CWL demonstrates:. `test.cwl`:; ```cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```; `test.yml`:; ```yml; bonus: [""first"", ""second""]; ```. Running with `cwltool test.cwl test.yml` yields:; `hello.txt`:; ```; --bonus first second; ```; Running with `/usr/bin/java -jar cromwell-32-0c557ab-SNAP.jar run -t cwl -i test.yml test.cwl` yields:; `cromwell-executions/test.cwl/6817868a-76e6-48b6-8702-4fe456b23277/call-test.cwl/execution/hello.txt`:; ```; --bonus --bonus first --bonus second; ```. The result from `cwltool` is what I would expect based on the `filesA` example in [this user guide](http://www.commonwl.org/user_guide/09-array-inputs/).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3589
https://github.com/broadinstitute/cromwell/issues/3589:612,Testability,test,test,612,"When running [this workflow step](https://github.com/genome/cancer-genomics-workflow/blob/c5eabd41d5cba375bec79eaffed97c80ab24babb/umi_alignment/extract_umis.cwl#L22), the constructed command-line was different than expected for the `--read-structure` parameter. This short CWL demonstrates:. `test.cwl`:; ```cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```; `test.yml`:; ```yml; bonus: [""first"", ""second""]; ```. Running with `cwltool test.cwl test.yml` yields:; `hello.txt`:; ```; --bonus first second; ```; Running with `/usr/bin/java -jar cromwell-32-0c557ab-SNAP.jar run -t cwl -i test.yml test.cwl` yields:; `cromwell-executions/test.cwl/6817868a-76e6-48b6-8702-4fe456b23277/call-test.cwl/execution/hello.txt`:; ```; --bonus --bonus first --bonus second; ```. The result from `cwltool` is what I would expect based on the `filesA` example in [this user guide](http://www.commonwl.org/user_guide/09-array-inputs/).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3589
https://github.com/broadinstitute/cromwell/issues/3589:621,Testability,test,test,621,"When running [this workflow step](https://github.com/genome/cancer-genomics-workflow/blob/c5eabd41d5cba375bec79eaffed97c80ab24babb/umi_alignment/extract_umis.cwl#L22), the constructed command-line was different than expected for the `--read-structure` parameter. This short CWL demonstrates:. `test.cwl`:; ```cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```; `test.yml`:; ```yml; bonus: [""first"", ""second""]; ```. Running with `cwltool test.cwl test.yml` yields:; `hello.txt`:; ```; --bonus first second; ```; Running with `/usr/bin/java -jar cromwell-32-0c557ab-SNAP.jar run -t cwl -i test.yml test.cwl` yields:; `cromwell-executions/test.cwl/6817868a-76e6-48b6-8702-4fe456b23277/call-test.cwl/execution/hello.txt`:; ```; --bonus --bonus first --bonus second; ```. The result from `cwltool` is what I would expect based on the `filesA` example in [this user guide](http://www.commonwl.org/user_guide/09-array-inputs/).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3589
https://github.com/broadinstitute/cromwell/issues/3589:762,Testability,test,test,762,"When running [this workflow step](https://github.com/genome/cancer-genomics-workflow/blob/c5eabd41d5cba375bec79eaffed97c80ab24babb/umi_alignment/extract_umis.cwl#L22), the constructed command-line was different than expected for the `--read-structure` parameter. This short CWL demonstrates:. `test.cwl`:; ```cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```; `test.yml`:; ```yml; bonus: [""first"", ""second""]; ```. Running with `cwltool test.cwl test.yml` yields:; `hello.txt`:; ```; --bonus first second; ```; Running with `/usr/bin/java -jar cromwell-32-0c557ab-SNAP.jar run -t cwl -i test.yml test.cwl` yields:; `cromwell-executions/test.cwl/6817868a-76e6-48b6-8702-4fe456b23277/call-test.cwl/execution/hello.txt`:; ```; --bonus --bonus first --bonus second; ```. The result from `cwltool` is what I would expect based on the `filesA` example in [this user guide](http://www.commonwl.org/user_guide/09-array-inputs/).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3589
https://github.com/broadinstitute/cromwell/issues/3589:771,Testability,test,test,771,"When running [this workflow step](https://github.com/genome/cancer-genomics-workflow/blob/c5eabd41d5cba375bec79eaffed97c80ab24babb/umi_alignment/extract_umis.cwl#L22), the constructed command-line was different than expected for the `--read-structure` parameter. This short CWL demonstrates:. `test.cwl`:; ```cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```; `test.yml`:; ```yml; bonus: [""first"", ""second""]; ```. Running with `cwltool test.cwl test.yml` yields:; `hello.txt`:; ```; --bonus first second; ```; Running with `/usr/bin/java -jar cromwell-32-0c557ab-SNAP.jar run -t cwl -i test.yml test.cwl` yields:; `cromwell-executions/test.cwl/6817868a-76e6-48b6-8702-4fe456b23277/call-test.cwl/execution/hello.txt`:; ```; --bonus --bonus first --bonus second; ```. The result from `cwltool` is what I would expect based on the `filesA` example in [this user guide](http://www.commonwl.org/user_guide/09-array-inputs/).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3589
https://github.com/broadinstitute/cromwell/issues/3589:811,Testability,test,test,811,"When running [this workflow step](https://github.com/genome/cancer-genomics-workflow/blob/c5eabd41d5cba375bec79eaffed97c80ab24babb/umi_alignment/extract_umis.cwl#L22), the constructed command-line was different than expected for the `--read-structure` parameter. This short CWL demonstrates:. `test.cwl`:; ```cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```; `test.yml`:; ```yml; bonus: [""first"", ""second""]; ```. Running with `cwltool test.cwl test.yml` yields:; `hello.txt`:; ```; --bonus first second; ```; Running with `/usr/bin/java -jar cromwell-32-0c557ab-SNAP.jar run -t cwl -i test.yml test.cwl` yields:; `cromwell-executions/test.cwl/6817868a-76e6-48b6-8702-4fe456b23277/call-test.cwl/execution/hello.txt`:; ```; --bonus --bonus first --bonus second; ```. The result from `cwltool` is what I would expect based on the `filesA` example in [this user guide](http://www.commonwl.org/user_guide/09-array-inputs/).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3589
https://github.com/broadinstitute/cromwell/issues/3589:862,Testability,test,test,862,"When running [this workflow step](https://github.com/genome/cancer-genomics-workflow/blob/c5eabd41d5cba375bec79eaffed97c80ab24babb/umi_alignment/extract_umis.cwl#L22), the constructed command-line was different than expected for the `--read-structure` parameter. This short CWL demonstrates:. `test.cwl`:; ```cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```; `test.yml`:; ```yml; bonus: [""first"", ""second""]; ```. Running with `cwltool test.cwl test.yml` yields:; `hello.txt`:; ```; --bonus first second; ```; Running with `/usr/bin/java -jar cromwell-32-0c557ab-SNAP.jar run -t cwl -i test.yml test.cwl` yields:; `cromwell-executions/test.cwl/6817868a-76e6-48b6-8702-4fe456b23277/call-test.cwl/execution/hello.txt`:; ```; --bonus --bonus first --bonus second; ```. The result from `cwltool` is what I would expect based on the `filesA` example in [this user guide](http://www.commonwl.org/user_guide/09-array-inputs/).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3589
https://github.com/broadinstitute/cromwell/issues/3589:1035,Usability,guid,guide,1035,"When running [this workflow step](https://github.com/genome/cancer-genomics-workflow/blob/c5eabd41d5cba375bec79eaffed97c80ab24babb/umi_alignment/extract_umis.cwl#L22), the constructed command-line was different than expected for the `--read-structure` parameter. This short CWL demonstrates:. `test.cwl`:; ```cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```; `test.yml`:; ```yml; bonus: [""first"", ""second""]; ```. Running with `cwltool test.cwl test.yml` yields:; `hello.txt`:; ```; --bonus first second; ```; Running with `/usr/bin/java -jar cromwell-32-0c557ab-SNAP.jar run -t cwl -i test.yml test.cwl` yields:; `cromwell-executions/test.cwl/6817868a-76e6-48b6-8702-4fe456b23277/call-test.cwl/execution/hello.txt`:; ```; --bonus --bonus first --bonus second; ```. The result from `cwltool` is what I would expect based on the `filesA` example in [this user guide](http://www.commonwl.org/user_guide/09-array-inputs/).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3589
https://github.com/broadinstitute/cromwell/pull/3591:18,Testability,test,test,18,Fixes conformance test 102 on local and PAPI,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3591
https://github.com/broadinstitute/cromwell/issues/3592:301,Modifiability,variab,variable,301,"I'm playing around with my first WDLs, and I realized that the QuickStart tutorial guides a user on how to assign task-level inputs, but not workflow-level. I used the womtool to generate a JSON and figured it out from there. Granted, it's what you'd expect (just omit the task name so it's ""workflow.variable"" rather than ""workflow.task.variable""), but I think this should be explicitly stated in the tutorial. It would only take a couple of lines, but it's important. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3592
https://github.com/broadinstitute/cromwell/issues/3592:338,Modifiability,variab,variable,338,"I'm playing around with my first WDLs, and I realized that the QuickStart tutorial guides a user on how to assign task-level inputs, but not workflow-level. I used the womtool to generate a JSON and figured it out from there. Granted, it's what you'd expect (just omit the task name so it's ""workflow.variable"" rather than ""workflow.task.variable""), but I think this should be explicitly stated in the tutorial. It would only take a couple of lines, but it's important. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3592
https://github.com/broadinstitute/cromwell/issues/3592:83,Usability,guid,guides,83,"I'm playing around with my first WDLs, and I realized that the QuickStart tutorial guides a user on how to assign task-level inputs, but not workflow-level. I used the womtool to generate a JSON and figured it out from there. Granted, it's what you'd expect (just omit the task name so it's ""workflow.variable"" rather than ""workflow.task.variable""), but I think this should be explicitly stated in the tutorial. It would only take a couple of lines, but it's important. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3592
https://github.com/broadinstitute/cromwell/pull/3594:18,Testability,test,test,18,Fixes conformance test 86 on local,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3594
https://github.com/broadinstitute/cromwell/pull/3596:72,Testability,test,tests,72,#3161 . - [x] Runtime attribute `maxRetries` implemented; - [x] Centaur tests; - [x] Changelog/Cromwell docs; - [x] More unit tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3596
https://github.com/broadinstitute/cromwell/pull/3596:126,Testability,test,tests,126,#3161 . - [x] Runtime attribute `maxRetries` implemented; - [x] Centaur tests; - [x] Changelog/Cromwell docs; - [x] More unit tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3596
https://github.com/broadinstitute/cromwell/pull/3597:43,Availability,error,error,43,"Basically changed the sbt assembly from ""--error"" to ""early(error)"" as they changed the flag. The other stuff is just 1.X cleanup",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3597
https://github.com/broadinstitute/cromwell/pull/3597:60,Availability,error,error,60,"Basically changed the sbt assembly from ""--error"" to ""early(error)"" as they changed the flag. The other stuff is just 1.X cleanup",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3597
https://github.com/broadinstitute/cromwell/pull/3600:16,Testability,test,test,16,CWL conformance test 106 on Local,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3600
https://github.com/broadinstitute/cromwell/pull/3601:143,Performance,Load,Load,143,"Takes WDL 1.0 case classes (`wdl.model.draft3.elements.FileElement` and its children) and writes them out as WDL source code. Test scheme:; 1. Load the WDL as case classes; 2. Write them out as source; 3. Load up the source from step 2 as case classes; 4. Verify the case classes from (1) and (3) are identical. I also uncovered some gaps in the set of WDLs we use for unit tests - I had gotten all the tests to pass, but there were still a handful of `???`'s in my code. Added test cases such that all of them were hit (and then implemented).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3601
https://github.com/broadinstitute/cromwell/pull/3601:205,Performance,Load,Load,205,"Takes WDL 1.0 case classes (`wdl.model.draft3.elements.FileElement` and its children) and writes them out as WDL source code. Test scheme:; 1. Load the WDL as case classes; 2. Write them out as source; 3. Load up the source from step 2 as case classes; 4. Verify the case classes from (1) and (3) are identical. I also uncovered some gaps in the set of WDLs we use for unit tests - I had gotten all the tests to pass, but there were still a handful of `???`'s in my code. Added test cases such that all of them were hit (and then implemented).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3601
https://github.com/broadinstitute/cromwell/pull/3601:126,Testability,Test,Test,126,"Takes WDL 1.0 case classes (`wdl.model.draft3.elements.FileElement` and its children) and writes them out as WDL source code. Test scheme:; 1. Load the WDL as case classes; 2. Write them out as source; 3. Load up the source from step 2 as case classes; 4. Verify the case classes from (1) and (3) are identical. I also uncovered some gaps in the set of WDLs we use for unit tests - I had gotten all the tests to pass, but there were still a handful of `???`'s in my code. Added test cases such that all of them were hit (and then implemented).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3601
https://github.com/broadinstitute/cromwell/pull/3601:374,Testability,test,tests,374,"Takes WDL 1.0 case classes (`wdl.model.draft3.elements.FileElement` and its children) and writes them out as WDL source code. Test scheme:; 1. Load the WDL as case classes; 2. Write them out as source; 3. Load up the source from step 2 as case classes; 4. Verify the case classes from (1) and (3) are identical. I also uncovered some gaps in the set of WDLs we use for unit tests - I had gotten all the tests to pass, but there were still a handful of `???`'s in my code. Added test cases such that all of them were hit (and then implemented).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3601
https://github.com/broadinstitute/cromwell/pull/3601:403,Testability,test,tests,403,"Takes WDL 1.0 case classes (`wdl.model.draft3.elements.FileElement` and its children) and writes them out as WDL source code. Test scheme:; 1. Load the WDL as case classes; 2. Write them out as source; 3. Load up the source from step 2 as case classes; 4. Verify the case classes from (1) and (3) are identical. I also uncovered some gaps in the set of WDLs we use for unit tests - I had gotten all the tests to pass, but there were still a handful of `???`'s in my code. Added test cases such that all of them were hit (and then implemented).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3601
https://github.com/broadinstitute/cromwell/pull/3601:478,Testability,test,test,478,"Takes WDL 1.0 case classes (`wdl.model.draft3.elements.FileElement` and its children) and writes them out as WDL source code. Test scheme:; 1. Load the WDL as case classes; 2. Write them out as source; 3. Load up the source from step 2 as case classes; 4. Verify the case classes from (1) and (3) are identical. I also uncovered some gaps in the set of WDLs we use for unit tests - I had gotten all the tests to pass, but there were still a handful of `???`'s in my code. Added test cases such that all of them were hit (and then implemented).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3601
https://github.com/broadinstitute/cromwell/pull/3602:221,Availability,failure,failures,221,It appears that the better file implementation of `nameWithoutExtension` can check for the existence of the file under some circumstances. For GCS files this means an http request which seems unnecessary and and prone to failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3602
https://github.com/broadinstitute/cromwell/issues/3607:43,Availability,error,error,43,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:490,Availability,error,error,490,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:653,Availability,Failure,Failure,653,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:108,Deployability,pipeline,pipeline,108,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:220,Deployability,pipeline,pipeline,220,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:416,Deployability,pipeline,pipeline,416,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:5911,Deployability,configurat,configuration,5911,"ala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; system {; workflow-restart = true; }; call-caching {; enabled = true; }. database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:/n/groups/bcbio/cwl/test_bcbio_cwl/somatic/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 20000; }; }. backend {; providers {; Local {; config {; runtime-attributes = """"""; Int? cpu; Int? memory_mb; """"""; submit-docker: """". filesystems {; local {; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }. }; }; ```; Does this provide enough information to identify what might be happening? Thanks for any thoughts or clues about avoid this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:4933,Energy Efficiency,adapt,adapted,4933,pl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:239); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:530,Integrability,message,message,530,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:4933,Modifiability,adapt,adapted,4933,pl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:239); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:5911,Modifiability,config,configuration,5911,"ala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; system {; workflow-restart = true; }; call-caching {; enabled = true; }. database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:/n/groups/bcbio/cwl/test_bcbio_cwl/somatic/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 20000; }; }. backend {; providers {; Local {; config {; runtime-attributes = """"""; Int? cpu; Int? memory_mb; """"""; submit-docker: """". filesystems {; local {; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }. }; }; ```; Does this provide enough information to identify what might be happening? Thanks for any thoughts or clues about avoid this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:6295,Modifiability,config,config,6295,"ala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; system {; workflow-restart = true; }; call-caching {; enabled = true; }. database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:/n/groups/bcbio/cwl/test_bcbio_cwl/somatic/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 20000; }; }. backend {; providers {; Local {; config {; runtime-attributes = """"""; Int? cpu; Int? memory_mb; """"""; submit-docker: """". filesystems {; local {; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }. }; }; ```; Does this provide enough information to identify what might be happening? Thanks for any thoughts or clues about avoid this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:677,Performance,cache,cache,677,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:2714,Performance,concurren,concurrent,2714,"sertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$1(DBIOAction.scala:186); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:186); 	at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:183); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); [2018-05-09 10:17:00,29] [[38;5;1merror[0m] WriteMetadataActor Failed to properly process data; java.sql.BatchUpdateException: data exception: string data, right truncation; table: METADATA_ENTRY column: METADATA_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:2799,Performance,concurren,concurrent,2799,"pleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$1(DBIOAction.scala:186); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:186); 	at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:183); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); [2018-05-09 10:17:00,29] [[38;5;1merror[0m] WriteMetadataActor Failed to properly process data; java.sql.BatchUpdateException: data exception: string data, right truncation; table: METADATA_ENTRY column: METADATA_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:5694,Performance,concurren,concurrent,5694,"ala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; system {; workflow-restart = true; }; call-caching {; enabled = true; }. database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:/n/groups/bcbio/cwl/test_bcbio_cwl/somatic/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 20000; }; }. backend {; providers {; Local {; config {; runtime-attributes = """"""; Int? cpu; Int? memory_mb; """"""; submit-docker: """". filesystems {; local {; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }. }; }; ```; Does this provide enough information to identify what might be happening? Thanks for any thoughts or clues about avoid this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:5779,Performance,concurren,concurrent,5779,"ala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; system {; workflow-restart = true; }; call-caching {; enabled = true; }. database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:/n/groups/bcbio/cwl/test_bcbio_cwl/somatic/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 20000; }; }. backend {; providers {; Local {; config {; runtime-attributes = """"""; Int? cpu; Int? memory_mb; """"""; submit-docker: """". filesystems {; local {; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }. }; }; ```; Does this provide enough information to identify what might be happening? Thanks for any thoughts or clues about avoid this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:6618,Safety,avoid,avoid,6618,"ala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; system {; workflow-restart = true; }; call-caching {; enabled = true; }. database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:/n/groups/bcbio/cwl/test_bcbio_cwl/somatic/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 20000; }; }. backend {; providers {; Local {; config {; runtime-attributes = """"""; Int? cpu; Int? memory_mb; """"""; submit-docker: """". filesystems {; local {; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }. }; }; ```; Does this provide enough information to identify what might be happening? Thanks for any thoughts or clues about avoid this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:6453,Security,hash,hashing-strategy,6453,"ala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; system {; workflow-restart = true; }; call-caching {; enabled = true; }. database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:/n/groups/bcbio/cwl/test_bcbio_cwl/somatic/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 20000; }; }. backend {; providers {; Local {; config {; runtime-attributes = """"""; Int? cpu; Int? memory_mb; """"""; submit-docker: """". filesystems {; local {; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }. }; }; ```; Does this provide enough information to identify what might be happening? Thanks for any thoughts or clues about avoid this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:290,Testability,test,test,290,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:1801,Usability,Simpl,SimpleJdbcProfileAction,1801,"eption: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$1(DBIOAction.scala:186); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:186); 	at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:183); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.conc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:1895,Usability,Simpl,SimpleJdbcProfileAction,1895,HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$1(DBIOAction.scala:186); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:186); 	at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:183); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Th,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:4027,Usability,Simpl,SimpleJdbcProfileAction,4027,"teException: data exception: string data, right truncation; table: METADATA_ENTRY column: METADATA_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:239); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at sc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3607:4121,Usability,Simpl,SimpleJdbcProfileAction,4121,DATA_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:239); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607
https://github.com/broadinstitute/cromwell/issues/3608:152,Availability,failure,failure,152,"A key feature for job manager is an ability to 'archive' a job so that it no longer shows in the UI when you don't want to see it anymore. Say it was a failure and you have dealt with the failure. You don't need to see it every time you look at recently failed jobs if you've dealt with it. The idea for implementing this was to have a label key called ""flag"" and have a value ""archived"" that we could apply to jobs a user wants to archive. The part we are struggling with is how to filter those out of view in the UI. If it's not difficult, the best way would be to be able to do queries where we say ""show me all the jobs that match this query and do not have the label `flag:archived` attached to them"". Is this something that would be possible?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3608
https://github.com/broadinstitute/cromwell/issues/3608:188,Availability,failure,failure,188,"A key feature for job manager is an ability to 'archive' a job so that it no longer shows in the UI when you don't want to see it anymore. Say it was a failure and you have dealt with the failure. You don't need to see it every time you look at recently failed jobs if you've dealt with it. The idea for implementing this was to have a label key called ""flag"" and have a value ""archived"" that we could apply to jobs a user wants to archive. The part we are struggling with is how to filter those out of view in the UI. If it's not difficult, the best way would be to be able to do queries where we say ""show me all the jobs that match this query and do not have the label `flag:archived` attached to them"". Is this something that would be possible?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3608
https://github.com/broadinstitute/cromwell/pull/3609:25,Testability,test,test,25,"Includes a Centaur smoke test. Pay no mind to the additional entry in `skipped_tests.csv`, nothing to see there.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3609
https://github.com/broadinstitute/cromwell/pull/3611:156,Testability,test,test,156,- Allows any womtool supported language / version to generate an `inputs.json`; - Gives the option to not generate optional inputs; - Adds a whole bunch of test cases which (I'll be honest) I didn't go through super-closely but looked about right; - Closes #3338 (using the `-o false` option,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3611
https://github.com/broadinstitute/cromwell/pull/3612:145,Testability,Test,Test,145,The regular expression that verifies docker image is modified to handle optional port. It allows using private and local registries with a port. Test cases covering this situation are provided.; ; **Resolves:** [Support docker images and tags from local registry](https://github.com/broadinstitute/cromwell/issues/2592),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3612
https://github.com/broadinstitute/cromwell/pull/3613:86,Deployability,configurat,configuration,86,"This PR adds initial support for tasks to run on an AWS Batch based backend It uses a configuration similar to other backends as documented in the AWS Batch 101 documentation (included in the PR). For now, the biggest gap is the lack of S3 Filesystem support. Job output is copied to the local filesystem for integration with the rest of the workflow. I will establish a new issue for that and issue a separate PR for that in the future. There are a few warts commented in the AwsBatchAsyncBackendJobExecutionActor that are related to this copying. I'd appreciate input on this, although ultimately I expect the warty code to be removed when the S3 Filesystem support is complete.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3613
https://github.com/broadinstitute/cromwell/pull/3613:309,Deployability,integrat,integration,309,"This PR adds initial support for tasks to run on an AWS Batch based backend It uses a configuration similar to other backends as documented in the AWS Batch 101 documentation (included in the PR). For now, the biggest gap is the lack of S3 Filesystem support. Job output is copied to the local filesystem for integration with the rest of the workflow. I will establish a new issue for that and issue a separate PR for that in the future. There are a few warts commented in the AwsBatchAsyncBackendJobExecutionActor that are related to this copying. I'd appreciate input on this, although ultimately I expect the warty code to be removed when the S3 Filesystem support is complete.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3613
https://github.com/broadinstitute/cromwell/pull/3613:309,Integrability,integrat,integration,309,"This PR adds initial support for tasks to run on an AWS Batch based backend It uses a configuration similar to other backends as documented in the AWS Batch 101 documentation (included in the PR). For now, the biggest gap is the lack of S3 Filesystem support. Job output is copied to the local filesystem for integration with the rest of the workflow. I will establish a new issue for that and issue a separate PR for that in the future. There are a few warts commented in the AwsBatchAsyncBackendJobExecutionActor that are related to this copying. I'd appreciate input on this, although ultimately I expect the warty code to be removed when the S3 Filesystem support is complete.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3613
https://github.com/broadinstitute/cromwell/pull/3613:86,Modifiability,config,configuration,86,"This PR adds initial support for tasks to run on an AWS Batch based backend It uses a configuration similar to other backends as documented in the AWS Batch 101 documentation (included in the PR). For now, the biggest gap is the lack of S3 Filesystem support. Job output is copied to the local filesystem for integration with the rest of the workflow. I will establish a new issue for that and issue a separate PR for that in the future. There are a few warts commented in the AwsBatchAsyncBackendJobExecutionActor that are related to this copying. I'd appreciate input on this, although ultimately I expect the warty code to be removed when the S3 Filesystem support is complete.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3613
https://github.com/broadinstitute/cromwell/issues/3614:344,Availability,error,error,344,"Hi there!. I'm new to both WDL and Cromwell. I'm trying to run a workflow and the main input is an Illumina run folder, which could be up to a Tb in size. So I don't want it to be copied or hard-linked (which is almost the same in time) but to be soft-linked. I changed the localization option in the backend but then I'm getting the following error:. `Cannot localize directory with symbolic links`. So, isn't this possible? Will I always have to hard-linked the directories? This is not an option in my case due to performance issues. Everything is in the same shared file system, so I don't see the point in hard-linking. Also, if I call the same directory in different tasks, will it be hard-linked every time?. Thank you very much in advance!. Best,; Santiago",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3614
https://github.com/broadinstitute/cromwell/issues/3614:517,Performance,perform,performance,517,"Hi there!. I'm new to both WDL and Cromwell. I'm trying to run a workflow and the main input is an Illumina run folder, which could be up to a Tb in size. So I don't want it to be copied or hard-linked (which is almost the same in time) but to be soft-linked. I changed the localization option in the backend but then I'm getting the following error:. `Cannot localize directory with symbolic links`. So, isn't this possible? Will I always have to hard-linked the directories? This is not an option in my case due to performance issues. Everything is in the same shared file system, so I don't see the point in hard-linking. Also, if I call the same directory in different tasks, will it be hard-linked every time?. Thank you very much in advance!. Best,; Santiago",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3614
https://github.com/broadinstitute/cromwell/issues/3615:499,Availability,error,error,499,"A task in a workflow failed because of the issue:; ```; Gsutil failed: Could not capture docker logs: Unable to capture docker logs exit status 1; ```; We could find all of the expected output files for the failed task in the cromwell-execution bucket, except `stdout.log` and `stderr.log` files. And we can make sure that this task only used nearly 2% of the disk space. Only 1 out of 8000 workflows ran into this issue, and re-run the failed workflow with call caching on didn't run into the same error, so it seems like an intermittent problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3615
https://github.com/broadinstitute/cromwell/issues/3615:96,Testability,log,logs,96,"A task in a workflow failed because of the issue:; ```; Gsutil failed: Could not capture docker logs: Unable to capture docker logs exit status 1; ```; We could find all of the expected output files for the failed task in the cromwell-execution bucket, except `stdout.log` and `stderr.log` files. And we can make sure that this task only used nearly 2% of the disk space. Only 1 out of 8000 workflows ran into this issue, and re-run the failed workflow with call caching on didn't run into the same error, so it seems like an intermittent problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3615
https://github.com/broadinstitute/cromwell/issues/3615:127,Testability,log,logs,127,"A task in a workflow failed because of the issue:; ```; Gsutil failed: Could not capture docker logs: Unable to capture docker logs exit status 1; ```; We could find all of the expected output files for the failed task in the cromwell-execution bucket, except `stdout.log` and `stderr.log` files. And we can make sure that this task only used nearly 2% of the disk space. Only 1 out of 8000 workflows ran into this issue, and re-run the failed workflow with call caching on didn't run into the same error, so it seems like an intermittent problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3615
https://github.com/broadinstitute/cromwell/issues/3615:268,Testability,log,log,268,"A task in a workflow failed because of the issue:; ```; Gsutil failed: Could not capture docker logs: Unable to capture docker logs exit status 1; ```; We could find all of the expected output files for the failed task in the cromwell-execution bucket, except `stdout.log` and `stderr.log` files. And we can make sure that this task only used nearly 2% of the disk space. Only 1 out of 8000 workflows ran into this issue, and re-run the failed workflow with call caching on didn't run into the same error, so it seems like an intermittent problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3615
https://github.com/broadinstitute/cromwell/issues/3615:285,Testability,log,log,285,"A task in a workflow failed because of the issue:; ```; Gsutil failed: Could not capture docker logs: Unable to capture docker logs exit status 1; ```; We could find all of the expected output files for the failed task in the cromwell-execution bucket, except `stdout.log` and `stderr.log` files. And we can make sure that this task only used nearly 2% of the disk space. Only 1 out of 8000 workflows ran into this issue, and re-run the failed workflow with call caching on didn't run into the same error, so it seems like an intermittent problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3615
https://github.com/broadinstitute/cromwell/issues/3618:495,Availability,error,error,495,"I ran a WDL file locally, by first creating a gatk jar and building a Docker image on my computer and then executing the WDL using Cromwell. I ran the following commands:; ./gradlew shadowJar; docker build -t us.gcr.io/broad-dsde-methods/broad-gatk-snapshots:testimage1 --build-arg DRELEASE=false .; java -jar cromwell-30.2.jar run cnv_somatic_pair_workflow.wdl --inputs cnv_somatic_pair_wgs_no-gc_workflow.json. Although all the necessary tasks seem to have finished, I still got the following error message:; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-6-0-mergePreferred#1200284127]] terminated abruptly; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-2-0-mergePreferred#-1631704537]] terminated abruptly; [2018-05-14 11:34:10,43] [info] Automatic shutdown of the async connection; [2018-05-14 11:34:10,43] [info] Gracefully shutdown sentry threads.; [2018-05-14 11:34:10,43] [info] Shutdown finished.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618
https://github.com/broadinstitute/cromwell/issues/3618:537,Availability,error,error,537,"I ran a WDL file locally, by first creating a gatk jar and building a Docker image on my computer and then executing the WDL using Cromwell. I ran the following commands:; ./gradlew shadowJar; docker build -t us.gcr.io/broad-dsde-methods/broad-gatk-snapshots:testimage1 --build-arg DRELEASE=false .; java -jar cromwell-30.2.jar run cnv_somatic_pair_workflow.wdl --inputs cnv_somatic_pair_wgs_no-gc_workflow.json. Although all the necessary tasks seem to have finished, I still got the following error message:; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-6-0-mergePreferred#1200284127]] terminated abruptly; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-2-0-mergePreferred#-1631704537]] terminated abruptly; [2018-05-14 11:34:10,43] [info] Automatic shutdown of the async connection; [2018-05-14 11:34:10,43] [info] Gracefully shutdown sentry threads.; [2018-05-14 11:34:10,43] [info] Shutdown finished.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618
https://github.com/broadinstitute/cromwell/issues/3618:568,Availability,error,error,568,"I ran a WDL file locally, by first creating a gatk jar and building a Docker image on my computer and then executing the WDL using Cromwell. I ran the following commands:; ./gradlew shadowJar; docker build -t us.gcr.io/broad-dsde-methods/broad-gatk-snapshots:testimage1 --build-arg DRELEASE=false .; java -jar cromwell-30.2.jar run cnv_somatic_pair_workflow.wdl --inputs cnv_somatic_pair_wgs_no-gc_workflow.json. Although all the necessary tasks seem to have finished, I still got the following error message:; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-6-0-mergePreferred#1200284127]] terminated abruptly; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-2-0-mergePreferred#-1631704537]] terminated abruptly; [2018-05-14 11:34:10,43] [info] Automatic shutdown of the async connection; [2018-05-14 11:34:10,43] [info] Gracefully shutdown sentry threads.; [2018-05-14 11:34:10,43] [info] Shutdown finished.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618
https://github.com/broadinstitute/cromwell/issues/3618:769,Availability,error,error,769,"I ran a WDL file locally, by first creating a gatk jar and building a Docker image on my computer and then executing the WDL using Cromwell. I ran the following commands:; ./gradlew shadowJar; docker build -t us.gcr.io/broad-dsde-methods/broad-gatk-snapshots:testimage1 --build-arg DRELEASE=false .; java -jar cromwell-30.2.jar run cnv_somatic_pair_workflow.wdl --inputs cnv_somatic_pair_wgs_no-gc_workflow.json. Although all the necessary tasks seem to have finished, I still got the following error message:; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-6-0-mergePreferred#1200284127]] terminated abruptly; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-2-0-mergePreferred#-1631704537]] terminated abruptly; [2018-05-14 11:34:10,43] [info] Automatic shutdown of the async connection; [2018-05-14 11:34:10,43] [info] Gracefully shutdown sentry threads.; [2018-05-14 11:34:10,43] [info] Shutdown finished.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618
https://github.com/broadinstitute/cromwell/issues/3618:800,Availability,error,error,800,"I ran a WDL file locally, by first creating a gatk jar and building a Docker image on my computer and then executing the WDL using Cromwell. I ran the following commands:; ./gradlew shadowJar; docker build -t us.gcr.io/broad-dsde-methods/broad-gatk-snapshots:testimage1 --build-arg DRELEASE=false .; java -jar cromwell-30.2.jar run cnv_somatic_pair_workflow.wdl --inputs cnv_somatic_pair_wgs_no-gc_workflow.json. Although all the necessary tasks seem to have finished, I still got the following error message:; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-6-0-mergePreferred#1200284127]] terminated abruptly; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-2-0-mergePreferred#-1631704537]] terminated abruptly; [2018-05-14 11:34:10,43] [info] Automatic shutdown of the async connection; [2018-05-14 11:34:10,43] [info] Gracefully shutdown sentry threads.; [2018-05-14 11:34:10,43] [info] Shutdown finished.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618
https://github.com/broadinstitute/cromwell/issues/3618:501,Integrability,message,message,501,"I ran a WDL file locally, by first creating a gatk jar and building a Docker image on my computer and then executing the WDL using Cromwell. I ran the following commands:; ./gradlew shadowJar; docker build -t us.gcr.io/broad-dsde-methods/broad-gatk-snapshots:testimage1 --build-arg DRELEASE=false .; java -jar cromwell-30.2.jar run cnv_somatic_pair_workflow.wdl --inputs cnv_somatic_pair_wgs_no-gc_workflow.json. Although all the necessary tasks seem to have finished, I still got the following error message:; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-6-0-mergePreferred#1200284127]] terminated abruptly; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-2-0-mergePreferred#-1631704537]] terminated abruptly; [2018-05-14 11:34:10,43] [info] Automatic shutdown of the async connection; [2018-05-14 11:34:10,43] [info] Gracefully shutdown sentry threads.; [2018-05-14 11:34:10,43] [info] Shutdown finished.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618
https://github.com/broadinstitute/cromwell/pull/3620:22,Availability,error,error,22,This should help with error messages when Cromwell shuts down,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620
https://github.com/broadinstitute/cromwell/pull/3620:57,Availability,down,down,57,This should help with error messages when Cromwell shuts down,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620
https://github.com/broadinstitute/cromwell/pull/3620:28,Integrability,message,messages,28,This should help with error messages when Cromwell shuts down,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620
https://github.com/broadinstitute/cromwell/pull/3621:5,Testability,test,tests,5,Some tests were just failing because their input were not on GCS,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3621
https://github.com/broadinstitute/cromwell/issues/3622:80,Availability,error,error,80,"Whenever SAM returns a successful response the content is JSON. When there's an error, say a `HTTP 500` the SAM response is HTML. However in all cases CromIAM tries to deserialize the response into JSON. The effect is that the original error message is lost producing a stack trace similar to the one below. A/C: SAM errors are reported and/or logged with original/redacted information, and NOT logged with the error below. . ```java; [INFO] [05/14/2018 18:08:50.471] [default-akka.actor.default-dispatcher-5853] [CromIamServer$(akka://default)] Requesting authorization for view access for user [REDACTED] on a request to view for collection [REDACTED]; [ERROR] [05/14/2018 18:08:50.507] [CromIamServer-akka.actor.default-dispatcher-443] [CromIamServer$(akka://default)] Request failed java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:236,Availability,error,error,236,"Whenever SAM returns a successful response the content is JSON. When there's an error, say a `HTTP 500` the SAM response is HTML. However in all cases CromIAM tries to deserialize the response into JSON. The effect is that the original error message is lost producing a stack trace similar to the one below. A/C: SAM errors are reported and/or logged with original/redacted information, and NOT logged with the error below. . ```java; [INFO] [05/14/2018 18:08:50.471] [default-akka.actor.default-dispatcher-5853] [CromIamServer$(akka://default)] Requesting authorization for view access for user [REDACTED] on a request to view for collection [REDACTED]; [ERROR] [05/14/2018 18:08:50.507] [CromIamServer-akka.actor.default-dispatcher-443] [CromIamServer$(akka://default)] Request failed java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:317,Availability,error,errors,317,"Whenever SAM returns a successful response the content is JSON. When there's an error, say a `HTTP 500` the SAM response is HTML. However in all cases CromIAM tries to deserialize the response into JSON. The effect is that the original error message is lost producing a stack trace similar to the one below. A/C: SAM errors are reported and/or logged with original/redacted information, and NOT logged with the error below. . ```java; [INFO] [05/14/2018 18:08:50.471] [default-akka.actor.default-dispatcher-5853] [CromIamServer$(akka://default)] Requesting authorization for view access for user [REDACTED] on a request to view for collection [REDACTED]; [ERROR] [05/14/2018 18:08:50.507] [CromIamServer-akka.actor.default-dispatcher-443] [CromIamServer$(akka://default)] Request failed java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:411,Availability,error,error,411,"Whenever SAM returns a successful response the content is JSON. When there's an error, say a `HTTP 500` the SAM response is HTML. However in all cases CromIAM tries to deserialize the response into JSON. The effect is that the original error message is lost producing a stack trace similar to the one below. A/C: SAM errors are reported and/or logged with original/redacted information, and NOT logged with the error below. . ```java; [INFO] [05/14/2018 18:08:50.471] [default-akka.actor.default-dispatcher-5853] [CromIamServer$(akka://default)] Requesting authorization for view access for user [REDACTED] on a request to view for collection [REDACTED]; [ERROR] [05/14/2018 18:08:50.507] [CromIamServer-akka.actor.default-dispatcher-443] [CromIamServer$(akka://default)] Request failed java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:656,Availability,ERROR,ERROR,656,"Whenever SAM returns a successful response the content is JSON. When there's an error, say a `HTTP 500` the SAM response is HTML. However in all cases CromIAM tries to deserialize the response into JSON. The effect is that the original error message is lost producing a stack trace similar to the one below. A/C: SAM errors are reported and/or logged with original/redacted information, and NOT logged with the error below. . ```java; [INFO] [05/14/2018 18:08:50.471] [default-akka.actor.default-dispatcher-5853] [CromIamServer$(akka://default)] Requesting authorization for view access for user [REDACTED] on a request to view for collection [REDACTED]; [ERROR] [05/14/2018 18:08:50.507] [CromIamServer-akka.actor.default-dispatcher-443] [CromIamServer$(akka://default)] Request failed java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:3923,Availability,error,error,3923,"at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException: Unsupported Content-Type, supported: application/json; 	at akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException$.apply(Unmarshaller.scala:158); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$EnhancedFromEntityUnmarshaller$.$anonfun$forContentTypes$3(Unmarshaller.scala:114); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshaller.$anonfun$transform$3(Unmarshaller.scala:23); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshaller.$anonfun$transform$3(Unmarshaller.scala:23); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshal.to(Unmarshal.scala:25); 	at cromiam.sam.SamClient.$anonfun$collectionsForUser$1(SamClient.scala:52); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	... 12 more; ```. Unmarshalling section where the response may not always be `HTTP 200` and the entity could actually be an error HTML string: https://github.com/broadinstitute/cromwell/blob/4252c85d417b727d7745d67fdec03f6f175b6e58/CromIAM/src/main/scala/cromiam/sam/SamClient.scala#L50-L53",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:242,Integrability,message,message,242,"Whenever SAM returns a successful response the content is JSON. When there's an error, say a `HTTP 500` the SAM response is HTML. However in all cases CromIAM tries to deserialize the response into JSON. The effect is that the original error message is lost producing a stack trace similar to the one below. A/C: SAM errors are reported and/or logged with original/redacted information, and NOT logged with the error below. . ```java; [INFO] [05/14/2018 18:08:50.471] [default-akka.actor.default-dispatcher-5853] [CromIamServer$(akka://default)] Requesting authorization for view access for user [REDACTED] on a request to view for collection [REDACTED]; [ERROR] [05/14/2018 18:08:50.507] [CromIamServer-akka.actor.default-dispatcher-443] [CromIamServer$(akka://default)] Request failed java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:2968,Modifiability,Enhance,EnhancedFromEntityUnmarshaller,2968,".apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException: Unsupported Content-Type, supported: application/json; 	at akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException$.apply(Unmarshaller.scala:158); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$EnhancedFromEntityUnmarshaller$.$anonfun$forContentTypes$3(Unmarshaller.scala:114); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshaller.$anonfun$transform$3(Unmarshaller.scala:23); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshaller.$anonfun$transform$3(Unmarshaller.scala:23); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshal.to(Unmarshal.scala:25); 	at cromiam.sam.SamClient.$anonfun$collectionsForUser$1(SamClient.scala:52); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	... 12 more; ```. Unmarshalling section where the response may not always be `HTTP 200` and the entity could actually be an error HTML string: https://github.com/broadinstitute/cr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:1697,Performance,concurren,concurrent,1697,"r-akka.actor.default-dispatcher-443] [CromIamServer$(akka://default)] Request failed java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: akka.http.scaladsl.unmarshalling.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:2022,Performance,concurren,concurrent,2022,"upported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException: Unsupported Content-Type, supported: application/json; 	at akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException$.apply(Unmarshaller.scala:158); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$EnhancedFromEntityUnmarshaller$.$anonfun$forContentTypes$3(U",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:3664,Performance,concurren,concurrent,3664,"at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException: Unsupported Content-Type, supported: application/json; 	at akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException$.apply(Unmarshaller.scala:158); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$EnhancedFromEntityUnmarshaller$.$anonfun$forContentTypes$3(Unmarshaller.scala:114); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshaller.$anonfun$transform$3(Unmarshaller.scala:23); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshaller.$anonfun$transform$3(Unmarshaller.scala:23); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshal.to(Unmarshal.scala:25); 	at cromiam.sam.SamClient.$anonfun$collectionsForUser$1(SamClient.scala:52); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	... 12 more; ```. Unmarshalling section where the response may not always be `HTTP 200` and the entity could actually be an error HTML string: https://github.com/broadinstitute/cromwell/blob/4252c85d417b727d7745d67fdec03f6f175b6e58/CromIAM/src/main/scala/cromiam/sam/SamClient.scala#L50-L53",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:3730,Performance,concurren,concurrent,3730,"at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException: Unsupported Content-Type, supported: application/json; 	at akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException$.apply(Unmarshaller.scala:158); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$EnhancedFromEntityUnmarshaller$.$anonfun$forContentTypes$3(Unmarshaller.scala:114); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshaller.$anonfun$transform$3(Unmarshaller.scala:23); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshaller.$anonfun$transform$3(Unmarshaller.scala:23); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshal.to(Unmarshal.scala:25); 	at cromiam.sam.SamClient.$anonfun$collectionsForUser$1(SamClient.scala:52); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	... 12 more; ```. Unmarshalling section where the response may not always be `HTTP 200` and the entity could actually be an error HTML string: https://github.com/broadinstitute/cromwell/blob/4252c85d417b727d7745d67fdec03f6f175b6e58/CromIAM/src/main/scala/cromiam/sam/SamClient.scala#L50-L53",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:557,Security,authoriz,authorization,557,"Whenever SAM returns a successful response the content is JSON. When there's an error, say a `HTTP 500` the SAM response is HTML. However in all cases CromIAM tries to deserialize the response into JSON. The effect is that the original error message is lost producing a stack trace similar to the one below. A/C: SAM errors are reported and/or logged with original/redacted information, and NOT logged with the error below. . ```java; [INFO] [05/14/2018 18:08:50.471] [default-akka.actor.default-dispatcher-5853] [CromIamServer$(akka://default)] Requesting authorization for view access for user [REDACTED] on a request to view for collection [REDACTED]; [ERROR] [05/14/2018 18:08:50.507] [CromIamServer-akka.actor.default-dispatcher-443] [CromIamServer$(akka://default)] Request failed java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:580,Security,access,access,580,"Whenever SAM returns a successful response the content is JSON. When there's an error, say a `HTTP 500` the SAM response is HTML. However in all cases CromIAM tries to deserialize the response into JSON. The effect is that the original error message is lost producing a stack trace similar to the one below. A/C: SAM errors are reported and/or logged with original/redacted information, and NOT logged with the error below. . ```java; [INFO] [05/14/2018 18:08:50.471] [default-akka.actor.default-dispatcher-5853] [CromIamServer$(akka://default)] Requesting authorization for view access for user [REDACTED] on a request to view for collection [REDACTED]; [ERROR] [05/14/2018 18:08:50.507] [CromIamServer-akka.actor.default-dispatcher-443] [CromIamServer$(akka://default)] Request failed java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:344,Testability,log,logged,344,"Whenever SAM returns a successful response the content is JSON. When there's an error, say a `HTTP 500` the SAM response is HTML. However in all cases CromIAM tries to deserialize the response into JSON. The effect is that the original error message is lost producing a stack trace similar to the one below. A/C: SAM errors are reported and/or logged with original/redacted information, and NOT logged with the error below. . ```java; [INFO] [05/14/2018 18:08:50.471] [default-akka.actor.default-dispatcher-5853] [CromIamServer$(akka://default)] Requesting authorization for view access for user [REDACTED] on a request to view for collection [REDACTED]; [ERROR] [05/14/2018 18:08:50.507] [CromIamServer-akka.actor.default-dispatcher-443] [CromIamServer$(akka://default)] Request failed java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3622:395,Testability,log,logged,395,"Whenever SAM returns a successful response the content is JSON. When there's an error, say a `HTTP 500` the SAM response is HTML. However in all cases CromIAM tries to deserialize the response into JSON. The effect is that the original error message is lost producing a stack trace similar to the one below. A/C: SAM errors are reported and/or logged with original/redacted information, and NOT logged with the error below. . ```java; [INFO] [05/14/2018 18:08:50.471] [default-akka.actor.default-dispatcher-5853] [CromIamServer$(akka://default)] Requesting authorization for view access for user [REDACTED] on a request to view for collection [REDACTED]; [ERROR] [05/14/2018 18:08:50.507] [CromIamServer-akka.actor.default-dispatcher-443] [CromIamServer$(akka://default)] Request failed java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622
https://github.com/broadinstitute/cromwell/issues/3623:672,Deployability,patch,patch,672,"The StandardBackend is caching some workflow information in the backend initialization data and then using it for things like returning the ""workflowRoot"":. https://github.com/broadinstitute/cromwell/blob/4252c85d417b727d7745d67fdec03f6f175b6e58/backend/src/main/scala/cromwell/backend/standard/StandardLifecycleActorFactory.scala#L199-L205. The problem with this is that there is only _one_ `initializationData` being passed around for a ""family"" of root and subworkflows. https://github.com/broadinstitute/cromwell/blob/4252c85d417b727d7745d67fdec03f6f175b6e58/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L562-L583. A patch for this particular issue may be a simple as _not_ overriding `getWorkflowExecutionRootPath` to ignore the `workflowRoot` stored in the initialization data. Ultimately it's possible the `workflowRoot` doesn't belong in `initializationData` if the data is supposed to be for a root-plus-subworkflows. A/C:; - Metadata for a subworkflow returns the `workflowRoot` for the subworkflow not the root.; - Other tickets filed as necessary if there are other issues regarding `initializationData` being shared between root and subworkflows. For example:; - The initializationData.workflowRoot` may be used incorrectly in other subworkflow cases; - Other ""workflow"" variables like the `executionRoot` may need a similar fix.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3623
https://github.com/broadinstitute/cromwell/issues/3623:1335,Modifiability,variab,variables,1335,"The StandardBackend is caching some workflow information in the backend initialization data and then using it for things like returning the ""workflowRoot"":. https://github.com/broadinstitute/cromwell/blob/4252c85d417b727d7745d67fdec03f6f175b6e58/backend/src/main/scala/cromwell/backend/standard/StandardLifecycleActorFactory.scala#L199-L205. The problem with this is that there is only _one_ `initializationData` being passed around for a ""family"" of root and subworkflows. https://github.com/broadinstitute/cromwell/blob/4252c85d417b727d7745d67fdec03f6f175b6e58/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L562-L583. A patch for this particular issue may be a simple as _not_ overriding `getWorkflowExecutionRootPath` to ignore the `workflowRoot` stored in the initialization data. Ultimately it's possible the `workflowRoot` doesn't belong in `initializationData` if the data is supposed to be for a root-plus-subworkflows. A/C:; - Metadata for a subworkflow returns the `workflowRoot` for the subworkflow not the root.; - Other tickets filed as necessary if there are other issues regarding `initializationData` being shared between root and subworkflows. For example:; - The initializationData.workflowRoot` may be used incorrectly in other subworkflow cases; - Other ""workflow"" variables like the `executionRoot` may need a similar fix.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3623
https://github.com/broadinstitute/cromwell/issues/3623:713,Usability,simpl,simple,713,"The StandardBackend is caching some workflow information in the backend initialization data and then using it for things like returning the ""workflowRoot"":. https://github.com/broadinstitute/cromwell/blob/4252c85d417b727d7745d67fdec03f6f175b6e58/backend/src/main/scala/cromwell/backend/standard/StandardLifecycleActorFactory.scala#L199-L205. The problem with this is that there is only _one_ `initializationData` being passed around for a ""family"" of root and subworkflows. https://github.com/broadinstitute/cromwell/blob/4252c85d417b727d7745d67fdec03f6f175b6e58/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L562-L583. A patch for this particular issue may be a simple as _not_ overriding `getWorkflowExecutionRootPath` to ignore the `workflowRoot` stored in the initialization data. Ultimately it's possible the `workflowRoot` doesn't belong in `initializationData` if the data is supposed to be for a root-plus-subworkflows. A/C:; - Metadata for a subworkflow returns the `workflowRoot` for the subworkflow not the root.; - Other tickets filed as necessary if there are other issues regarding `initializationData` being shared between root and subworkflows. For example:; - The initializationData.workflowRoot` may be used incorrectly in other subworkflow cases; - Other ""workflow"" variables like the `executionRoot` may need a similar fix.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3623
https://github.com/broadinstitute/cromwell/issues/3624:193,Modifiability,plugin,plugins,193,"For some reason Cromwell now makes jars for every subproject every time the `run` command is issued:. ```; computer:cromwell me$ sbt ""project server"" ""run server""; [info] Loading settings from plugins.sbt,swagger2markup.sbt ...; [info] Loading project definition from /Users/me/gitrepos/cromwell/project; [info] Loading settings from build.sbt ...; [info] Resolving key references (31064 settings) ...; [info] Set current project to root (in build file:/Users/me/gitrepos/cromwell/); [info] Set current project to cromwell (in build file:/Users/me/gitrepos/cromwell/); [info] Packaging /Users/me/gitrepos/cromwell/database/sql/target/scala-2.12/cromwell-database-sql_2.12-32-92c91d9-SNAP.jar ...; [info] Packaging /Users/me/gitrepos/cromwell/cromwellApiClient/target/scala-2.12/cromwell-api-client_2.12-32-92c91d9-SNAP.jar ...; [info] Done packaging.; [info] Packaging /Users/me/gitrepos/cromwell/common/target/scala-2.12/cromwell-common_2.12-32-92c91d9-SNAP.jar ...; ...; ```. This certainly isn't making the run launch any faster and clutters up the build directory with jars that makes it tough to find the Centaur CWL runner and Cromwell server jars needed for conformance testing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3624
https://github.com/broadinstitute/cromwell/issues/3624:171,Performance,Load,Loading,171,"For some reason Cromwell now makes jars for every subproject every time the `run` command is issued:. ```; computer:cromwell me$ sbt ""project server"" ""run server""; [info] Loading settings from plugins.sbt,swagger2markup.sbt ...; [info] Loading project definition from /Users/me/gitrepos/cromwell/project; [info] Loading settings from build.sbt ...; [info] Resolving key references (31064 settings) ...; [info] Set current project to root (in build file:/Users/me/gitrepos/cromwell/); [info] Set current project to cromwell (in build file:/Users/me/gitrepos/cromwell/); [info] Packaging /Users/me/gitrepos/cromwell/database/sql/target/scala-2.12/cromwell-database-sql_2.12-32-92c91d9-SNAP.jar ...; [info] Packaging /Users/me/gitrepos/cromwell/cromwellApiClient/target/scala-2.12/cromwell-api-client_2.12-32-92c91d9-SNAP.jar ...; [info] Done packaging.; [info] Packaging /Users/me/gitrepos/cromwell/common/target/scala-2.12/cromwell-common_2.12-32-92c91d9-SNAP.jar ...; ...; ```. This certainly isn't making the run launch any faster and clutters up the build directory with jars that makes it tough to find the Centaur CWL runner and Cromwell server jars needed for conformance testing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3624
https://github.com/broadinstitute/cromwell/issues/3624:236,Performance,Load,Loading,236,"For some reason Cromwell now makes jars for every subproject every time the `run` command is issued:. ```; computer:cromwell me$ sbt ""project server"" ""run server""; [info] Loading settings from plugins.sbt,swagger2markup.sbt ...; [info] Loading project definition from /Users/me/gitrepos/cromwell/project; [info] Loading settings from build.sbt ...; [info] Resolving key references (31064 settings) ...; [info] Set current project to root (in build file:/Users/me/gitrepos/cromwell/); [info] Set current project to cromwell (in build file:/Users/me/gitrepos/cromwell/); [info] Packaging /Users/me/gitrepos/cromwell/database/sql/target/scala-2.12/cromwell-database-sql_2.12-32-92c91d9-SNAP.jar ...; [info] Packaging /Users/me/gitrepos/cromwell/cromwellApiClient/target/scala-2.12/cromwell-api-client_2.12-32-92c91d9-SNAP.jar ...; [info] Done packaging.; [info] Packaging /Users/me/gitrepos/cromwell/common/target/scala-2.12/cromwell-common_2.12-32-92c91d9-SNAP.jar ...; ...; ```. This certainly isn't making the run launch any faster and clutters up the build directory with jars that makes it tough to find the Centaur CWL runner and Cromwell server jars needed for conformance testing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3624
https://github.com/broadinstitute/cromwell/issues/3624:312,Performance,Load,Loading,312,"For some reason Cromwell now makes jars for every subproject every time the `run` command is issued:. ```; computer:cromwell me$ sbt ""project server"" ""run server""; [info] Loading settings from plugins.sbt,swagger2markup.sbt ...; [info] Loading project definition from /Users/me/gitrepos/cromwell/project; [info] Loading settings from build.sbt ...; [info] Resolving key references (31064 settings) ...; [info] Set current project to root (in build file:/Users/me/gitrepos/cromwell/); [info] Set current project to cromwell (in build file:/Users/me/gitrepos/cromwell/); [info] Packaging /Users/me/gitrepos/cromwell/database/sql/target/scala-2.12/cromwell-database-sql_2.12-32-92c91d9-SNAP.jar ...; [info] Packaging /Users/me/gitrepos/cromwell/cromwellApiClient/target/scala-2.12/cromwell-api-client_2.12-32-92c91d9-SNAP.jar ...; [info] Done packaging.; [info] Packaging /Users/me/gitrepos/cromwell/common/target/scala-2.12/cromwell-common_2.12-32-92c91d9-SNAP.jar ...; ...; ```. This certainly isn't making the run launch any faster and clutters up the build directory with jars that makes it tough to find the Centaur CWL runner and Cromwell server jars needed for conformance testing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3624
https://github.com/broadinstitute/cromwell/issues/3624:1177,Testability,test,testing,1177,"For some reason Cromwell now makes jars for every subproject every time the `run` command is issued:. ```; computer:cromwell me$ sbt ""project server"" ""run server""; [info] Loading settings from plugins.sbt,swagger2markup.sbt ...; [info] Loading project definition from /Users/me/gitrepos/cromwell/project; [info] Loading settings from build.sbt ...; [info] Resolving key references (31064 settings) ...; [info] Set current project to root (in build file:/Users/me/gitrepos/cromwell/); [info] Set current project to cromwell (in build file:/Users/me/gitrepos/cromwell/); [info] Packaging /Users/me/gitrepos/cromwell/database/sql/target/scala-2.12/cromwell-database-sql_2.12-32-92c91d9-SNAP.jar ...; [info] Packaging /Users/me/gitrepos/cromwell/cromwellApiClient/target/scala-2.12/cromwell-api-client_2.12-32-92c91d9-SNAP.jar ...; [info] Done packaging.; [info] Packaging /Users/me/gitrepos/cromwell/common/target/scala-2.12/cromwell-common_2.12-32-92c91d9-SNAP.jar ...; ...; ```. This certainly isn't making the run launch any faster and clutters up the build directory with jars that makes it tough to find the Centaur CWL runner and Cromwell server jars needed for conformance testing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3624
https://github.com/broadinstitute/cromwell/pull/3628:75,Deployability,pipeline,pipeline,75,- [x] Rebase on develop after #3611 ; - Enables the germline single sample pipeline in WDL 1.0; - Fixes `womtool womgraph` with imports ; - Closes #3055 for WDL 1.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3628
https://github.com/broadinstitute/cromwell/pull/3631:33,Testability,test,test,33,"Remove bashisms from conformance test wdl.; Pick at most one assembly jar for the centaurCwlRunner.; Killing cromwell on parallel conformance test exit.; Renamed ""travis"" directory to the more generic ""ci"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3631
https://github.com/broadinstitute/cromwell/pull/3631:142,Testability,test,test,142,"Remove bashisms from conformance test wdl.; Pick at most one assembly jar for the centaurCwlRunner.; Killing cromwell on parallel conformance test exit.; Renamed ""travis"" directory to the more generic ""ci"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3631
https://github.com/broadinstitute/cromwell/issues/3632:71,Deployability,update,updated,71,"Hello,. So that I don't have to manually check to see if this has been updated, can the following snippet be moved into a seperate file?. https://github.com/broadinstitute/cromwell/blob/97b7d324c5cbec4b29093aa26c5b10f5086b54bb/src/bin/travis/testCentaurCwlConformanceLocal.sh#L32",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3632
https://github.com/broadinstitute/cromwell/issues/3632:242,Testability,test,testCentaurCwlConformanceLocal,242,"Hello,. So that I don't have to manually check to see if this has been updated, can the following snippet be moved into a seperate file?. https://github.com/broadinstitute/cromwell/blob/97b7d324c5cbec4b29093aa26c5b10f5086b54bb/src/bin/travis/testCentaurCwlConformanceLocal.sh#L32",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3632
https://github.com/broadinstitute/cromwell/issues/3633:13,Integrability,message,messages,13,"Lots of INFO messages still: https://ci.commonwl.org/job/cromwell/150/consoleText; Perhaps the logger's level needs to be adjusted? Then all those `if (!args.quiet) ` could be removed. With regards to https://github.com/broadinstitute/cromwell/blob/97b7d324c5cbec4b29093aa26c5b10f5086b54bb/centaurCwlRunner/src/main/scala/centaur/cwl/CentaurCwlRunner.scala#L192; The result should **always** be output to stdout (and nothing else), regardless of `--quiet` . Cheers,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3633
https://github.com/broadinstitute/cromwell/issues/3633:95,Testability,log,logger,95,"Lots of INFO messages still: https://ci.commonwl.org/job/cromwell/150/consoleText; Perhaps the logger's level needs to be adjusted? Then all those `if (!args.quiet) ` could be removed. With regards to https://github.com/broadinstitute/cromwell/blob/97b7d324c5cbec4b29093aa26c5b10f5086b54bb/centaurCwlRunner/src/main/scala/centaur/cwl/CentaurCwlRunner.scala#L192; The result should **always** be output to stdout (and nothing else), regardless of `--quiet` . Cheers,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3633
https://github.com/broadinstitute/cromwell/pull/3636:41,Testability,test,tests,41,No reason to limit this now that the BCS tests are turned off. (also I'm not sure this was actually doing what it looks like it should be doing),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3636
https://github.com/broadinstitute/cromwell/issues/3637:142,Availability,error,error,142,"As discussed at Workbench office hours with @cjllanwarne. @ruchim This affects @eitanbanks's work on PCAWG. We have been seeing the following error running the Mutect2 wdl on Firecloud. It seemed to start after we switched to an NIO wdl, although that fact may be a red herring. When running the M2 workflow we sometimes (about 5% of the time) get the error:; ```; Failed to import workflow https://api.firecloud.org/ga4gh/v1/tools/gatk:mutect2-gatk4/versions/8/plain-WDL/descriptor Error: Server Error. The server encountered a temporary error. Please try again in 30 seconds.; ```. Salient facts:; * The wdl it's trying to import from the Firecloud methods repository definitely exists.; * The issue always resolves by restarting the job.; * When scattering this wdl eg in the Mutect2 panel of normals workflow, which runs Mutect2 on many samples, the failure occurs on a random set of scatters, and the failing samples are different each time.; * As @cjllanwarne points out, it's odd that a subworkflow import would cause failure only for some scatters -- you would think that it is imported once for the whole job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3637
https://github.com/broadinstitute/cromwell/issues/3637:352,Availability,error,error,352,"As discussed at Workbench office hours with @cjllanwarne. @ruchim This affects @eitanbanks's work on PCAWG. We have been seeing the following error running the Mutect2 wdl on Firecloud. It seemed to start after we switched to an NIO wdl, although that fact may be a red herring. When running the M2 workflow we sometimes (about 5% of the time) get the error:; ```; Failed to import workflow https://api.firecloud.org/ga4gh/v1/tools/gatk:mutect2-gatk4/versions/8/plain-WDL/descriptor Error: Server Error. The server encountered a temporary error. Please try again in 30 seconds.; ```. Salient facts:; * The wdl it's trying to import from the Firecloud methods repository definitely exists.; * The issue always resolves by restarting the job.; * When scattering this wdl eg in the Mutect2 panel of normals workflow, which runs Mutect2 on many samples, the failure occurs on a random set of scatters, and the failing samples are different each time.; * As @cjllanwarne points out, it's odd that a subworkflow import would cause failure only for some scatters -- you would think that it is imported once for the whole job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3637
https://github.com/broadinstitute/cromwell/issues/3637:483,Availability,Error,Error,483,"As discussed at Workbench office hours with @cjllanwarne. @ruchim This affects @eitanbanks's work on PCAWG. We have been seeing the following error running the Mutect2 wdl on Firecloud. It seemed to start after we switched to an NIO wdl, although that fact may be a red herring. When running the M2 workflow we sometimes (about 5% of the time) get the error:; ```; Failed to import workflow https://api.firecloud.org/ga4gh/v1/tools/gatk:mutect2-gatk4/versions/8/plain-WDL/descriptor Error: Server Error. The server encountered a temporary error. Please try again in 30 seconds.; ```. Salient facts:; * The wdl it's trying to import from the Firecloud methods repository definitely exists.; * The issue always resolves by restarting the job.; * When scattering this wdl eg in the Mutect2 panel of normals workflow, which runs Mutect2 on many samples, the failure occurs on a random set of scatters, and the failing samples are different each time.; * As @cjllanwarne points out, it's odd that a subworkflow import would cause failure only for some scatters -- you would think that it is imported once for the whole job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3637
https://github.com/broadinstitute/cromwell/issues/3637:497,Availability,Error,Error,497,"As discussed at Workbench office hours with @cjllanwarne. @ruchim This affects @eitanbanks's work on PCAWG. We have been seeing the following error running the Mutect2 wdl on Firecloud. It seemed to start after we switched to an NIO wdl, although that fact may be a red herring. When running the M2 workflow we sometimes (about 5% of the time) get the error:; ```; Failed to import workflow https://api.firecloud.org/ga4gh/v1/tools/gatk:mutect2-gatk4/versions/8/plain-WDL/descriptor Error: Server Error. The server encountered a temporary error. Please try again in 30 seconds.; ```. Salient facts:; * The wdl it's trying to import from the Firecloud methods repository definitely exists.; * The issue always resolves by restarting the job.; * When scattering this wdl eg in the Mutect2 panel of normals workflow, which runs Mutect2 on many samples, the failure occurs on a random set of scatters, and the failing samples are different each time.; * As @cjllanwarne points out, it's odd that a subworkflow import would cause failure only for some scatters -- you would think that it is imported once for the whole job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3637
https://github.com/broadinstitute/cromwell/issues/3637:539,Availability,error,error,539,"As discussed at Workbench office hours with @cjllanwarne. @ruchim This affects @eitanbanks's work on PCAWG. We have been seeing the following error running the Mutect2 wdl on Firecloud. It seemed to start after we switched to an NIO wdl, although that fact may be a red herring. When running the M2 workflow we sometimes (about 5% of the time) get the error:; ```; Failed to import workflow https://api.firecloud.org/ga4gh/v1/tools/gatk:mutect2-gatk4/versions/8/plain-WDL/descriptor Error: Server Error. The server encountered a temporary error. Please try again in 30 seconds.; ```. Salient facts:; * The wdl it's trying to import from the Firecloud methods repository definitely exists.; * The issue always resolves by restarting the job.; * When scattering this wdl eg in the Mutect2 panel of normals workflow, which runs Mutect2 on many samples, the failure occurs on a random set of scatters, and the failing samples are different each time.; * As @cjllanwarne points out, it's odd that a subworkflow import would cause failure only for some scatters -- you would think that it is imported once for the whole job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3637
https://github.com/broadinstitute/cromwell/issues/3637:854,Availability,failure,failure,854,"As discussed at Workbench office hours with @cjllanwarne. @ruchim This affects @eitanbanks's work on PCAWG. We have been seeing the following error running the Mutect2 wdl on Firecloud. It seemed to start after we switched to an NIO wdl, although that fact may be a red herring. When running the M2 workflow we sometimes (about 5% of the time) get the error:; ```; Failed to import workflow https://api.firecloud.org/ga4gh/v1/tools/gatk:mutect2-gatk4/versions/8/plain-WDL/descriptor Error: Server Error. The server encountered a temporary error. Please try again in 30 seconds.; ```. Salient facts:; * The wdl it's trying to import from the Firecloud methods repository definitely exists.; * The issue always resolves by restarting the job.; * When scattering this wdl eg in the Mutect2 panel of normals workflow, which runs Mutect2 on many samples, the failure occurs on a random set of scatters, and the failing samples are different each time.; * As @cjllanwarne points out, it's odd that a subworkflow import would cause failure only for some scatters -- you would think that it is imported once for the whole job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3637
https://github.com/broadinstitute/cromwell/issues/3637:1025,Availability,failure,failure,1025,"As discussed at Workbench office hours with @cjllanwarne. @ruchim This affects @eitanbanks's work on PCAWG. We have been seeing the following error running the Mutect2 wdl on Firecloud. It seemed to start after we switched to an NIO wdl, although that fact may be a red herring. When running the M2 workflow we sometimes (about 5% of the time) get the error:; ```; Failed to import workflow https://api.firecloud.org/ga4gh/v1/tools/gatk:mutect2-gatk4/versions/8/plain-WDL/descriptor Error: Server Error. The server encountered a temporary error. Please try again in 30 seconds.; ```. Salient facts:; * The wdl it's trying to import from the Firecloud methods repository definitely exists.; * The issue always resolves by restarting the job.; * When scattering this wdl eg in the Mutect2 panel of normals workflow, which runs Mutect2 on many samples, the failure occurs on a random set of scatters, and the failing samples are different each time.; * As @cjllanwarne points out, it's odd that a subworkflow import would cause failure only for some scatters -- you would think that it is imported once for the whole job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3637
https://github.com/broadinstitute/cromwell/pull/3638:258,Deployability,configurat,configuration,258,"Requester pays is now added to Cromwell. The user can set the billing project id using one of the methods listed below:. - It can be added as `'google_project':'project-id'` as part of workflow options during workflow submission; - It can be included inside configuration file as shown in Getting started on Google Pipelines API where you need to replace the <google-billing-project-id> with the project id; - If it is not mentioned using above 2 ways, Cromwell will use the default project that has been configured with gcloud. More information about Requester pays can be found [here](https://cloud.google.com/storage/docs/requester-pays)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3638
https://github.com/broadinstitute/cromwell/pull/3638:315,Deployability,Pipeline,Pipelines,315,"Requester pays is now added to Cromwell. The user can set the billing project id using one of the methods listed below:. - It can be added as `'google_project':'project-id'` as part of workflow options during workflow submission; - It can be included inside configuration file as shown in Getting started on Google Pipelines API where you need to replace the <google-billing-project-id> with the project id; - If it is not mentioned using above 2 ways, Cromwell will use the default project that has been configured with gcloud. More information about Requester pays can be found [here](https://cloud.google.com/storage/docs/requester-pays)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3638
https://github.com/broadinstitute/cromwell/pull/3638:258,Modifiability,config,configuration,258,"Requester pays is now added to Cromwell. The user can set the billing project id using one of the methods listed below:. - It can be added as `'google_project':'project-id'` as part of workflow options during workflow submission; - It can be included inside configuration file as shown in Getting started on Google Pipelines API where you need to replace the <google-billing-project-id> with the project id; - If it is not mentioned using above 2 ways, Cromwell will use the default project that has been configured with gcloud. More information about Requester pays can be found [here](https://cloud.google.com/storage/docs/requester-pays)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3638
https://github.com/broadinstitute/cromwell/pull/3638:505,Modifiability,config,configured,505,"Requester pays is now added to Cromwell. The user can set the billing project id using one of the methods listed below:. - It can be added as `'google_project':'project-id'` as part of workflow options during workflow submission; - It can be included inside configuration file as shown in Getting started on Google Pipelines API where you need to replace the <google-billing-project-id> with the project id; - If it is not mentioned using above 2 ways, Cromwell will use the default project that has been configured with gcloud. More information about Requester pays can be found [here](https://cloud.google.com/storage/docs/requester-pays)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3638
https://github.com/broadinstitute/cromwell/pull/3641:18,Availability,error,error,18,- [x] Some of the error messages will change/improve following #3628 (or vice versa); - Red thumb required for the womtool changes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3641
https://github.com/broadinstitute/cromwell/pull/3641:24,Integrability,message,messages,24,- [x] Some of the error messages will change/improve following #3628 (or vice versa); - Red thumb required for the womtool changes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3641
https://github.com/broadinstitute/cromwell/pull/3642:0,Deployability,Update,Updates,0,Updates:; - Restores cron build.; - Moved chunks of duplicated scripts and confs into includes.; - Scripts and confs are now each under 100 lines.; - Switch Local backend CWL conformance testing from serial to parallel.; - With minimal prep (ex: vault auth) run test scripts on your laptop. Future:; - More refactoring might possible but at some point conditionals-in-common-code grows exponentially.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3642
https://github.com/broadinstitute/cromwell/pull/3642:307,Modifiability,refactor,refactoring,307,Updates:; - Restores cron build.; - Moved chunks of duplicated scripts and confs into includes.; - Scripts and confs are now each under 100 lines.; - Switch Local backend CWL conformance testing from serial to parallel.; - With minimal prep (ex: vault auth) run test scripts on your laptop. Future:; - More refactoring might possible but at some point conditionals-in-common-code grows exponentially.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3642
https://github.com/broadinstitute/cromwell/pull/3642:187,Testability,test,testing,187,Updates:; - Restores cron build.; - Moved chunks of duplicated scripts and confs into includes.; - Scripts and confs are now each under 100 lines.; - Switch Local backend CWL conformance testing from serial to parallel.; - With minimal prep (ex: vault auth) run test scripts on your laptop. Future:; - More refactoring might possible but at some point conditionals-in-common-code grows exponentially.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3642
https://github.com/broadinstitute/cromwell/pull/3642:262,Testability,test,test,262,Updates:; - Restores cron build.; - Moved chunks of duplicated scripts and confs into includes.; - Scripts and confs are now each under 100 lines.; - Switch Local backend CWL conformance testing from serial to parallel.; - With minimal prep (ex: vault auth) run test scripts on your laptop. Future:; - More refactoring might possible but at some point conditionals-in-common-code grows exponentially.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3642
https://github.com/broadinstitute/cromwell/issues/3647:229,Availability,error,error,229,"**WORKAROUND:** Explicitly `mkdir` (as necessary) and `export` a new `$TMPDIR` at the top of your task command, for example `export TMPDIR=/tmp` should work. Setting the `TMPDIR` environment variable to a long path will cause an error in Python `mulitprocessing` library. ```; Process SyncManager-1: ; Traceback (most recent call last):; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 258, in _bootstrap; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 114, in run; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 550, in _run_server; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 162, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 132, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 256, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647
https://github.com/broadinstitute/cromwell/issues/3647:1151,Availability,error,error,1151,"work. Setting the `TMPDIR` environment variable to a long path will cause an error in Python `mulitprocessing` library. ```; Process SyncManager-1: ; Traceback (most recent call last):; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 258, in _bootstrap; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 114, in run; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 550, in _run_server; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 162, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 132, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 256, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py 6ca4c640-758d-455d-ba5b-b965069a39b4/nationwidechildrens.org_biospecimen.TCGA-4C-A93U.xml; '; ```. A/C:; - A centaur test that checks that generated `",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647
https://github.com/broadinstitute/cromwell/issues/3647:1833,Availability,echo,echo,1833,"nt/out00-PYZ.pyz/multiprocessing.managers"", line 162, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 132, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 256, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py 6ca4c640-758d-455d-ba5b-b965069a39b4/nationwidechildrens.org_biospecimen.TCGA-4C-A93U.xml; '; ```. A/C:; - A centaur test that checks that generated `$TMPDIR` paths are always shorter than some reasonable length. Because different backends with-or-without docker may generate different paths for `$TMPDIR` the test should only use `<=` not `=` to verify the length of the environment variable. Other links:; - https://gatkforums.broadinstitute.org/firecloud/discussion/11980/problem-running-gdc-client-tool; - https://stackoverflow.com/questions/34829600/why-is-the-maximal-path-length-allowed-for-unix-sockets-on-linux-108; - https://unix.stackexchange.com/questions/367008/why-is-socket-path-length-limited-to-a-hundred-chars; - https://gith",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647
https://github.com/broadinstitute/cromwell/issues/3647:1854,Availability,echo,echo,1854,"nt/out00-PYZ.pyz/multiprocessing.managers"", line 162, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 132, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 256, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py 6ca4c640-758d-455d-ba5b-b965069a39b4/nationwidechildrens.org_biospecimen.TCGA-4C-A93U.xml; '; ```. A/C:; - A centaur test that checks that generated `$TMPDIR` paths are always shorter than some reasonable length. Because different backends with-or-without docker may generate different paths for `$TMPDIR` the test should only use `<=` not `=` to verify the length of the environment variable. Other links:; - https://gatkforums.broadinstitute.org/firecloud/discussion/11980/problem-running-gdc-client-tool; - https://stackoverflow.com/questions/34829600/why-is-the-maximal-path-length-allowed-for-unix-sockets-on-linux-108; - https://unix.stackexchange.com/questions/367008/why-is-socket-path-length-limited-to-a-hundred-chars; - https://gith",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647
https://github.com/broadinstitute/cromwell/issues/3647:191,Modifiability,variab,variable,191,"**WORKAROUND:** Explicitly `mkdir` (as necessary) and `export` a new `$TMPDIR` at the top of your task command, for example `export TMPDIR=/tmp` should work. Setting the `TMPDIR` environment variable to a long path will cause an error in Python `mulitprocessing` library. ```; Process SyncManager-1: ; Traceback (most recent call last):; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 258, in _bootstrap; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 114, in run; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 550, in _run_server; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 162, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 132, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 256, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647
https://github.com/broadinstitute/cromwell/issues/3647:1325,Modifiability,extend,extends,1325," recent call last):; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 258, in _bootstrap; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 114, in run; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 550, in _run_server; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 162, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 132, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 256, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py 6ca4c640-758d-455d-ba5b-b965069a39b4/nationwidechildrens.org_biospecimen.TCGA-4C-A93U.xml; '; ```. A/C:; - A centaur test that checks that generated `$TMPDIR` paths are always shorter than some reasonable length. Because different backends with-or-without docker may generate different paths for `$TMPDIR` the test s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647
https://github.com/broadinstitute/cromwell/issues/3647:2386,Modifiability,variab,variable,2386,"__init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py 6ca4c640-758d-455d-ba5b-b965069a39b4/nationwidechildrens.org_biospecimen.TCGA-4C-A93U.xml; '; ```. A/C:; - A centaur test that checks that generated `$TMPDIR` paths are always shorter than some reasonable length. Because different backends with-or-without docker may generate different paths for `$TMPDIR` the test should only use `<=` not `=` to verify the length of the environment variable. Other links:; - https://gatkforums.broadinstitute.org/firecloud/discussion/11980/problem-running-gdc-client-tool; - https://stackoverflow.com/questions/34829600/why-is-the-maximal-path-length-allowed-for-unix-sockets-on-linux-108; - https://unix.stackexchange.com/questions/367008/why-is-socket-path-length-limited-to-a-hundred-chars; - https://github.com/Ericsson/codechecker/issues/1533; - https://bugs.chromium.org/p/chromium/issues/detail?id=326203; - https://docs.python.org/2/library/multiprocessing.html; - https://docs.python.org/3/library/multiprocessing.html; - https://superuser.com/questions/807573/how-to-find-length-of-string-in-shell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647
https://github.com/broadinstitute/cromwell/issues/3647:2119,Testability,test,test,2119,"__init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py 6ca4c640-758d-455d-ba5b-b965069a39b4/nationwidechildrens.org_biospecimen.TCGA-4C-A93U.xml; '; ```. A/C:; - A centaur test that checks that generated `$TMPDIR` paths are always shorter than some reasonable length. Because different backends with-or-without docker may generate different paths for `$TMPDIR` the test should only use `<=` not `=` to verify the length of the environment variable. Other links:; - https://gatkforums.broadinstitute.org/firecloud/discussion/11980/problem-running-gdc-client-tool; - https://stackoverflow.com/questions/34829600/why-is-the-maximal-path-length-allowed-for-unix-sockets-on-linux-108; - https://unix.stackexchange.com/questions/367008/why-is-socket-path-length-limited-to-a-hundred-chars; - https://github.com/Ericsson/codechecker/issues/1533; - https://bugs.chromium.org/p/chromium/issues/detail?id=326203; - https://docs.python.org/2/library/multiprocessing.html; - https://docs.python.org/3/library/multiprocessing.html; - https://superuser.com/questions/807573/how-to-find-length-of-string-in-shell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647
https://github.com/broadinstitute/cromwell/issues/3647:2312,Testability,test,test,2312,"__init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py 6ca4c640-758d-455d-ba5b-b965069a39b4/nationwidechildrens.org_biospecimen.TCGA-4C-A93U.xml; '; ```. A/C:; - A centaur test that checks that generated `$TMPDIR` paths are always shorter than some reasonable length. Because different backends with-or-without docker may generate different paths for `$TMPDIR` the test should only use `<=` not `=` to verify the length of the environment variable. Other links:; - https://gatkforums.broadinstitute.org/firecloud/discussion/11980/problem-running-gdc-client-tool; - https://stackoverflow.com/questions/34829600/why-is-the-maximal-path-length-allowed-for-unix-sockets-on-linux-108; - https://unix.stackexchange.com/questions/367008/why-is-socket-path-length-limited-to-a-hundred-chars; - https://github.com/Ericsson/codechecker/issues/1533; - https://bugs.chromium.org/p/chromium/issues/detail?id=326203; - https://docs.python.org/2/library/multiprocessing.html; - https://docs.python.org/3/library/multiprocessing.html; - https://superuser.com/questions/807573/how-to-find-length-of-string-in-shell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647
https://github.com/broadinstitute/cromwell/issues/3648:489,Availability,avail,available,489,"So far cromwell is working great on our cluster. Thanks a lot for this splendid effort. However there is one issue we run into that other people might run into as well. Our cluster uses NFS as its filesystem backend. This means that if one node completes a job, it might take a while before the files that were created. In other workflows we can set an I/O timeout option: if the file did not appear within 3 minutes, the job failed. How do we do this in cromwell. All the settings I have available is `number-of-requests`, `per` and `number-of-attempts`. Currently we have; ```HOCON; io {; number-of-requests = 10; per = 10 seconds; number-of-attempts = 180; }; ```; This should make sure that failing files are attempted for 180 seconds, but this is not very elegant. There is a `timeout` option in I/O. But this is the timeout for the I/O operation to respond. If the file is not ""visible"" then the I/O operation will respond immediately, and the job will have failed. Is there a fix to this option in the current cromwell configuration?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648
https://github.com/broadinstitute/cromwell/issues/3648:1026,Deployability,configurat,configuration,1026,"So far cromwell is working great on our cluster. Thanks a lot for this splendid effort. However there is one issue we run into that other people might run into as well. Our cluster uses NFS as its filesystem backend. This means that if one node completes a job, it might take a while before the files that were created. In other workflows we can set an I/O timeout option: if the file did not appear within 3 minutes, the job failed. How do we do this in cromwell. All the settings I have available is `number-of-requests`, `per` and `number-of-attempts`. Currently we have; ```HOCON; io {; number-of-requests = 10; per = 10 seconds; number-of-attempts = 180; }; ```; This should make sure that failing files are attempted for 180 seconds, but this is not very elegant. There is a `timeout` option in I/O. But this is the timeout for the I/O operation to respond. If the file is not ""visible"" then the I/O operation will respond immediately, and the job will have failed. Is there a fix to this option in the current cromwell configuration?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648
https://github.com/broadinstitute/cromwell/issues/3648:1026,Modifiability,config,configuration,1026,"So far cromwell is working great on our cluster. Thanks a lot for this splendid effort. However there is one issue we run into that other people might run into as well. Our cluster uses NFS as its filesystem backend. This means that if one node completes a job, it might take a while before the files that were created. In other workflows we can set an I/O timeout option: if the file did not appear within 3 minutes, the job failed. How do we do this in cromwell. All the settings I have available is `number-of-requests`, `per` and `number-of-attempts`. Currently we have; ```HOCON; io {; number-of-requests = 10; per = 10 seconds; number-of-attempts = 180; }; ```; This should make sure that failing files are attempted for 180 seconds, but this is not very elegant. There is a `timeout` option in I/O. But this is the timeout for the I/O operation to respond. If the file is not ""visible"" then the I/O operation will respond immediately, and the job will have failed. Is there a fix to this option in the current cromwell configuration?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648
https://github.com/broadinstitute/cromwell/issues/3648:357,Safety,timeout,timeout,357,"So far cromwell is working great on our cluster. Thanks a lot for this splendid effort. However there is one issue we run into that other people might run into as well. Our cluster uses NFS as its filesystem backend. This means that if one node completes a job, it might take a while before the files that were created. In other workflows we can set an I/O timeout option: if the file did not appear within 3 minutes, the job failed. How do we do this in cromwell. All the settings I have available is `number-of-requests`, `per` and `number-of-attempts`. Currently we have; ```HOCON; io {; number-of-requests = 10; per = 10 seconds; number-of-attempts = 180; }; ```; This should make sure that failing files are attempted for 180 seconds, but this is not very elegant. There is a `timeout` option in I/O. But this is the timeout for the I/O operation to respond. If the file is not ""visible"" then the I/O operation will respond immediately, and the job will have failed. Is there a fix to this option in the current cromwell configuration?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648
https://github.com/broadinstitute/cromwell/issues/3648:782,Safety,timeout,timeout,782,"So far cromwell is working great on our cluster. Thanks a lot for this splendid effort. However there is one issue we run into that other people might run into as well. Our cluster uses NFS as its filesystem backend. This means that if one node completes a job, it might take a while before the files that were created. In other workflows we can set an I/O timeout option: if the file did not appear within 3 minutes, the job failed. How do we do this in cromwell. All the settings I have available is `number-of-requests`, `per` and `number-of-attempts`. Currently we have; ```HOCON; io {; number-of-requests = 10; per = 10 seconds; number-of-attempts = 180; }; ```; This should make sure that failing files are attempted for 180 seconds, but this is not very elegant. There is a `timeout` option in I/O. But this is the timeout for the I/O operation to respond. If the file is not ""visible"" then the I/O operation will respond immediately, and the job will have failed. Is there a fix to this option in the current cromwell configuration?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648
https://github.com/broadinstitute/cromwell/issues/3648:822,Safety,timeout,timeout,822,"So far cromwell is working great on our cluster. Thanks a lot for this splendid effort. However there is one issue we run into that other people might run into as well. Our cluster uses NFS as its filesystem backend. This means that if one node completes a job, it might take a while before the files that were created. In other workflows we can set an I/O timeout option: if the file did not appear within 3 minutes, the job failed. How do we do this in cromwell. All the settings I have available is `number-of-requests`, `per` and `number-of-attempts`. Currently we have; ```HOCON; io {; number-of-requests = 10; per = 10 seconds; number-of-attempts = 180; }; ```; This should make sure that failing files are attempted for 180 seconds, but this is not very elegant. There is a `timeout` option in I/O. But this is the timeout for the I/O operation to respond. If the file is not ""visible"" then the I/O operation will respond immediately, and the job will have failed. Is there a fix to this option in the current cromwell configuration?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648
https://github.com/broadinstitute/cromwell/pull/3649:23,Security,hash,hash,23,Re-pin the conformance hash and fix the most recently added CWL conformance test by creating a directory for Directories without a `path` or `location`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3649
https://github.com/broadinstitute/cromwell/pull/3649:76,Testability,test,test,76,Re-pin the conformance hash and fix the most recently added CWL conformance test by creating a directory for Directories without a `path` or `location`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3649
https://github.com/broadinstitute/cromwell/issues/3650:39,Availability,error,error,39,"The query endpoint responds with a 400 error when trying filter by ""On Hold"" status. curl -X GET ""https://[cromwell_url]/api/workflows/v1/query?status=On%20Hold"" -H ""accept: application/json"". 400; {; ""status"": ""fail"",; ""message"": ""Unrecognized status values: On Hold""; }. This makes it difficult to use On Hold for queuing because you can't find the On Hold workflows unless you query for everything and then filter the results by status on the client side, which is not very efficient.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3650
https://github.com/broadinstitute/cromwell/issues/3650:477,Energy Efficiency,efficient,efficient,477,"The query endpoint responds with a 400 error when trying filter by ""On Hold"" status. curl -X GET ""https://[cromwell_url]/api/workflows/v1/query?status=On%20Hold"" -H ""accept: application/json"". 400; {; ""status"": ""fail"",; ""message"": ""Unrecognized status values: On Hold""; }. This makes it difficult to use On Hold for queuing because you can't find the On Hold workflows unless you query for everything and then filter the results by status on the client side, which is not very efficient.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3650
https://github.com/broadinstitute/cromwell/issues/3650:221,Integrability,message,message,221,"The query endpoint responds with a 400 error when trying filter by ""On Hold"" status. curl -X GET ""https://[cromwell_url]/api/workflows/v1/query?status=On%20Hold"" -H ""accept: application/json"". 400; {; ""status"": ""fail"",; ""message"": ""Unrecognized status values: On Hold""; }. This makes it difficult to use On Hold for queuing because you can't find the On Hold workflows unless you query for everything and then filter the results by status on the client side, which is not very efficient.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3650
https://github.com/broadinstitute/cromwell/pull/3651:258,Deployability,configurat,configuration,258,"Requester pays is now added to Cromwell. The user can set the billing project id using one of the methods listed below:. - It can be added as `'google_project':'project-id'` as part of workflow options during workflow submission; - It can be included inside configuration file as shown in Getting started on Google Pipelines API where you need to replace the <google-billing-project-id> with the project id; - If it is not mentioned using above 2 ways, Cromwell will use the default project that has been configured with gcloud. More information about Requester pays can be found [here](https://cloud.google.com/storage/docs/requester-pays)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3651
https://github.com/broadinstitute/cromwell/pull/3651:315,Deployability,Pipeline,Pipelines,315,"Requester pays is now added to Cromwell. The user can set the billing project id using one of the methods listed below:. - It can be added as `'google_project':'project-id'` as part of workflow options during workflow submission; - It can be included inside configuration file as shown in Getting started on Google Pipelines API where you need to replace the <google-billing-project-id> with the project id; - If it is not mentioned using above 2 ways, Cromwell will use the default project that has been configured with gcloud. More information about Requester pays can be found [here](https://cloud.google.com/storage/docs/requester-pays)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3651
https://github.com/broadinstitute/cromwell/pull/3651:258,Modifiability,config,configuration,258,"Requester pays is now added to Cromwell. The user can set the billing project id using one of the methods listed below:. - It can be added as `'google_project':'project-id'` as part of workflow options during workflow submission; - It can be included inside configuration file as shown in Getting started on Google Pipelines API where you need to replace the <google-billing-project-id> with the project id; - If it is not mentioned using above 2 ways, Cromwell will use the default project that has been configured with gcloud. More information about Requester pays can be found [here](https://cloud.google.com/storage/docs/requester-pays)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3651
https://github.com/broadinstitute/cromwell/pull/3651:505,Modifiability,config,configured,505,"Requester pays is now added to Cromwell. The user can set the billing project id using one of the methods listed below:. - It can be added as `'google_project':'project-id'` as part of workflow options during workflow submission; - It can be included inside configuration file as shown in Getting started on Google Pipelines API where you need to replace the <google-billing-project-id> with the project id; - If it is not mentioned using above 2 ways, Cromwell will use the default project that has been configured with gcloud. More information about Requester pays can be found [here](https://cloud.google.com/storage/docs/requester-pays)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3651
https://github.com/broadinstitute/cromwell/pull/3653:169,Performance,cache,cache,169,"Hint: these are barely changed from the draft-2 versions. ; You probably only *need* to review the `.scala` changes, although obviously another pair of eyes on the call cache capoeira tests never hurt. I made a mistake - no red thumb is required now but a red thumb is of course always welcome...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3653
https://github.com/broadinstitute/cromwell/pull/3653:184,Testability,test,tests,184,"Hint: these are barely changed from the draft-2 versions. ; You probably only *need* to review the `.scala` changes, although obviously another pair of eyes on the call cache capoeira tests never hurt. I made a mistake - no red thumb is required now but a red thumb is of course always welcome...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3653
https://github.com/broadinstitute/cromwell/issues/3654:348,Availability,error,error,348,"A few workflows that we aborted in Cromwell-as-a-Service have the status ""Aborted"" (e.g. ""429e0aaf-c429-4438-a12d-734f1f444801"") but the subworkflow that was running when the parent workflow was aborted is still in the ""running"" status. When trying to abort the subworkflow that is still running (""34074359-f8ed-4402-ba65-c92ab550e999""), I see the error:. ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 34074359-f8ed-4402-ba65-c92ab550e999 because no workflow with that ID is in progress""; }; ```. It looks like Cromwell thinks the workflow is not running, but it's metadata says that it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654
https://github.com/broadinstitute/cromwell/issues/3654:375,Availability,error,error,375,"A few workflows that we aborted in Cromwell-as-a-Service have the status ""Aborted"" (e.g. ""429e0aaf-c429-4438-a12d-734f1f444801"") but the subworkflow that was running when the parent workflow was aborted is still in the ""running"" status. When trying to abort the subworkflow that is still running (""34074359-f8ed-4402-ba65-c92ab550e999""), I see the error:. ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 34074359-f8ed-4402-ba65-c92ab550e999 because no workflow with that ID is in progress""; }; ```. It looks like Cromwell thinks the workflow is not running, but it's metadata says that it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654
https://github.com/broadinstitute/cromwell/issues/3654:385,Integrability,message,message,385,"A few workflows that we aborted in Cromwell-as-a-Service have the status ""Aborted"" (e.g. ""429e0aaf-c429-4438-a12d-734f1f444801"") but the subworkflow that was running when the parent workflow was aborted is still in the ""running"" status. When trying to abort the subworkflow that is still running (""34074359-f8ed-4402-ba65-c92ab550e999""), I see the error:. ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 34074359-f8ed-4402-ba65-c92ab550e999 because no workflow with that ID is in progress""; }; ```. It looks like Cromwell thinks the workflow is not running, but it's metadata says that it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654
https://github.com/broadinstitute/cromwell/issues/3654:24,Safety,abort,aborted,24,"A few workflows that we aborted in Cromwell-as-a-Service have the status ""Aborted"" (e.g. ""429e0aaf-c429-4438-a12d-734f1f444801"") but the subworkflow that was running when the parent workflow was aborted is still in the ""running"" status. When trying to abort the subworkflow that is still running (""34074359-f8ed-4402-ba65-c92ab550e999""), I see the error:. ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 34074359-f8ed-4402-ba65-c92ab550e999 because no workflow with that ID is in progress""; }; ```. It looks like Cromwell thinks the workflow is not running, but it's metadata says that it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654
https://github.com/broadinstitute/cromwell/issues/3654:74,Safety,Abort,Aborted,74,"A few workflows that we aborted in Cromwell-as-a-Service have the status ""Aborted"" (e.g. ""429e0aaf-c429-4438-a12d-734f1f444801"") but the subworkflow that was running when the parent workflow was aborted is still in the ""running"" status. When trying to abort the subworkflow that is still running (""34074359-f8ed-4402-ba65-c92ab550e999""), I see the error:. ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 34074359-f8ed-4402-ba65-c92ab550e999 because no workflow with that ID is in progress""; }; ```. It looks like Cromwell thinks the workflow is not running, but it's metadata says that it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654
https://github.com/broadinstitute/cromwell/issues/3654:195,Safety,abort,aborted,195,"A few workflows that we aborted in Cromwell-as-a-Service have the status ""Aborted"" (e.g. ""429e0aaf-c429-4438-a12d-734f1f444801"") but the subworkflow that was running when the parent workflow was aborted is still in the ""running"" status. When trying to abort the subworkflow that is still running (""34074359-f8ed-4402-ba65-c92ab550e999""), I see the error:. ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 34074359-f8ed-4402-ba65-c92ab550e999 because no workflow with that ID is in progress""; }; ```. It looks like Cromwell thinks the workflow is not running, but it's metadata says that it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654
https://github.com/broadinstitute/cromwell/issues/3654:252,Safety,abort,abort,252,"A few workflows that we aborted in Cromwell-as-a-Service have the status ""Aborted"" (e.g. ""429e0aaf-c429-4438-a12d-734f1f444801"") but the subworkflow that was running when the parent workflow was aborted is still in the ""running"" status. When trying to abort the subworkflow that is still running (""34074359-f8ed-4402-ba65-c92ab550e999""), I see the error:. ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 34074359-f8ed-4402-ba65-c92ab550e999 because no workflow with that ID is in progress""; }; ```. It looks like Cromwell thinks the workflow is not running, but it's metadata says that it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654
https://github.com/broadinstitute/cromwell/issues/3654:405,Safety,abort,abort,405,"A few workflows that we aborted in Cromwell-as-a-Service have the status ""Aborted"" (e.g. ""429e0aaf-c429-4438-a12d-734f1f444801"") but the subworkflow that was running when the parent workflow was aborted is still in the ""running"" status. When trying to abort the subworkflow that is still running (""34074359-f8ed-4402-ba65-c92ab550e999""), I see the error:. ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 34074359-f8ed-4402-ba65-c92ab550e999 because no workflow with that ID is in progress""; }; ```. It looks like Cromwell thinks the workflow is not running, but it's metadata says that it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654
https://github.com/broadinstitute/cromwell/issues/3655:124,Deployability,update,updated,124,Seeing this a lot now: I clear out all my old Cromwell fat jars and do an `sbt assembly`. Whichever jars did not need to be updated will claim to be up-to-date but actually they no longer exist. I'm hacking around this by making false edits to common files but this is annoying. ```; computer: me$ find . -name '*.jar' | xargs rm; computer: me$ sbt assembly; ...; [info] Assembly up to date: /Users/me/gitrepos/cromwell/server/target/scala-2.12/cromwell-32-b8d3d2f-SNAP.jar ; computer: me$ find . -name '*.jar'; computer: me$ # nothing; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3655
https://github.com/broadinstitute/cromwell/issues/3655:25,Usability,clear,clear,25,Seeing this a lot now: I clear out all my old Cromwell fat jars and do an `sbt assembly`. Whichever jars did not need to be updated will claim to be up-to-date but actually they no longer exist. I'm hacking around this by making false edits to common files but this is annoying. ```; computer: me$ find . -name '*.jar' | xargs rm; computer: me$ sbt assembly; ...; [info] Assembly up to date: /Users/me/gitrepos/cromwell/server/target/scala-2.12/cromwell-32-b8d3d2f-SNAP.jar ; computer: me$ find . -name '*.jar'; computer: me$ # nothing; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3655
https://github.com/broadinstitute/cromwell/issues/3656:395,Modifiability,variab,variable,395,"Cromwell Appears to be setting the HOME Override in a really really wonky way that completely breaks several tools. Additionally the initial `$HOME` value is being set to `/root` as opposed to the `/cromwell_root`. I am not sure if that is by design or if that is a slight oversight. This bug however effectually renders GenomicsDBImport useless unless the user once again overrides the `$HOME` variable. the offending line in the `exec.sh` looks like the following:; ```bash; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME"". ```. IN previous versions of cromwell (v29) this Home override did not exist. ```bash; tmpDir=$(mktemp -d /cromwell_root/tmp.XXXXXX); chmod 777 $tmpDir; export _JAVA_OPTIONS=-Djava.io.tmpdir=$tmpDir; export TMPDIR=$tmpDir; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3656
https://github.com/broadinstitute/cromwell/issues/3657:154,Availability,error,error,154,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:663,Availability,error,error,663,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2041,Availability,error,error,2041,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2558,Availability,Error,Errors,2558,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:46,Deployability,configurat,configuration,46,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:976,Deployability,configurat,configuration,976,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1540,Deployability,configurat,configuration,1540," activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#wo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1719,Deployability,configurat,configuration,1719,"r and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1862,Deployability,configurat,configurations,1862,"indings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/bl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2141,Deployability,Update,Update,2141,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2229,Deployability,Update,Update,2229,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2256,Deployability,configurat,configuration,2256,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2347,Deployability,configurat,configuration,2347,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:700,Energy Efficiency,monitor,monitor,700,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:160,Integrability,message,message,160,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:182,Integrability,message,message,182,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:330,Integrability,message,message,330,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1609,Integrability,depend,dependency,1609,"io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:46,Modifiability,config,configuration,46,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:92,Modifiability,config,configs,92,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:546,Modifiability,config,configured,546,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:976,Modifiability,config,configuration,976,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1215,Modifiability,Config,Configuring,1215," message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1328,Modifiability,Config,Config,1328,"leWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1444,Modifiability,variab,variable,1444," [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1540,Modifiability,config,configuration,1540," activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#wo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1719,Modifiability,config,configuration,1719,"r and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1862,Modifiability,config,configurations,1862,"indings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/bl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2209,Modifiability,Config,Configuring,2209,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2256,Modifiability,config,configuration,2256,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2347,Modifiability,config,configuration,2347,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2418,Modifiability,Config,Configuring,2418,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2789,Modifiability,config,config,2789,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:213,Testability,log,logback,213,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:498,Testability,log,logback,498,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:523,Testability,log,logback,523,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:810,Testability,log,logback,810,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:832,Testability,log,logback,832,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:882,Testability,log,logback,882,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1105,Testability,Log,Logging,1105,"of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1271,Testability,log,logback,1271,"leWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1516,Testability,log,logback,1516," activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#wo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1600,Testability,log,logback,1600,"io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:1633,Testability,log,logback,1633,"io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2187,Testability,Log,Logging,2187,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2440,Testability,log,log-directory,2440,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2499,Testability,Log,Logging,2499,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2517,Testability,log,logs,2517,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2741,Testability,log,logback,2741,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3657:2887,Testability,log,logback,2887,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657
https://github.com/broadinstitute/cromwell/issues/3658:322,Availability,error,error,322,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:1041,Availability,Error,Errors,1041," builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:1159,Availability,error,error,1159,"t will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:1680,Availability,error,error,1680,"sts. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be tr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:1902,Availability,error,error,1902,"aybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch test",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2014,Availability,error,errors,2014," Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2117,Availability,error,error,2117," events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:3126,Availability,error,error,3126,"y be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2533,Deployability,Update,Update,2533,"ry twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:3173,Deployability,configurat,configuration,3173,"y be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:3206,Deployability,Update,Update,3206,"y be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:3348,Deployability,configurat,configuration,3348,"e tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestCanceled.html; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/resources/org/scalatest/ScalaTestBundle.properties#L664; - https://github.com/scalatest/scalatest/b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:3494,Deployability,configurat,configuration,3494," if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestCanceled.html; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/resources/org/scalatest/ScalaTestBundle.properties#L664; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/scala/org/scalatest/Retries.scala#L565-L577; - https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2578,Integrability,depend,dependsOn,2578,"stCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON li",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2654,Integrability,depend,dependencies,2654,"nal error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - ht",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2239,Modifiability,refactor,refactored,2239,"ows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:3004,Modifiability,config,configurable,3004,"eton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:3173,Modifiability,config,configuration,3173,"y be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:3348,Modifiability,config,configuration,3348,"e tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestCanceled.html; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/resources/org/scalatest/ScalaTestBundle.properties#L664; - https://github.com/scalatest/scalatest/b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:3494,Modifiability,config,configuration,3494," if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestCanceled.html; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/resources/org/scalatest/ScalaTestBundle.properties#L664; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/scala/org/scalatest/Retries.scala#L565-L577; - https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:1967,Performance,cache,cache,1967,"aybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch test",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:3876,Performance,concurren,concurrent,3876,"roject in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestCanceled.html; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/resources/org/scalatest/ScalaTestBundle.properties#L664; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/scala/org/scalatest/Retries.scala#L565-L577; - https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/project/Testing.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:53,Testability,test,test,53,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:94,Testability,test,test,94,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:139,Testability,test,test,139,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:217,Testability,test,test,217,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:261,Testability,test,test,261,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:401,Testability,test,tests,401,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:449,Testability,test,tests,449,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:549,Testability,test,tests,549,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:589,Testability,test,tests,589,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:703,Testability,test,tests,703,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:748,Testability,test,test,748,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:792,Testability,test,tests,792,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:910,Testability,Test,Test,910,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:1095,Testability,Test,TestCanceled,1095,"t will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:1268,Testability,Test,TestFailed,1268,"ntire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:1284,Testability,Test,TestCanceled,1284,"ntire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:1379,Testability,test,test,1379," to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:1594,Testability,Test,TestCanceled,1594,"`Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:1876,Testability,test,test,1876,"aybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch test",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2044,Testability,log,logit,2044," Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2197,Testability,test,tests,2197,"ows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2303,Testability,test,testing,2303,"ows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2348,Testability,test,tests,2348,"he failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2597,Testability,test,test,2597,"stCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON li",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2603,Testability,test,test,2603,"stCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON li",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2763,Testability,test,test,2763,"withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2855,Testability,test,tests,2855,"eton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:2997,Testability,test,test,2997,"eton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:3082,Testability,Test,Testing,3082,"eton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:4096,Testability,Test,TestFailed,4096,"roject in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestCanceled.html; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/resources/org/scalatest/ScalaTestBundle.properties#L664; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/scala/org/scalatest/Retries.scala#L565-L577; - https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/project/Testing.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:4172,Testability,Test,TestCanceled,4172,"roject in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestCanceled.html; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/resources/org/scalatest/ScalaTestBundle.properties#L664; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/scala/org/scalatest/Retries.scala#L565-L577; - https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/project/Testing.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/issues/3658:4534,Testability,Test,Testing,4534,"roject in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestCanceled.html; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/resources/org/scalatest/ScalaTestBundle.properties#L664; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/scala/org/scalatest/Retries.scala#L565-L577; - https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/project/Testing.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658
https://github.com/broadinstitute/cromwell/pull/3660:31,Testability,test,test,31,"Allows the requester pays glob test to work in WDL 1.0 by supporting a `read_string` pattern like this:; ```wdl; output {; Array[File] globArray = glob(""*.txt""); String globArrayContent = read_string(globArray[0]); }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3660
https://github.com/broadinstitute/cromwell/pull/3661:692,Deployability,update,updates,692,"Give Centaur-managed Cromwell more time to restart and a custom exit code.; Publish artifacts again on each build tag.; Login to docker before trying to push images.; Functions using secure variables ensure that xtrace is not enabled, thus no longer need a subshell, thus do not need to be exported.; Artifactory and Docker Hub credentials added to vault.; Docker login can use environment variables or vault once dsde-toolbox is public.; Split setup_secure_environment into setup_common_environment and setup_secure_resources.; Sbt environment variables prefixed as CROMWELL_SBT_*.; Print out a warning instead of exiting when vault resources cannot be rendered when testing locally.; Minor updates for more consistent shell variable usage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3661
https://github.com/broadinstitute/cromwell/pull/3661:190,Modifiability,variab,variables,190,"Give Centaur-managed Cromwell more time to restart and a custom exit code.; Publish artifacts again on each build tag.; Login to docker before trying to push images.; Functions using secure variables ensure that xtrace is not enabled, thus no longer need a subshell, thus do not need to be exported.; Artifactory and Docker Hub credentials added to vault.; Docker login can use environment variables or vault once dsde-toolbox is public.; Split setup_secure_environment into setup_common_environment and setup_secure_resources.; Sbt environment variables prefixed as CROMWELL_SBT_*.; Print out a warning instead of exiting when vault resources cannot be rendered when testing locally.; Minor updates for more consistent shell variable usage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3661
https://github.com/broadinstitute/cromwell/pull/3661:390,Modifiability,variab,variables,390,"Give Centaur-managed Cromwell more time to restart and a custom exit code.; Publish artifacts again on each build tag.; Login to docker before trying to push images.; Functions using secure variables ensure that xtrace is not enabled, thus no longer need a subshell, thus do not need to be exported.; Artifactory and Docker Hub credentials added to vault.; Docker login can use environment variables or vault once dsde-toolbox is public.; Split setup_secure_environment into setup_common_environment and setup_secure_resources.; Sbt environment variables prefixed as CROMWELL_SBT_*.; Print out a warning instead of exiting when vault resources cannot be rendered when testing locally.; Minor updates for more consistent shell variable usage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3661
https://github.com/broadinstitute/cromwell/pull/3661:545,Modifiability,variab,variables,545,"Give Centaur-managed Cromwell more time to restart and a custom exit code.; Publish artifacts again on each build tag.; Login to docker before trying to push images.; Functions using secure variables ensure that xtrace is not enabled, thus no longer need a subshell, thus do not need to be exported.; Artifactory and Docker Hub credentials added to vault.; Docker login can use environment variables or vault once dsde-toolbox is public.; Split setup_secure_environment into setup_common_environment and setup_secure_resources.; Sbt environment variables prefixed as CROMWELL_SBT_*.; Print out a warning instead of exiting when vault resources cannot be rendered when testing locally.; Minor updates for more consistent shell variable usage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3661
https://github.com/broadinstitute/cromwell/pull/3661:726,Modifiability,variab,variable,726,"Give Centaur-managed Cromwell more time to restart and a custom exit code.; Publish artifacts again on each build tag.; Login to docker before trying to push images.; Functions using secure variables ensure that xtrace is not enabled, thus no longer need a subshell, thus do not need to be exported.; Artifactory and Docker Hub credentials added to vault.; Docker login can use environment variables or vault once dsde-toolbox is public.; Split setup_secure_environment into setup_common_environment and setup_secure_resources.; Sbt environment variables prefixed as CROMWELL_SBT_*.; Print out a warning instead of exiting when vault resources cannot be rendered when testing locally.; Minor updates for more consistent shell variable usage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3661
https://github.com/broadinstitute/cromwell/pull/3661:183,Security,secur,secure,183,"Give Centaur-managed Cromwell more time to restart and a custom exit code.; Publish artifacts again on each build tag.; Login to docker before trying to push images.; Functions using secure variables ensure that xtrace is not enabled, thus no longer need a subshell, thus do not need to be exported.; Artifactory and Docker Hub credentials added to vault.; Docker login can use environment variables or vault once dsde-toolbox is public.; Split setup_secure_environment into setup_common_environment and setup_secure_resources.; Sbt environment variables prefixed as CROMWELL_SBT_*.; Print out a warning instead of exiting when vault resources cannot be rendered when testing locally.; Minor updates for more consistent shell variable usage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3661
https://github.com/broadinstitute/cromwell/pull/3661:120,Testability,Log,Login,120,"Give Centaur-managed Cromwell more time to restart and a custom exit code.; Publish artifacts again on each build tag.; Login to docker before trying to push images.; Functions using secure variables ensure that xtrace is not enabled, thus no longer need a subshell, thus do not need to be exported.; Artifactory and Docker Hub credentials added to vault.; Docker login can use environment variables or vault once dsde-toolbox is public.; Split setup_secure_environment into setup_common_environment and setup_secure_resources.; Sbt environment variables prefixed as CROMWELL_SBT_*.; Print out a warning instead of exiting when vault resources cannot be rendered when testing locally.; Minor updates for more consistent shell variable usage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3661
https://github.com/broadinstitute/cromwell/pull/3661:364,Testability,log,login,364,"Give Centaur-managed Cromwell more time to restart and a custom exit code.; Publish artifacts again on each build tag.; Login to docker before trying to push images.; Functions using secure variables ensure that xtrace is not enabled, thus no longer need a subshell, thus do not need to be exported.; Artifactory and Docker Hub credentials added to vault.; Docker login can use environment variables or vault once dsde-toolbox is public.; Split setup_secure_environment into setup_common_environment and setup_secure_resources.; Sbt environment variables prefixed as CROMWELL_SBT_*.; Print out a warning instead of exiting when vault resources cannot be rendered when testing locally.; Minor updates for more consistent shell variable usage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3661
https://github.com/broadinstitute/cromwell/pull/3661:668,Testability,test,testing,668,"Give Centaur-managed Cromwell more time to restart and a custom exit code.; Publish artifacts again on each build tag.; Login to docker before trying to push images.; Functions using secure variables ensure that xtrace is not enabled, thus no longer need a subshell, thus do not need to be exported.; Artifactory and Docker Hub credentials added to vault.; Docker login can use environment variables or vault once dsde-toolbox is public.; Split setup_secure_environment into setup_common_environment and setup_secure_resources.; Sbt environment variables prefixed as CROMWELL_SBT_*.; Print out a warning instead of exiting when vault resources cannot be rendered when testing locally.; Minor updates for more consistent shell variable usage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3661
https://github.com/broadinstitute/cromwell/pull/3662:55,Availability,fault,faulty,55,"Having default runtime attributes in jes config caused faulty WARN messages about ""Unrecognized configuration key(s) for Jes"". This PR should fix those.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3662
https://github.com/broadinstitute/cromwell/pull/3662:96,Deployability,configurat,configuration,96,"Having default runtime attributes in jes config caused faulty WARN messages about ""Unrecognized configuration key(s) for Jes"". This PR should fix those.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3662
https://github.com/broadinstitute/cromwell/pull/3662:67,Integrability,message,messages,67,"Having default runtime attributes in jes config caused faulty WARN messages about ""Unrecognized configuration key(s) for Jes"". This PR should fix those.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3662
https://github.com/broadinstitute/cromwell/pull/3662:41,Modifiability,config,config,41,"Having default runtime attributes in jes config caused faulty WARN messages about ""Unrecognized configuration key(s) for Jes"". This PR should fix those.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3662
https://github.com/broadinstitute/cromwell/pull/3662:96,Modifiability,config,configuration,96,"Having default runtime attributes in jes config caused faulty WARN messages about ""Unrecognized configuration key(s) for Jes"". This PR should fix those.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3662
https://github.com/broadinstitute/cromwell/issues/3665:86,Availability,avail,available,86,"See linked epic for related postmortem. * happens If # of VMs > maximum number of IPs available; * throttling of API calls to GCE prevents destruction of finished VMs (DB: t sure ; * Finished VMs are holding IP addresses, preventing new calls from obtaining them. # IP Exhaustion. * New networks are in /20 CIDR block, allowing 2^12 = 4096 IP addresses; * PAPI v1 is limited to default network; * PAPI v2 can specify network per project (TODO: confirm per project? per call?); * PAPI v2 non-default networks can use /16 and thus 65K IP addresses. # Context. * Can only occur when quota increase is requested to put max # cpus > available IPs. # Mechanics. * Pass in network name as workflow option from rawls -> cromwell. # Questions. * What caused the API throttling in the first place? We allocated too many VM's and just generally sent too much traffic to GCE?; * Does PAPI v2 address the IP exhaustion situation? Otherwise we have to manage the resources for it.; * Migration of old projects to use PAPI v2 ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3665
https://github.com/broadinstitute/cromwell/issues/3665:628,Availability,avail,available,628,"See linked epic for related postmortem. * happens If # of VMs > maximum number of IPs available; * throttling of API calls to GCE prevents destruction of finished VMs (DB: t sure ; * Finished VMs are holding IP addresses, preventing new calls from obtaining them. # IP Exhaustion. * New networks are in /20 CIDR block, allowing 2^12 = 4096 IP addresses; * PAPI v1 is limited to default network; * PAPI v2 can specify network per project (TODO: confirm per project? per call?); * PAPI v2 non-default networks can use /16 and thus 65K IP addresses. # Context. * Can only occur when quota increase is requested to put max # cpus > available IPs. # Mechanics. * Pass in network name as workflow option from rawls -> cromwell. # Questions. * What caused the API throttling in the first place? We allocated too many VM's and just generally sent too much traffic to GCE?; * Does PAPI v2 address the IP exhaustion situation? Otherwise we have to manage the resources for it.; * Migration of old projects to use PAPI v2 ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3665
https://github.com/broadinstitute/cromwell/issues/3665:791,Energy Efficiency,allocate,allocated,791,"See linked epic for related postmortem. * happens If # of VMs > maximum number of IPs available; * throttling of API calls to GCE prevents destruction of finished VMs (DB: t sure ; * Finished VMs are holding IP addresses, preventing new calls from obtaining them. # IP Exhaustion. * New networks are in /20 CIDR block, allowing 2^12 = 4096 IP addresses; * PAPI v1 is limited to default network; * PAPI v2 can specify network per project (TODO: confirm per project? per call?); * PAPI v2 non-default networks can use /16 and thus 65K IP addresses. # Context. * Can only occur when quota increase is requested to put max # cpus > available IPs. # Mechanics. * Pass in network name as workflow option from rawls -> cromwell. # Questions. * What caused the API throttling in the first place? We allocated too many VM's and just generally sent too much traffic to GCE?; * Does PAPI v2 address the IP exhaustion situation? Otherwise we have to manage the resources for it.; * Migration of old projects to use PAPI v2 ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3665
https://github.com/broadinstitute/cromwell/issues/3666:351,Availability,down,down,351,"# Thursday. 1. Jobs ""queued in cromwell""; 2. Doug started job, 8 hours to start; 3. Graphite under-reporting. # Friday. 1. No improvement; 2. User1 suggests his own wf running w/ 25k jobs; 3. disk quota hit; 4. **No limit on # jobs running / project**; 4. Console quotas page revealed this; 5. User1 aborts wf; 6. 10 minutes of improvement, then back down to low throughput; 7. **Difficult to understand who is running what**; 8. We found a crude query to ask ^; 9. Bubbled up sub-wf's manually; 10. Found User2 running a large job; 11. User had upped quotas on # cpus, persistent disk and IP's; 12. **Exhausted IP quota, PAPI throttles itself as a result of too many API calls**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3666
https://github.com/broadinstitute/cromwell/issues/3666:21,Performance,queue,queued,21,"# Thursday. 1. Jobs ""queued in cromwell""; 2. Doug started job, 8 hours to start; 3. Graphite under-reporting. # Friday. 1. No improvement; 2. User1 suggests his own wf running w/ 25k jobs; 3. disk quota hit; 4. **No limit on # jobs running / project**; 4. Console quotas page revealed this; 5. User1 aborts wf; 6. 10 minutes of improvement, then back down to low throughput; 7. **Difficult to understand who is running what**; 8. We found a crude query to ask ^; 9. Bubbled up sub-wf's manually; 10. Found User2 running a large job; 11. User had upped quotas on # cpus, persistent disk and IP's; 12. **Exhausted IP quota, PAPI throttles itself as a result of too many API calls**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3666
https://github.com/broadinstitute/cromwell/issues/3666:363,Performance,throughput,throughput,363,"# Thursday. 1. Jobs ""queued in cromwell""; 2. Doug started job, 8 hours to start; 3. Graphite under-reporting. # Friday. 1. No improvement; 2. User1 suggests his own wf running w/ 25k jobs; 3. disk quota hit; 4. **No limit on # jobs running / project**; 4. Console quotas page revealed this; 5. User1 aborts wf; 6. 10 minutes of improvement, then back down to low throughput; 7. **Difficult to understand who is running what**; 8. We found a crude query to ask ^; 9. Bubbled up sub-wf's manually; 10. Found User2 running a large job; 11. User had upped quotas on # cpus, persistent disk and IP's; 12. **Exhausted IP quota, PAPI throttles itself as a result of too many API calls**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3666
https://github.com/broadinstitute/cromwell/issues/3666:627,Performance,throttle,throttles,627,"# Thursday. 1. Jobs ""queued in cromwell""; 2. Doug started job, 8 hours to start; 3. Graphite under-reporting. # Friday. 1. No improvement; 2. User1 suggests his own wf running w/ 25k jobs; 3. disk quota hit; 4. **No limit on # jobs running / project**; 4. Console quotas page revealed this; 5. User1 aborts wf; 6. 10 minutes of improvement, then back down to low throughput; 7. **Difficult to understand who is running what**; 8. We found a crude query to ask ^; 9. Bubbled up sub-wf's manually; 10. Found User2 running a large job; 11. User had upped quotas on # cpus, persistent disk and IP's; 12. **Exhausted IP quota, PAPI throttles itself as a result of too many API calls**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3666
https://github.com/broadinstitute/cromwell/issues/3666:300,Safety,abort,aborts,300,"# Thursday. 1. Jobs ""queued in cromwell""; 2. Doug started job, 8 hours to start; 3. Graphite under-reporting. # Friday. 1. No improvement; 2. User1 suggests his own wf running w/ 25k jobs; 3. disk quota hit; 4. **No limit on # jobs running / project**; 4. Console quotas page revealed this; 5. User1 aborts wf; 6. 10 minutes of improvement, then back down to low throughput; 7. **Difficult to understand who is running what**; 8. We found a crude query to ask ^; 9. Bubbled up sub-wf's manually; 10. Found User2 running a large job; 11. User had upped quotas on # cpus, persistent disk and IP's; 12. **Exhausted IP quota, PAPI throttles itself as a result of too many API calls**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3666
https://github.com/broadinstitute/cromwell/pull/3668:419,Availability,error,error,419,"Issues addressed:. - Custom-named stdout or stderr files were not being delocalized.; - The wrong stdout and stderr files were being delocalized (user action instead of user command).; - Optional output files that were not produced were being published to metadata as if they had been produced.; - Existence check for optional files was a `-a` test not actually supported by Alpine's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package.; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3668
https://github.com/broadinstitute/cromwell/pull/3668:344,Testability,test,test,344,"Issues addressed:. - Custom-named stdout or stderr files were not being delocalized.; - The wrong stdout and stderr files were being delocalized (user action instead of user command).; - Optional output files that were not produced were being published to metadata as if they had been produced.; - Existence check for optional files was a `-a` test not actually supported by Alpine's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package.; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3668
https://github.com/broadinstitute/cromwell/pull/3668:777,Testability,test,test,777,"Issues addressed:. - Custom-named stdout or stderr files were not being delocalized.; - The wrong stdout and stderr files were being delocalized (user action instead of user command).; - Optional output files that were not produced were being published to metadata as if they had been produced.; - Existence check for optional files was a `-a` test not actually supported by Alpine's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package.; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3668
https://github.com/broadinstitute/cromwell/pull/3668:867,Testability,test,test,867,"Issues addressed:. - Custom-named stdout or stderr files were not being delocalized.; - The wrong stdout and stderr files were being delocalized (user action instead of user command).; - Optional output files that were not produced were being published to metadata as if they had been produced.; - Existence check for optional files was a `-a` test not actually supported by Alpine's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package.; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3668
https://github.com/broadinstitute/cromwell/pull/3669:161,Deployability,configurat,configuration,161,@danbills I ended up using the global conf in the interest of time.; I'm sure there's a better way to do it (@cjllanwarne suggested through the language factory configuration which seems a good idea). This is what the original ticket actually intended: https://github.com/broadinstitute/cromwell/issues/2611,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3669
https://github.com/broadinstitute/cromwell/pull/3669:161,Modifiability,config,configuration,161,@danbills I ended up using the global conf in the interest of time.; I'm sure there's a better way to do it (@cjllanwarne suggested through the language factory configuration which seems a good idea). This is what the original ticket actually intended: https://github.com/broadinstitute/cromwell/issues/2611,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3669
https://github.com/broadinstitute/cromwell/issues/3672:118,Integrability,interface,interface,118,The WES to Cromwell Shim (wes2cromwell) is being built to demonstrate use of Cromwell through the GA4GH-specified WES interface. The shim is an independent executable that provides the WES REST API and calls the Cromwell REST API to perform the workflow operations. WES client ⇒ wes2cromwell ⇒ Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3672
https://github.com/broadinstitute/cromwell/issues/3672:233,Performance,perform,perform,233,The WES to Cromwell Shim (wes2cromwell) is being built to demonstrate use of Cromwell through the GA4GH-specified WES interface. The shim is an independent executable that provides the WES REST API and calls the Cromwell REST API to perform the workflow operations. WES client ⇒ wes2cromwell ⇒ Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3672
https://github.com/broadinstitute/cromwell/issues/3673:2041,Availability,error,error,2041,"Runnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I see 13 of these:; ```; 024bf23f-b7a3-4ede-bddf-938321ac570f; 26707981-d32b-4814-ba1f-4e5f27f739dc; 52fe6d61-2ba8-4c79-8a50-e365b355e36b; 5cbeca9f-c686-45a9-ab57-167379029964; 627a48a3-1584-42de-9b57-ee7a859b08d1; 6ed1070c-e478-47f2-8ea9-7ccc656bbba9; 70786146-ac4e-4d26-9906-ba211fde03f9; 8de76a93-6b66-4c29-a2fe-31e6cd1f969e; 8f07ade2-0a6d-40df-b886-cf99e3a1ed13; 9bda3e3d-1e17-4406-87e3-9ec7f71f4822; cb4b3331-193a-4c22-a95d-40f1ac9b53d6; dc9ded6f-463f-4cb1-a71a-0503c53f702a; f6644044-f4af-412a-a978-12ff080af3e1; ```; (edited). mcovarr [4:51 PM]; yeah that's from the 2/3 of horizontal Cromwell we implemented. mcovarr [4:52 PM]; but not sure why this is happening in this specific case. kshaki",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:2171,Availability,heartbeat,heartbeats,2171,"ngExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I see 13 of these:; ```; 024bf23f-b7a3-4ede-bddf-938321ac570f; 26707981-d32b-4814-ba1f-4e5f27f739dc; 52fe6d61-2ba8-4c79-8a50-e365b355e36b; 5cbeca9f-c686-45a9-ab57-167379029964; 627a48a3-1584-42de-9b57-ee7a859b08d1; 6ed1070c-e478-47f2-8ea9-7ccc656bbba9; 70786146-ac4e-4d26-9906-ba211fde03f9; 8de76a93-6b66-4c29-a2fe-31e6cd1f969e; 8f07ade2-0a6d-40df-b886-cf99e3a1ed13; 9bda3e3d-1e17-4406-87e3-9ec7f71f4822; cb4b3331-193a-4c22-a95d-40f1ac9b53d6; dc9ded6f-463f-4cb1-a71a-0503c53f702a; f6644044-f4af-412a-a978-12ff080af3e1; ```; (edited). mcovarr [4:51 PM]; yeah that's from the 2/3 of horizontal Cromwell we implemented. mcovarr [4:52 PM]; but not sure why this is happening in this specific case. kshakir [4:52 PM]; How does :heartbeat:-ing work? There was a restart event in the middle of these workflows.; i think. mcovarr [4:55 PM]; timer in t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:3071,Availability,heartbeat,heartbeat,3071,"kQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I see 13 of these:; ```; 024bf23f-b7a3-4ede-bddf-938321ac570f; 26707981-d32b-4814-ba1f-4e5f27f739dc; 52fe6d61-2ba8-4c79-8a50-e365b355e36b; 5cbeca9f-c686-45a9-ab57-167379029964; 627a48a3-1584-42de-9b57-ee7a859b08d1; 6ed1070c-e478-47f2-8ea9-7ccc656bbba9; 70786146-ac4e-4d26-9906-ba211fde03f9; 8de76a93-6b66-4c29-a2fe-31e6cd1f969e; 8f07ade2-0a6d-40df-b886-cf99e3a1ed13; 9bda3e3d-1e17-4406-87e3-9ec7f71f4822; cb4b3331-193a-4c22-a95d-40f1ac9b53d6; dc9ded6f-463f-4cb1-a71a-0503c53f702a; f6644044-f4af-412a-a978-12ff080af3e1; ```; (edited). mcovarr [4:51 PM]; yeah that's from the 2/3 of horizontal Cromwell we implemented. mcovarr [4:52 PM]; but not sure why this is happening in this specific case. kshakir [4:52 PM]; How does :heartbeat:-ing work? There was a restart event in the middle of these workflows.; i think. mcovarr [4:55 PM]; timer in the WorkflowExecutionActor. kshakir [4:59 PM]; uploaded and commented on this file ; caas_cromwell_logs.txt; 6 MB Plain Text; Scratch the-restart-in-the-middle idea. I think they might have been submitted after the last restart. Gotta take off for now but here's the log dump for chatting at standup tomorrow. mcovarr [5:02 PM]; that looks odd, it didn't actually restart anything; how was this shut down?. kshakir [5:05 PM]; I assume caas shutdowns are _not_ clean. mcovarr [5:06 PM]; Just curious, I *thought* Cromwell always put the workflow store into a sane state when it started up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:3590,Availability,down,down,3590,"kQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I see 13 of these:; ```; 024bf23f-b7a3-4ede-bddf-938321ac570f; 26707981-d32b-4814-ba1f-4e5f27f739dc; 52fe6d61-2ba8-4c79-8a50-e365b355e36b; 5cbeca9f-c686-45a9-ab57-167379029964; 627a48a3-1584-42de-9b57-ee7a859b08d1; 6ed1070c-e478-47f2-8ea9-7ccc656bbba9; 70786146-ac4e-4d26-9906-ba211fde03f9; 8de76a93-6b66-4c29-a2fe-31e6cd1f969e; 8f07ade2-0a6d-40df-b886-cf99e3a1ed13; 9bda3e3d-1e17-4406-87e3-9ec7f71f4822; cb4b3331-193a-4c22-a95d-40f1ac9b53d6; dc9ded6f-463f-4cb1-a71a-0503c53f702a; f6644044-f4af-412a-a978-12ff080af3e1; ```; (edited). mcovarr [4:51 PM]; yeah that's from the 2/3 of horizontal Cromwell we implemented. mcovarr [4:52 PM]; but not sure why this is happening in this specific case. kshakir [4:52 PM]; How does :heartbeat:-ing work? There was a restart event in the middle of these workflows.; i think. mcovarr [4:55 PM]; timer in the WorkflowExecutionActor. kshakir [4:59 PM]; uploaded and commented on this file ; caas_cromwell_logs.txt; 6 MB Plain Text; Scratch the-restart-in-the-middle idea. I think they might have been submitted after the last restart. Gotta take off for now but here's the log dump for chatting at standup tomorrow. mcovarr [5:02 PM]; that looks odd, it didn't actually restart anything; how was this shut down?. kshakir [5:05 PM]; I assume caas shutdowns are _not_ clean. mcovarr [5:06 PM]; Just curious, I *thought* Cromwell always put the workflow store into a sane state when it started up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:2047,Integrability,message,message,2047,"Runnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I see 13 of these:; ```; 024bf23f-b7a3-4ede-bddf-938321ac570f; 26707981-d32b-4814-ba1f-4e5f27f739dc; 52fe6d61-2ba8-4c79-8a50-e365b355e36b; 5cbeca9f-c686-45a9-ab57-167379029964; 627a48a3-1584-42de-9b57-ee7a859b08d1; 6ed1070c-e478-47f2-8ea9-7ccc656bbba9; 70786146-ac4e-4d26-9906-ba211fde03f9; 8de76a93-6b66-4c29-a2fe-31e6cd1f969e; 8f07ade2-0a6d-40df-b886-cf99e3a1ed13; 9bda3e3d-1e17-4406-87e3-9ec7f71f4822; cb4b3331-193a-4c22-a95d-40f1ac9b53d6; dc9ded6f-463f-4cb1-a71a-0503c53f702a; f6644044-f4af-412a-a978-12ff080af3e1; ```; (edited). mcovarr [4:51 PM]; yeah that's from the 2/3 of horizontal Cromwell we implemented. mcovarr [4:52 PM]; but not sure why this is happening in this specific case. kshaki",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:2265,Integrability,message,message,2265,"ngExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I see 13 of these:; ```; 024bf23f-b7a3-4ede-bddf-938321ac570f; 26707981-d32b-4814-ba1f-4e5f27f739dc; 52fe6d61-2ba8-4c79-8a50-e365b355e36b; 5cbeca9f-c686-45a9-ab57-167379029964; 627a48a3-1584-42de-9b57-ee7a859b08d1; 6ed1070c-e478-47f2-8ea9-7ccc656bbba9; 70786146-ac4e-4d26-9906-ba211fde03f9; 8de76a93-6b66-4c29-a2fe-31e6cd1f969e; 8f07ade2-0a6d-40df-b886-cf99e3a1ed13; 9bda3e3d-1e17-4406-87e3-9ec7f71f4822; cb4b3331-193a-4c22-a95d-40f1ac9b53d6; dc9ded6f-463f-4cb1-a71a-0503c53f702a; f6644044-f4af-412a-a978-12ff080af3e1; ```; (edited). mcovarr [4:51 PM]; yeah that's from the 2/3 of horizontal Cromwell we implemented. mcovarr [4:52 PM]; but not sure why this is happening in this specific case. kshakir [4:52 PM]; How does :heartbeat:-ing work? There was a restart event in the middle of these workflows.; i think. mcovarr [4:55 PM]; timer in t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:823,Performance,concurren,concurrent,823,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:884,Performance,concurren,concurrent,884,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:950,Performance,concurren,concurrent,950,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:1023,Performance,concurren,concurrent,1023,". dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:1344,Performance,concurren,concurrent,1344," and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:435,Security,validat,validation,435,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:446,Security,Validat,Validation,446,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:457,Security,Validat,ValidationTry,457,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:489,Security,Validat,Validation,489,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:521,Security,validat,validation,521,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:532,Security,Validat,Validation,532,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:543,Security,Validat,ValidationTry,543,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:575,Security,Validat,Validation,575,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:13,Testability,log,log,13,"Copying chat log below:. ---. dshiga [4:46 PM]; hi @kshakir, we're seeing a lot of workflows that have been stuck in submitted status for the past ~45 minutes. is something wrong in caas?; for example this workflow is stuck: 8de76a93-6b66-4c29-a2fe-31e6cd1f969e. kshakir [4:48 PM]; ```; Workflow 8de76a93-6b66-4c29-a2fe-31e6cd1f969e in state Running and restarted = false cannot be started and should not have been fetched.; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.engine.workflow.workflowstore.SqlWorkflowStore.$anonfun$fetchStartableWorkflows$1(SqlWorkflowStore.scala:57); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3673:3457,Testability,log,log,3457,"kQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I see 13 of these:; ```; 024bf23f-b7a3-4ede-bddf-938321ac570f; 26707981-d32b-4814-ba1f-4e5f27f739dc; 52fe6d61-2ba8-4c79-8a50-e365b355e36b; 5cbeca9f-c686-45a9-ab57-167379029964; 627a48a3-1584-42de-9b57-ee7a859b08d1; 6ed1070c-e478-47f2-8ea9-7ccc656bbba9; 70786146-ac4e-4d26-9906-ba211fde03f9; 8de76a93-6b66-4c29-a2fe-31e6cd1f969e; 8f07ade2-0a6d-40df-b886-cf99e3a1ed13; 9bda3e3d-1e17-4406-87e3-9ec7f71f4822; cb4b3331-193a-4c22-a95d-40f1ac9b53d6; dc9ded6f-463f-4cb1-a71a-0503c53f702a; f6644044-f4af-412a-a978-12ff080af3e1; ```; (edited). mcovarr [4:51 PM]; yeah that's from the 2/3 of horizontal Cromwell we implemented. mcovarr [4:52 PM]; but not sure why this is happening in this specific case. kshakir [4:52 PM]; How does :heartbeat:-ing work? There was a restart event in the middle of these workflows.; i think. mcovarr [4:55 PM]; timer in the WorkflowExecutionActor. kshakir [4:59 PM]; uploaded and commented on this file ; caas_cromwell_logs.txt; 6 MB Plain Text; Scratch the-restart-in-the-middle idea. I think they might have been submitted after the last restart. Gotta take off for now but here's the log dump for chatting at standup tomorrow. mcovarr [5:02 PM]; that looks odd, it didn't actually restart anything; how was this shut down?. kshakir [5:05 PM]; I assume caas shutdowns are _not_ clean. mcovarr [5:06 PM]; Just curious, I *thought* Cromwell always put the workflow store into a sane state when it started up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673
https://github.com/broadinstitute/cromwell/issues/3674:369,Deployability,deploy,deploy,369,"The command as it is now stops the container and gives 10s grace period. This should be increased, probably to whatever cromwell is using as its hard stop limit. > $SSHCMD $SSH_USER@$SSH_HOST ""docker-compose -p $PROJECT -f $COMPOSE_FILE stop"". As seen here:; https://github.com/broadinstitute/cromwell-develop/blob/551b25a1b097407010dc13ea16260222ab4955b7/dsde-jenkins-deploy.sh#L114",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3674
https://github.com/broadinstitute/cromwell/pull/3675:15,Availability,down,down,15,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675
https://github.com/broadinstitute/cromwell/pull/3675:337,Availability,down,down,337,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675
https://github.com/broadinstitute/cromwell/pull/3675:599,Availability,heartbeat,heartbeats,599,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675
https://github.com/broadinstitute/cromwell/pull/3675:652,Availability,heartbeat,heartbeat,652,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675
https://github.com/broadinstitute/cromwell/pull/3675:937,Availability,heartbeat,heartbeat,937,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675
https://github.com/broadinstitute/cromwell/pull/3675:1044,Availability,down,down,1044,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675
https://github.com/broadinstitute/cromwell/pull/3675:957,Integrability,depend,depending,957,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675
https://github.com/broadinstitute/cromwell/pull/3675:905,Safety,Abort,Aborted,905,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675
https://github.com/broadinstitute/cromwell/pull/3675:726,Testability,log,logic,726,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675
https://github.com/broadinstitute/cromwell/pull/3676:56,Testability,log,logs,56,Redo of #3667 with language and version inference. Also logs and metadata's the *actual* language factory we end up using,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3676
https://github.com/broadinstitute/cromwell/pull/3677:132,Security,validat,validation,132,"~This is now somewhat functional in that it processes the file `scripts/test_upgrade/scatter_files.wdl` into a form that passes WOM validation.~. ~PR'ing for early feedback and to start making progress on tests.~. This PR adds `WomToWdlom`, the second of two transformations required to go from WOM to WDL 1.0 source code `WdlWriter`.; ```; [ WOM ] ==> [ Wdlom ] ==> [ WDL 1.0 source ]; WomToWdlom WdlWriter; ```; We already know how to convert WDL draft-2 source into WOM, so now we can go from WDL draft-2 source to WDL 1.0 source, accomplishing the goal of upgrading user code. N.B. imports do not work in this iteration, to be added soon.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3677
https://github.com/broadinstitute/cromwell/pull/3677:205,Testability,test,tests,205,"~This is now somewhat functional in that it processes the file `scripts/test_upgrade/scatter_files.wdl` into a form that passes WOM validation.~. ~PR'ing for early feedback and to start making progress on tests.~. This PR adds `WomToWdlom`, the second of two transformations required to go from WOM to WDL 1.0 source code `WdlWriter`.; ```; [ WOM ] ==> [ Wdlom ] ==> [ WDL 1.0 source ]; WomToWdlom WdlWriter; ```; We already know how to convert WDL draft-2 source into WOM, so now we can go from WDL draft-2 source to WDL 1.0 source, accomplishing the goal of upgrading user code. N.B. imports do not work in this iteration, to be added soon.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3677
https://github.com/broadinstitute/cromwell/pull/3677:164,Usability,feedback,feedback,164,"~This is now somewhat functional in that it processes the file `scripts/test_upgrade/scatter_files.wdl` into a form that passes WOM validation.~. ~PR'ing for early feedback and to start making progress on tests.~. This PR adds `WomToWdlom`, the second of two transformations required to go from WOM to WDL 1.0 source code `WdlWriter`.; ```; [ WOM ] ==> [ Wdlom ] ==> [ WDL 1.0 source ]; WomToWdlom WdlWriter; ```; We already know how to convert WDL draft-2 source into WOM, so now we can go from WDL draft-2 source to WDL 1.0 source, accomplishing the goal of upgrading user code. N.B. imports do not work in this iteration, to be added soon.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3677
https://github.com/broadinstitute/cromwell/issues/3680:166,Integrability,Depend,Dependency,166,At the moment the PAPI v2 backend uses hardcoded public docker images to localize and delocalize files / directories.; This is not desirable for several reasons:. 1) Dependency on external images; 2) Lack of flexibility; 3) Potentially unoptimized or oversized images. Infrastructure should be put in place so that we can have control over those images while ensuring they can be accessed by all Cromwell users.; Those images (along with the command they run maybe ?) should be configurable.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3680
https://github.com/broadinstitute/cromwell/issues/3680:478,Modifiability,config,configurable,478,At the moment the PAPI v2 backend uses hardcoded public docker images to localize and delocalize files / directories.; This is not desirable for several reasons:. 1) Dependency on external images; 2) Lack of flexibility; 3) Potentially unoptimized or oversized images. Infrastructure should be put in place so that we can have control over those images while ensuring they can be accessed by all Cromwell users.; Those images (along with the command they run maybe ?) should be configurable.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3680
https://github.com/broadinstitute/cromwell/issues/3680:380,Security,access,accessed,380,At the moment the PAPI v2 backend uses hardcoded public docker images to localize and delocalize files / directories.; This is not desirable for several reasons:. 1) Dependency on external images; 2) Lack of flexibility; 3) Potentially unoptimized or oversized images. Infrastructure should be put in place so that we can have control over those images while ensuring they can be accessed by all Cromwell users.; Those images (along with the command they run maybe ?) should be configurable.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3680
https://github.com/broadinstitute/cromwell/pull/3681:112,Availability,error,error,112,- Remove the throttling of jobs during PapiV1 cron.; - Assemble jars if they haven't been already.; - Ensure an error is printed before exiting.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3681
https://github.com/broadinstitute/cromwell/pull/3682:564,Availability,error,error,564,"Issues addressed:. - ~~Custom-named stdout or stderr files were not being delocalized.~~ Handled in #3695 instead; - ~~The wrong stdout and stderr files were being delocalized (user action instead of user command).~~ Handled in #3695 instead; - ~~Optional output files that were not produced were being published to metadata as if they had been produced.~~ Revert fixes for this due to existence check performance issues; - Existence check for optional files was a `-a` test not actually supported by the delocalization Docker's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - ~~The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.~~ Reverted by reviewer request.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package. Now correctly moved thanks to reviewer input. 🙂 ; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations. With the existence checks removed from Cromwell proper this has reverted back to ""failing to fail"" which is mistaken for success.; - File literals now actually create a file, needed because the OutputManipulator does existence checks on outputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3682
https://github.com/broadinstitute/cromwell/pull/3682:402,Performance,perform,performance,402,"Issues addressed:. - ~~Custom-named stdout or stderr files were not being delocalized.~~ Handled in #3695 instead; - ~~The wrong stdout and stderr files were being delocalized (user action instead of user command).~~ Handled in #3695 instead; - ~~Optional output files that were not produced were being published to metadata as if they had been produced.~~ Revert fixes for this due to existence check performance issues; - Existence check for optional files was a `-a` test not actually supported by the delocalization Docker's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - ~~The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.~~ Reverted by reviewer request.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package. Now correctly moved thanks to reviewer input. 🙂 ; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations. With the existence checks removed from Cromwell proper this has reverted back to ""failing to fail"" which is mistaken for success.; - File literals now actually create a file, needed because the OutputManipulator does existence checks on outputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3682
https://github.com/broadinstitute/cromwell/pull/3682:470,Testability,test,test,470,"Issues addressed:. - ~~Custom-named stdout or stderr files were not being delocalized.~~ Handled in #3695 instead; - ~~The wrong stdout and stderr files were being delocalized (user action instead of user command).~~ Handled in #3695 instead; - ~~Optional output files that were not produced were being published to metadata as if they had been produced.~~ Revert fixes for this due to existence check performance issues; - Existence check for optional files was a `-a` test not actually supported by the delocalization Docker's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - ~~The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.~~ Reverted by reviewer request.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package. Now correctly moved thanks to reviewer input. 🙂 ; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations. With the existence checks removed from Cromwell proper this has reverted back to ""failing to fail"" which is mistaken for success.; - File literals now actually create a file, needed because the OutputManipulator does existence checks on outputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3682
https://github.com/broadinstitute/cromwell/pull/3682:956,Testability,test,test,956,"Issues addressed:. - ~~Custom-named stdout or stderr files were not being delocalized.~~ Handled in #3695 instead; - ~~The wrong stdout and stderr files were being delocalized (user action instead of user command).~~ Handled in #3695 instead; - ~~Optional output files that were not produced were being published to metadata as if they had been produced.~~ Revert fixes for this due to existence check performance issues; - Existence check for optional files was a `-a` test not actually supported by the delocalization Docker's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - ~~The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.~~ Reverted by reviewer request.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package. Now correctly moved thanks to reviewer input. 🙂 ; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations. With the existence checks removed from Cromwell proper this has reverted back to ""failing to fail"" which is mistaken for success.; - File literals now actually create a file, needed because the OutputManipulator does existence checks on outputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3682
https://github.com/broadinstitute/cromwell/pull/3682:1095,Testability,test,test,1095,"Issues addressed:. - ~~Custom-named stdout or stderr files were not being delocalized.~~ Handled in #3695 instead; - ~~The wrong stdout and stderr files were being delocalized (user action instead of user command).~~ Handled in #3695 instead; - ~~Optional output files that were not produced were being published to metadata as if they had been produced.~~ Revert fixes for this due to existence check performance issues; - Existence check for optional files was a `-a` test not actually supported by the delocalization Docker's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - ~~The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.~~ Reverted by reviewer request.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package. Now correctly moved thanks to reviewer input. 🙂 ; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations. With the existence checks removed from Cromwell proper this has reverted back to ""failing to fail"" which is mistaken for success.; - File literals now actually create a file, needed because the OutputManipulator does existence checks on outputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3682
https://github.com/broadinstitute/cromwell/issues/3683:302,Availability,recover,recover,302,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683
https://github.com/broadinstitute/cromwell/issues/3683:509,Energy Efficiency,efficient,efficient,509,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683
https://github.com/broadinstitute/cromwell/issues/3683:602,Energy Efficiency,efficient,efficient,602,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683
https://github.com/broadinstitute/cromwell/issues/3683:201,Integrability,message,messages,201,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683
https://github.com/broadinstitute/cromwell/issues/3683:428,Integrability,message,messages,428,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683
https://github.com/broadinstitute/cromwell/issues/3683:1042,Integrability,message,message,1042,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683
https://github.com/broadinstitute/cromwell/issues/3683:1229,Integrability,message,message,1229,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683
https://github.com/broadinstitute/cromwell/issues/3683:22,Modifiability,config,config,22,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683
https://github.com/broadinstitute/cromwell/issues/3683:302,Safety,recover,recover,302,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683
https://github.com/broadinstitute/cromwell/issues/3683:236,Testability,log,logs,236,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683
https://github.com/broadinstitute/cromwell/issues/3690:226,Availability,error,error,226,"Two days ago I successfully ran my first wdl on Cromwell using the Google Pipelines API. Then I tried to change my service account and it broke. I'm not able to get it running anymore at all. Stacktrace can be seen below, the error is ""Scopes not configured for service account."". **stdout**. ```; ...; tsv_string += '\n' + ""unmapped"". with open(""sequence_grouping_with_unmapped.txt"",""w"") as tsv_file_with_unmapped:; tsv_file_with_unmapped.write(tsv_string); tsv_file_with_unmapped.close(); CODE`; 2018-05-25 12:55:24,629 cromwell-system-akka.dispatchers.backend-dispatcher-137 ERROR - Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:342); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at cromwell.cloudsupport.gcp.genomics.GenomicsFactory$$anon$1.initialize(GenomicsFactory.scala:18); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:277); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest$lzycompute(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:578,Availability,ERROR,ERROR,578,"Two days ago I successfully ran my first wdl on Cromwell using the Google Pipelines API. Then I tried to change my service account and it broke. I'm not able to get it running anymore at all. Stacktrace can be seen below, the error is ""Scopes not configured for service account."". **stdout**. ```; ...; tsv_string += '\n' + ""unmapped"". with open(""sequence_grouping_with_unmapped.txt"",""w"") as tsv_file_with_unmapped:; tsv_file_with_unmapped.write(tsv_string); tsv_file_with_unmapped.close(); CODE`; 2018-05-25 12:55:24,629 cromwell-system-akka.dispatchers.backend-dispatcher-137 ERROR - Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:342); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at cromwell.cloudsupport.gcp.genomics.GenomicsFactory$$anon$1.initialize(GenomicsFactory.scala:18); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:277); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest$lzycompute(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:5618,Availability,down,downloaded,5618," for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud config set project project-test1; gsutil ls gs://project-test1 // This command works, so authentication is successful.; ```; **project-test1-59b66448c3ab.json**; ```; {; ""type"": ""service_account"",; ""project_id"": ""project-test1"",; ""private_key_id"": ""59b66448c3ab730097135e1dba83b375a6b57ea3"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\n(Omitted)\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""project-service@project-test1.iam.gserviceaccount.com"",; ""client_id"": ""104927211954691424974"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/project-service%40project-test1.iam.gserviceaccount.com""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:74,Deployability,Pipeline,Pipelines,74,"Two days ago I successfully ran my first wdl on Cromwell using the Google Pipelines API. Then I tried to change my service account and it broke. I'm not able to get it running anymore at all. Stacktrace can be seen below, the error is ""Scopes not configured for service account."". **stdout**. ```; ...; tsv_string += '\n' + ""unmapped"". with open(""sequence_grouping_with_unmapped.txt"",""w"") as tsv_file_with_unmapped:; tsv_file_with_unmapped.write(tsv_string); tsv_file_with_unmapped.close(); CODE`; 2018-05-25 12:55:24,629 cromwell-system-akka.dispatchers.backend-dispatcher-137 ERROR - Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:342); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at cromwell.cloudsupport.gcp.genomics.GenomicsFactory$$anon$1.initialize(GenomicsFactory.scala:18); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:277); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest$lzycompute(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:4937,Deployability,Pipeline,Pipelines,4937," changed the actual project name to generic ""project""); ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""project-test1""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""project-test1"". // Base bucket for workflow executions; root = ""gs://project-test1/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud config set project project-test1; gsutil ls gs://project-test1 // This command works, so authentication is successful.; ```; **project-test1-59b66448c3ab.json**; ```; {; ""type"": ""service",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:247,Modifiability,config,configured,247,"Two days ago I successfully ran my first wdl on Cromwell using the Google Pipelines API. Then I tried to change my service account and it broke. I'm not able to get it running anymore at all. Stacktrace can be seen below, the error is ""Scopes not configured for service account."". **stdout**. ```; ...; tsv_string += '\n' + ""unmapped"". with open(""sequence_grouping_with_unmapped.txt"",""w"") as tsv_file_with_unmapped:; tsv_file_with_unmapped.write(tsv_string); tsv_file_with_unmapped.close(); CODE`; 2018-05-25 12:55:24,629 cromwell-system-akka.dispatchers.backend-dispatcher-137 ERROR - Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:342); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at cromwell.cloudsupport.gcp.genomics.GenomicsFactory$$anon$1.initialize(GenomicsFactory.scala:18); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:277); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest$lzycompute(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:597,Modifiability,config,configured,597,"Two days ago I successfully ran my first wdl on Cromwell using the Google Pipelines API. Then I tried to change my service account and it broke. I'm not able to get it running anymore at all. Stacktrace can be seen below, the error is ""Scopes not configured for service account."". **stdout**. ```; ...; tsv_string += '\n' + ""unmapped"". with open(""sequence_grouping_with_unmapped.txt"",""w"") as tsv_file_with_unmapped:; tsv_file_with_unmapped.write(tsv_string); tsv_file_with_unmapped.close(); CODE`; 2018-05-25 12:55:24,629 cromwell-system-akka.dispatchers.backend-dispatcher-137 ERROR - Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:342); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at cromwell.cloudsupport.gcp.genomics.GenomicsFactory$$anon$1.initialize(GenomicsFactory.scala:18); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:277); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest$lzycompute(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:747,Modifiability,config,configured,747,"Two days ago I successfully ran my first wdl on Cromwell using the Google Pipelines API. Then I tried to change my service account and it broke. I'm not able to get it running anymore at all. Stacktrace can be seen below, the error is ""Scopes not configured for service account."". **stdout**. ```; ...; tsv_string += '\n' + ""unmapped"". with open(""sequence_grouping_with_unmapped.txt"",""w"") as tsv_file_with_unmapped:; tsv_file_with_unmapped.write(tsv_string); tsv_file_with_unmapped.close(); CODE`; 2018-05-25 12:55:24,629 cromwell-system-akka.dispatchers.backend-dispatcher-137 ERROR - Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:342); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at cromwell.cloudsupport.gcp.genomics.GenomicsFactory$$anon$1.initialize(GenomicsFactory.scala:18); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:277); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest$lzycompute(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:4392,Modifiability,config,config,4392,"(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; I also tried on another computer and another GCP project just to verify that it is not a cache problem. I don't know what is wrong. Seems like the service account has a problem, but I did everything the same way as when it worked. **Some detailed information:**. I have tried Cromwell 31.1 and 31. I start Cromwell using this command; `java -Dconfig.file=google.conf -jar cromwell-31.jar server`. **google.conf**; (I have changed the actual project name to generic ""project""); ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""project-test1""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""project-test1"". // Base bucket for workflow executions; root = ""gs://project-test1/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; aut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:5753,Modifiability,config,config,5753," for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud config set project project-test1; gsutil ls gs://project-test1 // This command works, so authentication is successful.; ```; **project-test1-59b66448c3ab.json**; ```; {; ""type"": ""service_account"",; ""project_id"": ""project-test1"",; ""private_key_id"": ""59b66448c3ab730097135e1dba83b375a6b57ea3"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\n(Omitted)\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""project-service@project-test1.iam.gserviceaccount.com"",; ""client_id"": ""104927211954691424974"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/project-service%40project-test1.iam.gserviceaccount.com""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:3606,Performance,cache,cache,3606,".scala:512); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager.akka$actor$Timers$$super$aroundReceive(JesApiQueryManager.scala:33); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager.aroundReceive(JesApiQueryManager.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; I also tried on another computer and another GCP project just to verify that it is not a cache problem. I don't know what is wrong. Seems like the service account has a problem, but I did everything the same way as when it worked. **Some detailed information:**. I have tried Cromwell 31.1 and 31. I start Cromwell using this command; `java -Dconfig.file=google.conf -jar cromwell-31.jar server`. **google.conf**; (I have changed the actual project name to generic ""project""); ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""project-test1""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""project-test1"". // Base bucket for workflow executions; root = ""gs://project-test1/cromwell-execution"". // Polling for completion",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:4745,Security,access,access,4745,"ome detailed information:**. I have tried Cromwell 31.1 and 31. I start Cromwell using this command; `java -Dconfig.file=google.conf -jar cromwell-31.jar server`. **google.conf**; (I have changed the actual project name to generic ""project""); ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""project-test1""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""project-test1"". // Base bucket for workflow executions; root = ""gs://project-test1/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:5545,Security,authenticat,authentication,5545,"adually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud config set project project-test1; gsutil ls gs://project-test1 // This command works, so authentication is successful.; ```; **project-test1-59b66448c3ab.json**; ```; {; ""type"": ""service_account"",; ""project_id"": ""project-test1"",; ""private_key_id"": ""59b66448c3ab730097135e1dba83b375a6b57ea3"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\n(Omitted)\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""project-service@project-test1.iam.gserviceaccount.com"",; ""client_id"": ""104927211954691424974"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/project-service%40project-test1.iam.gserviceaccount.com"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:5842,Security,authenticat,authentication,5842," for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud config set project project-test1; gsutil ls gs://project-test1 // This command works, so authentication is successful.; ```; **project-test1-59b66448c3ab.json**; ```; {; ""type"": ""service_account"",; ""project_id"": ""project-test1"",; ""private_key_id"": ""59b66448c3ab730097135e1dba83b375a6b57ea3"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\n(Omitted)\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""project-service@project-test1.iam.gserviceaccount.com"",; ""client_id"": ""104927211954691424974"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/project-service%40project-test1.iam.gserviceaccount.com""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:5678,Testability,log,login,5678," for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud config set project project-test1; gsutil ls gs://project-test1 // This command works, so authentication is successful.; ```; **project-test1-59b66448c3ab.json**; ```; {; ""type"": ""service_account"",; ""project_id"": ""project-test1"",; ""private_key_id"": ""59b66448c3ab730097135e1dba83b375a6b57ea3"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\n(Omitted)\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""project-service@project-test1.iam.gserviceaccount.com"",; ""client_id"": ""104927211954691424974"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/project-service%40project-test1.iam.gserviceaccount.com""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/issues/3690:5739,Testability,log,login,5739," for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud config set project project-test1; gsutil ls gs://project-test1 // This command works, so authentication is successful.; ```; **project-test1-59b66448c3ab.json**; ```; {; ""type"": ""service_account"",; ""project_id"": ""project-test1"",; ""private_key_id"": ""59b66448c3ab730097135e1dba83b375a6b57ea3"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\n(Omitted)\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""project-service@project-test1.iam.gserviceaccount.com"",; ""client_id"": ""104927211954691424974"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/project-service%40project-test1.iam.gserviceaccount.com""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690
https://github.com/broadinstitute/cromwell/pull/3694:37,Deployability,Pipeline,Pipelines,37,- Enables 121 on PAPIv2; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3694
https://github.com/broadinstitute/cromwell/pull/3694:27,Modifiability,Refactor,Refactors,27,- Enables 121 on PAPIv2; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3694
https://github.com/broadinstitute/cromwell/pull/3694:208,Security,validat,validation,208,- Enables 121 on PAPIv2; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3694
https://github.com/broadinstitute/cromwell/pull/3695:157,Availability,error,error,157,Standardize PAPI detritus naming to match SFS for the benefit of the existing detritus filtering code. This also fixes the conflation of standard output and error from the user command with that of the whole exec script on PAPI that we've had since the beginning of time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3695
https://github.com/broadinstitute/cromwell/pull/3697:462,Deployability,Pipeline,Pipelines,462,"- Better localization and delocalization of directories in PAPI2 using hidden files to cover for empty directories; - IWDR localization is not baked in the CWL code anymore but left to the backend. This allows for the PAPI backend to opt out of it since localization is done directly on the VM.; - ~~Use configurable `job-shell` instead of hardcoded `/bin/bash`~~ It fixes 117 but also makes a bunch of centaur tests fail, so leaving as is for now.; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known. This only partially covers the possible cases. It needs a deeper tech talk discussion. This is orthogonal to the above and only here to avoid a later rebase (the files changed overlap with the refactoring mentioned).; - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697
https://github.com/broadinstitute/cromwell/pull/3697:304,Modifiability,config,configurable,304,"- Better localization and delocalization of directories in PAPI2 using hidden files to cover for empty directories; - IWDR localization is not baked in the CWL code anymore but left to the backend. This allows for the PAPI backend to opt out of it since localization is done directly on the VM.; - ~~Use configurable `job-shell` instead of hardcoded `/bin/bash`~~ It fixes 117 but also makes a bunch of centaur tests fail, so leaving as is for now.; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known. This only partially covers the possible cases. It needs a deeper tech talk discussion. This is orthogonal to the above and only here to avoid a later rebase (the files changed overlap with the refactoring mentioned).; - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697
https://github.com/broadinstitute/cromwell/pull/3697:452,Modifiability,Refactor,Refactors,452,"- Better localization and delocalization of directories in PAPI2 using hidden files to cover for empty directories; - IWDR localization is not baked in the CWL code anymore but left to the backend. This allows for the PAPI backend to opt out of it since localization is done directly on the VM.; - ~~Use configurable `job-shell` instead of hardcoded `/bin/bash`~~ It fixes 117 but also makes a bunch of centaur tests fail, so leaving as is for now.; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known. This only partially covers the possible cases. It needs a deeper tech talk discussion. This is orthogonal to the above and only here to avoid a later rebase (the files changed overlap with the refactoring mentioned).; - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697
https://github.com/broadinstitute/cromwell/pull/3697:850,Modifiability,refactor,refactoring,850,"- Better localization and delocalization of directories in PAPI2 using hidden files to cover for empty directories; - IWDR localization is not baked in the CWL code anymore but left to the backend. This allows for the PAPI backend to opt out of it since localization is done directly on the VM.; - ~~Use configurable `job-shell` instead of hardcoded `/bin/bash`~~ It fixes 117 but also makes a bunch of centaur tests fail, so leaving as is for now.; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known. This only partially covers the possible cases. It needs a deeper tech talk discussion. This is orthogonal to the above and only here to avoid a later rebase (the files changed overlap with the refactoring mentioned).; - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697
https://github.com/broadinstitute/cromwell/pull/3697:793,Safety,avoid,avoid,793,"- Better localization and delocalization of directories in PAPI2 using hidden files to cover for empty directories; - IWDR localization is not baked in the CWL code anymore but left to the backend. This allows for the PAPI backend to opt out of it since localization is done directly on the VM.; - ~~Use configurable `job-shell` instead of hardcoded `/bin/bash`~~ It fixes 117 but also makes a bunch of centaur tests fail, so leaving as is for now.; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known. This only partially covers the possible cases. It needs a deeper tech talk discussion. This is orthogonal to the above and only here to avoid a later rebase (the files changed overlap with the refactoring mentioned).; - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697
https://github.com/broadinstitute/cromwell/pull/3697:998,Security,validat,validation,998,"- Better localization and delocalization of directories in PAPI2 using hidden files to cover for empty directories; - IWDR localization is not baked in the CWL code anymore but left to the backend. This allows for the PAPI backend to opt out of it since localization is done directly on the VM.; - ~~Use configurable `job-shell` instead of hardcoded `/bin/bash`~~ It fixes 117 but also makes a bunch of centaur tests fail, so leaving as is for now.; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known. This only partially covers the possible cases. It needs a deeper tech talk discussion. This is orthogonal to the above and only here to avoid a later rebase (the files changed overlap with the refactoring mentioned).; - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697
https://github.com/broadinstitute/cromwell/pull/3697:411,Testability,test,tests,411,"- Better localization and delocalization of directories in PAPI2 using hidden files to cover for empty directories; - IWDR localization is not baked in the CWL code anymore but left to the backend. This allows for the PAPI backend to opt out of it since localization is done directly on the VM.; - ~~Use configurable `job-shell` instead of hardcoded `/bin/bash`~~ It fixes 117 but also makes a bunch of centaur tests fail, so leaving as is for now.; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known. This only partially covers the possible cases. It needs a deeper tech talk discussion. This is orthogonal to the above and only here to avoid a later rebase (the files changed overlap with the refactoring mentioned).; - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697
https://github.com/broadinstitute/cromwell/issues/3698:23,Testability,test,test,23,The `read_tsv` Centaur test wrongly succeeds on the PAPI backend when in fact none of the files referenced in the generated TSV were delocalized from the VM. This is a more general problem of indirect file references not unlike `cwl.output.json` or globbing which would possibly benefit from a general solution.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3698
https://github.com/broadinstitute/cromwell/pull/3700:53,Deployability,update,update,53,"The ones I've added are draft-2, but will eventually update to draft-3!. This PR is contingent on [cron job](https://travis-ci.org/broadinstitute/cromwell/builds/385209234) going green.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3700
https://github.com/broadinstitute/cromwell/pull/3700:179,Energy Efficiency,green,green,179,"The ones I've added are draft-2, but will eventually update to draft-3!. This PR is contingent on [cron job](https://travis-ci.org/broadinstitute/cromwell/builds/385209234) going green.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3700
https://github.com/broadinstitute/cromwell/issues/3701:270,Deployability,update,updated,270,You need to enable the [Google Cloud Storage JSON API](https://console.cloud.google.com/apis/api/storage-api.googleapis.com/overview) in order to run Cromwell on PAPI. The [PAPI tutorial](http://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/) needs to be updated to include this API with the other 3.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3701
https://github.com/broadinstitute/cromwell/issues/3705:144,Deployability,configurat,configuration,144,"https://github.com/broadinstitute/cromwell/blob/215cca97e6efafa7406fd89b38e21a39ce2d0d4e/cromwell.examples.conf#L440. It seems like the default configuration sends the stdout and stderr to the same filenames twice. In the config above, qsub is called with -o EG:; ```e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout```. In the script that will be executed on the node, there is some shenanigans:; ```; (; cd .../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution. bla bla bla my command invocation; ) > >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout') 2> >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stderr' >&2); ```. Those 'tees' are generated by cromwell somewhere, I do not know if the config has control of that. I don't know the details of what will happen, but I do not think it will be healthy. I smell race conditions. I came across this trying to debug missing log data when SGE aborts a job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705
https://github.com/broadinstitute/cromwell/issues/3705:144,Modifiability,config,configuration,144,"https://github.com/broadinstitute/cromwell/blob/215cca97e6efafa7406fd89b38e21a39ce2d0d4e/cromwell.examples.conf#L440. It seems like the default configuration sends the stdout and stderr to the same filenames twice. In the config above, qsub is called with -o EG:; ```e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout```. In the script that will be executed on the node, there is some shenanigans:; ```; (; cd .../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution. bla bla bla my command invocation; ) > >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout') 2> >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stderr' >&2); ```. Those 'tees' are generated by cromwell somewhere, I do not know if the config has control of that. I don't know the details of what will happen, but I do not think it will be healthy. I smell race conditions. I came across this trying to debug missing log data when SGE aborts a job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705
https://github.com/broadinstitute/cromwell/issues/3705:222,Modifiability,config,config,222,"https://github.com/broadinstitute/cromwell/blob/215cca97e6efafa7406fd89b38e21a39ce2d0d4e/cromwell.examples.conf#L440. It seems like the default configuration sends the stdout and stderr to the same filenames twice. In the config above, qsub is called with -o EG:; ```e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout```. In the script that will be executed on the node, there is some shenanigans:; ```; (; cd .../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution. bla bla bla my command invocation; ) > >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout') 2> >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stderr' >&2); ```. Those 'tees' are generated by cromwell somewhere, I do not know if the config has control of that. I don't know the details of what will happen, but I do not think it will be healthy. I smell race conditions. I came across this trying to debug missing log data when SGE aborts a job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705
https://github.com/broadinstitute/cromwell/issues/3705:766,Modifiability,config,config,766,"https://github.com/broadinstitute/cromwell/blob/215cca97e6efafa7406fd89b38e21a39ce2d0d4e/cromwell.examples.conf#L440. It seems like the default configuration sends the stdout and stderr to the same filenames twice. In the config above, qsub is called with -o EG:; ```e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout```. In the script that will be executed on the node, there is some shenanigans:; ```; (; cd .../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution. bla bla bla my command invocation; ) > >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout') 2> >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stderr' >&2); ```. Those 'tees' are generated by cromwell somewhere, I do not know if the config has control of that. I don't know the details of what will happen, but I do not think it will be healthy. I smell race conditions. I came across this trying to debug missing log data when SGE aborts a job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705
https://github.com/broadinstitute/cromwell/issues/3705:887,Performance,race condition,race conditions,887,"https://github.com/broadinstitute/cromwell/blob/215cca97e6efafa7406fd89b38e21a39ce2d0d4e/cromwell.examples.conf#L440. It seems like the default configuration sends the stdout and stderr to the same filenames twice. In the config above, qsub is called with -o EG:; ```e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout```. In the script that will be executed on the node, there is some shenanigans:; ```; (; cd .../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution. bla bla bla my command invocation; ) > >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout') 2> >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stderr' >&2); ```. Those 'tees' are generated by cromwell somewhere, I do not know if the config has control of that. I don't know the details of what will happen, but I do not think it will be healthy. I smell race conditions. I came across this trying to debug missing log data when SGE aborts a job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705
https://github.com/broadinstitute/cromwell/issues/3705:965,Safety,abort,aborts,965,"https://github.com/broadinstitute/cromwell/blob/215cca97e6efafa7406fd89b38e21a39ce2d0d4e/cromwell.examples.conf#L440. It seems like the default configuration sends the stdout and stderr to the same filenames twice. In the config above, qsub is called with -o EG:; ```e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout```. In the script that will be executed on the node, there is some shenanigans:; ```; (; cd .../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution. bla bla bla my command invocation; ) > >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout') 2> >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stderr' >&2); ```. Those 'tees' are generated by cromwell somewhere, I do not know if the config has control of that. I don't know the details of what will happen, but I do not think it will be healthy. I smell race conditions. I came across this trying to debug missing log data when SGE aborts a job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705
https://github.com/broadinstitute/cromwell/issues/3705:947,Testability,log,log,947,"https://github.com/broadinstitute/cromwell/blob/215cca97e6efafa7406fd89b38e21a39ce2d0d4e/cromwell.examples.conf#L440. It seems like the default configuration sends the stdout and stderr to the same filenames twice. In the config above, qsub is called with -o EG:; ```e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout```. In the script that will be executed on the node, there is some shenanigans:; ```; (; cd .../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution. bla bla bla my command invocation; ) > >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout') 2> >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stderr' >&2); ```. Those 'tees' are generated by cromwell somewhere, I do not know if the config has control of that. I don't know the details of what will happen, but I do not think it will be healthy. I smell race conditions. I came across this trying to debug missing log data when SGE aborts a job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705
https://github.com/broadinstitute/cromwell/issues/3706:152,Performance,bottleneck,bottlenecked,152,"Not sure if ""scatter"" is the right summary word here but:. Chris Whelan ran a job that sharded 60 different ways, each with 555 samples. . Cromwell was bottlenecked trying to hash all these, and ultimately many of the has requests timed out. It's potentially a spurious correlation, but CPU was pegged at 100% during this time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3706
https://github.com/broadinstitute/cromwell/issues/3706:175,Security,hash,hash,175,"Not sure if ""scatter"" is the right summary word here but:. Chris Whelan ran a job that sharded 60 different ways, each with 555 samples. . Cromwell was bottlenecked trying to hash all these, and ultimately many of the has requests timed out. It's potentially a spurious correlation, but CPU was pegged at 100% during this time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3706
https://github.com/broadinstitute/cromwell/pull/3707:106,Integrability,wrap,wrapper,106,Addresses #3705. This requires the fixes in #3695 that puts command stdout/err in separate files from the wrapper exec script.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3707
https://github.com/broadinstitute/cromwell/issues/3710:1001,Availability,echo,echo,1001,"ommonwl.org/v1.0/CommandLineTool.html#Parameter_references; >If the value of a field has non-whitespace leading or trailing characters around a parameter reference, it is subject to string interpolation. The effective value of the field is a string containing the leading characters, followed by the string value of the parameter reference, followed by the trailing characters. The string value of the parameter reference is its textual **JSON** representation with the following rules:; > ; > * Leading and trailing quotes are stripped from strings; > * Objects entries are sorted by key; >; > Multiple parameter references may appear in a single field. This case must be treated as a string interpolation. After interpolating the first parameter reference, interpolation must be recursively applied to the trailing characters to yield the final string value. Test code: `runtime-dump.cwl`; ``` cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: echo; inputs: []; arguments: [ '{""runtime"":$(runtime)}' ]; stdout: runtime.json; outputs:; runtime: stdout; ```; Example contents of the generated `runtime.json` (linewrapped for readability):; ``` json; {""runtime"":object {ram: 4, cores: 1, tmpdir: ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-e842171dde6e/call-c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/tmp.095e5005"",; outdir: ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-e842171dde6e/call-c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/execution"",; outdirSize: 2147483647, tmpdirSize: 2147483647}}; ```; Corrected version of the generated `runtime.json`:; 1. No `output ` prefix; 2. JSON, not YAML: key names are in quotes; 3. Objects entries are sorted by key; ``` json; {""runtime"":{""cores"": 1, ""outdir"": ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-e842171dde6e/call-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3710
https://github.com/broadinstitute/cromwell/issues/3710:879,Testability,Test,Test,879,"From https://www.commonwl.org/v1.0/CommandLineTool.html#Parameter_references; >If the value of a field has non-whitespace leading or trailing characters around a parameter reference, it is subject to string interpolation. The effective value of the field is a string containing the leading characters, followed by the string value of the parameter reference, followed by the trailing characters. The string value of the parameter reference is its textual **JSON** representation with the following rules:; > ; > * Leading and trailing quotes are stripped from strings; > * Objects entries are sorted by key; >; > Multiple parameter references may appear in a single field. This case must be treated as a string interpolation. After interpolating the first parameter reference, interpolation must be recursively applied to the trailing characters to yield the final string value. Test code: `runtime-dump.cwl`; ``` cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: echo; inputs: []; arguments: [ '{""runtime"":$(runtime)}' ]; stdout: runtime.json; outputs:; runtime: stdout; ```; Example contents of the generated `runtime.json` (linewrapped for readability):; ``` json; {""runtime"":object {ram: 4, cores: 1, tmpdir: ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-e842171dde6e/call-c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/tmp.095e5005"",; outdir: ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-e842171dde6e/call-c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/execution"",; outdirSize: 2147483647, tmpdirSize: 2147483647}}; ```; Corrected version of the generated `runtime.json`:; 1. No `output ` prefix; 2. JSON, not YAML: key names are in quotes; 3. Objects entries are sorted by key; ``` json; {""runtime"":{""cores"": 1, ""outdir"": ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3710
https://github.com/broadinstitute/cromwell/issues/3710:2381,Testability,test,test,2381,"representation with the following rules:; > ; > * Leading and trailing quotes are stripped from strings; > * Objects entries are sorted by key; >; > Multiple parameter references may appear in a single field. This case must be treated as a string interpolation. After interpolating the first parameter reference, interpolation must be recursively applied to the trailing characters to yield the final string value. Test code: `runtime-dump.cwl`; ``` cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: echo; inputs: []; arguments: [ '{""runtime"":$(runtime)}' ]; stdout: runtime.json; outputs:; runtime: stdout; ```; Example contents of the generated `runtime.json` (linewrapped for readability):; ``` json; {""runtime"":object {ram: 4, cores: 1, tmpdir: ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-e842171dde6e/call-c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/tmp.095e5005"",; outdir: ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-e842171dde6e/call-c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/execution"",; outdirSize: 2147483647, tmpdirSize: 2147483647}}; ```; Corrected version of the generated `runtime.json`:; 1. No `output ` prefix; 2. JSON, not YAML: key names are in quotes; 3. Objects entries are sorted by key; ``` json; {""runtime"":{""cores"": 1, ""outdir"": ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-e842171dde6e/call-c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/execution"",; ""outdirSize"": 2147483647, ""ram"": 4, ""tmpdir"": ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-e842171dde6e/call-c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/tmp.095e5005"",; ""tmpdirSize"": 2147483647}}; ```. This was discovered by a new conformance test: https://github.com/common-workflow-language/common-workflow-language/pull/706",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3710
https://github.com/broadinstitute/cromwell/pull/3711:107,Integrability,message,message,107,"Bug where querying for workflows which are in 'On Hold' state and would give . ```; {; ""status"": ""fail"",; ""message"": ""Unrecognized status values: On Hold""; }; ```. is resolved. Comparison of string for state matching is now case insensitive, and instead of sending the user submitted state, the actual string associated with each WorkflowState is sent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3711
https://github.com/broadinstitute/cromwell/issues/3712:644,Modifiability,flexible,flexible,644,"## Discussion \#1; ```; bshifaw [3:59 PM]; Hi Chris, ; The featured joint calling method is using NIO.; https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl; Is this the method you are referencing? (edited). bshifaw [4:28 PM]; @vdauwera, just confirmed with @jsoto. The wdl isn’t using NIO when importing the GVCFs. Due to a change in the wdl we decide to implement to best leverage the FC data model (using an array of input files instead of a sample name map file). (edited). Collapse; cwhelan [9:48 PM]; right, that’s the method i was using. vdauwera [11:22 PM]; oooh that’s an interesting case that would benefit from the flexible data models work — this would be great to show @andreah; ```. ## Discussion \#2. ```; cwhelan [11:17 AM]; ie it’s trying to localize each gvcf to each shard instance. tjeandet [11:17 AM]; do you have an idea of how many input files each shard has ?. Collapse; cwhelan [11:17 AM]; 555 samples; ```. # Takeaways. Run https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl in a non-production environment w/ 555 samples and try to reproduce issue w/ hashing timeouts. We predict they will not occur as cromwell production was seeing elevated CPU usage due to it's /stats endpoint being hit repeatedly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712
https://github.com/broadinstitute/cromwell/issues/3712:1124,Safety,timeout,timeouts,1124,"## Discussion \#1; ```; bshifaw [3:59 PM]; Hi Chris, ; The featured joint calling method is using NIO.; https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl; Is this the method you are referencing? (edited). bshifaw [4:28 PM]; @vdauwera, just confirmed with @jsoto. The wdl isn’t using NIO when importing the GVCFs. Due to a change in the wdl we decide to implement to best leverage the FC data model (using an array of input files instead of a sample name map file). (edited). Collapse; cwhelan [9:48 PM]; right, that’s the method i was using. vdauwera [11:22 PM]; oooh that’s an interesting case that would benefit from the flexible data models work — this would be great to show @andreah; ```. ## Discussion \#2. ```; cwhelan [11:17 AM]; ie it’s trying to localize each gvcf to each shard instance. tjeandet [11:17 AM]; do you have an idea of how many input files each shard has ?. Collapse; cwhelan [11:17 AM]; 555 samples; ```. # Takeaways. Run https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl in a non-production environment w/ 555 samples and try to reproduce issue w/ hashing timeouts. We predict they will not occur as cromwell production was seeing elevated CPU usage due to it's /stats endpoint being hit repeatedly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712
https://github.com/broadinstitute/cromwell/issues/3712:1137,Safety,predict,predict,1137,"## Discussion \#1; ```; bshifaw [3:59 PM]; Hi Chris, ; The featured joint calling method is using NIO.; https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl; Is this the method you are referencing? (edited). bshifaw [4:28 PM]; @vdauwera, just confirmed with @jsoto. The wdl isn’t using NIO when importing the GVCFs. Due to a change in the wdl we decide to implement to best leverage the FC data model (using an array of input files instead of a sample name map file). (edited). Collapse; cwhelan [9:48 PM]; right, that’s the method i was using. vdauwera [11:22 PM]; oooh that’s an interesting case that would benefit from the flexible data models work — this would be great to show @andreah; ```. ## Discussion \#2. ```; cwhelan [11:17 AM]; ie it’s trying to localize each gvcf to each shard instance. tjeandet [11:17 AM]; do you have an idea of how many input files each shard has ?. Collapse; cwhelan [11:17 AM]; 555 samples; ```. # Takeaways. Run https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl in a non-production environment w/ 555 samples and try to reproduce issue w/ hashing timeouts. We predict they will not occur as cromwell production was seeing elevated CPU usage due to it's /stats endpoint being hit repeatedly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712
https://github.com/broadinstitute/cromwell/issues/3712:1116,Security,hash,hashing,1116,"## Discussion \#1; ```; bshifaw [3:59 PM]; Hi Chris, ; The featured joint calling method is using NIO.; https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl; Is this the method you are referencing? (edited). bshifaw [4:28 PM]; @vdauwera, just confirmed with @jsoto. The wdl isn’t using NIO when importing the GVCFs. Due to a change in the wdl we decide to implement to best leverage the FC data model (using an array of input files instead of a sample name map file). (edited). Collapse; cwhelan [9:48 PM]; right, that’s the method i was using. vdauwera [11:22 PM]; oooh that’s an interesting case that would benefit from the flexible data models work — this would be great to show @andreah; ```. ## Discussion \#2. ```; cwhelan [11:17 AM]; ie it’s trying to localize each gvcf to each shard instance. tjeandet [11:17 AM]; do you have an idea of how many input files each shard has ?. Collapse; cwhelan [11:17 AM]; 555 samples; ```. # Takeaways. Run https://portal.firecloud.org/#methods/gatk/joint-discovery-gatk4/9/wdl in a non-production environment w/ 555 samples and try to reproduce issue w/ hashing timeouts. We predict they will not occur as cromwell production was seeing elevated CPU usage due to it's /stats endpoint being hit repeatedly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712
https://github.com/broadinstitute/cromwell/pull/3715:77,Availability,avail,available,77,"The `validateWomNamespace` method was using `NoIoFunctionSet` instead of the available ioFunctions, causing the workflow in the centaur test to fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3715
https://github.com/broadinstitute/cromwell/pull/3715:5,Security,validat,validateWomNamespace,5,"The `validateWomNamespace` method was using `NoIoFunctionSet` instead of the available ioFunctions, causing the workflow in the centaur test to fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3715
