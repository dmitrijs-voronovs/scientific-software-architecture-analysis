quality_attribute,keyword,matched_word,sentence,source,author,repo,version,wiki,url
Usability,simpl,simpleAssert,We added `simpleAssert`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/39#issuecomment-235158444
Deployability,integrat,integrated,Yup! Mitja and I have been talking and sharing code on this issue. But clearly there is some work to be done for this functionality to be integrated naturally in Hail. And obviously the phasing stuff would be neat :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/104#issuecomment-279858875
Integrability,integrat,integrated,Yup! Mitja and I have been talking and sharing code on this issue. But clearly there is some work to be done for this functionality to be integrated naturally in Hail. And obviously the phasing stuff would be neat :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/104#issuecomment-279858875
Usability,clear,clearly,Yup! Mitja and I have been talking and sharing code on this issue. But clearly there is some work to be done for this functionality to be integrated naturally in Hail. And obviously the phasing stuff would be neat :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/104#issuecomment-279858875
Usability,simpl,simply,"Added in #115. We did not add AN, since this is simply `2 * nCalled`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/112#issuecomment-169052284
Availability,error,error,Our error message on functions that read TSV are much clearer than they used to be. I don't think this needs to be a separate command.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/216#issuecomment-279518768
Integrability,message,message,Our error message on functions that read TSV are much clearer than they used to be. I don't think this needs to be a separate command.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/216#issuecomment-279518768
Usability,clear,clearer,Our error message on functions that read TSV are much clearer than they used to be. I don't think this needs to be a separate command.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/216#issuecomment-279518768
Availability,down,down,"`hail` was hanging after all commands completed when running kudu commands against the quickstart. From the thread dump, it looked like it was spinning in the kudu client. Shutting down the kudu context seemed to fix the problem. See any problems with this patch? Also, I removed latest. It didn't seem to be used. ```; diff --git a/src/main/scala/org/kududb/spark/KuduContext.scala b/src/main/scala/org/kududb/spark/KuduContext.scala; index c48dcd4..71be7d2 100644; --- a/src/main/scala/org/kududb/spark/KuduContext.scala; +++ b/src/main/scala/org/kududb/spark/KuduContext.scala; @@ -41,8 +41,6 @@ class KuduContext(@transient sc: SparkContext,. val broadcastedKuduMaster = sc.broadcast(kuduMaster). - LatestKuduContextCache.latest = this; -; /**; * A simple enrichment of the traditional Spark RDD foreachPartition.; * This function differs from the original in that it offers the; @@ -169,10 +167,6 @@ class KuduContext(@transient sc: SparkContext,; def fakeClassTag[T]: ClassTag[T] = ClassTag.AnyRef.asInstanceOf[ClassTag[T]]; }. -object LatestKuduContextCache {; - var latest:KuduContext = null; -}; -; object KuduClientCache {; var kuduClient: KuduClient = null; var asyncKuduClient: AsyncKuduClient = null; @@ -195,4 +189,14 @@ object KuduClientCache {; asyncKuduClient; }. + def close() {; + if (kuduClient != null) {; + kuduClient.close(); + kuduClient = null; + }; + if (asyncKuduClient != null) {; + asyncKuduClient.close(); + asyncKuduClient = null; + }; + }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220667612
Deployability,patch,patch,"`hail` was hanging after all commands completed when running kudu commands against the quickstart. From the thread dump, it looked like it was spinning in the kudu client. Shutting down the kudu context seemed to fix the problem. See any problems with this patch? Also, I removed latest. It didn't seem to be used. ```; diff --git a/src/main/scala/org/kududb/spark/KuduContext.scala b/src/main/scala/org/kududb/spark/KuduContext.scala; index c48dcd4..71be7d2 100644; --- a/src/main/scala/org/kududb/spark/KuduContext.scala; +++ b/src/main/scala/org/kududb/spark/KuduContext.scala; @@ -41,8 +41,6 @@ class KuduContext(@transient sc: SparkContext,. val broadcastedKuduMaster = sc.broadcast(kuduMaster). - LatestKuduContextCache.latest = this; -; /**; * A simple enrichment of the traditional Spark RDD foreachPartition.; * This function differs from the original in that it offers the; @@ -169,10 +167,6 @@ class KuduContext(@transient sc: SparkContext,; def fakeClassTag[T]: ClassTag[T] = ClassTag.AnyRef.asInstanceOf[ClassTag[T]]; }. -object LatestKuduContextCache {; - var latest:KuduContext = null; -}; -; object KuduClientCache {; var kuduClient: KuduClient = null; var asyncKuduClient: AsyncKuduClient = null; @@ -195,4 +189,14 @@ object KuduClientCache {; asyncKuduClient; }. + def close() {; + if (kuduClient != null) {; + kuduClient.close(); + kuduClient = null; + }; + if (asyncKuduClient != null) {; + asyncKuduClient.close(); + asyncKuduClient = null; + }; + }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220667612
Usability,simpl,simple,"`hail` was hanging after all commands completed when running kudu commands against the quickstart. From the thread dump, it looked like it was spinning in the kudu client. Shutting down the kudu context seemed to fix the problem. See any problems with this patch? Also, I removed latest. It didn't seem to be used. ```; diff --git a/src/main/scala/org/kududb/spark/KuduContext.scala b/src/main/scala/org/kududb/spark/KuduContext.scala; index c48dcd4..71be7d2 100644; --- a/src/main/scala/org/kududb/spark/KuduContext.scala; +++ b/src/main/scala/org/kududb/spark/KuduContext.scala; @@ -41,8 +41,6 @@ class KuduContext(@transient sc: SparkContext,. val broadcastedKuduMaster = sc.broadcast(kuduMaster). - LatestKuduContextCache.latest = this; -; /**; * A simple enrichment of the traditional Spark RDD foreachPartition.; * This function differs from the original in that it offers the; @@ -169,10 +167,6 @@ class KuduContext(@transient sc: SparkContext,; def fakeClassTag[T]: ClassTag[T] = ClassTag.AnyRef.asInstanceOf[ClassTag[T]]; }. -object LatestKuduContextCache {; - var latest:KuduContext = null; -}; -; object KuduClientCache {; var kuduClient: KuduClient = null; var asyncKuduClient: AsyncKuduClient = null; @@ -195,4 +189,14 @@ object KuduClientCache {; asyncKuduClient; }. + def close() {; + if (kuduClient != null) {; + kuduClient.close(); + kuduClient = null; + }; + if (asyncKuduClient != null) {; + asyncKuduClient.close(); + asyncKuduClient = null; + }; + }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220667612
Testability,test,testing,What should we do about testing Cassandra? We need an `annotatevariants cass` as well I think. This stuff looks good though (awesome that it can be so simple!),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/282#issuecomment-208341414
Usability,simpl,simple,What should we do about testing Cassandra? We need an `annotatevariants cass` as well I think. This stuff looks good though (awesome that it can be so simple!),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/282#issuecomment-208341414
Usability,clear,clear,I think this is due to HDFS filling up. Possibly related to the fact Tim was creating a copy of ExAC. I will retry the job once we clear up some space.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/301#issuecomment-210901646
Usability,clear,clear,"ok, let me know when I can try again. I’m now importing another large WES study, hope doesn’t have the same problem.; cheers,. > On Apr 16, 2016, at 5:08 PM, cseed notifications@github.com wrote:; > ; > I think this is due to HDFS filling up. Possibly related to the fact Tim was creating a copy of ExAC. I will retry the job once we clear up some space.; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly or view it on GitHub https://github.com/broadinstitute/hail/issues/301#issuecomment-210901646",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/301#issuecomment-210901896
Usability,clear,clearly,And we should clearly document what we've implemented so there is no ambiguity.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/313#issuecomment-212074652
Integrability,interface,interface,"Proposed interface changes:. class TextTableConfiguration. class TextTableReader. TextTableReader(conf); TextTableReader(delimiter = ""#"", ...). // only read fields ; TextTableReader.read(columnTypes: Map[String, Type], path: String, [select]): (TStruct, RDD[Annotation]). (and JSON). for JSON:. JSONReader.read(t: Type, path: String): RDD[Annotation]. in expr language:. support. Variant(""chr:pos:ref:alt1,...,altN"") . (so Variant(v.toString) == v) and. Variant(chr: String, pos: Int, ref: String, alts: Array[String]). Then we can do:. annotatevariants table -v 'Variant(Chrom, Pos, Ref, Alts.split("",""))'. annotatevariants table -v 'Variant(Variant)'. To get this behavior, you'll have to build the EvalContext from the table type. Add. TStruct.filter(predicate: (Field) => Boolean): (TStruct, Filterer). where. type Filterer = (Annotation) => Annotation. This should make implementating importvariants table simple and elegant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232740494
Usability,simpl,simple,"Proposed interface changes:. class TextTableConfiguration. class TextTableReader. TextTableReader(conf); TextTableReader(delimiter = ""#"", ...). // only read fields ; TextTableReader.read(columnTypes: Map[String, Type], path: String, [select]): (TStruct, RDD[Annotation]). (and JSON). for JSON:. JSONReader.read(t: Type, path: String): RDD[Annotation]. in expr language:. support. Variant(""chr:pos:ref:alt1,...,altN"") . (so Variant(v.toString) == v) and. Variant(chr: String, pos: Int, ref: String, alts: Array[String]). Then we can do:. annotatevariants table -v 'Variant(Chrom, Pos, Ref, Alts.split("",""))'. annotatevariants table -v 'Variant(Variant)'. To get this behavior, you'll have to build the EvalContext from the table type. Add. TStruct.filter(predicate: (Field) => Boolean): (TStruct, Filterer). where. type Filterer = (Annotation) => Annotation. This should make implementating importvariants table simple and elegant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232740494
Availability,error,error,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
Integrability,message,messages,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
Performance,load,load,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
Security,validat,validation,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
Usability,clear,clear,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581
Integrability,interoperab,interoperability,"Thanks, @tomwhite! This is great. Is there a Hive CLI equivalent of `LIKE PARQUET <file>`? I can't figure out how to get Hive to infer the schema from the Parquet file rather than specifying it explicitly. It would be awesome to be able to query the genotypes, too. It seems like we could write a SerDe (now I'm thinking ImpEx isn't so bad :)) to unpack the genotypes. Does that sound like the right approach?. On a related note, we've played with storing VDS natively as Parquet as (variant, variant annotations, array(genotype)). Even when I ported over some of the GenotypeStream encoding tricks (OD instead of DP, etc.), it was 2-3x larger (using Snappy compression vs. our internal LZ4 compression). That's disappointing, esp. when we have 30+TB datasets on the way. What's worse, simple operations like counting genotypes (`count -g`) are 5-10x in the native representation. Current master:. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read -i profile225.vds count -g; hail: info: timing:; importvcf: 508.829ms; write: 3m6.7s; read: 1.629s; count: 13.934s; $ du -sh profile225.vds; 2.0G profile225.vds; ```. And with the `jg_dataframe1` experimental branch, which uses native Parquet and computes the count using a UDF that computes the sum per array (fastest Parquet-based implementation we've found so far):. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read2 -i profile225.vds count2; hail: info: timing:; importvcf: 492.354ms; write: 5m57.1s; read2: 1.466s; count2: 1m44.1; $ du -sh profile225.vds; 5.4G profile225.vds; ```. That's 2.7x larger and >7x slower. This includes the fact that the Parquet version is only loading the GT field of the genotypes (!). This might be a non-starter for us. We'd love the flexibility and interoperability of standard Parquet. If you have other ideas about how to get Parquet close to what we currently have, I'd love to talk more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/480#issuecomment-234027310
Performance,load,loading,"Thanks, @tomwhite! This is great. Is there a Hive CLI equivalent of `LIKE PARQUET <file>`? I can't figure out how to get Hive to infer the schema from the Parquet file rather than specifying it explicitly. It would be awesome to be able to query the genotypes, too. It seems like we could write a SerDe (now I'm thinking ImpEx isn't so bad :)) to unpack the genotypes. Does that sound like the right approach?. On a related note, we've played with storing VDS natively as Parquet as (variant, variant annotations, array(genotype)). Even when I ported over some of the GenotypeStream encoding tricks (OD instead of DP, etc.), it was 2-3x larger (using Snappy compression vs. our internal LZ4 compression). That's disappointing, esp. when we have 30+TB datasets on the way. What's worse, simple operations like counting genotypes (`count -g`) are 5-10x in the native representation. Current master:. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read -i profile225.vds count -g; hail: info: timing:; importvcf: 508.829ms; write: 3m6.7s; read: 1.629s; count: 13.934s; $ du -sh profile225.vds; 2.0G profile225.vds; ```. And with the `jg_dataframe1` experimental branch, which uses native Parquet and computes the count using a UDF that computes the sum per array (fastest Parquet-based implementation we've found so far):. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read2 -i profile225.vds count2; hail: info: timing:; importvcf: 492.354ms; write: 5m57.1s; read2: 1.466s; count2: 1m44.1; $ du -sh profile225.vds; 5.4G profile225.vds; ```. That's 2.7x larger and >7x slower. This includes the fact that the Parquet version is only loading the GT field of the genotypes (!). This might be a non-starter for us. We'd love the flexibility and interoperability of standard Parquet. If you have other ideas about how to get Parquet close to what we currently have, I'd love to talk more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/480#issuecomment-234027310
Usability,simpl,simple,"Thanks, @tomwhite! This is great. Is there a Hive CLI equivalent of `LIKE PARQUET <file>`? I can't figure out how to get Hive to infer the schema from the Parquet file rather than specifying it explicitly. It would be awesome to be able to query the genotypes, too. It seems like we could write a SerDe (now I'm thinking ImpEx isn't so bad :)) to unpack the genotypes. Does that sound like the right approach?. On a related note, we've played with storing VDS natively as Parquet as (variant, variant annotations, array(genotype)). Even when I ported over some of the GenotypeStream encoding tricks (OD instead of DP, etc.), it was 2-3x larger (using Snappy compression vs. our internal LZ4 compression). That's disappointing, esp. when we have 30+TB datasets on the way. What's worse, simple operations like counting genotypes (`count -g`) are 5-10x in the native representation. Current master:. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read -i profile225.vds count -g; hail: info: timing:; importvcf: 508.829ms; write: 3m6.7s; read: 1.629s; count: 13.934s; $ du -sh profile225.vds; 2.0G profile225.vds; ```. And with the `jg_dataframe1` experimental branch, which uses native Parquet and computes the count using a UDF that computes the sum per array (fastest Parquet-based implementation we've found so far):. ```; $ hail importvcf profile225.vcf.bgz write -o profile225.vds read2 -i profile225.vds count2; hail: info: timing:; importvcf: 492.354ms; write: 5m57.1s; read2: 1.466s; count2: 1m44.1; $ du -sh profile225.vds; 5.4G profile225.vds; ```. That's 2.7x larger and >7x slower. This includes the fact that the Parquet version is only loading the GT field of the genotypes (!). This might be a non-starter for us. We'd love the flexibility and interoperability of standard Parquet. If you have other ideas about how to get Parquet close to what we currently have, I'd love to talk more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/480#issuecomment-234027310
Integrability,depend,depends,"`inHemiX` depends on sex. How about `inXPar` and `inXNonPar`, so that Non clearly just refers to Par? This reads as ""in the X pseudo-autosomal region"" or ""in the X non-pseudo-autosomal-region""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235665809
Usability,clear,clearly,"`inHemiX` depends on sex. How about `inXPar` and `inXNonPar`, so that Non clearly just refers to Par? This reads as ""in the X pseudo-autosomal region"" or ""in the X non-pseudo-autosomal-region""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235665809
Usability,simpl,simple,"Well, fraction reads supporting that alternate allele (`1-Ref` for the bi-allelic case, but not necessarily for the multi-allelic) - but that's possibly a separate discussion. GQ histogram I think is a simple application here. For each variant, a histogram of GQs - I think `gs.hist()` is the right idea though yes (this is an idea, does not exist yet, right?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/509#issuecomment-237703639
Modifiability,extend,extend,"If/when the need is pressing, we can extend parsing to deal with both unnamed and named and optional args in full generality. With sort and sortBy, I handled the possibilities more directly and think the documentation is clear as is. I also assume the Boolean parameter is a constant rather than an expression handling null etc, but I think that covers the use cases of these functions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236266169
Usability,clear,clear,"If/when the need is pressing, we can extend parsing to deal with both unnamed and named and optional args in full generality. With sort and sortBy, I handled the possibilities more directly and think the documentation is clear as is. I also assume the Boolean parameter is a constant rather than an expression handling null etc, but I think that covers the use cases of these functions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236266169
Availability,error,error,I moved to TBoolean and simplified the error messages. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236389770
Integrability,message,messages,I moved to TBoolean and simplified the error messages. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236389770
Usability,simpl,simplified,I moved to TBoolean and simplified the error messages. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236389770
Usability,clear,clear,"You're right, they don't. @tpoterba Let's make this clear in the documentation. Although it is on the roadmap, it is not easy to support nested aggregators in full generality (what amounts to subqueries in SQL). I propose adding a `oneHot(x, arr)` where `x: Whatever`, `arr: Array[Whatever]` and `oneHot` returns the one hot encoding of `x` with respect to the ordered values in `arr` as an `Array[Int]` (and missing if `x` is not in `arr`). Then, using Tim's new changes that support aggregating over arrays (https://github.com/broadinstitute/hail/pull/584, still being reviewed but very close), you can write the above example as:. ```; annotateglobal expr -c 'global.pops = [""AFR"", ""NFE""]'; annotateglobals expr -c 'global.pop_counts = samples.sum(oneHot(sa.meta_test.POP, global.pops))'; ```. Reasonable?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/549#issuecomment-240003377
Usability,clear,clear,"Two small comments:; - make it clear the name/description tables are describing the scope of the corresponding expression. We might need to do this elsewhere in the documentation, too.; - I'd write `aIndices[newIndex] = oldIndex` just to make it clear you're talking about the indices, and not the alleles.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240494452
Usability,simpl,simpler,@cseed Merging this PR will make the diff for #536 simpler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/558#issuecomment-238264351
Usability,simpl,simplified,Greatly simplified the evaluation matching.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/574#issuecomment-240128679
Energy Efficiency,reduce,reduced,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
Integrability,wrap,wrap,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
Performance,perform,performance,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
Testability,test,test,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
Usability,feedback,feedback,"Ready for another look. I had to modify the classes some to make it work, particularly for getting the `type` out of the test. Now the type is with the Test rather than the TestResult, perhaps you see a better way?. Related notes, mostly relevant to future PRs once we have some feedback and a sense of performance:. I think LogisticRegressionNullFit should be a separate class, as it plays a conceptually and practically different role. I don't want to attach vectors of length nSamples (like mu) to each LogisticRegressionFit output, even though they would speed up the score test and first iteration of fitting per variant to not recompute them for every variant. I did put some of this efficiency in the score test (only computing the extra coordinate of score and row / column of fisher per variant). df would also then go away for LogisticRegressionFit, but I'd add the diagonal of its inverse for use in Wald (see below). The model fit function would then take a LogisticRegressionNullFit to use in the first iteration. The bigger future gains will come from not computing or inverting the Fisher matrix at all in the iteration, but rather using QR magic. val sqrtW = sqrt(mu :\* (1d - mu)); val QR = qr.reduced(X(::, _) :_ sqrtW); solve QR.R \* deltaB = QR.Q.t \* (y - mu) with R upper triangular (need to wrap lapack function). for Wald: return diagonal of inverse as well, namely diagonal of inv(R)^T \* inv(R), rather than inverting fisher again. for Score, this version of this may be faster:; val sqrtW = sqrt(mu :\* (1d - mu)); val Qty0 = qr.reduced.justQ(X(::, _) :_ sqrtW).t \* ((y - mu) :/ sqrtW); val chi2 = Qty0 dot Qty0. for Firth, modify score using:; val QQ = QR.Q :\* QR.Q; val h = sum(QQ(*, ::))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-241153168
Deployability,install,installed,"Hi Jerome, yup, the first three require plink 1.9 and the fourth requires qctool. I'm surprised FisherExactSuite didn't fail as well, perhaps you have R installed or pulled Hail before that went into master. Thanks for the feedback, super helpful!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240399174
Usability,feedback,feedback,"Hi Jerome, yup, the first three require plink 1.9 and the fourth requires qctool. I'm surprised FisherExactSuite didn't fail as well, perhaps you have R installed or pulled Hail before that went into master. Thanks for the feedback, super helpful!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240399174
Usability,clear,clear,@cseed this should be clear for merge now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/696#issuecomment-244097334
Modifiability,rewrite,rewrite,"Here's a larger rewrite of Github readme, ready for feedback. The gitter links reflect hail and hail-dev as we want them to be, so before merging we should rename hail to hail-dev and create hail. I also think it'd be good to give a bit more context for users on what ""pre-alpha, very active dev"" does and does not mean. In particular, that Hail is usable and tested now, but liable to change in non backward-compatible ways. Thoughts on including / wording this?. We should also consider moving the Roadmap somewhere on the forum. I think the development forum is a good place for more detailed instructions on collaboration (forking, etc) and best practices.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/699#issuecomment-243136925
Testability,test,tested,"Here's a larger rewrite of Github readme, ready for feedback. The gitter links reflect hail and hail-dev as we want them to be, so before merging we should rename hail to hail-dev and create hail. I also think it'd be good to give a bit more context for users on what ""pre-alpha, very active dev"" does and does not mean. In particular, that Hail is usable and tested now, but liable to change in non backward-compatible ways. Thoughts on including / wording this?. We should also consider moving the Roadmap somewhere on the forum. I think the development forum is a good place for more detailed instructions on collaboration (forking, etc) and best practices.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/699#issuecomment-243136925
Usability,feedback,feedback,"Here's a larger rewrite of Github readme, ready for feedback. The gitter links reflect hail and hail-dev as we want them to be, so before merging we should rename hail to hail-dev and create hail. I also think it'd be good to give a bit more context for users on what ""pre-alpha, very active dev"" does and does not mean. In particular, that Hail is usable and tested now, but liable to change in non backward-compatible ways. Thoughts on including / wording this?. We should also consider moving the Roadmap somewhere on the forum. I think the development forum is a good place for more detailed instructions on collaboration (forking, etc) and best practices.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/699#issuecomment-243136925
Performance,perform,performance,"@tpoterba @cseed . If y'all can take a look at the docs, tests, and implementation, I want to merge this. I included a log of running on `profile.vcf.bgz` (which has 2500 samples) below, total time is about 3.5 minutes. I expect it to scale roughly like `O(nSamples^2)`. For Kyle's use case this performance is acceptable. Further performance, model, and usability improvements will be separate PRs. ```; dking@wmb16-359 # hail read -i profile.vds ibd -o foo; hail: info: running: read -i profile.vds; [Stage 0:==============> (1 + 3) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:==================================================> (197 + 4) / 214]hail: info: timing:; read: 3.523s; ibd: 3m33.8s. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-250288185
Testability,test,tests,"@tpoterba @cseed . If y'all can take a look at the docs, tests, and implementation, I want to merge this. I included a log of running on `profile.vcf.bgz` (which has 2500 samples) below, total time is about 3.5 minutes. I expect it to scale roughly like `O(nSamples^2)`. For Kyle's use case this performance is acceptable. Further performance, model, and usability improvements will be separate PRs. ```; dking@wmb16-359 # hail read -i profile.vds ibd -o foo; hail: info: running: read -i profile.vds; [Stage 0:==============> (1 + 3) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:==================================================> (197 + 4) / 214]hail: info: timing:; read: 3.523s; ibd: 3m33.8s. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-250288185
Usability,usab,usability,"@tpoterba @cseed . If y'all can take a look at the docs, tests, and implementation, I want to merge this. I included a log of running on `profile.vcf.bgz` (which has 2500 samples) below, total time is about 3.5 minutes. I expect it to scale roughly like `O(nSamples^2)`. For Kyle's use case this performance is acceptable. Further performance, model, and usability improvements will be separate PRs. ```; dking@wmb16-359 # hail read -i profile.vds ibd -o foo; hail: info: running: read -i profile.vds; [Stage 0:==============> (1 + 3) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:==================================================> (197 + 4) / 214]hail: info: timing:; read: 3.523s; ibd: 3m33.8s. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-250288185
Usability,simpl,simpler,"Another (simpler) check that I think will produce a `ClassCastException` :. ``` scala; eval[Int]("""""" (if (true) 0 else 0.toLong).toInt """""" ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/748#issuecomment-245384057
Availability,error,errors,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143
Deployability,patch,patch,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143
Usability,simpl,simple,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143
Deployability,patch,patch,Thank you for the clear and easy-to-follow instructions! I've rebased my commit and applied your patch. What do I need to do now?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-251409770
Usability,clear,clear,Thank you for the clear and easy-to-follow instructions! I've rebased my commit and applied your patch. What do I need to do now?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-251409770
Availability,down,down,"When I run the current GRM with BlockMatrix locally on profile225.hardcalls.vds (2535 samples, 225k variants), I get:. ```; java.lang.ArrayIndexOutOfBoundsException: 1048578; ```. When I run on profile.hardcalls.vds (only 25k variants), I get:. ```; java.lang.OutOfMemoryError: Java heap space; ```. When I cut profile down to only 10k variants, grm takes about 66s:. ```; read: 1.874s; grm: 1m5.9s; ```. The respective numbers using this PR are 9m36s, 39s, and 12s (so a ~5x speedup in the last case). I'd like to try this on a cluster as well with profile225k. Comparing the output for 10k, the doubles look to agree to around 16 digits (we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
Deployability,update,updates,": 1.874s; grm: 1m5.9s; ```. The respective numbers using this PR are 9m36s, 39s, and 12s (so a ~5x speedup in the last case). I'd like to try this on a cluster as well with profile225k. Comparing the output for 10k, the doubles look to agree to around 16 digits (we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoft",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
Energy Efficiency,schedul,scheduler,Map.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
Performance,concurren,concurrent,Map.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
Safety,avoid,avoiding,"When I run the current GRM with BlockMatrix locally on profile225.hardcalls.vds (2535 samples, 225k variants), I get:. ```; java.lang.ArrayIndexOutOfBoundsException: 1048578; ```. When I run on profile.hardcalls.vds (only 25k variants), I get:. ```; java.lang.OutOfMemoryError: Java heap space; ```. When I cut profile down to only 10k variants, grm takes about 66s:. ```; read: 1.874s; grm: 1m5.9s; ```. The respective numbers using this PR are 9m36s, 39s, and 12s (so a ~5x speedup in the last case). I'd like to try this on a cluster as well with profile225k. Comparing the output for 10k, the doubles look to agree to around 16 digits (we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
Usability,clear,clear,"```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:194); at org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:147); at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:185); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703
Usability,feedback,feedback,@bw2 any feedback on above? Should I close this issue or do you have more inquiries on symbolic variants?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/803#issuecomment-279597422
Availability,error,errors,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
Security,access,access,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
Testability,log,log,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
Usability,guid,guides,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848
Availability,down,download,Two high-level comments:; - Here is the default documentation:. https://ci.hail.is/repository/download/HailSourceCode_HailCi/846:id/docs/index.html#exportaggregate. Some documentation of the output format and maybe and example or two (with and without `--by-matrix`) would be awesome.; - We need at least some testing. I think a simple aggregation on a small file that you verify by hand would be sufficient. I'll look over the code and let you know if I have additional comments.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/837#issuecomment-249244766
Testability,test,testing,Two high-level comments:; - Here is the default documentation:. https://ci.hail.is/repository/download/HailSourceCode_HailCi/846:id/docs/index.html#exportaggregate. Some documentation of the output format and maybe and example or two (with and without `--by-matrix`) would be awesome.; - We need at least some testing. I think a simple aggregation on a small file that you verify by hand would be sufficient. I'll look over the code and let you know if I have additional comments.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/837#issuecomment-249244766
Usability,simpl,simple,Two high-level comments:; - Here is the default documentation:. https://ci.hail.is/repository/download/HailSourceCode_HailCi/846:id/docs/index.html#exportaggregate. Some documentation of the output format and maybe and example or two (with and without `--by-matrix`) would be awesome.; - We need at least some testing. I think a simple aggregation on a small file that you verify by hand would be sufficient. I'll look over the code and let you know if I have additional comments.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/837#issuecomment-249244766
Usability,simpl,simple,"More directly, this is known as the maximal independent set (MIS) problem, for which there are simple parallel algorithms: https://cstar.iiit.ac.in/~kkishore/MISStudy.pdf. MIS should be implemented in Spark GraphX, but I can't find it!; http://ilpubs.stanford.edu:8090/1085/2/primitives_tr_sig_alternate.pdf",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/863#issuecomment-250805502
Usability,clear,clearer,"Suggestions:; 1) change `va.maf` to `va.panel_maf` in the example, say first something like ""Suppose we have already added a variant annotation `va.panel_maf` with allele frequencies computed from a reference panel.""; 2) add ""if unspecified, MAF will be estimated from the dataset"" to `-m`. How about using `-maf` instead of `-m`, it's still super short and much clearer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/894#issuecomment-251433966
Usability,feedback,feedback,"Great, and thanks for the feedback. I'll close the issue now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-256958955
Usability,clear,clearer,"Great, this is much clearer now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1000#issuecomment-257306183
Usability,clear,clear,"@jigold Did commit 23c9e2f fix the build or did the CI server break something? I guess either way, it's not clear why we had different results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1012#issuecomment-257353710
Testability,test,test,"The command is hidden because to generate reasonable memory overhead and subsequent task sizes for even 10k samples and 10k variants, we should parallelize data generation of the RDD, rather than using `sc.parallelize` on a matrix at driver. That will be a subsequent improvement. For reference, running an IntelliJ test that simply generates the integer matrix of genotypes for 10k samples and 10k variants and 4 populations with the rest default takes about 7 seconds (one core). Small examples still work fine using this command, but PCA fails at the following scale unless repartition is used first:. ```; hail \; baldingnichols \; -k 3 \; -n 2000 \; -m 10000 \; -f .02,.03,.1 \; -d .2,.3,.5 \; -s 0 \; repartition -n 8 \; printschema -o ~/data/baldingnichols/schema.json \; pca -k 3 -s 'sa.pc' -e 'global.evals' \; showglobals -o ~/data/baldingnichols/global.tsv \; exportsamples -c 'sample = s, pop = sa.bn.pop, pc = sa.pc.*' -o ~/data/baldingnichols/samples.tsv \; exportvariants -c 'variant = v, freq = va.bn.*' -o ~/data/baldingnichols/variants.tsv; ```. Here is the annotation scheme created by `baldingnichols`:. ```; Global annotation schema:; global: Struct {; bn: Struct {; seed: Int,; nPops: Int,; nSamples: Int,; nVariants: Int,; popDist: Array[Double],; Fst: Array[Double]; }; }. Sample annotation schema:; sa: Struct {; bn: Struct {; pop: Int; }; }. Variant annotation schema:; va: Struct {; bn: Struct {; ancAF: Double,; AF0: Double,; AF1: Double,; AF2: Double; }; }; ```. The following python code shows three tight clusters corresponding to population using PC1 and PC2, and that PC3 is noise:. ```; import numpy as np; import matplotlib.pyplot as plt; import pandas as pd. %matplotlib inline. df = pd.read_table(""samples.tsv""); colors = {0: 'r', 1: 'b', 2: 'g'}. df.plot('pc.PC1', 'pc.PC2', 'scatter', c=df['pop'].map(colors), alpha=.3); plt.show(). df.plot('pc.PC1', 'pc.PC3', 'scatter', c=df['pop'].map(colors), alpha=.3); plt.show(). df.plot('pc.PC2', 'pc.PC3', 'scatter', c=d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1029#issuecomment-257211215
Usability,simpl,simply,"The command is hidden because to generate reasonable memory overhead and subsequent task sizes for even 10k samples and 10k variants, we should parallelize data generation of the RDD, rather than using `sc.parallelize` on a matrix at driver. That will be a subsequent improvement. For reference, running an IntelliJ test that simply generates the integer matrix of genotypes for 10k samples and 10k variants and 4 populations with the rest default takes about 7 seconds (one core). Small examples still work fine using this command, but PCA fails at the following scale unless repartition is used first:. ```; hail \; baldingnichols \; -k 3 \; -n 2000 \; -m 10000 \; -f .02,.03,.1 \; -d .2,.3,.5 \; -s 0 \; repartition -n 8 \; printschema -o ~/data/baldingnichols/schema.json \; pca -k 3 -s 'sa.pc' -e 'global.evals' \; showglobals -o ~/data/baldingnichols/global.tsv \; exportsamples -c 'sample = s, pop = sa.bn.pop, pc = sa.pc.*' -o ~/data/baldingnichols/samples.tsv \; exportvariants -c 'variant = v, freq = va.bn.*' -o ~/data/baldingnichols/variants.tsv; ```. Here is the annotation scheme created by `baldingnichols`:. ```; Global annotation schema:; global: Struct {; bn: Struct {; seed: Int,; nPops: Int,; nSamples: Int,; nVariants: Int,; popDist: Array[Double],; Fst: Array[Double]; }; }. Sample annotation schema:; sa: Struct {; bn: Struct {; pop: Int; }; }. Variant annotation schema:; va: Struct {; bn: Struct {; ancAF: Double,; AF0: Double,; AF1: Double,; AF2: Double; }; }; ```. The following python code shows three tight clusters corresponding to population using PC1 and PC2, and that PC3 is noise:. ```; import numpy as np; import matplotlib.pyplot as plt; import pandas as pd. %matplotlib inline. df = pd.read_table(""samples.tsv""); colors = {0: 'r', 1: 'b', 2: 'g'}. df.plot('pc.PC1', 'pc.PC2', 'scatter', c=df['pop'].map(colors), alpha=.3); plt.show(). df.plot('pc.PC1', 'pc.PC3', 'scatter', c=df['pop'].map(colors), alpha=.3); plt.show(). df.plot('pc.PC2', 'pc.PC3', 'scatter', c=d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1029#issuecomment-257211215
Availability,down,down,"Hi TJ! @tpoterba tells me he told you to make an Issue, but he forgot that we're trying to limit Issues to bug reports. Would you mind reposting this feature request on the forum and we'll follow up there?. http://discuss.hail.is/c/features. Can you also spell out a bit more what information you'd like for each parent-proband trio and how this information can be useful? We think the forum will be an easier place to get community feedback to nail down the best spec for all. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1055#issuecomment-258299456
Usability,feedback,feedback,"Hi TJ! @tpoterba tells me he told you to make an Issue, but he forgot that we're trying to limit Issues to bug reports. Would you mind reposting this feature request on the forum and we'll follow up there?. http://discuss.hail.is/c/features. Can you also spell out a bit more what information you'd like for each parent-proband trio and how this information can be useful? We think the forum will be an easier place to get community feedback to nail down the best spec for all. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1055#issuecomment-258299456
Usability,feedback,feedback,"Great feedback, thanks! I think I addressed all the comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1110#issuecomment-265057616
Deployability,update,update,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1114#issuecomment-266934270
Usability,guid,guide,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1114#issuecomment-266934270
Usability,feedback,feedback,"Great feedback, @tpoterba . I think I addressed all the comments. Back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1116#issuecomment-262160105
Deployability,update,update,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1144#issuecomment-266934292
Usability,guid,guide,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1144#issuecomment-266934292
Deployability,update,update,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1145#issuecomment-266934335
Usability,guid,guide,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1145#issuecomment-266934335
Testability,test,test,"Extremely nice @lfrancioli, very elegantly done. Rebase and address the minor comments, and it should be good to go. A simple test would be nice, too, but I'll put that on our todo list if you don't get to it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1147#issuecomment-265653872
Usability,simpl,simple,"Extremely nice @lfrancioli, very elegantly done. Rebase and address the minor comments, and it should be good to go. A simple test would be nice, too, but I'll put that on our todo list if you don't get to it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1147#issuecomment-265653872
Availability,failure,failure,"I've replicated the issue. invocation:; ```bash; ./pyhail-submit cluster-2 foo.py; ```; `foo.py`:; ```python; #!/usr/bin/python. from pyhail import *. hc = HailContext(log=""/tmp/hail.log""). (hc.read(<andrea's file here>); .write('gs://hail-1kg/trash.vds')); ```; first failure:; ```; 2016-12-15 19:05:43 ERROR Utils:91 - Uncaught exception in thread task-result-getter-1; java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
Energy Efficiency,schedul,scheduler,ternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
Modifiability,config,config,"rk.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.useraccounts.readonly"",; ""https://www.googleapis.com/auth/devstorage.full_control"",; ""https://www.googleapis.com/auth/devstorage.read_write"",; ""https://www.googleapis.com/auth/logging.write""; ]; },; ""masterConfig"": {; ""numInstances"": 1,; ""instanceNames"": [; ""cluster-2-m""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
Performance,concurren,concurrent,"gory.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
Security,hash,hash,"spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.user",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
Testability,log,log,"I've replicated the issue. invocation:; ```bash; ./pyhail-submit cluster-2 foo.py; ```; `foo.py`:; ```python; #!/usr/bin/python. from pyhail import *. hc = HailContext(log=""/tmp/hail.log""). (hc.read(<andrea's file here>); .write('gs://hail-1kg/trash.vds')); ```; first failure:; ```; 2016-12-15 19:05:43 ERROR Utils:91 - Uncaught exception in thread task-result-getter-1; java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
Usability,progress bar,progress bar,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027
Modifiability,variab,variable,"Great feedback. Addressed comments, back to you. In the scope lists, I use the format: `variable (*Type*): description`, where the type is in italics but not a hyperlink, but I put a hyperlink in the description when it seemed appropriate. ```; *:ref:`foo`; ```. didn't format the hyperlink. Also, I don't think we can put hyperlinks in double-back-quote literal/code blocks. I didn't address the math stuff. I think we can merge this (and other doc migrations) when it is ready and fix that separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1225#issuecomment-271382041
Usability,feedback,feedback,"Great feedback. Addressed comments, back to you. In the scope lists, I use the format: `variable (*Type*): description`, where the type is in italics but not a hyperlink, but I put a hyperlink in the description when it seemed appropriate. ```; *:ref:`foo`; ```. didn't format the hyperlink. Also, I don't think we can put hyperlinks in double-back-quote literal/code blocks. I didn't address the math stuff. I think we can merge this (and other doc migrations) when it is ready and fix that separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1225#issuecomment-271382041
Testability,benchmark,benchmark,what if you do a simpler benchmark: `filter_genotypes('g.gq > 20')`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1292#issuecomment-275122784
Usability,simpl,simpler,what if you do a simpler benchmark: `filter_genotypes('g.gq > 20')`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1292#issuecomment-275122784
Energy Efficiency,allocate,allocates,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468
Integrability,rout,route,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468
Modifiability,extend,extend,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468
Safety,avoid,avoids,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468
Usability,simpl,simplest,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468
Usability,simpl,simple,Looks good! Next steps:. - Use it!; - Test it! I'm not sure how much will break.; - Time it! Do something simple filter genotypes gq >= 20 and a sampleqc or something. Does it help? How much?; - Figure out how you're going to deal with annotations like `va.rare_genos = gs.filter(g => ... some rare condition ...).collect()`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1322#issuecomment-276272639
Usability,clear,clear,"I don't think the whitespace stuff belongs in a method, since that's expr language doc. I think if anything, the nMales/nFemales/nSamples stuff should go in query_samples, but I think the docs are pretty clear now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1349#issuecomment-284852670
Usability,clear,clear,"There are some tiny formatting issues and typos, but the main thing is to add an short examples section in docs that makes clear what this is useful for. I'd put one example for INFO field and one for FORMAT field. For other commands for format, like:; https://hail.is/hail/hail.VariantDataset.html#hail.VariantDataset.pca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1373#issuecomment-279790111
Integrability,interface,interface,"I agree with your criticism, although my feeling is that rows and r in your proposal are noisy and unnecessary. Two thoughts:. I think this is best resolved in the context of embedding the expression language in Python. I think understanding pandas and what's involved in building a pandas-like interface for VariantDatasets is a good way to start. If we do address it in the current setup, what do we want to write? How about `kt.aggregate('filter(col1 < col2).count()` or, assuming we're doing a summing col1, `filter(col1 < col2).sum(col1)`. Then all the lambdas go away. We clearly need the scope in aggregators. Why not make that explicit, and throw out the single implicit value? Then `filter(col1 < col2).with(col3 = col1 * col2).mean(col3)`. I'm not sure about flatMap. `flatWith(col3 = <array expr>)`? I guess that's the same as `with(col3 = <array expr>).explode(col3)`. Then Aggregables look like Structs:. ```; Aggregable {; col1: Int,; col2: Int, ...; }; ```. Then there's nothing funny going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1410#issuecomment-282784540
Usability,clear,clearly,"I agree with your criticism, although my feeling is that rows and r in your proposal are noisy and unnecessary. Two thoughts:. I think this is best resolved in the context of embedding the expression language in Python. I think understanding pandas and what's involved in building a pandas-like interface for VariantDatasets is a good way to start. If we do address it in the current setup, what do we want to write? How about `kt.aggregate('filter(col1 < col2).count()` or, assuming we're doing a summing col1, `filter(col1 < col2).sum(col1)`. Then all the lambdas go away. We clearly need the scope in aggregators. Why not make that explicit, and throw out the single implicit value? Then `filter(col1 < col2).with(col3 = col1 * col2).mean(col3)`. I'm not sure about flatMap. `flatWith(col3 = <array expr>)`? I guess that's the same as `with(col3 = <array expr>).explode(col3)`. Then Aggregables look like Structs:. ```; Aggregable {; col1: Int,; col2: Int, ...; }; ```. Then there's nothing funny going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1410#issuecomment-282784540
Availability,failure,failure,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
Testability,test,tests,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
Usability,usab,usable,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771
Usability,clear,clear,"Ok. In ""Setting limit to negative disables limiting the number of split and trailing empty strings are preserved"", you're missing ""s"" on split, and it's not clear if negative is necessary for trailing empty strings to be preserved. How about:. Setting limit to negative disables limiting the number of splits. Trailing empty strings are preserved, so ""a,b,,"".split("","", -1) gives [""a"", ""b"", """"]",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1423#issuecomment-281771114
Usability,clear,clearer,yes much clearer. Corrected!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1423#issuecomment-281794307
Usability,clear,clear,"It's not clear what happens to empty strings at the beginning. I think if you replace with this, we'll be all set:. Setting `limit` to negative disables limiting the number of splits. Trailing empty strings are preserved, so "",a,b,,"".split("","", -1) gives ["""", ""a"", ""b"", """", """"] whereas "",a,b,,"".split("","") gives ["""", ""a"", ""b""].",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1423#issuecomment-281819089
Usability,intuit,intuition,My intuition is that the reference should be directed the other way. PCA should refer to the GRM docs when discussing the GRM. The GRM docs should actually contain the text explaining the GRM.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1439#issuecomment-283120768
Usability,clear,clear,"Things aren't really consistent, so there's no clear pattern to follow. I'm happy to mimic python set/list functionality",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1491#issuecomment-284746931
Usability,guid,guide,"I just noticed that the spacing doesn't follow the style guide. Not critical, but try to autoformat before PRing",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1494#issuecomment-284814488
Usability,guid,guide,"Sorry about this! Will do!. On Tue, Mar 7, 2017 at 13:36 Tim Poterba <notifications@github.com> wrote:. > I just noticed that the spacing doesn't follow the style guide. Not; > critical, but try to autoformat before PRing; >; >; >; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/1494#issuecomment-284814488>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ADVxgc012iIeaevvZPZfYZuXXj1nuiE2ks5rjaOQgaJpZM4MVwyK>; > .; >; >; >; >; >; >; >; >; >; >; >; >; >; >; >; >; >; >; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1494#issuecomment-284817572
Energy Efficiency,efficient,efficient,"I prefer reworking count, killing the genotypes parameter, so that it's always just a simple/efficient way to get (nSamples, nVariants, nGenotypes, nCalled, callRate). I don't see why a tuple is better than a dict.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1505#issuecomment-284860282
Usability,simpl,simple,"I prefer reworking count, killing the genotypes parameter, so that it's always just a simple/efficient way to get (nSamples, nVariants, nGenotypes, nCalled, callRate). I don't see why a tuple is better than a dict.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1505#issuecomment-284860282
Availability,error,errors,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
Safety,abort,abort,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
Testability,assert,assert,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
Usability,usab,usability,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990
Availability,error,error,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290
Integrability,message,messages,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290
Usability,clear,clear,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290
Usability,simpl,simple,"It's working on a simple example on the Cray:. ```; >>> (hc; ... .import_vcf('file:///mnt/lustre/cseed/sample.vcf'); ... .vep('/mnt/lustre/cseed/vep.properties'); ... .write('file:///mnt/lustre/cseed/sample.vds', overwrite=True)); hail: info: Coerced sorted dataset; hail: info: vep: annotated 346 variants; >>> vds = hc.read('file:///mnt/lustre/cseed/sample.vds'); >>> vds.count(); {u'nVariants': 346L, u'nSamples': 100, u'nGenotypes': 34600L}; >>> vds.filter_variants_expr('isDefined(va.vep)').count(); {u'nVariants': 346L, u'nSamples': 100, u'nGenotypes': 34600L}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1558#issuecomment-287177936
Testability,test,tests,"Failing tests likely due to recent commit, should be simple fix and re-push.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1648#issuecomment-294171904
Usability,simpl,simple,"Failing tests likely due to recent commit, should be simple fix and re-push.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1648#issuecomment-294171904
Performance,perform,performance,"FYI, I create a `yDummy` of all zeros in order to very simply reuse the regression utils we have. Effect on performance is negligible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1655#issuecomment-292960736
Usability,simpl,simply,"FYI, I create a `yDummy` of all zeros in order to very simply reuse the regression utils we have. Effect on performance is negligible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1655#issuecomment-292960736
Availability,failure,failure,"@danking and I are giving feedback on this branch. Patrick, the test failure is due to you testing on files in scratch that are only local to your system. You should remove the test annotation @Test on scratch before pushing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-297083904
Testability,test,test,"@danking and I are giving feedback on this branch. Patrick, the test failure is due to you testing on files in scratch that are only local to your system. You should remove the test annotation @Test on scratch before pushing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-297083904
Usability,feedback,feedback,"@danking and I are giving feedback on this branch. Patrick, the test failure is due to you testing on files in scratch that are only local to your system. You should remove the test annotation @Test on scratch before pushing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-297083904
Performance,load,load,"> I thought we were going to load the genome reference upon initializing the HailContext. That is correct. I just want to double-check the VDS reference matches the global one. We won't support multiple datasets with different references in a single Hail session with this change. That's an intentional assumption to make the first version of reference support simpler. . Right, so you should also add a reference option the HailContext constructor (last argument, named with default to GRCh37 not to break backward compatibility) to set the reference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302433569
Usability,simpl,simpler,"> I thought we were going to load the genome reference upon initializing the HailContext. That is correct. I just want to double-check the VDS reference matches the global one. We won't support multiple datasets with different references in a single Hail session with this change. That's an intentional assumption to make the first version of reference support simpler. . Right, so you should also add a reference option the HailContext constructor (last argument, named with default to GRCh37 not to break backward compatibility) to set the reference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1789#issuecomment-302433569
Deployability,pipeline,pipeline,"Ah, you're totally right, this is unnecessary. I'm looking at a pipeline: split_multi, sampleqc. There wasn't a clear indication in the WebUI Spark wasn't recomputing this (it isn't shown as a green dot like persist), but after the job is complete, the shuffle is marked as ""skipped"" and wasn't recomputed. I don't know how long intermediate shuffle results are kept around or if/when they are flushed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1814#issuecomment-301566601
Energy Efficiency,green,green,"Ah, you're totally right, this is unnecessary. I'm looking at a pipeline: split_multi, sampleqc. There wasn't a clear indication in the WebUI Spark wasn't recomputing this (it isn't shown as a green dot like persist), but after the job is complete, the shuffle is marked as ""skipped"" and wasn't recomputed. I don't know how long intermediate shuffle results are kept around or if/when they are flushed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1814#issuecomment-301566601
Usability,clear,clear,"Ah, you're totally right, this is unnecessary. I'm looking at a pipeline: split_multi, sampleqc. There wasn't a clear indication in the WebUI Spark wasn't recomputing this (it isn't shown as a green dot like persist), but after the job is complete, the shuffle is marked as ""skipped"" and wasn't recomputed. I don't know how long intermediate shuffle results are kept around or if/when they are flushed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1814#issuecomment-301566601
Availability,down,downsides,"1. Functionally, it's already possible -- should be able to do `vds.annotate_variants_db('va.dann')` and get the `va.dann.score` annotation in your VDS. Taking a step further, you can even do `vds.annotate_variants_db('va')` and get all of the annotations. It's just a matter of designing the query builder to encourage people using the function in whatever way we think is optimal, I suppose. What are the downsides to carrying around a lot of annotations in a VDS? I worry that if we supplied a select-all `va` option, everybody would just use that -- but if there aren't any major drawbacks, maybe that's the way to go and we don't even really need a query builder. Or maybe just allowing top-level selections like `va.dann`, `va.chromHMM`, etc. would be a good intermediate solution. 2. Yes! I'd definitely be interested in working on a Scala implementation, if one of you would be willing to work with a newbie :). Though I think if we can get this Python version working & usable first, that may be best. 3. I'll look into this. I don't have a solution yet. That's why in the method documentation, I used ; `.. code-block:: python` statements to add example code snippets. With the `>>> ...` syntax, the build was trying to run those examples and throwing weird errors like you anticipated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1914#issuecomment-308780378
Usability,usab,usable,"1. Functionally, it's already possible -- should be able to do `vds.annotate_variants_db('va.dann')` and get the `va.dann.score` annotation in your VDS. Taking a step further, you can even do `vds.annotate_variants_db('va')` and get all of the annotations. It's just a matter of designing the query builder to encourage people using the function in whatever way we think is optimal, I suppose. What are the downsides to carrying around a lot of annotations in a VDS? I worry that if we supplied a select-all `va` option, everybody would just use that -- but if there aren't any major drawbacks, maybe that's the way to go and we don't even really need a query builder. Or maybe just allowing top-level selections like `va.dann`, `va.chromHMM`, etc. would be a good intermediate solution. 2. Yes! I'd definitely be interested in working on a Scala implementation, if one of you would be willing to work with a newbie :). Though I think if we can get this Python version working & usable first, that may be best. 3. I'll look into this. I don't have a solution yet. That's why in the method documentation, I used ; `.. code-block:: python` statements to add example code snippets. With the `>>> ...` syntax, the build was trying to run those examples and throwing weird errors like you anticipated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1914#issuecomment-308780378
Deployability,update,updates,@tpoterba @jigold committed some updates based on your feedback.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1936#issuecomment-314784714
Usability,feedback,feedback,@tpoterba @jigold committed some updates based on your feedback.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1936#issuecomment-314784714
Energy Efficiency,reduce,reduce,"@jigold addressed those changes. . Regarding the margin of the `div.wy-nav-content` element, I'm reducing the padding on the right rather than increasing the max-width, I think that should keep the left margin aligned. Though I think that it might not be a bad idea to reduce the left margin across all of the doc pages. I changed one of the treeview parameters as well, hopefully will help with selection issue you were experiencing. Though it is still a bit finicky in certain situations, usually when selecting/unselecting some combination of parent and child nodes (such as in gnomad.exomes). Selecting child nodes on selection of the parent isn't a basic option in the treeview class unfortunately, and I haven't figured out a way to do it that is completely to my satisfaction yet. The clear selections button seems to reset everything appropriately, at least.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1936#issuecomment-315459328
Usability,clear,clear,"@jigold addressed those changes. . Regarding the margin of the `div.wy-nav-content` element, I'm reducing the padding on the right rather than increasing the max-width, I think that should keep the left margin aligned. Though I think that it might not be a bad idea to reduce the left margin across all of the doc pages. I changed one of the treeview parameters as well, hopefully will help with selection issue you were experiencing. Though it is still a bit finicky in certain situations, usually when selecting/unselecting some combination of parent and child nodes (such as in gnomad.exomes). Selecting child nodes on selection of the parent isn't a basic option in the treeview class unfortunately, and I haven't figured out a way to do it that is completely to my satisfaction yet. The clear selections button seems to reset everything appropriately, at least.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1936#issuecomment-315459328
Usability,clear,clearer,"@danking I changed the homepage banner to ""Hail is hiring engineers!"" so it's clearer. I'll merge now but happy to keep iterating.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1948#issuecomment-311694515
Usability,clear,clear,"@cseed FWIW, Java has a [`BitSet` class](https://docs.oracle.com/javase/8/docs/api/java/util/BitSet.html) that implements dynamically growing BitSets. It implements `set`, `clear` (for every bit), `clear` (your `reset`), and `get` (your `apply`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1955#issuecomment-311982284
Testability,test,test,We'll test this when we add keytable random gens. For now I think this is a clear fix,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1972#issuecomment-316468753
Usability,clear,clear,We'll test this when we add keytable random gens. For now I think this is a clear fix,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1972#issuecomment-316468753
Integrability,interface,interface,"The creeping expansion of the interface is on me as we tried to not break 0.1. I'd appreciate discussing in person what makes the most sense for 0.1. Here are the pieces I think we should separate for devel, though we could consider providing a meta-interface as well that combines some of them for usability. I'm writing (U, S, L) for a local matrix of eigenvectors U, an array of eigenvalues S, and an array of labels L on the rows of U (as with labels for SymmetricMatrix). 1) VDS to a (labeled) symmetric matrix (we have these: GRM, RRM, LD matrix, and Dan is working on a way to read and write them). 2) Symmetric matrix to (U, S, L), which we'll want to write and read. This modularizes the single-core eigen-decomposition bottleneck. 3) Variant-labeled (V, S, L) and VDS with those variants to transport (V, S, L) to sample-labeled (U, S, L). Currently this also requires knowing the number of samples used to make the LDMatrix since that number is used in its normalization. I agree it feels unnatural to need to remember this; to avoid it we'd need an unnormalized version. 4) Sample-labeled (U, S, L) and VDS to global fit of LMM including delta. This is currently a local computation that's been pretty fast in practice but as sample sizes increase we will want to distribute evaluating many values of delta in parallel. Note this step only uses the sample annotations on the VDS, so logically it could also be on KeyTable (which would be the sample KeyTable of the VDS). 5) Sample-labeled (U, S, L), VDS, and delta to per-variant-fit of LMM. This VDS can now contain exactly the variants one wants to fit. (5) should eventually be decomposed as well. The first command should project from Matrix to Matrix (projecting both numeric cells and a list of numeric sample annotations) and some additional small data. Then (6) will do per variant tests starting from after this projection (that is, after what is the BIG computation when you have tens of millions of variants). That way users can",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-319971210
Performance,bottleneck,bottleneck,"The creeping expansion of the interface is on me as we tried to not break 0.1. I'd appreciate discussing in person what makes the most sense for 0.1. Here are the pieces I think we should separate for devel, though we could consider providing a meta-interface as well that combines some of them for usability. I'm writing (U, S, L) for a local matrix of eigenvectors U, an array of eigenvalues S, and an array of labels L on the rows of U (as with labels for SymmetricMatrix). 1) VDS to a (labeled) symmetric matrix (we have these: GRM, RRM, LD matrix, and Dan is working on a way to read and write them). 2) Symmetric matrix to (U, S, L), which we'll want to write and read. This modularizes the single-core eigen-decomposition bottleneck. 3) Variant-labeled (V, S, L) and VDS with those variants to transport (V, S, L) to sample-labeled (U, S, L). Currently this also requires knowing the number of samples used to make the LDMatrix since that number is used in its normalization. I agree it feels unnatural to need to remember this; to avoid it we'd need an unnormalized version. 4) Sample-labeled (U, S, L) and VDS to global fit of LMM including delta. This is currently a local computation that's been pretty fast in practice but as sample sizes increase we will want to distribute evaluating many values of delta in parallel. Note this step only uses the sample annotations on the VDS, so logically it could also be on KeyTable (which would be the sample KeyTable of the VDS). 5) Sample-labeled (U, S, L), VDS, and delta to per-variant-fit of LMM. This VDS can now contain exactly the variants one wants to fit. (5) should eventually be decomposed as well. The first command should project from Matrix to Matrix (projecting both numeric cells and a list of numeric sample annotations) and some additional small data. Then (6) will do per variant tests starting from after this projection (that is, after what is the BIG computation when you have tens of millions of variants). That way users can",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-319971210
Safety,avoid,avoid,"terface is on me as we tried to not break 0.1. I'd appreciate discussing in person what makes the most sense for 0.1. Here are the pieces I think we should separate for devel, though we could consider providing a meta-interface as well that combines some of them for usability. I'm writing (U, S, L) for a local matrix of eigenvectors U, an array of eigenvalues S, and an array of labels L on the rows of U (as with labels for SymmetricMatrix). 1) VDS to a (labeled) symmetric matrix (we have these: GRM, RRM, LD matrix, and Dan is working on a way to read and write them). 2) Symmetric matrix to (U, S, L), which we'll want to write and read. This modularizes the single-core eigen-decomposition bottleneck. 3) Variant-labeled (V, S, L) and VDS with those variants to transport (V, S, L) to sample-labeled (U, S, L). Currently this also requires knowing the number of samples used to make the LDMatrix since that number is used in its normalization. I agree it feels unnatural to need to remember this; to avoid it we'd need an unnormalized version. 4) Sample-labeled (U, S, L) and VDS to global fit of LMM including delta. This is currently a local computation that's been pretty fast in practice but as sample sizes increase we will want to distribute evaluating many values of delta in parallel. Note this step only uses the sample annotations on the VDS, so logically it could also be on KeyTable (which would be the sample KeyTable of the VDS). 5) Sample-labeled (U, S, L), VDS, and delta to per-variant-fit of LMM. This VDS can now contain exactly the variants one wants to fit. (5) should eventually be decomposed as well. The first command should project from Matrix to Matrix (projecting both numeric cells and a list of numeric sample annotations) and some additional small data. Then (6) will do per variant tests starting from after this projection (that is, after what is the BIG computation when you have tens of millions of variants). That way users can quickly iterate on exploration",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-319971210
Testability,log,logically,"ng in person what makes the most sense for 0.1. Here are the pieces I think we should separate for devel, though we could consider providing a meta-interface as well that combines some of them for usability. I'm writing (U, S, L) for a local matrix of eigenvectors U, an array of eigenvalues S, and an array of labels L on the rows of U (as with labels for SymmetricMatrix). 1) VDS to a (labeled) symmetric matrix (we have these: GRM, RRM, LD matrix, and Dan is working on a way to read and write them). 2) Symmetric matrix to (U, S, L), which we'll want to write and read. This modularizes the single-core eigen-decomposition bottleneck. 3) Variant-labeled (V, S, L) and VDS with those variants to transport (V, S, L) to sample-labeled (U, S, L). Currently this also requires knowing the number of samples used to make the LDMatrix since that number is used in its normalization. I agree it feels unnatural to need to remember this; to avoid it we'd need an unnormalized version. 4) Sample-labeled (U, S, L) and VDS to global fit of LMM including delta. This is currently a local computation that's been pretty fast in practice but as sample sizes increase we will want to distribute evaluating many values of delta in parallel. Note this step only uses the sample annotations on the VDS, so logically it could also be on KeyTable (which would be the sample KeyTable of the VDS). 5) Sample-labeled (U, S, L), VDS, and delta to per-variant-fit of LMM. This VDS can now contain exactly the variants one wants to fit. (5) should eventually be decomposed as well. The first command should project from Matrix to Matrix (projecting both numeric cells and a list of numeric sample annotations) and some additional small data. Then (6) will do per variant tests starting from after this projection (that is, after what is the BIG computation when you have tens of millions of variants). That way users can quickly iterate on exploration of covariates, fine mapping, and so on, for any fixed set of samples.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-319971210
Usability,usab,usability,"The creeping expansion of the interface is on me as we tried to not break 0.1. I'd appreciate discussing in person what makes the most sense for 0.1. Here are the pieces I think we should separate for devel, though we could consider providing a meta-interface as well that combines some of them for usability. I'm writing (U, S, L) for a local matrix of eigenvectors U, an array of eigenvalues S, and an array of labels L on the rows of U (as with labels for SymmetricMatrix). 1) VDS to a (labeled) symmetric matrix (we have these: GRM, RRM, LD matrix, and Dan is working on a way to read and write them). 2) Symmetric matrix to (U, S, L), which we'll want to write and read. This modularizes the single-core eigen-decomposition bottleneck. 3) Variant-labeled (V, S, L) and VDS with those variants to transport (V, S, L) to sample-labeled (U, S, L). Currently this also requires knowing the number of samples used to make the LDMatrix since that number is used in its normalization. I agree it feels unnatural to need to remember this; to avoid it we'd need an unnormalized version. 4) Sample-labeled (U, S, L) and VDS to global fit of LMM including delta. This is currently a local computation that's been pretty fast in practice but as sample sizes increase we will want to distribute evaluating many values of delta in parallel. Note this step only uses the sample annotations on the VDS, so logically it could also be on KeyTable (which would be the sample KeyTable of the VDS). 5) Sample-labeled (U, S, L), VDS, and delta to per-variant-fit of LMM. This VDS can now contain exactly the variants one wants to fit. (5) should eventually be decomposed as well. The first command should project from Matrix to Matrix (projecting both numeric cells and a list of numeric sample annotations) and some additional small data. Then (6) will do per variant tests starting from after this projection (that is, after what is the BIG computation when you have tens of millions of variants). That way users can",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-319971210
Deployability,upgrade,upgrade,"Buy it, use it, break it, fix it; Trash it, change it, mail - upgrade it; Charge it, point it, zoom it, press it; Snap it, work it, quick - erase it; Write it, cut it, paste it, save it; Load it, check it, quick - rewrite it; Plug it, play it, burn it, rip it; Drag and drop it, zip - unzip it; Lock it, fill it, call it, find it; View it, code it, jam - unlock it; Surf it, scroll it, pause it, click it; Cross it, crack it, switch - update it; Name it, read it, tune it, print it; Scan it, send it, fax - rename it; Touch it, bring it, pay it, watch it; Turn it, leave it, start - format it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817
Modifiability,rewrite,rewrite,"Buy it, use it, break it, fix it; Trash it, change it, mail - upgrade it; Charge it, point it, zoom it, press it; Snap it, work it, quick - erase it; Write it, cut it, paste it, save it; Load it, check it, quick - rewrite it; Plug it, play it, burn it, rip it; Drag and drop it, zip - unzip it; Lock it, fill it, call it, find it; View it, code it, jam - unlock it; Surf it, scroll it, pause it, click it; Cross it, crack it, switch - update it; Name it, read it, tune it, print it; Scan it, send it, fax - rename it; Touch it, bring it, pay it, watch it; Turn it, leave it, start - format it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817
Performance,tune,tune,"Buy it, use it, break it, fix it; Trash it, change it, mail - upgrade it; Charge it, point it, zoom it, press it; Snap it, work it, quick - erase it; Write it, cut it, paste it, save it; Load it, check it, quick - rewrite it; Plug it, play it, burn it, rip it; Drag and drop it, zip - unzip it; Lock it, fill it, call it, find it; View it, code it, jam - unlock it; Surf it, scroll it, pause it, click it; Cross it, crack it, switch - update it; Name it, read it, tune it, print it; Scan it, send it, fax - rename it; Touch it, bring it, pay it, watch it; Turn it, leave it, start - format it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817
Usability,pause,pause,"Buy it, use it, break it, fix it; Trash it, change it, mail - upgrade it; Charge it, point it, zoom it, press it; Snap it, work it, quick - erase it; Write it, cut it, paste it, save it; Load it, check it, quick - rewrite it; Plug it, play it, burn it, rip it; Drag and drop it, zip - unzip it; Lock it, fill it, call it, find it; View it, code it, jam - unlock it; Surf it, scroll it, pause it, click it; Cross it, crack it, switch - update it; Name it, read it, tune it, print it; Scan it, send it, fax - rename it; Touch it, bring it, pay it, watch it; Turn it, leave it, start - format it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817
Availability,avail,available,"@danking IIUC the TeamCity build is now working with spark-2.1.0 but not spark-2.0.2; (even though running `./gradlew shadowJar archiveZip` on my laptop with spark-2.0.2 works fine.). From looking at the Maven repo; https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20_2.11; and the elasticsearch-spark connector docs; https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html; there's no indication that some versions only support v2.1, though it does say; ```; elasticsearch-hadoop allows Elasticsearch to be used in Spark in two ways: through the dedicated support available since 2.1 or through the Map/Reduce bridge since 2.0. Spark 2.0 is supported in elasticsearch-hadoop since version 5.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2049#issuecomment-320274234
Integrability,bridg,bridge,"@danking IIUC the TeamCity build is now working with spark-2.1.0 but not spark-2.0.2; (even though running `./gradlew shadowJar archiveZip` on my laptop with spark-2.0.2 works fine.). From looking at the Maven repo; https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20_2.11; and the elasticsearch-spark connector docs; https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html; there's no indication that some versions only support v2.1, though it does say; ```; elasticsearch-hadoop allows Elasticsearch to be used in Spark in two ways: through the dedicated support available since 2.1 or through the Map/Reduce bridge since 2.0. Spark 2.0 is supported in elasticsearch-hadoop since version 5.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2049#issuecomment-320274234
Usability,guid,guide,"@danking IIUC the TeamCity build is now working with spark-2.1.0 but not spark-2.0.2; (even though running `./gradlew shadowJar archiveZip` on my laptop with spark-2.0.2 works fine.). From looking at the Maven repo; https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20_2.11; and the elasticsearch-spark connector docs; https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html; there's no indication that some versions only support v2.1, though it does say; ```; elasticsearch-hadoop allows Elasticsearch to be used in Spark in two ways: through the dedicated support available since 2.1 or through the Map/Reduce bridge since 2.0. Spark 2.0 is supported in elasticsearch-hadoop since version 5.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2049#issuecomment-320274234
Availability,error,error,"Regarding the `pyspark` issue, it looks like you have to use `--properties-file` and then put that comma-sepearted list as a newline-separated list in a file. That error message is pyspark's rather terrible way of telling you that it doesn't support a `--properties` option. Regarding the old version of VDS, the `master` branch of hail is now an unstable development branch. If you want a consistent user experience with backwards compatible interfaces, please check out and exclusively use the `0.1` branch. Tim discusses the wider change [here](http://discuss.hail.is/t/deployment-changes-branching-off-for-faster-development/261/1). The VDS format will likely change on the scale of days on the `master` branch. Regarding the `hail/scripts` folder, that is a repository of scripts that our build system uses as templates to create a pre-compiled, ready-to-go distribution that only requires a Spark installation. These distributions are available from the Google Storage API at gs://hail-common/distributions. If you're building from source, I recommend following exactly the steps listed [here](https://hail.is/docs/stable/getting_started.html#building-hail-from-source) so as to avoid any future hiccups. NB: the steps for [Running Hail Locally](https://hail.is/docs/stable/getting_started.html#running-hail-locally) are for using the pre-compiled distribution, not for the result of building hail directly from source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551
Deployability,deploy,deployment-changes-branching-off-for-faster-development,"Regarding the `pyspark` issue, it looks like you have to use `--properties-file` and then put that comma-sepearted list as a newline-separated list in a file. That error message is pyspark's rather terrible way of telling you that it doesn't support a `--properties` option. Regarding the old version of VDS, the `master` branch of hail is now an unstable development branch. If you want a consistent user experience with backwards compatible interfaces, please check out and exclusively use the `0.1` branch. Tim discusses the wider change [here](http://discuss.hail.is/t/deployment-changes-branching-off-for-faster-development/261/1). The VDS format will likely change on the scale of days on the `master` branch. Regarding the `hail/scripts` folder, that is a repository of scripts that our build system uses as templates to create a pre-compiled, ready-to-go distribution that only requires a Spark installation. These distributions are available from the Google Storage API at gs://hail-common/distributions. If you're building from source, I recommend following exactly the steps listed [here](https://hail.is/docs/stable/getting_started.html#building-hail-from-source) so as to avoid any future hiccups. NB: the steps for [Running Hail Locally](https://hail.is/docs/stable/getting_started.html#running-hail-locally) are for using the pre-compiled distribution, not for the result of building hail directly from source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551
Integrability,message,message,"Regarding the `pyspark` issue, it looks like you have to use `--properties-file` and then put that comma-sepearted list as a newline-separated list in a file. That error message is pyspark's rather terrible way of telling you that it doesn't support a `--properties` option. Regarding the old version of VDS, the `master` branch of hail is now an unstable development branch. If you want a consistent user experience with backwards compatible interfaces, please check out and exclusively use the `0.1` branch. Tim discusses the wider change [here](http://discuss.hail.is/t/deployment-changes-branching-off-for-faster-development/261/1). The VDS format will likely change on the scale of days on the `master` branch. Regarding the `hail/scripts` folder, that is a repository of scripts that our build system uses as templates to create a pre-compiled, ready-to-go distribution that only requires a Spark installation. These distributions are available from the Google Storage API at gs://hail-common/distributions. If you're building from source, I recommend following exactly the steps listed [here](https://hail.is/docs/stable/getting_started.html#building-hail-from-source) so as to avoid any future hiccups. NB: the steps for [Running Hail Locally](https://hail.is/docs/stable/getting_started.html#running-hail-locally) are for using the pre-compiled distribution, not for the result of building hail directly from source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551
Safety,avoid,avoid,"Regarding the `pyspark` issue, it looks like you have to use `--properties-file` and then put that comma-sepearted list as a newline-separated list in a file. That error message is pyspark's rather terrible way of telling you that it doesn't support a `--properties` option. Regarding the old version of VDS, the `master` branch of hail is now an unstable development branch. If you want a consistent user experience with backwards compatible interfaces, please check out and exclusively use the `0.1` branch. Tim discusses the wider change [here](http://discuss.hail.is/t/deployment-changes-branching-off-for-faster-development/261/1). The VDS format will likely change on the scale of days on the `master` branch. Regarding the `hail/scripts` folder, that is a repository of scripts that our build system uses as templates to create a pre-compiled, ready-to-go distribution that only requires a Spark installation. These distributions are available from the Google Storage API at gs://hail-common/distributions. If you're building from source, I recommend following exactly the steps listed [here](https://hail.is/docs/stable/getting_started.html#building-hail-from-source) so as to avoid any future hiccups. NB: the steps for [Running Hail Locally](https://hail.is/docs/stable/getting_started.html#running-hail-locally) are for using the pre-compiled distribution, not for the result of building hail directly from source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551
Usability,user experience,user experience,"Regarding the `pyspark` issue, it looks like you have to use `--properties-file` and then put that comma-sepearted list as a newline-separated list in a file. That error message is pyspark's rather terrible way of telling you that it doesn't support a `--properties` option. Regarding the old version of VDS, the `master` branch of hail is now an unstable development branch. If you want a consistent user experience with backwards compatible interfaces, please check out and exclusively use the `0.1` branch. Tim discusses the wider change [here](http://discuss.hail.is/t/deployment-changes-branching-off-for-faster-development/261/1). The VDS format will likely change on the scale of days on the `master` branch. Regarding the `hail/scripts` folder, that is a repository of scripts that our build system uses as templates to create a pre-compiled, ready-to-go distribution that only requires a Spark installation. These distributions are available from the Google Storage API at gs://hail-common/distributions. If you're building from source, I recommend following exactly the steps listed [here](https://hail.is/docs/stable/getting_started.html#building-hail-from-source) so as to avoid any future hiccups. NB: the steps for [Running Hail Locally](https://hail.is/docs/stable/getting_started.html#running-hail-locally) are for using the pre-compiled distribution, not for the result of building hail directly from source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551
Modifiability,variab,variables,"I didn't set that to a value,and kept it by default.; I have no idea about which variables should be set to some value, is there a guide to show all the variables I should set ? I didn't see something like this in the hail website?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338462427
Usability,guid,guide,"I didn't set that to a value,and kept it by default.; I have no idea about which variables should be set to some value, is there a guide to show all the variables I should set ? I didn't see something like this in the hail website?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338462427
Testability,benchmark,benchmarking,"Yeah, OK, this isn't great. Do you have exac sites file snippet you're using somewhere? I clearly need to start doing more systematic benchmarking.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2089#issuecomment-321580378
Usability,clear,clearly,"Yeah, OK, this isn't great. Do you have exac sites file snippet you're using somewhere? I clearly need to start doing more systematic benchmarking.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2089#issuecomment-321580378
Integrability,wrap,wrapper,"As I mentioned, I think this, plus KinshipMatrix and LDMatrix are getting lost in the domain-specific details. I suggest the following structure:; - an abstract Python `Matrix` class for numeric matrices. This should have (at least) three implementations: local, indexed-row and block. It should have read/write methods. It should support at least basic operations: *, +, -. They might not all be supported on all combination of implementations. There should be operations for converting between them. @danking is working on freeing us from Spark matrices and building on Breeze. You might coordinate here.; - a `Vector` class; - a `KeyedMatrix` which has row and column keys with schemas, or possibly a SymmetricKeyedMatrix to start if that is all we need (e.g. for Kinship and LD). This should again have read/write.; - then Eigen is just a KeyedMatrix with a Vector; - I'd nuke Kinship and LD, or if it is necessary to keep n{Samples, Variants}Used, it should be a simple wrapper class with the integer value and the underlying keyed matrix. Get the structure in place to start, don't worry so much about documentation. The user-facing part should be pretty thin/lightweight.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2160#issuecomment-326643889
Usability,simpl,simple,"As I mentioned, I think this, plus KinshipMatrix and LDMatrix are getting lost in the domain-specific details. I suggest the following structure:; - an abstract Python `Matrix` class for numeric matrices. This should have (at least) three implementations: local, indexed-row and block. It should have read/write methods. It should support at least basic operations: *, +, -. They might not all be supported on all combination of implementations. There should be operations for converting between them. @danking is working on freeing us from Spark matrices and building on Breeze. You might coordinate here.; - a `Vector` class; - a `KeyedMatrix` which has row and column keys with schemas, or possibly a SymmetricKeyedMatrix to start if that is all we need (e.g. for Kinship and LD). This should again have read/write.; - then Eigen is just a KeyedMatrix with a Vector; - I'd nuke Kinship and LD, or if it is necessary to keep n{Samples, Variants}Used, it should be a simple wrapper class with the integer value and the underlying keyed matrix. Get the structure in place to start, don't worry so much about documentation. The user-facing part should be pretty thin/lightweight.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2160#issuecomment-326643889
Testability,test,test,I took a look at what we are currently outputting. I think this is relatively straightforward except for the HWE test. I don't know of a multiallelic version of HWE. A simple approach would be to compute HWE for each alternate allele compared to the reference allele. Where this gets tricky is how to handle heterozygotes where the second allele is not the reference allele. Example: 1/2 genotype.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2206#issuecomment-328648857
Usability,simpl,simple,I took a look at what we are currently outputting. I think this is relatively straightforward except for the HWE test. I don't know of a multiallelic version of HWE. A simple approach would be to compute HWE for each alternate allele compared to the reference allele. Where this gets tricky is how to handle heterozygotes where the second allele is not the reference allele. Example: 1/2 genotype.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2206#issuecomment-328648857
Usability,simpl,simple,"These examples suggest to me that the main problem is loss of type specificity in the substituted type. As for digging through the fields of `TVariant`, I personally am not bothered by `vt.gr.inXPar _`. I don't see any simple way to preserve the fact that `vt` will be a `TVariant`. I'll modify it to the approach you suggested.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2226#issuecomment-334178886
Integrability,wrap,wrapper,"Looks my comment got lost! Sorry. I said, I'd prefer we didn't copy the HailContext whole hog, but just write a simple wrapper that calls from hail2.HailContext to hail.HailContext, so something like:. ```; class HailContext:; def __init__(args...):; self.hc1 = hail.HailContext(args...). def import_bgen(args...):; return self.hc1.import_bgen(args...).to_hail2(); ```. etc. I don't think you even need docs unless there is something specifically different between the two. That way, we won't need to maintain two versions for things like doc changes and we won't get confused about which one is the ""real"" HailContext. Then, when we're ready to switch over, we can pull the docs across and throw away the stub.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2244#issuecomment-337717963
Testability,stub,stub,"Looks my comment got lost! Sorry. I said, I'd prefer we didn't copy the HailContext whole hog, but just write a simple wrapper that calls from hail2.HailContext to hail.HailContext, so something like:. ```; class HailContext:; def __init__(args...):; self.hc1 = hail.HailContext(args...). def import_bgen(args...):; return self.hc1.import_bgen(args...).to_hail2(); ```. etc. I don't think you even need docs unless there is something specifically different between the two. That way, we won't need to maintain two versions for things like doc changes and we won't get confused about which one is the ""real"" HailContext. Then, when we're ready to switch over, we can pull the docs across and throw away the stub.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2244#issuecomment-337717963
Usability,simpl,simple,"Looks my comment got lost! Sorry. I said, I'd prefer we didn't copy the HailContext whole hog, but just write a simple wrapper that calls from hail2.HailContext to hail.HailContext, so something like:. ```; class HailContext:; def __init__(args...):; self.hc1 = hail.HailContext(args...). def import_bgen(args...):; return self.hc1.import_bgen(args...).to_hail2(); ```. etc. I don't think you even need docs unless there is something specifically different between the two. That way, we won't need to maintain two versions for things like doc changes and we won't get confused about which one is the ""real"" HailContext. Then, when we're ready to switch over, we can pull the docs across and throw away the stub.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2244#issuecomment-337717963
Performance,perform,performances,"A quick and dirty local test of different performances:; ```; 2017-09-22 18:05:57 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; [Stage 0:> (0 + 10) / 10]2017-09-22 18:05:58 Hail: INFO: Coerced sorted dataset; [Stage 374:==========================================> (3 + 1) / 4]. phi 27.4091310501. 2017-09-22 18:06:24 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; 2017-09-22 18:06:24 Hail: INFO: Coerced sorted dataset; [Stage 735:==========================================> (3 + 1) / 4]. phik2 34.3392460346. 2017-09-22 18:06:58 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; 2017-09-22 18:06:59 Hail: INFO: Coerced sorted dataset; [Stage 1192:==========================================> (3 + 1) / 4]. phik2k0 67.0002729893. 2017-09-22 18:08:05 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; 2017-09-22 18:08:06 Hail: INFO: Coerced sorted dataset; [Stage 1561:==========================================> (3 + 1) / 4]. all 102.006611109. ```. Time is in seconds. The most painful operation is clearly k0, but I bet most people will only want phi, maybe phi and k2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2249#issuecomment-331572653
Testability,test,test,"A quick and dirty local test of different performances:; ```; 2017-09-22 18:05:57 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; [Stage 0:> (0 + 10) / 10]2017-09-22 18:05:58 Hail: INFO: Coerced sorted dataset; [Stage 374:==========================================> (3 + 1) / 4]. phi 27.4091310501. 2017-09-22 18:06:24 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; 2017-09-22 18:06:24 Hail: INFO: Coerced sorted dataset; [Stage 735:==========================================> (3 + 1) / 4]. phik2 34.3392460346. 2017-09-22 18:06:58 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; 2017-09-22 18:06:59 Hail: INFO: Coerced sorted dataset; [Stage 1192:==========================================> (3 + 1) / 4]. phik2k0 67.0002729893. 2017-09-22 18:08:05 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; 2017-09-22 18:08:06 Hail: INFO: Coerced sorted dataset; [Stage 1561:==========================================> (3 + 1) / 4]. all 102.006611109. ```. Time is in seconds. The most painful operation is clearly k0, but I bet most people will only want phi, maybe phi and k2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2249#issuecomment-331572653
Usability,clear,clearly,"A quick and dirty local test of different performances:; ```; 2017-09-22 18:05:57 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; [Stage 0:> (0 + 10) / 10]2017-09-22 18:05:58 Hail: INFO: Coerced sorted dataset; [Stage 374:==========================================> (3 + 1) / 4]. phi 27.4091310501. 2017-09-22 18:06:24 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; 2017-09-22 18:06:24 Hail: INFO: Coerced sorted dataset; [Stage 735:==========================================> (3 + 1) / 4]. phik2 34.3392460346. 2017-09-22 18:06:58 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; 2017-09-22 18:06:59 Hail: INFO: Coerced sorted dataset; [Stage 1192:==========================================> (3 + 1) / 4]. phik2k0 67.0002729893. 2017-09-22 18:08:05 Hail: INFO: baldingnichols: generating genotypes for 20 populations, 1000 samples, and 10000 variants...; 2017-09-22 18:08:06 Hail: INFO: Coerced sorted dataset; [Stage 1561:==========================================> (3 + 1) / 4]. all 102.006611109. ```. Time is in seconds. The most painful operation is clearly k0, but I bet most people will only want phi, maybe phi and k2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2249#issuecomment-331572653
Usability,simpl,simplify,@catoverdrive review after #2238 goes in because this PR will simplify after #2238 merges.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2249#issuecomment-332245478
Usability,clear,clearer,"Sorry, I should have been clearer. I think the first line shouldn't have any ""#""s and for the following lines, ""###"" is fine.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2256#issuecomment-332298462
Energy Efficiency,allocate,allocate,"@catoverdrive I dismissed your review because I added more changes to address your comment on the FIXME. I think addRegionValue now does a minimal amount of work. In particular, if you write add a region value at the top level to the same region (rvb.start(t); rvb.addRegionValue(rv); rvb.end), it doesn't modify the region but simply sets start = rv.offset. This means that rvb.start can't actually do anything, and some other add routines need to check if they need to allocate. This adds some overhead that should get compiled away in the staged version. I also improved the tests to test adding to the same as well as a different region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2299#issuecomment-336900902
Integrability,rout,routines,"@catoverdrive I dismissed your review because I added more changes to address your comment on the FIXME. I think addRegionValue now does a minimal amount of work. In particular, if you write add a region value at the top level to the same region (rvb.start(t); rvb.addRegionValue(rv); rvb.end), it doesn't modify the region but simply sets start = rv.offset. This means that rvb.start can't actually do anything, and some other add routines need to check if they need to allocate. This adds some overhead that should get compiled away in the staged version. I also improved the tests to test adding to the same as well as a different region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2299#issuecomment-336900902
Testability,test,tests,"@catoverdrive I dismissed your review because I added more changes to address your comment on the FIXME. I think addRegionValue now does a minimal amount of work. In particular, if you write add a region value at the top level to the same region (rvb.start(t); rvb.addRegionValue(rv); rvb.end), it doesn't modify the region but simply sets start = rv.offset. This means that rvb.start can't actually do anything, and some other add routines need to check if they need to allocate. This adds some overhead that should get compiled away in the staged version. I also improved the tests to test adding to the same as well as a different region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2299#issuecomment-336900902
Usability,simpl,simply,"@catoverdrive I dismissed your review because I added more changes to address your comment on the FIXME. I think addRegionValue now does a minimal amount of work. In particular, if you write add a region value at the top level to the same region (rvb.start(t); rvb.addRegionValue(rv); rvb.end), it doesn't modify the region but simply sets start = rv.offset. This means that rvb.start can't actually do anything, and some other add routines need to check if they need to allocate. This adds some overhead that should get compiled away in the staged version. I also improved the tests to test adding to the same as well as a different region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2299#issuecomment-336900902
Availability,down,download,"""the implementations should rely directly on java.util.Random"" Umm, why? From my outsiders perspective I would have assumed that high quality software worked on by the Broad Institute would use a half decent Random Number Generator (RNG). . If I had time to spend on this I would be pushing to change it to something else, perhaps the Apache Commons RNG: commons.apache.org/proper/commons-rng/userguide/rng.html I'm not a Java programmer though, and don't really aspire to be. . The C++ standard rand() function is also well known to be quite bad, though C++11 distributions use an ok implementation (I think default is Mersenne Twister, but there are also other options http://en.cppreference.com/w/cpp/numeric/random). In some C code I was replacing rand() and found this nice library: http://www.pcg-random.org/ no Java implementation though http://www.pcg-random.org/download.html#java-implementation T.T, but the PCG algorithm is actually really simple to implement. The PCG site is worth exploring in general to understand the important differences between RNGs. The GNU Scientific Library also provides C/C++ coders with some RNG implementations https://www.gnu.org/software/gsl/manual/html_node/Random-Number-Generation.html . I realize that it is quite early in development (in terms of versioning, 0.2) so maybe this seems like an insignificant thing, but I also hear that it is actively being used, so.... it also may not matter much, I just would like to bring some attention to it in case it hasn't been considered because I know that the RNG is something that is frequently overlooked.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2314#issuecomment-384085976
Usability,simpl,simple,"""the implementations should rely directly on java.util.Random"" Umm, why? From my outsiders perspective I would have assumed that high quality software worked on by the Broad Institute would use a half decent Random Number Generator (RNG). . If I had time to spend on this I would be pushing to change it to something else, perhaps the Apache Commons RNG: commons.apache.org/proper/commons-rng/userguide/rng.html I'm not a Java programmer though, and don't really aspire to be. . The C++ standard rand() function is also well known to be quite bad, though C++11 distributions use an ok implementation (I think default is Mersenne Twister, but there are also other options http://en.cppreference.com/w/cpp/numeric/random). In some C code I was replacing rand() and found this nice library: http://www.pcg-random.org/ no Java implementation though http://www.pcg-random.org/download.html#java-implementation T.T, but the PCG algorithm is actually really simple to implement. The PCG site is worth exploring in general to understand the important differences between RNGs. The GNU Scientific Library also provides C/C++ coders with some RNG implementations https://www.gnu.org/software/gsl/manual/html_node/Random-Number-Generation.html . I realize that it is quite early in development (in terms of versioning, 0.2) so maybe this seems like an insignificant thing, but I also hear that it is actively being used, so.... it also may not matter much, I just would like to bring some attention to it in case it hasn't been considered because I know that the RNG is something that is frequently overlooked.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2314#issuecomment-384085976
Integrability,wrap,wrapper,"I used `RegionValueVariant` to clean up some of the code, and fixed a couple of bugs from #2451 in the process. I also replaced the `aggregatePartitions` method I wrote in e5f87c3 following @danking's comment, which was defined on `OrderedRDD2`, with `aggregateWithContext`, which is defined on a rich wrapper around `RDD`. I put it in the spark package to get access to private methods, making the implementation cleaner. It is now a simple modification of the implementation of `RDD.aggregate`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2423#issuecomment-347624179
Security,access,access,"I used `RegionValueVariant` to clean up some of the code, and fixed a couple of bugs from #2451 in the process. I also replaced the `aggregatePartitions` method I wrote in e5f87c3 following @danking's comment, which was defined on `OrderedRDD2`, with `aggregateWithContext`, which is defined on a rich wrapper around `RDD`. I put it in the spark package to get access to private methods, making the implementation cleaner. It is now a simple modification of the implementation of `RDD.aggregate`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2423#issuecomment-347624179
Usability,simpl,simple,"I used `RegionValueVariant` to clean up some of the code, and fixed a couple of bugs from #2451 in the process. I also replaced the `aggregatePartitions` method I wrote in e5f87c3 following @danking's comment, which was defined on `OrderedRDD2`, with `aggregateWithContext`, which is defined on a rich wrapper around `RDD`. I put it in the spark package to get access to private methods, making the implementation cleaner. It is now a simple modification of the implementation of `RDD.aggregate`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2423#issuecomment-347624179
Usability,simpl,simplified,"> This looks like it would work. Do you think think that after the region value transition, the Variant, AltAllele, etc., classes will be replaced with their associated views?. I think the classes and the views are roughly the same thing. Make the AltAllele, Variant, etc. classes abstract, and have a concrete implementation (in terms of Scala types) and another mutable one in terms of a RegionValue. See my altAllele comment above. I'm not completely happy with the existing type hierarchy. In particular, the way things are stored now, the contig and start are duplicated in the row (pk vs k) and the ref is duplicated per allele. We did this so AltAllele was a sensible object, but I think this can be rethought and simplified, esp. as we move stuff into the expr language. I still think `Locus` is good (but it should be a `Long`, the global genomic position on the reference) and Variant should be split apart where the alleles are just an `Array[String]` which contains all the alleles including ref. Then the row type for a variant indexed data set should look like:. ```; Struct {; locus: Locus [= Long],; alleles: Array[String],; va: ...; gs: Array[...]; }; ```. Then the question becomes, how do we formulate things like `isSNP`? Does it become `isSNP(alleles, i)`? Is it enough to hang the genome reference off the Locus type and the alleles? Also, we don't carry types with Python values which might cause problems, e.g. rendering Locus. @tpoterba might have thoughts on this, too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2434#issuecomment-344634493
Usability,simpl,simplified,@catoverdrive @tpoterba rebased and simplified. have at it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2451#issuecomment-345880084
Integrability,interface,interface,"Overall this is good, but I think we should simplify the interface. 1. Require `entry_to_double`. Don't support genotypes or do normalization. 2. Only have the one version that returns the triple. The user can reannotate the original dataset if that's what they want. 3. Write a `VariantDataset.genotype_matrix_pca` in Python that looks at the `.GT` field and does the necessary normalization before calling `pca`. This should be written completely in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2454#issuecomment-348531819
Usability,simpl,simplify,"Overall this is good, but I think we should simplify the interface. 1. Require `entry_to_double`. Don't support genotypes or do normalization. 2. Only have the one version that returns the triple. The user can reannotate the original dataset if that's what they want. 3. Write a `VariantDataset.genotype_matrix_pca` in Python that looks at the `.GT` field and does the necessary normalization before calling `pca`. This should be written completely in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2454#issuecomment-348531819
Performance,load,loadings,"so I can remove the computeEigenvalues option and just always return them, but I'm less clear on the loadings. @tpoterba are you suggesting that I can remove the computeLoadings option because the computation is lazy? Does passing the KeyTable object through to python count as ""using"", though?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2454#issuecomment-348580540
Usability,clear,clear,"so I can remove the computeEigenvalues option and just always return them, but I'm less clear on the loadings. @tpoterba are you suggesting that I can remove the computeLoadings option because the computation is lazy? Does passing the KeyTable object through to python count as ""using"", though?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2454#issuecomment-348580540
Usability,feedback,feedback,"@jigold you won the PR lottery, definitely ask me if you find something unclear (which may suggest ways to improve it). We can ask for feedback from @cseed or @tpoterba on the Spark question above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2494#issuecomment-348243800
Usability,clear,clear,"Personally, I'm not a fan of the `n` prefix that Spark and friends use everywhere they what ""number of"". I think `rows` (the Breeze naming style) is always clear in context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2542#issuecomment-350358236
Testability,log,logic,"A couple of notes:. - I moved the actual writing of the InsnNodes to the method's InsnList onto the MethodBuilder (MethodBuilder.close()) itself, per Dan's suggestion. This is a little weird because it gets called in fb.classAsBytes(), and so calling it earlier will basically add the instructions again, and we should never do this. I'm thinking of adding some logic to check that a method isn't ""closed"", or at least clearing out the instruction buffer afterwards.; - I want to implement `<init>` in terms of the method builder, but we don't have a way to deal with Unit return types well yet. Dan's made a crack at this as part of #2555, so I'm going to hold off on that until I can use that.; - We realized that the auto-adding of a return op at the end of the method was causing some extra bytecode to be added at the end of the method if you explicitly called Code._return() to return the last Code object in the method. We decided that keeping the return op in MethodBuilder and just not calling _return unless returning in the middle of a method was nicer, since Scala doesn't use return x either.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2569#issuecomment-351811149
Usability,clear,clearing,"A couple of notes:. - I moved the actual writing of the InsnNodes to the method's InsnList onto the MethodBuilder (MethodBuilder.close()) itself, per Dan's suggestion. This is a little weird because it gets called in fb.classAsBytes(), and so calling it earlier will basically add the instructions again, and we should never do this. I'm thinking of adding some logic to check that a method isn't ""closed"", or at least clearing out the instruction buffer afterwards.; - I want to implement `<init>` in terms of the method builder, but we don't have a way to deal with Unit return types well yet. Dan's made a crack at this as part of #2555, so I'm going to hold off on that until I can use that.; - We realized that the auto-adding of a return op at the end of the method was causing some extra bytecode to be added at the end of the method if you explicitly called Code._return() to return the last Code object in the method. We decided that keeping the return op in MethodBuilder and just not calling _return unless returning in the middle of a method was nicer, since Scala doesn't use return x either.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2569#issuecomment-351811149
Deployability,integrat,integrate,"y current rough list of things to be done before hail2 is as usable as hail1. It's still pretty long!. ## Necessary code work:; - Add the rest of the core methods from VDS/KT to api2 (#2591 does most for KT, order_by is the only outstanding KT method that's not moved to table there. Same needs to be done for VDS, this isn't too hard); - Add the non-core methods to `hail.methods` / `hail.genetics.methods`; - some stuff here is much harder than the rest, like `filter_alleles`; - This is mostly just labor, but some require more thought than others, like moving TDT to use hail2 expr; - Support intervals in the `index_*` methods. It's possible now to join by locus, but not using the `annotateLociTable` fast path.; - Move to Python 3 so argument order is preserved; - Test the hail2 api much more rigorously than we do now (at the very least, call each parameter branch for each method!; - Typecheck the expression language. This isn't super trivial, and making a nice system to integrate our `typecheck` module and expressions will require some thoughtful design work.; - Some more organization around the package: monkey patching with `import hail.genetics` is an idea I like, but want to think about the edge cases first. ## Documentation; - Document the `index_*` methods / joins; - Translate the _Hail Overview_ tutorial; - Make new tutorials to replace the 2 expr ones we have; - Fill in docs on api2 methods (they're not all there yet); - Fill in docs on expression language (things like __mul__ on NumericExpression haven't been documented); - Write ""integrative docs"" that provide how-tos for common types of workflows. Show the power of annotate / select / group_by/aggregate, etc. ## Longer term QoL:; - Move over tests to Python as much as possible. I looked at the linear regression suite and it can be moved entirely into Python without many problems.; - Write a type parser in Python. The nested calls into the JVM for Type._from_java make the library feel extremely sluggish on tee",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554
Energy Efficiency,power,power,"not moved to table there. Same needs to be done for VDS, this isn't too hard); - Add the non-core methods to `hail.methods` / `hail.genetics.methods`; - some stuff here is much harder than the rest, like `filter_alleles`; - This is mostly just labor, but some require more thought than others, like moving TDT to use hail2 expr; - Support intervals in the `index_*` methods. It's possible now to join by locus, but not using the `annotateLociTable` fast path.; - Move to Python 3 so argument order is preserved; - Test the hail2 api much more rigorously than we do now (at the very least, call each parameter branch for each method!; - Typecheck the expression language. This isn't super trivial, and making a nice system to integrate our `typecheck` module and expressions will require some thoughtful design work.; - Some more organization around the package: monkey patching with `import hail.genetics` is an idea I like, but want to think about the edge cases first. ## Documentation; - Document the `index_*` methods / joins; - Translate the _Hail Overview_ tutorial; - Make new tutorials to replace the 2 expr ones we have; - Fill in docs on api2 methods (they're not all there yet); - Fill in docs on expression language (things like __mul__ on NumericExpression haven't been documented); - Write ""integrative docs"" that provide how-tos for common types of workflows. Show the power of annotate / select / group_by/aggregate, etc. ## Longer term QoL:; - Move over tests to Python as much as possible. I looked at the linear regression suite and it can be moved entirely into Python without many problems.; - Write a type parser in Python. The nested calls into the JVM for Type._from_java make the library feel extremely sluggish on teensy data.; - Integrate RV with C/C++, so we can transmit data much more efficiently between Python and Java.; - Rethink the expr language function registry, because many functions there can be implemented in terms of others in Python.; - add back in de novo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554
Integrability,integrat,integrate,"y current rough list of things to be done before hail2 is as usable as hail1. It's still pretty long!. ## Necessary code work:; - Add the rest of the core methods from VDS/KT to api2 (#2591 does most for KT, order_by is the only outstanding KT method that's not moved to table there. Same needs to be done for VDS, this isn't too hard); - Add the non-core methods to `hail.methods` / `hail.genetics.methods`; - some stuff here is much harder than the rest, like `filter_alleles`; - This is mostly just labor, but some require more thought than others, like moving TDT to use hail2 expr; - Support intervals in the `index_*` methods. It's possible now to join by locus, but not using the `annotateLociTable` fast path.; - Move to Python 3 so argument order is preserved; - Test the hail2 api much more rigorously than we do now (at the very least, call each parameter branch for each method!; - Typecheck the expression language. This isn't super trivial, and making a nice system to integrate our `typecheck` module and expressions will require some thoughtful design work.; - Some more organization around the package: monkey patching with `import hail.genetics` is an idea I like, but want to think about the edge cases first. ## Documentation; - Document the `index_*` methods / joins; - Translate the _Hail Overview_ tutorial; - Make new tutorials to replace the 2 expr ones we have; - Fill in docs on api2 methods (they're not all there yet); - Fill in docs on expression language (things like __mul__ on NumericExpression haven't been documented); - Write ""integrative docs"" that provide how-tos for common types of workflows. Show the power of annotate / select / group_by/aggregate, etc. ## Longer term QoL:; - Move over tests to Python as much as possible. I looked at the linear regression suite and it can be moved entirely into Python without many problems.; - Write a type parser in Python. The nested calls into the JVM for Type._from_java make the library feel extremely sluggish on tee",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554
Testability,test,tests,"not moved to table there. Same needs to be done for VDS, this isn't too hard); - Add the non-core methods to `hail.methods` / `hail.genetics.methods`; - some stuff here is much harder than the rest, like `filter_alleles`; - This is mostly just labor, but some require more thought than others, like moving TDT to use hail2 expr; - Support intervals in the `index_*` methods. It's possible now to join by locus, but not using the `annotateLociTable` fast path.; - Move to Python 3 so argument order is preserved; - Test the hail2 api much more rigorously than we do now (at the very least, call each parameter branch for each method!; - Typecheck the expression language. This isn't super trivial, and making a nice system to integrate our `typecheck` module and expressions will require some thoughtful design work.; - Some more organization around the package: monkey patching with `import hail.genetics` is an idea I like, but want to think about the edge cases first. ## Documentation; - Document the `index_*` methods / joins; - Translate the _Hail Overview_ tutorial; - Make new tutorials to replace the 2 expr ones we have; - Fill in docs on api2 methods (they're not all there yet); - Fill in docs on expression language (things like __mul__ on NumericExpression haven't been documented); - Write ""integrative docs"" that provide how-tos for common types of workflows. Show the power of annotate / select / group_by/aggregate, etc. ## Longer term QoL:; - Move over tests to Python as much as possible. I looked at the linear regression suite and it can be moved entirely into Python without many problems.; - Write a type parser in Python. The nested calls into the JVM for Type._from_java make the library feel extremely sluggish on teensy data.; - Integrate RV with C/C++, so we can transmit data much more efficiently between Python and Java.; - Rethink the expr language function registry, because many functions there can be implemented in terms of others in Python.; - add back in de novo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554
Usability,usab,usable,"Here's my current rough list of things to be done before hail2 is as usable as hail1. It's still pretty long!. ## Necessary code work:; - Add the rest of the core methods from VDS/KT to api2 (#2591 does most for KT, order_by is the only outstanding KT method that's not moved to table there. Same needs to be done for VDS, this isn't too hard); - Add the non-core methods to `hail.methods` / `hail.genetics.methods`; - some stuff here is much harder than the rest, like `filter_alleles`; - This is mostly just labor, but some require more thought than others, like moving TDT to use hail2 expr; - Support intervals in the `index_*` methods. It's possible now to join by locus, but not using the `annotateLociTable` fast path.; - Move to Python 3 so argument order is preserved; - Test the hail2 api much more rigorously than we do now (at the very least, call each parameter branch for each method!; - Typecheck the expression language. This isn't super trivial, and making a nice system to integrate our `typecheck` module and expressions will require some thoughtful design work.; - Some more organization around the package: monkey patching with `import hail.genetics` is an idea I like, but want to think about the edge cases first. ## Documentation; - Document the `index_*` methods / joins; - Translate the _Hail Overview_ tutorial; - Make new tutorials to replace the 2 expr ones we have; - Fill in docs on api2 methods (they're not all there yet); - Fill in docs on expression language (things like __mul__ on NumericExpression haven't been documented); - Write ""integrative docs"" that provide how-tos for common types of workflows. Show the power of annotate / select / group_by/aggregate, etc. ## Longer term QoL:; - Move over tests to Python as much as possible. I looked at the linear regression suite and it can be moved entirely into Python without many problems.; - Write a type parser in Python. The nested calls into the JVM for Type._from_java make the library feel extremely sluggis",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554
Usability,simpl,simplify,"One more thing: I feel like you can simplify some additional stuff by getting rid of the T parameter on CodeAggregator and call the invoke instance that takes arrays of `Class`es and `Code`s. That way, you don't have to track T. Again, I think you can get the necessary types from the ApplyAggOp instance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2623#issuecomment-359279810
Usability,intuit,intuitive,"Ok, I like all but `view_join_X` - I hate it less than what it was before (i.e. `index_X`) but it's not super intuitive",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2701#issuecomment-357081571
Availability,redundant,redundant,"@catoverdrive this came up while Konrad and I were trying to understand a discrepancy with PCA in python sklearn, which automatically mean centers. This simplest solution would be to add a map that mean centers between irm and computeSVD here:; `val svd = irm.computeSVD(k, computeLoadings)`; But this is redundant when the data is already mean-centered, as in pca_of_normalized_genotypes. Let's discuss when you're back and I can make the changes and update the docs which need some work anyhow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2734#issuecomment-358096966
Deployability,update,update,"@catoverdrive this came up while Konrad and I were trying to understand a discrepancy with PCA in python sklearn, which automatically mean centers. This simplest solution would be to add a map that mean centers between irm and computeSVD here:; `val svd = irm.computeSVD(k, computeLoadings)`; But this is redundant when the data is already mean-centered, as in pca_of_normalized_genotypes. Let's discuss when you're back and I can make the changes and update the docs which need some work anyhow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2734#issuecomment-358096966
Safety,redund,redundant,"@catoverdrive this came up while Konrad and I were trying to understand a discrepancy with PCA in python sklearn, which automatically mean centers. This simplest solution would be to add a map that mean centers between irm and computeSVD here:; `val svd = irm.computeSVD(k, computeLoadings)`; But this is redundant when the data is already mean-centered, as in pca_of_normalized_genotypes. Let's discuss when you're back and I can make the changes and update the docs which need some work anyhow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2734#issuecomment-358096966
Usability,simpl,simplest,"@catoverdrive this came up while Konrad and I were trying to understand a discrepancy with PCA in python sklearn, which automatically mean centers. This simplest solution would be to add a map that mean centers between irm and computeSVD here:; `val svd = irm.computeSVD(k, computeLoadings)`; But this is redundant when the data is already mean-centered, as in pca_of_normalized_genotypes. Let's discuss when you're back and I can make the changes and update the docs which need some work anyhow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2734#issuecomment-358096966
Usability,clear,clear,Thanks for the clear bug report @uqrmaie1! Let me know if you have any more trouble.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763#issuecomment-360660531
Availability,down,down,"I wanted to chime in on this briefly because I think it a good example use case and its design will influence many future methods, so it is important to get the design right. Thoughts:. - the underscore stuff is a non-starter in my opinion, and too clever by half. A lot of my feedback on your stuff is guided by the general heuristic that you should start by writing down the code you want, and then decide how to implement. You'd never want to write this _ stuff if you didn't have to. - I'm still not quite sure what tablify does (in part because the name is too clever by half and in part because it doesn't appear to always return tables). - But I think the idea of tablify is something we want, which is to convert (possibly indexed expressions) back into relational objects (Table, MatrixTable) because the latter support a wider set of operations and don't have the ""source mismatch problem"". Tim and I discussed this yesterday and we suggest the following interface:. ```; t = build_table(); .set_globals(x = 5, batch = batch); .set_rows(locus = locus, aaf = aaf); .build(); ```. and. ```; mt = build_table_matix(); .set_globals(dataset = dataset); .set_rows(locus = locus, aaf = aaf); .set_entries(GT = GT); .build(); ```. where the input expressions for each part must all come from the same source (or be compatible, e.g., constants) and the resulting (matrix) table inherits the keys from the original table. I think there is an unresolved question about how to handle potential name conflicts (e.g. a column key named locus).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2852#issuecomment-363841964
Integrability,interface,interface,"I wanted to chime in on this briefly because I think it a good example use case and its design will influence many future methods, so it is important to get the design right. Thoughts:. - the underscore stuff is a non-starter in my opinion, and too clever by half. A lot of my feedback on your stuff is guided by the general heuristic that you should start by writing down the code you want, and then decide how to implement. You'd never want to write this _ stuff if you didn't have to. - I'm still not quite sure what tablify does (in part because the name is too clever by half and in part because it doesn't appear to always return tables). - But I think the idea of tablify is something we want, which is to convert (possibly indexed expressions) back into relational objects (Table, MatrixTable) because the latter support a wider set of operations and don't have the ""source mismatch problem"". Tim and I discussed this yesterday and we suggest the following interface:. ```; t = build_table(); .set_globals(x = 5, batch = batch); .set_rows(locus = locus, aaf = aaf); .build(); ```. and. ```; mt = build_table_matix(); .set_globals(dataset = dataset); .set_rows(locus = locus, aaf = aaf); .set_entries(GT = GT); .build(); ```. where the input expressions for each part must all come from the same source (or be compatible, e.g., constants) and the resulting (matrix) table inherits the keys from the original table. I think there is an unresolved question about how to handle potential name conflicts (e.g. a column key named locus).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2852#issuecomment-363841964
Modifiability,inherit,inherits,"I wanted to chime in on this briefly because I think it a good example use case and its design will influence many future methods, so it is important to get the design right. Thoughts:. - the underscore stuff is a non-starter in my opinion, and too clever by half. A lot of my feedback on your stuff is guided by the general heuristic that you should start by writing down the code you want, and then decide how to implement. You'd never want to write this _ stuff if you didn't have to. - I'm still not quite sure what tablify does (in part because the name is too clever by half and in part because it doesn't appear to always return tables). - But I think the idea of tablify is something we want, which is to convert (possibly indexed expressions) back into relational objects (Table, MatrixTable) because the latter support a wider set of operations and don't have the ""source mismatch problem"". Tim and I discussed this yesterday and we suggest the following interface:. ```; t = build_table(); .set_globals(x = 5, batch = batch); .set_rows(locus = locus, aaf = aaf); .build(); ```. and. ```; mt = build_table_matix(); .set_globals(dataset = dataset); .set_rows(locus = locus, aaf = aaf); .set_entries(GT = GT); .build(); ```. where the input expressions for each part must all come from the same source (or be compatible, e.g., constants) and the resulting (matrix) table inherits the keys from the original table. I think there is an unresolved question about how to handle potential name conflicts (e.g. a column key named locus).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2852#issuecomment-363841964
Usability,feedback,feedback,"I wanted to chime in on this briefly because I think it a good example use case and its design will influence many future methods, so it is important to get the design right. Thoughts:. - the underscore stuff is a non-starter in my opinion, and too clever by half. A lot of my feedback on your stuff is guided by the general heuristic that you should start by writing down the code you want, and then decide how to implement. You'd never want to write this _ stuff if you didn't have to. - I'm still not quite sure what tablify does (in part because the name is too clever by half and in part because it doesn't appear to always return tables). - But I think the idea of tablify is something we want, which is to convert (possibly indexed expressions) back into relational objects (Table, MatrixTable) because the latter support a wider set of operations and don't have the ""source mismatch problem"". Tim and I discussed this yesterday and we suggest the following interface:. ```; t = build_table(); .set_globals(x = 5, batch = batch); .set_rows(locus = locus, aaf = aaf); .build(); ```. and. ```; mt = build_table_matix(); .set_globals(dataset = dataset); .set_rows(locus = locus, aaf = aaf); .set_entries(GT = GT); .build(); ```. where the input expressions for each part must all come from the same source (or be compatible, e.g., constants) and the resulting (matrix) table inherits the keys from the original table. I think there is an unresolved question about how to handle potential name conflicts (e.g. a column key named locus).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2852#issuecomment-363841964
Deployability,pipeline,pipeline,@cseed I'll close this and just ask for feedback on the branch diff once I have a pipeline working end-to-end and thoughts on proper integration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2890#issuecomment-366455460
Integrability,integrat,integration,@cseed I'll close this and just ask for feedback on the branch diff once I have a pipeline working end-to-end and thoughts on proper integration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2890#issuecomment-366455460
Usability,feedback,feedback,@cseed I'll close this and just ask for feedback on the branch diff once I have a pipeline working end-to-end and thoughts on proper integration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2890#issuecomment-366455460
Usability,simpl,simple,"@jigold addressed your comments and, after seeing how simple it'd be to do, I came around to your point that I might as well make the same change in v1.1. Simplified the docs accordingly. We still may want to drop v1.1 at some point but Cotton agrees there is no urgency while it's not causing problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2930#issuecomment-367113994
Usability,feedback,feedback,the getting_started docs now point to using conda envs with the environment.yml. Thanks for the feedback @verdurin!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2978#issuecomment-378608438
Usability,clear,clear,Should have been clear about that though!!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3004#issuecomment-369286513
Availability,error,error,"your intuition is exactly what we're doing. We look for bind and lambda AST nodes, add those to the declared scope. If we find a `top_level=False` reference not in that scope, we error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3055#issuecomment-369986005
Usability,intuit,intuition,"your intuition is exactly what we're doing. We look for bind and lambda AST nodes, add those to the declared scope. If we find a `top_level=False` reference not in that scope, we error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3055#issuecomment-369986005
Usability,simpl,simplifying,"What do you think about simplifying the name to `annotation`? I think that's just as clear as `annotationFromRG`, especially as the RG is optional.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3062#issuecomment-370081471
Deployability,update,update,will add this to the style guide when I update it,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3065#issuecomment-370182016
Usability,guid,guide,will add this to the style guide when I update it,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3065#issuecomment-370182016
Performance,perform,performance,"Interesting to see the benchmarks, thanks. I didn't realize there were any per-variant usages, I figured these were per-RDD. That makes me more okay with the original, but it's completely up to you. On a side note, I can't wait until we can work in C++, where using library facilities to simplify code isn't such a performance hit!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3094#issuecomment-372723395
Testability,benchmark,benchmarks,"Interesting to see the benchmarks, thanks. I didn't realize there were any per-variant usages, I figured these were per-RDD. That makes me more okay with the original, but it's completely up to you. On a side note, I can't wait until we can work in C++, where using library facilities to simplify code isn't such a performance hit!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3094#issuecomment-372723395
Usability,simpl,simplify,"Interesting to see the benchmarks, thanks. I didn't realize there were any per-variant usages, I figured these were per-RDD. That makes me more okay with the original, but it's completely up to you. On a side note, I can't wait until we can work in C++, where using library facilities to simplify code isn't such a performance hit!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3094#issuecomment-372723395
Performance,cache,cache,@danking do you have an intuition for what the block size and capacity defaults should be for the cache?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3095#issuecomment-371984225
Usability,intuit,intuition,@danking do you have an intuition for what the block size and capacity defaults should be for the cache?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3095#issuecomment-371984225
Usability,feedback,feedback,"Ack, this isn't working as written in cluster mode on GCP due to different meaning of file names locally and through Hadoop. I'll think on it but would also appreciate feedback before I go further on this branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-371968730
Integrability,interoperab,interoperability,"I've removed the Python `tempfile` approach in favor of adding `new_local_temp_file` to utils and a corresponding function to HailContext in Scala, which currently hardcodes `file:///temp` as the local temp directory. It may be more natural to have a localTmpDir on HailContext like we have tmpDir. ; I see there is a notion of local temp files on TempDir on the Scala side, but it doesn't seem to be used on the Python side. I also don't see if/where we wipe temp files on exit. In any case, I've tested that now it all works nicely on GCP, so ready for feedback/review. I think factoring through `tofile` and `fromfile` is useful for wider interoperability for the same reason that NumPy exposes them, but it's also good if you don’t want to actually load the NumPy array into driver memory but just save it to read/copy later, or to load it multiple time without recomputing the BlockMatrix. And I've provided the simpler interface of `to_numpy` and `from_numpy` for the common case. I suspect that (de)serializing over the network and building the local matrix dominates local read/write, so that using a socket isn't going to do much better. I can profile more closely if/when we feel it's high priority to make this faster still.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-372165433
Performance,load,load,"I've removed the Python `tempfile` approach in favor of adding `new_local_temp_file` to utils and a corresponding function to HailContext in Scala, which currently hardcodes `file:///temp` as the local temp directory. It may be more natural to have a localTmpDir on HailContext like we have tmpDir. ; I see there is a notion of local temp files on TempDir on the Scala side, but it doesn't seem to be used on the Python side. I also don't see if/where we wipe temp files on exit. In any case, I've tested that now it all works nicely on GCP, so ready for feedback/review. I think factoring through `tofile` and `fromfile` is useful for wider interoperability for the same reason that NumPy exposes them, but it's also good if you don’t want to actually load the NumPy array into driver memory but just save it to read/copy later, or to load it multiple time without recomputing the BlockMatrix. And I've provided the simpler interface of `to_numpy` and `from_numpy` for the common case. I suspect that (de)serializing over the network and building the local matrix dominates local read/write, so that using a socket isn't going to do much better. I can profile more closely if/when we feel it's high priority to make this faster still.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-372165433
Security,expose,exposes,"I've removed the Python `tempfile` approach in favor of adding `new_local_temp_file` to utils and a corresponding function to HailContext in Scala, which currently hardcodes `file:///temp` as the local temp directory. It may be more natural to have a localTmpDir on HailContext like we have tmpDir. ; I see there is a notion of local temp files on TempDir on the Scala side, but it doesn't seem to be used on the Python side. I also don't see if/where we wipe temp files on exit. In any case, I've tested that now it all works nicely on GCP, so ready for feedback/review. I think factoring through `tofile` and `fromfile` is useful for wider interoperability for the same reason that NumPy exposes them, but it's also good if you don’t want to actually load the NumPy array into driver memory but just save it to read/copy later, or to load it multiple time without recomputing the BlockMatrix. And I've provided the simpler interface of `to_numpy` and `from_numpy` for the common case. I suspect that (de)serializing over the network and building the local matrix dominates local read/write, so that using a socket isn't going to do much better. I can profile more closely if/when we feel it's high priority to make this faster still.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-372165433
Testability,test,tested,"I've removed the Python `tempfile` approach in favor of adding `new_local_temp_file` to utils and a corresponding function to HailContext in Scala, which currently hardcodes `file:///temp` as the local temp directory. It may be more natural to have a localTmpDir on HailContext like we have tmpDir. ; I see there is a notion of local temp files on TempDir on the Scala side, but it doesn't seem to be used on the Python side. I also don't see if/where we wipe temp files on exit. In any case, I've tested that now it all works nicely on GCP, so ready for feedback/review. I think factoring through `tofile` and `fromfile` is useful for wider interoperability for the same reason that NumPy exposes them, but it's also good if you don’t want to actually load the NumPy array into driver memory but just save it to read/copy later, or to load it multiple time without recomputing the BlockMatrix. And I've provided the simpler interface of `to_numpy` and `from_numpy` for the common case. I suspect that (de)serializing over the network and building the local matrix dominates local read/write, so that using a socket isn't going to do much better. I can profile more closely if/when we feel it's high priority to make this faster still.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-372165433
Usability,feedback,feedback,"I've removed the Python `tempfile` approach in favor of adding `new_local_temp_file` to utils and a corresponding function to HailContext in Scala, which currently hardcodes `file:///temp` as the local temp directory. It may be more natural to have a localTmpDir on HailContext like we have tmpDir. ; I see there is a notion of local temp files on TempDir on the Scala side, but it doesn't seem to be used on the Python side. I also don't see if/where we wipe temp files on exit. In any case, I've tested that now it all works nicely on GCP, so ready for feedback/review. I think factoring through `tofile` and `fromfile` is useful for wider interoperability for the same reason that NumPy exposes them, but it's also good if you don’t want to actually load the NumPy array into driver memory but just save it to read/copy later, or to load it multiple time without recomputing the BlockMatrix. And I've provided the simpler interface of `to_numpy` and `from_numpy` for the common case. I suspect that (de)serializing over the network and building the local matrix dominates local read/write, so that using a socket isn't going to do much better. I can profile more closely if/when we feel it's high priority to make this faster still.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-372165433
Usability,undo,undo,"Ok, I think I get it now. The hacky solution (import `hail` in `typecheck`) doesn't seem too bad, especially if we document it so we remember to undo it if we find a better solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3138#issuecomment-373051517
Usability,simpl,simple,"We should discuss the struct ordering in person. I think there are orderings that can be defined on the space of all tuples (since the names don't matter) of arbitrary lengths, which are very helpful in working with changing keys and partition keys. In principle, it should be easy to repartition an OrderedRVD with a longer partition key to a partitioner with a shorter partition key, but currently that doesn't look simple to do. I tried to lay the groundwork here to make that trivial.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3159#issuecomment-373723920
Usability,clear,clear,"To be clear, I'm approving the Python code, and the fact that it runs to completion (will check for correctness shortly). Someone else might want to look over the Scala code changes)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3211#issuecomment-376013809
Availability,toler,tolerance,"I now pass the scores_table through as a Table rather than localizing and passing through colKeys, colKeyType, and scores annotations. The column key can now be any type. Both string and integer keys are tested from Python. However, `requireUniqueSamples` still requires a single string ID (this was the remaining problem of going generic), so I've removed this check and would appreciate feedback on the best approach to checking uniqueness, preferably on the localized `keys` in PCRelate so as not to trigger additional actions. I could use keyType.valuesSimilar to compare any two elements...it's a bit weird to have a tolerance on floats here. As noted, I'm also a bit wary that I'm relying on `scores` from `pca` to be in the same order as the columns on the matrix table. This is currently true, but could change. @danking I think the joins in `fuse` should also be zipPartitions, I've noted it in a FIXME. I'm also concerned that the number of diagonal blocks is an upper bound on parallelism for the matrix multiply. We should be able to fix that by immediately writing and then reading phi.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3211#issuecomment-376385065
Testability,test,tested,"I now pass the scores_table through as a Table rather than localizing and passing through colKeys, colKeyType, and scores annotations. The column key can now be any type. Both string and integer keys are tested from Python. However, `requireUniqueSamples` still requires a single string ID (this was the remaining problem of going generic), so I've removed this check and would appreciate feedback on the best approach to checking uniqueness, preferably on the localized `keys` in PCRelate so as not to trigger additional actions. I could use keyType.valuesSimilar to compare any two elements...it's a bit weird to have a tolerance on floats here. As noted, I'm also a bit wary that I'm relying on `scores` from `pca` to be in the same order as the columns on the matrix table. This is currently true, but could change. @danking I think the joins in `fuse` should also be zipPartitions, I've noted it in a FIXME. I'm also concerned that the number of diagonal blocks is an upper bound on parallelism for the matrix multiply. We should be able to fix that by immediately writing and then reading phi.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3211#issuecomment-376385065
Usability,feedback,feedback,"I now pass the scores_table through as a Table rather than localizing and passing through colKeys, colKeyType, and scores annotations. The column key can now be any type. Both string and integer keys are tested from Python. However, `requireUniqueSamples` still requires a single string ID (this was the remaining problem of going generic), so I've removed this check and would appreciate feedback on the best approach to checking uniqueness, preferably on the localized `keys` in PCRelate so as not to trigger additional actions. I could use keyType.valuesSimilar to compare any two elements...it's a bit weird to have a tolerance on floats here. As noted, I'm also a bit wary that I'm relying on `scores` from `pca` to be in the same order as the columns on the matrix table. This is currently true, but could change. @danking I think the joins in `fuse` should also be zipPartitions, I've noted it in a FIXME. I'm also concerned that the number of diagonal blocks is an upper bound on parallelism for the matrix multiply. We should be able to fix that by immediately writing and then reading phi.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3211#issuecomment-376385065
Deployability,install,installing,"I think we also need to be clear when installing something that will break everyone's local tests (email, dev post, something).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377700023
Testability,test,tests,"I think we also need to be clear when installing something that will break everyone's local tests (email, dev post, something).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377700023
Usability,clear,clear,"I think we also need to be clear when installing something that will break everyone's local tests (email, dev post, something).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377700023
Usability,simpl,simple,"Looking over the tutorial, it looks like the primitive functionality we would need to add are versions of intersect and merge. Once we have ordered point-interval joins (annotateRowsIntervalTable), I think both of these should be simple additions. It looks like most of the other bedtools functionality could be implemented in Python on top of those primitives.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3302#issuecomment-379259962
Usability,simpl,simplified,"Nice, and then steps 4 - 8 will be simplified / accelerated by #3185. I think once you've revised the latter, you'll find it cleaner to work in terms of indices the whole way through (no 7), indexing the MatrixTable rows to apply the final filter in 10.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3335#issuecomment-379866339
Usability,clear,clearly,"With natives, it clearly isn't dominated by the matrix multiply and I get something comparable to master: ~33 (master) vs ~36 (this branch, plus some fixes I will suggest in comments) on an example I cooked up (10K variants already mostly independent, a bad case for ld_prune).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3335#issuecomment-385311049
Deployability,release,released,"I think tying the reset to the iterator is a mistake. First, iterator is the wrong abstraction here. Whole-stage code generation should use the aggregator/array strategy we're using in Emit to generate nothing, conditionals and loops for map, filter and flatMap, respectively. Ideally read ... do stuff ... write will generate an RDD with no per-element iterators at all. I want to make sure this picture is clear. Second, we want to vectorize in the database sense: we want to process multiple rows together in batches. Then overall structure of a stage is a loop over the batches, and and a loop within batches. Thus, the common case should not be we reset after every element, so I think it's the wrong direction to bake it in. The place where we do this should be interface points with the Spark stack which should be looked at with scorn and derision and as the organizing model. Finally, this points to an ongoing difference in our views about the meaning of context. I see context as serving two purposes (neither of which involve reset):. - First, context is a set of resources needed to process a partition that should be released when the partition is complete. For example, I'm working on GenomicsDB which needs to localize a GenomicsDB shard to a local file that needs to be cleaned up when the partition is complete. - Second, it is a way to tell an iterator where to return its value. (This is the ""current"" region business.). I'd be happy to separate these, but I don't see clean way. In no case do I see generic logic to manage the lifetime of regions (e.g. knowing when to call reset) inside the Context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3365#issuecomment-381180739
Integrability,interface,interface,"I think tying the reset to the iterator is a mistake. First, iterator is the wrong abstraction here. Whole-stage code generation should use the aggregator/array strategy we're using in Emit to generate nothing, conditionals and loops for map, filter and flatMap, respectively. Ideally read ... do stuff ... write will generate an RDD with no per-element iterators at all. I want to make sure this picture is clear. Second, we want to vectorize in the database sense: we want to process multiple rows together in batches. Then overall structure of a stage is a loop over the batches, and and a loop within batches. Thus, the common case should not be we reset after every element, so I think it's the wrong direction to bake it in. The place where we do this should be interface points with the Spark stack which should be looked at with scorn and derision and as the organizing model. Finally, this points to an ongoing difference in our views about the meaning of context. I see context as serving two purposes (neither of which involve reset):. - First, context is a set of resources needed to process a partition that should be released when the partition is complete. For example, I'm working on GenomicsDB which needs to localize a GenomicsDB shard to a local file that needs to be cleaned up when the partition is complete. - Second, it is a way to tell an iterator where to return its value. (This is the ""current"" region business.). I'd be happy to separate these, but I don't see clean way. In no case do I see generic logic to manage the lifetime of regions (e.g. knowing when to call reset) inside the Context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3365#issuecomment-381180739
Testability,log,logic,"I think tying the reset to the iterator is a mistake. First, iterator is the wrong abstraction here. Whole-stage code generation should use the aggregator/array strategy we're using in Emit to generate nothing, conditionals and loops for map, filter and flatMap, respectively. Ideally read ... do stuff ... write will generate an RDD with no per-element iterators at all. I want to make sure this picture is clear. Second, we want to vectorize in the database sense: we want to process multiple rows together in batches. Then overall structure of a stage is a loop over the batches, and and a loop within batches. Thus, the common case should not be we reset after every element, so I think it's the wrong direction to bake it in. The place where we do this should be interface points with the Spark stack which should be looked at with scorn and derision and as the organizing model. Finally, this points to an ongoing difference in our views about the meaning of context. I see context as serving two purposes (neither of which involve reset):. - First, context is a set of resources needed to process a partition that should be released when the partition is complete. For example, I'm working on GenomicsDB which needs to localize a GenomicsDB shard to a local file that needs to be cleaned up when the partition is complete. - Second, it is a way to tell an iterator where to return its value. (This is the ""current"" region business.). I'd be happy to separate these, but I don't see clean way. In no case do I see generic logic to manage the lifetime of regions (e.g. knowing when to call reset) inside the Context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3365#issuecomment-381180739
Usability,clear,clear,"I think tying the reset to the iterator is a mistake. First, iterator is the wrong abstraction here. Whole-stage code generation should use the aggregator/array strategy we're using in Emit to generate nothing, conditionals and loops for map, filter and flatMap, respectively. Ideally read ... do stuff ... write will generate an RDD with no per-element iterators at all. I want to make sure this picture is clear. Second, we want to vectorize in the database sense: we want to process multiple rows together in batches. Then overall structure of a stage is a loop over the batches, and and a loop within batches. Thus, the common case should not be we reset after every element, so I think it's the wrong direction to bake it in. The place where we do this should be interface points with the Spark stack which should be looked at with scorn and derision and as the organizing model. Finally, this points to an ongoing difference in our views about the meaning of context. I see context as serving two purposes (neither of which involve reset):. - First, context is a set of resources needed to process a partition that should be released when the partition is complete. For example, I'm working on GenomicsDB which needs to localize a GenomicsDB shard to a local file that needs to be cleaned up when the partition is complete. - Second, it is a way to tell an iterator where to return its value. (This is the ""current"" region business.). I'd be happy to separate these, but I don't see clean way. In no case do I see generic logic to manage the lifetime of regions (e.g. knowing when to call reset) inside the Context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3365#issuecomment-381180739
Usability,clear,clear,Ah. One of them is the region.clear bug rear'ing its head in a new form. At least for that one I know roughly where to look.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3414#issuecomment-383178762
Usability,clear,clear,Ok squashed the LDPrune one. I forgot clear in persist. (Still on the stack is to figure out why not clearing causes all these weird issues.),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3414#issuecomment-383200056
Usability,clear,clearly,"Ah, got it. Pairwise is clearly no good. OK, one last question: what are the using for keys that they can compute the scores purely from the keys?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3425#issuecomment-385088990
Performance,cache,cache,"Timing is not clear, gonna try an LRU cache. I will close until I have time to work on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3450#issuecomment-385008102
Usability,clear,clear,"Timing is not clear, gonna try an LRU cache. I will close until I have time to work on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3450#issuecomment-385008102
Availability,error,error,"@jjfarrell Thanks for the information! This will be very helpful as I try to tease out what the issue is here. Also, I'm sorry my initial response was curt! I was a bit tired at the time and probably shouldn't have been responding to GitHub issues 😅. Hail's version of pc-relate does not identify an initial set of related and unrelated individuals. The R `pcrelate` implementation (the official / reference implementation by the authors of the paper) does this to identify a set of individuals on which to run the principal components analysis. It is not entirely clear to me why this is necessary, and we don't currently have a mechanism for doing so (since pc_relate _is_ our mechanism for determining related and unrelated individuals when there is population structure in the data set). If you have prior knowledge about related samples, you might try filtering to an known unrelated set and computing the scores from that set. I'm curious if that makes any difference in the results. Your invocations look very reasonable. I'll get in touch with the gnomAD team here at the broad to learn more about their experiences with pc_relate and see if I can better understand what's happening with the replicate samples. It's definitely possible there is an implementation error; however, I also want to rule out that the pc_relate model itself isn't breaking down here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-386639531
Usability,clear,clear,"@jjfarrell Thanks for the information! This will be very helpful as I try to tease out what the issue is here. Also, I'm sorry my initial response was curt! I was a bit tired at the time and probably shouldn't have been responding to GitHub issues 😅. Hail's version of pc-relate does not identify an initial set of related and unrelated individuals. The R `pcrelate` implementation (the official / reference implementation by the authors of the paper) does this to identify a set of individuals on which to run the principal components analysis. It is not entirely clear to me why this is necessary, and we don't currently have a mechanism for doing so (since pc_relate _is_ our mechanism for determining related and unrelated individuals when there is population structure in the data set). If you have prior knowledge about related samples, you might try filtering to an known unrelated set and computing the scores from that set. I'm curious if that makes any difference in the results. Your invocations look very reasonable. I'll get in touch with the gnomAD team here at the broad to learn more about their experiences with pc_relate and see if I can better understand what's happening with the replicate samples. It's definitely possible there is an implementation error; however, I also want to rule out that the pc_relate model itself isn't breaking down here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-386639531
Safety,predict,predicts,"Hmm. Am I correctly reading from this that there are 8 of the exact same genome in the dataset?. I am feeling more confident that this is a symptom of the model. Theoretically, if a group of replicates were perfectly separated from the rest of the dataset by a PC or group of PCs, then the estimator for kinship will get zero's because the μ perfectly predicts the genotype.; <img width=""825"" alt=""screen shot 2018-05-07 at 10 44 55 am"" src=""https://user-images.githubusercontent.com/106194/39707912-af6d2bac-51e3-11e8-928b-4dc8d08474b2.png"">. It seems a little odd that 8 samples out of 5000 would manage to get at least one PC to differentiate them from the rest of the dataset. However, if that _is_ happening, then it follows that PC-Relate would dramatically decrease the estimated kinship because all the shared alleles are being marked as markers of ancestral relatedness rather than familial relatedness. Basically, it would be interesting to see the _ancestral_ relatedness as well. If you plot the top 10 PCs and color the replicates a different color, are they clearly separated by any of the PCs?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-387091122
Usability,clear,clearly,"Hmm. Am I correctly reading from this that there are 8 of the exact same genome in the dataset?. I am feeling more confident that this is a symptom of the model. Theoretically, if a group of replicates were perfectly separated from the rest of the dataset by a PC or group of PCs, then the estimator for kinship will get zero's because the μ perfectly predicts the genotype.; <img width=""825"" alt=""screen shot 2018-05-07 at 10 44 55 am"" src=""https://user-images.githubusercontent.com/106194/39707912-af6d2bac-51e3-11e8-928b-4dc8d08474b2.png"">. It seems a little odd that 8 samples out of 5000 would manage to get at least one PC to differentiate them from the rest of the dataset. However, if that _is_ happening, then it follows that PC-Relate would dramatically decrease the estimated kinship because all the shared alleles are being marked as markers of ancestral relatedness rather than familial relatedness. Basically, it would be interesting to see the _ancestral_ relatedness as well. If you plot the top 10 PCs and color the replicates a different color, are they clearly separated by any of the PCs?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-387091122
Usability,clear,clear,"@danking I have a tab delimited file of pc-air pcs to try to run with the hail pc-relate (with header--SampleId, PC1, PC2,...PC10) . But I am not clear on the format of the scores_table.scores object needed to pass to pc-relate. Is it sorted by SampleId or indexed somehow? What are the steps in Hail to create that table from a tab delimited file? . `rel = hl.pc_relate(dataset.GT, 0.01,scores_expr=scores_table[dataset.col_key].scores, min_kinship=0.1)`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-390736100
Availability,down,down,"@jjfarrell Thanks for sharing that! This is really interesting information. I'm quite surprised, but the evidence is pointing to there being a PC that largely separates those 8 replicates from the entire remaining dataset (!!!). I'm personally quite surprised that 8 samples out of thousands could pull this off, but that definitely seems to be the issue, given that the PCs from PC-AiR avoid the issue. Thank you so much for hunting this down! It's very valuable information for us. ### Next Steps . So, clearly we need a solution for users that have substantial numbers of related individuals in their source dataset (especially if the pedigrees are unknown). For your _particular_ use case, I can add a blurb to the docs that recommends removing known replicates _before_ PCA and then projecting them using the loadings from PCA. A longer term solution is to simply implement PC-AiR in hail. I skimmed the implementation section of the paper earlier this week and it looks very straightforward. It seems to boil down to using the KING estimator to estimate relatedness, compute PCA on unrelated individuals, project related individuals into unrelated PC space. Finally, we can use pc_relate to improve on our original estimates of relatedness from KING. The timeline for the latter thing is kind of unclear and a bit further out given some other work I need to finish. I'll get the documentation improvement in this week. Is there anything else I can do that would have helped you avoid this issue? Is there anything else you need to resolve the issue now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-391739992
Performance,load,loadings,"@jjfarrell Thanks for sharing that! This is really interesting information. I'm quite surprised, but the evidence is pointing to there being a PC that largely separates those 8 replicates from the entire remaining dataset (!!!). I'm personally quite surprised that 8 samples out of thousands could pull this off, but that definitely seems to be the issue, given that the PCs from PC-AiR avoid the issue. Thank you so much for hunting this down! It's very valuable information for us. ### Next Steps . So, clearly we need a solution for users that have substantial numbers of related individuals in their source dataset (especially if the pedigrees are unknown). For your _particular_ use case, I can add a blurb to the docs that recommends removing known replicates _before_ PCA and then projecting them using the loadings from PCA. A longer term solution is to simply implement PC-AiR in hail. I skimmed the implementation section of the paper earlier this week and it looks very straightforward. It seems to boil down to using the KING estimator to estimate relatedness, compute PCA on unrelated individuals, project related individuals into unrelated PC space. Finally, we can use pc_relate to improve on our original estimates of relatedness from KING. The timeline for the latter thing is kind of unclear and a bit further out given some other work I need to finish. I'll get the documentation improvement in this week. Is there anything else I can do that would have helped you avoid this issue? Is there anything else you need to resolve the issue now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-391739992
Safety,avoid,avoid,"@jjfarrell Thanks for sharing that! This is really interesting information. I'm quite surprised, but the evidence is pointing to there being a PC that largely separates those 8 replicates from the entire remaining dataset (!!!). I'm personally quite surprised that 8 samples out of thousands could pull this off, but that definitely seems to be the issue, given that the PCs from PC-AiR avoid the issue. Thank you so much for hunting this down! It's very valuable information for us. ### Next Steps . So, clearly we need a solution for users that have substantial numbers of related individuals in their source dataset (especially if the pedigrees are unknown). For your _particular_ use case, I can add a blurb to the docs that recommends removing known replicates _before_ PCA and then projecting them using the loadings from PCA. A longer term solution is to simply implement PC-AiR in hail. I skimmed the implementation section of the paper earlier this week and it looks very straightforward. It seems to boil down to using the KING estimator to estimate relatedness, compute PCA on unrelated individuals, project related individuals into unrelated PC space. Finally, we can use pc_relate to improve on our original estimates of relatedness from KING. The timeline for the latter thing is kind of unclear and a bit further out given some other work I need to finish. I'll get the documentation improvement in this week. Is there anything else I can do that would have helped you avoid this issue? Is there anything else you need to resolve the issue now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-391739992
Usability,clear,clearly,"@jjfarrell Thanks for sharing that! This is really interesting information. I'm quite surprised, but the evidence is pointing to there being a PC that largely separates those 8 replicates from the entire remaining dataset (!!!). I'm personally quite surprised that 8 samples out of thousands could pull this off, but that definitely seems to be the issue, given that the PCs from PC-AiR avoid the issue. Thank you so much for hunting this down! It's very valuable information for us. ### Next Steps . So, clearly we need a solution for users that have substantial numbers of related individuals in their source dataset (especially if the pedigrees are unknown). For your _particular_ use case, I can add a blurb to the docs that recommends removing known replicates _before_ PCA and then projecting them using the loadings from PCA. A longer term solution is to simply implement PC-AiR in hail. I skimmed the implementation section of the paper earlier this week and it looks very straightforward. It seems to boil down to using the KING estimator to estimate relatedness, compute PCA on unrelated individuals, project related individuals into unrelated PC space. Finally, we can use pc_relate to improve on our original estimates of relatedness from KING. The timeline for the latter thing is kind of unclear and a bit further out given some other work I need to finish. I'll get the documentation improvement in this week. Is there anything else I can do that would have helped you avoid this issue? Is there anything else you need to resolve the issue now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-391739992
Testability,test,test,"@danking danking There is a lot of things going on in this dataset that makes PCs challenging. This has 3 replicate related samples (mother and 2 children). It also contains EA, AA and Dominicans. So there is lots of recent admixture. Among the 5000 samples, there are 800 related samples. We ran eigenstrat with the 5000 samples + 2500 1000 genomes samples and those PCs look good. . So I am not sure it is the replicates that are the underlying cause. It may simply be the lack of the 1000 Genomes samples in the dataset for the PCs. I am will try adding the 1000 genomes samples to see if that fixes things. If that works, the docs would just need to recommend merging in the 1000 genomes data. I will get back to you on those results. That would be great to implement PC-Air in Hail to help streamline the processing of this type of dataset! I will be glad to test it out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-391760451
Usability,simpl,simply,"@danking danking There is a lot of things going on in this dataset that makes PCs challenging. This has 3 replicate related samples (mother and 2 children). It also contains EA, AA and Dominicans. So there is lots of recent admixture. Among the 5000 samples, there are 800 related samples. We ran eigenstrat with the 5000 samples + 2500 1000 genomes samples and those PCs look good. . So I am not sure it is the replicates that are the underlying cause. It may simply be the lack of the 1000 Genomes samples in the dataset for the PCs. I am will try adding the 1000 genomes samples to see if that fixes things. If that works, the docs would just need to recommend merging in the 1000 genomes data. I will get back to you on those results. That would be great to implement PC-Air in Hail to help streamline the processing of this type of dataset! I will be glad to test it out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-391760451
Performance,load,loadings,You will use the principal components space that you learned from the unrelated samples. You can project the withheld samples by summing over all variants and multiplying the variant loadings by the withheld samples' GTs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-1284296019
Usability,learn,learned,You will use the principal components space that you learned from the unrelated samples. You can project the withheld samples by summing over all variants and multiplying the variant loadings by the withheld samples' GTs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-1284296019
Availability,error,erroring,"I'd argue this is a nicer UX - Having an ""invalid"" or ""unknown"" type lets people with weird alleles (and people do have weird alleles) actually run their pipelines instead of erroring out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3491#issuecomment-386425195
Deployability,pipeline,pipelines,"I'd argue this is a nicer UX - Having an ""invalid"" or ""unknown"" type lets people with weird alleles (and people do have weird alleles) actually run their pipelines instead of erroring out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3491#issuecomment-386425195
Usability,UX,UX,"I'd argue this is a nicer UX - Having an ""invalid"" or ""unknown"" type lets people with weird alleles (and people do have weird alleles) actually run their pipelines instead of erroring out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3491#issuecomment-386425195
Testability,test,tests,"I also feel like we should have some tests that assert correctness of very simple comparisons. Like 0 < 1, NA != 1, NA == NA. Do these exist in python?. In the pain of my recent work on contextrdd and off heap regions I've spent a lot of time reducing our test cases to actual minimal examples. It would save engineering time in the long run to add simple, tiny examples every time we make changes or add functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3582#issuecomment-389883212
Usability,simpl,simple,"I also feel like we should have some tests that assert correctness of very simple comparisons. Like 0 < 1, NA != 1, NA == NA. Do these exist in python?. In the pain of my recent work on contextrdd and off heap regions I've spent a lot of time reducing our test cases to actual minimal examples. It would save engineering time in the long run to add simple, tiny examples every time we make changes or add functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3582#issuecomment-389883212
Testability,log,logic,"Thanks for the feedback, I've simplified the logic, made the python test more comprehensive, and made element retrieval more direct and applicable to sparse block matrices. I'll clarify that the latter is supported in the sparse case as soon as #3539 goes in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3611#issuecomment-389940384
Usability,feedback,feedback,"Thanks for the feedback, I've simplified the logic, made the python test more comprehensive, and made element retrieval more direct and applicable to sparse block matrices. I'll clarify that the latter is supported in the sparse case as soon as #3539 goes in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3611#issuecomment-389940384
Usability,clear,clear,"To be clear `.select_globals()` fixes the problem, but...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3639#issuecomment-391115483
Performance,perform,performance,"It's good that you documented it in #3706. When fixed I can simplify `tie_breaker` to `hl.signum(r.twice_maf - l.twice_maf)`, but I don't expect that to make a noticeable performance difference in the scheme of the full computation so it's not my highest priority.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704#issuecomment-395117104
Usability,simpl,simplify,"It's good that you documented it in #3706. When fixed I can simplify `tie_breaker` to `hl.signum(r.twice_maf - l.twice_maf)`, but I don't expect that to make a noticeable performance difference in the scheme of the full computation so it's not my highest priority.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704#issuecomment-395117104
Safety,avoid,avoided,"I have a fairly bad case of PTSD around over-use of std::unique_ptr and std::move at; Oracle/Endeca. I think std::unique_ptr<T> is deeply confusing and evil because, in the; simplest terms, it doesn't have the normal semantics of a ""pointer"", i.e. two or more pointers; can refer to a single object. And that problem becomes massively aggravated in the; almost-universal situation of ""borrowing"" a pointer for the duration of a procedure call. Once you let std::unique_ptr<T> into your code, it can creep out into a whole lot of places; where it adds complexity and confusion without solving any real problem. In this particular case, the complexity of managing the memory chunks isn't that hard,; they all get cleaned up by the Region destructor, and adding another layer of software; with the Chunk class seems to obscure rather than clarify what is happening. C++11 added some wonderful features, and some lousy ones. std::unique_ptr is best avoided.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396297556
Usability,simpl,simplest,"I have a fairly bad case of PTSD around over-use of std::unique_ptr and std::move at; Oracle/Endeca. I think std::unique_ptr<T> is deeply confusing and evil because, in the; simplest terms, it doesn't have the normal semantics of a ""pointer"", i.e. two or more pointers; can refer to a single object. And that problem becomes massively aggravated in the; almost-universal situation of ""borrowing"" a pointer for the duration of a procedure call. Once you let std::unique_ptr<T> into your code, it can creep out into a whole lot of places; where it adds complexity and confusion without solving any real problem. In this particular case, the complexity of managing the memory chunks isn't that hard,; they all get cleaned up by the Region destructor, and adding another layer of software; with the Chunk class seems to obscure rather than clarify what is happening. C++11 added some wonderful features, and some lousy ones. std::unique_ptr is best avoided.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396297556
Performance,perform,performance,"On std::unique_ptr, I may be a contrarian, but I don't care what the ""C++ community"" thinks about it.; If you buy into using std::unique_ptr<T>, then everyone who writes or reads the code has to get ; their head around the massively confusing and counter-intuitive concept of move semantics (a ; form of assignment which modifies the source) *and* the somewhat bizarre terminology and syntax; used to express that in C++. And then you get into a whole host of associated design decisions (I'm holding this as a unique_ptr,; but I want to pass it to a function - should I pass it as a raw pointer ? a raw reference ? a reference; to the unique_ptr ?). . I would be fine with that extra learning curve and complexity if unique_ptr<T> solved a difficult; problem. But - by definition! - it doesn't. It only works for the easy case where you have one; pointer to each object. And anywhere that you *might* want to use unique_ptr<T>, shared_ptr<T> provides a superset; of the functionality at only a small extra cost in memory and runtime. So my rule is, if you need; a smart pointer, use shared_ptr<T>. And if there's some place where the memory or performance; cost of shared_ptr<T> is truly proved to be painful, then use a few raw pointers where absolutely; necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396320515
Usability,intuit,intuitive,"On std::unique_ptr, I may be a contrarian, but I don't care what the ""C++ community"" thinks about it.; If you buy into using std::unique_ptr<T>, then everyone who writes or reads the code has to get ; their head around the massively confusing and counter-intuitive concept of move semantics (a ; form of assignment which modifies the source) *and* the somewhat bizarre terminology and syntax; used to express that in C++. And then you get into a whole host of associated design decisions (I'm holding this as a unique_ptr,; but I want to pass it to a function - should I pass it as a raw pointer ? a raw reference ? a reference; to the unique_ptr ?). . I would be fine with that extra learning curve and complexity if unique_ptr<T> solved a difficult; problem. But - by definition! - it doesn't. It only works for the easy case where you have one; pointer to each object. And anywhere that you *might* want to use unique_ptr<T>, shared_ptr<T> provides a superset; of the functionality at only a small extra cost in memory and runtime. So my rule is, if you need; a smart pointer, use shared_ptr<T>. And if there's some place where the memory or performance; cost of shared_ptr<T> is truly proved to be painful, then use a few raw pointers where absolutely; necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396320515
Deployability,integrat,integrated,"I want to respond to some of your objections to unique_ptr. > If you buy into using std::unique_ptr, then everyone who writes or reads the code has to get; their head around the massively confusing and counter-intuitive concept of move semantics (a; form of assignment which modifies the source) and the somewhat bizarre terminology and syntax; used to express that in C++. I agree that move semantics takes getting used to, but I think it is much too integrated into modern C++ to ignore, going far beyond unique_ptr. Writing interfaces that take advantage of move semantics requires understanding rvalue-references in more detail, but for users of those move-enabled interfaces I think the guidelines are easy to teach: a variable will only be modified by moving if it is explicitly tagged with a `std::move`, so all you have to remember is ""after a `std::move(foo)`, the variable `foo` may only be assigned to or deleted."". > And then you get into a whole host of associated design decisions (I'm holding this as a unique_ptr,; but I want to pass it to a function - should I pass it as a raw pointer ? a raw reference ? a reference; to the unique_ptr ?). Keeping in mind the model that letting a function/class `foo` hold a `unique_ptr<Widget>` means explicitly ""`foo` owns this Widget, and is responsible for deleting it or passing ownership somewhere else"", these questions have pretty clear answers. * If a function `bar` takes a `Widget` but isn't concerned with its lifetime management, it should take its argument as a `Widget*` or `Widget&`, with the usual reasoning to choose between them. The caller owns the widget, and the lifetime of `bar` is nested inside that of its caller, so lifetime management isn't an issue.; * If `bar` takes a `Widget` and needs to take ownership, it should take its argument as a `unique_ptr<Widget>`. This serves as documentation that the function is taking over responsibility for deleting the Widget, in a way enforced by the compiler.; * A `unique_ptr<Wid",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396669638
Integrability,integrat,integrated,"I want to respond to some of your objections to unique_ptr. > If you buy into using std::unique_ptr, then everyone who writes or reads the code has to get; their head around the massively confusing and counter-intuitive concept of move semantics (a; form of assignment which modifies the source) and the somewhat bizarre terminology and syntax; used to express that in C++. I agree that move semantics takes getting used to, but I think it is much too integrated into modern C++ to ignore, going far beyond unique_ptr. Writing interfaces that take advantage of move semantics requires understanding rvalue-references in more detail, but for users of those move-enabled interfaces I think the guidelines are easy to teach: a variable will only be modified by moving if it is explicitly tagged with a `std::move`, so all you have to remember is ""after a `std::move(foo)`, the variable `foo` may only be assigned to or deleted."". > And then you get into a whole host of associated design decisions (I'm holding this as a unique_ptr,; but I want to pass it to a function - should I pass it as a raw pointer ? a raw reference ? a reference; to the unique_ptr ?). Keeping in mind the model that letting a function/class `foo` hold a `unique_ptr<Widget>` means explicitly ""`foo` owns this Widget, and is responsible for deleting it or passing ownership somewhere else"", these questions have pretty clear answers. * If a function `bar` takes a `Widget` but isn't concerned with its lifetime management, it should take its argument as a `Widget*` or `Widget&`, with the usual reasoning to choose between them. The caller owns the widget, and the lifetime of `bar` is nested inside that of its caller, so lifetime management isn't an issue.; * If `bar` takes a `Widget` and needs to take ownership, it should take its argument as a `unique_ptr<Widget>`. This serves as documentation that the function is taking over responsibility for deleting the Widget, in a way enforced by the compiler.; * A `unique_ptr<Wid",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396669638
Modifiability,variab,variable,"I want to respond to some of your objections to unique_ptr. > If you buy into using std::unique_ptr, then everyone who writes or reads the code has to get; their head around the massively confusing and counter-intuitive concept of move semantics (a; form of assignment which modifies the source) and the somewhat bizarre terminology and syntax; used to express that in C++. I agree that move semantics takes getting used to, but I think it is much too integrated into modern C++ to ignore, going far beyond unique_ptr. Writing interfaces that take advantage of move semantics requires understanding rvalue-references in more detail, but for users of those move-enabled interfaces I think the guidelines are easy to teach: a variable will only be modified by moving if it is explicitly tagged with a `std::move`, so all you have to remember is ""after a `std::move(foo)`, the variable `foo` may only be assigned to or deleted."". > And then you get into a whole host of associated design decisions (I'm holding this as a unique_ptr,; but I want to pass it to a function - should I pass it as a raw pointer ? a raw reference ? a reference; to the unique_ptr ?). Keeping in mind the model that letting a function/class `foo` hold a `unique_ptr<Widget>` means explicitly ""`foo` owns this Widget, and is responsible for deleting it or passing ownership somewhere else"", these questions have pretty clear answers. * If a function `bar` takes a `Widget` but isn't concerned with its lifetime management, it should take its argument as a `Widget*` or `Widget&`, with the usual reasoning to choose between them. The caller owns the widget, and the lifetime of `bar` is nested inside that of its caller, so lifetime management isn't an issue.; * If `bar` takes a `Widget` and needs to take ownership, it should take its argument as a `unique_ptr<Widget>`. This serves as documentation that the function is taking over responsibility for deleting the Widget, in a way enforced by the compiler.; * A `unique_ptr<Wid",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396669638
Usability,intuit,intuitive,"I want to respond to some of your objections to unique_ptr. > If you buy into using std::unique_ptr, then everyone who writes or reads the code has to get; their head around the massively confusing and counter-intuitive concept of move semantics (a; form of assignment which modifies the source) and the somewhat bizarre terminology and syntax; used to express that in C++. I agree that move semantics takes getting used to, but I think it is much too integrated into modern C++ to ignore, going far beyond unique_ptr. Writing interfaces that take advantage of move semantics requires understanding rvalue-references in more detail, but for users of those move-enabled interfaces I think the guidelines are easy to teach: a variable will only be modified by moving if it is explicitly tagged with a `std::move`, so all you have to remember is ""after a `std::move(foo)`, the variable `foo` may only be assigned to or deleted."". > And then you get into a whole host of associated design decisions (I'm holding this as a unique_ptr,; but I want to pass it to a function - should I pass it as a raw pointer ? a raw reference ? a reference; to the unique_ptr ?). Keeping in mind the model that letting a function/class `foo` hold a `unique_ptr<Widget>` means explicitly ""`foo` owns this Widget, and is responsible for deleting it or passing ownership somewhere else"", these questions have pretty clear answers. * If a function `bar` takes a `Widget` but isn't concerned with its lifetime management, it should take its argument as a `Widget*` or `Widget&`, with the usual reasoning to choose between them. The caller owns the widget, and the lifetime of `bar` is nested inside that of its caller, so lifetime management isn't an issue.; * If `bar` takes a `Widget` and needs to take ownership, it should take its argument as a `unique_ptr<Widget>`. This serves as documentation that the function is taking over responsibility for deleting the Widget, in a way enforced by the compiler.; * A `unique_ptr<Wid",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396669638
Performance,optimiz,optimizing,"ember of the ""getting things done"" community than the; ""modern C++"" community. I have seen std::unique_ptr used in practice, and it was a bad experience.; And I stand by my contention that it doesn't solve any of the hard problems (whereas shared_ptr; very much does). Now I realize that people writing books about C++ write a good deal about move semantics and; unique_ptr. My interpretation is that there's a lot of writing about it because it involves concepts; which simply don't occur in any other commonly-used languages, and as such it requires a; good deal of explanation and justification because it's peculiar and unfamiliar. I suggest that; other languages haven't invented this concept because it's a) confusing and b) not particularly; useful. There's one really good thing you get from move semantics: the ability to resize a std::vector<T>; or std::unordered_map<T> without constructing deep copies of each T object. In the cases; where that's useful, it's very useful for optimizing performance without totally bypassing all your; abstraction mechanisms. The other ways people attempt to exploit move semantics are IMO; just a bad idea: if you want to pass around a large expensive-to-create object, then do it the; Java way by putting it on the heap and passing around some kind of reference, and *everyone* can understand it, not just experts in modern C++. Another angle on this debate would be to look at some open-source C++ projects and see how; often they actually use unique_ptr and/or std::move. My guess is that it's much less common; in practice than you might think from reading books about C++, because the overlap between; ""object ownership is passed around"" and ""... but we always know precisely who has ownership""; is not a very big part of the design space - compared to a whole lot of ""always owned by the object which created it"" and ""used in several places at once and we don't know who will be the last to drop it"". [Update: the LLVM codebase, including tests, is ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396683489
Testability,test,tests,"e any of the hard problems (whereas shared_ptr; very much does). Now I realize that people writing books about C++ write a good deal about move semantics and; unique_ptr. My interpretation is that there's a lot of writing about it because it involves concepts; which simply don't occur in any other commonly-used languages, and as such it requires a; good deal of explanation and justification because it's peculiar and unfamiliar. I suggest that; other languages haven't invented this concept because it's a) confusing and b) not particularly; useful. There's one really good thing you get from move semantics: the ability to resize a std::vector<T>; or std::unordered_map<T> without constructing deep copies of each T object. In the cases; where that's useful, it's very useful for optimizing performance without totally bypassing all your; abstraction mechanisms. The other ways people attempt to exploit move semantics are IMO; just a bad idea: if you want to pass around a large expensive-to-create object, then do it the; Java way by putting it on the heap and passing around some kind of reference, and *everyone* can understand it, not just experts in modern C++. Another angle on this debate would be to look at some open-source C++ projects and see how; often they actually use unique_ptr and/or std::move. My guess is that it's much less common; in practice than you might think from reading books about C++, because the overlap between; ""object ownership is passed around"" and ""... but we always know precisely who has ownership""; is not a very big part of the design space - compared to a whole lot of ""always owned by the object which created it"" and ""used in several places at once and we don't know who will be the last to drop it"". [Update: the LLVM codebase, including tests, is 2.00M lines of C++ (.h and .cpp files), of which; ""unique_ptr"" occurs 4500 times, and ""std::move"" 3558 times. That's a ""unique_ptr"" on average once every 444 lines, and ""std::move"" once every 562 lines.]",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396683489
Usability,simpl,simple,"I guess in simple terms I'm more of a member of the ""getting things done"" community than the; ""modern C++"" community. I have seen std::unique_ptr used in practice, and it was a bad experience.; And I stand by my contention that it doesn't solve any of the hard problems (whereas shared_ptr; very much does). Now I realize that people writing books about C++ write a good deal about move semantics and; unique_ptr. My interpretation is that there's a lot of writing about it because it involves concepts; which simply don't occur in any other commonly-used languages, and as such it requires a; good deal of explanation and justification because it's peculiar and unfamiliar. I suggest that; other languages haven't invented this concept because it's a) confusing and b) not particularly; useful. There's one really good thing you get from move semantics: the ability to resize a std::vector<T>; or std::unordered_map<T> without constructing deep copies of each T object. In the cases; where that's useful, it's very useful for optimizing performance without totally bypassing all your; abstraction mechanisms. The other ways people attempt to exploit move semantics are IMO; just a bad idea: if you want to pass around a large expensive-to-create object, then do it the; Java way by putting it on the heap and passing around some kind of reference, and *everyone* can understand it, not just experts in modern C++. Another angle on this debate would be to look at some open-source C++ projects and see how; often they actually use unique_ptr and/or std::move. My guess is that it's much less common; in practice than you might think from reading books about C++, because the overlap between; ""object ownership is passed around"" and ""... but we always know precisely who has ownership""; is not a very big part of the design space - compared to a whole lot of ""always owned by the object which created it"" and ""used in several places at once and we don't know who will be the last to drop it"". [Update: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396683489
Availability,failure,failure,I've simplified / improved the test to show both modes of failure that indeed occur on master.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3749#issuecomment-396754583
Testability,test,test,I've simplified / improved the test to show both modes of failure that indeed occur on master.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3749#issuecomment-396754583
Usability,simpl,simplified,I've simplified / improved the test to show both modes of failure that indeed occur on master.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3749#issuecomment-396754583
Usability,simpl,simplify,"I want to explore having a KeyedSeqOp IR class instead of the key as an argument to the SeqOp IR. I think that would simplify the code, especially if we have a second kind of SeqOp such as windowed. I can't remember why this didn't work before when I tried it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3824#issuecomment-399712664
Availability,robust,robust,"@liameabbott I think you should go ahead and merge #3859. Once this is in, you can then use `locus_windows` to simplify, reduce memory req, and be more robust to catching out-of-order loci.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3873#issuecomment-401440022
Energy Efficiency,reduce,reduce,"@liameabbott I think you should go ahead and merge #3859. Once this is in, you can then use `locus_windows` to simplify, reduce memory req, and be more robust to catching out-of-order loci.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3873#issuecomment-401440022
Usability,simpl,simplify,"@liameabbott I think you should go ahead and merge #3859. Once this is in, you can then use `locus_windows` to simplify, reduce memory req, and be more robust to catching out-of-order loci.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3873#issuecomment-401440022
Usability,clear,clearer,"@patrick-schultz So, I made the change. It doesn't change the speed to get the keys (which was already at ~ SSD read speed). I'm not sure it's any clearer. Is there another way I could have done it?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3893#issuecomment-403986055
Integrability,interface,interface,"All that messy state twiddling is because Scala's `Iterator` is the wrong model for most things we use it for, which is why I made `FlipbookIterator`. Using that, what you have would become; ```scala; private class BgenRecordStateMachine(; ctx: RVDContext,; p: BgenPartition,; settings: BgenSettings; ) extends StateMachine[RegionValue] {; private[this] val bfis = p.makeInputStream; private[this] val rv = RegionValue(ctx.region); private[this] val rvb = ctx.rvb; ; def isValid: Boolean = p.isValid; def value: RegionValue = rv; def advance() { p.advance(); findNextVariant() }; private def findNextVariant() {; // same as existing advance(), but without advancing p; }. findNextVariant() // make sure iterator is initialized in first valid state; }; ```; giving `BgenPartition` a `FlipbookIterator` style interface, with `isValid`, `value`, and `advance()` instead of `hasNext()` and `next()`. Then to create a new iterator `FlipbookIterator(new BgenRecordStateMachine(...))`. But honestly, what you had was clear enough, so if you benchmarked and the allocation isn't an issue, you should do whatever you find most readable. I've been conditioned to avoid `Option` in low-level code, but I don't have a good intuition for when it is or isn't actually a problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507
Modifiability,extend,extends,"All that messy state twiddling is because Scala's `Iterator` is the wrong model for most things we use it for, which is why I made `FlipbookIterator`. Using that, what you have would become; ```scala; private class BgenRecordStateMachine(; ctx: RVDContext,; p: BgenPartition,; settings: BgenSettings; ) extends StateMachine[RegionValue] {; private[this] val bfis = p.makeInputStream; private[this] val rv = RegionValue(ctx.region); private[this] val rvb = ctx.rvb; ; def isValid: Boolean = p.isValid; def value: RegionValue = rv; def advance() { p.advance(); findNextVariant() }; private def findNextVariant() {; // same as existing advance(), but without advancing p; }. findNextVariant() // make sure iterator is initialized in first valid state; }; ```; giving `BgenPartition` a `FlipbookIterator` style interface, with `isValid`, `value`, and `advance()` instead of `hasNext()` and `next()`. Then to create a new iterator `FlipbookIterator(new BgenRecordStateMachine(...))`. But honestly, what you had was clear enough, so if you benchmarked and the allocation isn't an issue, you should do whatever you find most readable. I've been conditioned to avoid `Option` in low-level code, but I don't have a good intuition for when it is or isn't actually a problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507
Safety,avoid,avoid,"All that messy state twiddling is because Scala's `Iterator` is the wrong model for most things we use it for, which is why I made `FlipbookIterator`. Using that, what you have would become; ```scala; private class BgenRecordStateMachine(; ctx: RVDContext,; p: BgenPartition,; settings: BgenSettings; ) extends StateMachine[RegionValue] {; private[this] val bfis = p.makeInputStream; private[this] val rv = RegionValue(ctx.region); private[this] val rvb = ctx.rvb; ; def isValid: Boolean = p.isValid; def value: RegionValue = rv; def advance() { p.advance(); findNextVariant() }; private def findNextVariant() {; // same as existing advance(), but without advancing p; }. findNextVariant() // make sure iterator is initialized in first valid state; }; ```; giving `BgenPartition` a `FlipbookIterator` style interface, with `isValid`, `value`, and `advance()` instead of `hasNext()` and `next()`. Then to create a new iterator `FlipbookIterator(new BgenRecordStateMachine(...))`. But honestly, what you had was clear enough, so if you benchmarked and the allocation isn't an issue, you should do whatever you find most readable. I've been conditioned to avoid `Option` in low-level code, but I don't have a good intuition for when it is or isn't actually a problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507
Testability,benchmark,benchmarked,"All that messy state twiddling is because Scala's `Iterator` is the wrong model for most things we use it for, which is why I made `FlipbookIterator`. Using that, what you have would become; ```scala; private class BgenRecordStateMachine(; ctx: RVDContext,; p: BgenPartition,; settings: BgenSettings; ) extends StateMachine[RegionValue] {; private[this] val bfis = p.makeInputStream; private[this] val rv = RegionValue(ctx.region); private[this] val rvb = ctx.rvb; ; def isValid: Boolean = p.isValid; def value: RegionValue = rv; def advance() { p.advance(); findNextVariant() }; private def findNextVariant() {; // same as existing advance(), but without advancing p; }. findNextVariant() // make sure iterator is initialized in first valid state; }; ```; giving `BgenPartition` a `FlipbookIterator` style interface, with `isValid`, `value`, and `advance()` instead of `hasNext()` and `next()`. Then to create a new iterator `FlipbookIterator(new BgenRecordStateMachine(...))`. But honestly, what you had was clear enough, so if you benchmarked and the allocation isn't an issue, you should do whatever you find most readable. I've been conditioned to avoid `Option` in low-level code, but I don't have a good intuition for when it is or isn't actually a problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507
Usability,clear,clear,"All that messy state twiddling is because Scala's `Iterator` is the wrong model for most things we use it for, which is why I made `FlipbookIterator`. Using that, what you have would become; ```scala; private class BgenRecordStateMachine(; ctx: RVDContext,; p: BgenPartition,; settings: BgenSettings; ) extends StateMachine[RegionValue] {; private[this] val bfis = p.makeInputStream; private[this] val rv = RegionValue(ctx.region); private[this] val rvb = ctx.rvb; ; def isValid: Boolean = p.isValid; def value: RegionValue = rv; def advance() { p.advance(); findNextVariant() }; private def findNextVariant() {; // same as existing advance(), but without advancing p; }. findNextVariant() // make sure iterator is initialized in first valid state; }; ```; giving `BgenPartition` a `FlipbookIterator` style interface, with `isValid`, `value`, and `advance()` instead of `hasNext()` and `next()`. Then to create a new iterator `FlipbookIterator(new BgenRecordStateMachine(...))`. But honestly, what you had was clear enough, so if you benchmarked and the allocation isn't an issue, you should do whatever you find most readable. I've been conditioned to avoid `Option` in low-level code, but I don't have a good intuition for when it is or isn't actually a problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507
Usability,clear,clearer,We could add a warning to the docs to make it even clearer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3912#issuecomment-403844335
Usability,clear,clearer,closing this because i ripped some stuff out prematurely and i'm not sure it's actually clearer to have this as a separate PR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3939#issuecomment-406155895
Usability,simpl,simpler,"Great, thanks Jackie! Do you remember about how long the import step took for 0.1?. Second, you're running many linear regressions, right? If those still fail (or run much slower than 0.1), can you also try just a single linear regression? That's a simpler baseline to start with.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3945#issuecomment-405978394
Availability,down,download," directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of linux, we can reevaluate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414
Deployability,pipeline,pipelines,"Strange, I can't reply to directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414
Modifiability,config,configuration,"Strange, I can't reply to directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414
Performance,latency,latency," directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of linux, we can reevaluate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414
Safety,risk,risks,"Strange, I can't reply to directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414
Testability,test,testing,"Strange, I can't reply to directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414
Usability,clear,clear," directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of linux, we can reevaluate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414
Security,access,access,"The default is to put the modules in ${HOME}/hail_modules, which in many; environments; would indeed be an NFS directory. It only goes to /tmp/hail_modules if; ${HOME} is undefined. The thinking behind this is that there's a huge amount of re-use of code; for an individual; from one Hail analysis to the next, but probably much less overlap between; different users.; And while multi-user sharing ought to work, it raises potential issues; about file access; permissions which seemed like trouble without a clear benefit. On Fri, Aug 3, 2018 at 10:49 AM Patrick Schultz <notifications@github.com>; wrote:. > *@patrick-schultz* commented on this pull request.; > ------------------------------; >; > In src/main/c/NativeModule.cpp; > <https://github.com/hail-is/hail/pull/3973#discussion_r207567962>:; >; > > +#include <string>; > +#include <vector>; > +; > +#if 0; > +#define D(fmt, ...) { \; > + char buf[1024]; \; > + sprintf(buf, fmt, ##__VA_ARGS__); \; > + fprintf(stderr, ""DEBUG: %s,%d: %s"", __FILE__, __LINE__, buf); \; > +}; > +#else; > +#define D(fmt, ...) { }; > +#endif; > +; > +namespace hail {; > +; > +namespace {; >; > The anonymous namespace can't be named, so no names introduced in an; > anonymous namespace can be referenced from outside the namespace. The; > exception is that on closing an anonymous namespace, it is automatically; > opened into the enclosing namespace. The typical use is to make things; > file-local.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/3973#discussion_r207567962>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AJzExuxprnaVF62eonAgCjSmqAERvBJiks5uNGLtgaJpZM4VbZpP>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410321049
Usability,clear,clear,"The default is to put the modules in ${HOME}/hail_modules, which in many; environments; would indeed be an NFS directory. It only goes to /tmp/hail_modules if; ${HOME} is undefined. The thinking behind this is that there's a huge amount of re-use of code; for an individual; from one Hail analysis to the next, but probably much less overlap between; different users.; And while multi-user sharing ought to work, it raises potential issues; about file access; permissions which seemed like trouble without a clear benefit. On Fri, Aug 3, 2018 at 10:49 AM Patrick Schultz <notifications@github.com>; wrote:. > *@patrick-schultz* commented on this pull request.; > ------------------------------; >; > In src/main/c/NativeModule.cpp; > <https://github.com/hail-is/hail/pull/3973#discussion_r207567962>:; >; > > +#include <string>; > +#include <vector>; > +; > +#if 0; > +#define D(fmt, ...) { \; > + char buf[1024]; \; > + sprintf(buf, fmt, ##__VA_ARGS__); \; > + fprintf(stderr, ""DEBUG: %s,%d: %s"", __FILE__, __LINE__, buf); \; > +}; > +#else; > +#define D(fmt, ...) { }; > +#endif; > +; > +namespace hail {; > +; > +namespace {; >; > The anonymous namespace can't be named, so no names introduced in an; > anonymous namespace can be referenced from outside the namespace. The; > exception is that on closing an anonymous namespace, it is automatically; > opened into the enclosing namespace. The typical use is to make things; > file-local.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/3973#discussion_r207567962>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AJzExuxprnaVF62eonAgCjSmqAERvBJiks5uNGLtgaJpZM4VbZpP>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410321049
Availability,down,down,"> The thinking behind this is that there's a huge amount of re-use of code for an individual from one Hail analysis to the next. So I'm not sure I buy this. Hail is a exploratory data analysis platform, it isn't a SQL engine running nightly billing reports. In particular, as we do whole stage optimization and code generation. It isn't clear to me how you do code-reuse when we're specializing each operation into the global context (happy to hear the plan). For example, tables that have many fields, you will need to load different ones for different queries and in general it is infeasible (exponential) to generate them all. Also, to get sharing you need to break the code up and now you're running the compiler multiple times which also seems bad. Given our focus on large-scale analysis, introducing optimization boundaries for code reuse seems like a bad trade off to me. A significant amount of analysis happens in the cloud where $HOME is ephemeral so you won't get savings between sessions. Finally, there are pipelines that are more standardized but it is my impression they are run on extremely large datasets (hours, overnight) in which case compilation speed isn't important. Finally, there's the complexity around locking that isn't easy and have real technical risk. A compiler cache potentially becomes more appealing in the context of an always-on service. There's no locking issue, and you can start to do things like speculative compilation (e.g. immediately start compiling the decoder (the full decoder? Hmm.) when a user opens the dataset.). I would say getting in a 3x decoder improvement is way more important than this. I would have punted it down the road and instrumented to estimate cache hit rates before building this out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596
Deployability,pipeline,pipelines,"> The thinking behind this is that there's a huge amount of re-use of code for an individual from one Hail analysis to the next. So I'm not sure I buy this. Hail is a exploratory data analysis platform, it isn't a SQL engine running nightly billing reports. In particular, as we do whole stage optimization and code generation. It isn't clear to me how you do code-reuse when we're specializing each operation into the global context (happy to hear the plan). For example, tables that have many fields, you will need to load different ones for different queries and in general it is infeasible (exponential) to generate them all. Also, to get sharing you need to break the code up and now you're running the compiler multiple times which also seems bad. Given our focus on large-scale analysis, introducing optimization boundaries for code reuse seems like a bad trade off to me. A significant amount of analysis happens in the cloud where $HOME is ephemeral so you won't get savings between sessions. Finally, there are pipelines that are more standardized but it is my impression they are run on extremely large datasets (hours, overnight) in which case compilation speed isn't important. Finally, there's the complexity around locking that isn't easy and have real technical risk. A compiler cache potentially becomes more appealing in the context of an always-on service. There's no locking issue, and you can start to do things like speculative compilation (e.g. immediately start compiling the decoder (the full decoder? Hmm.) when a user opens the dataset.). I would say getting in a 3x decoder improvement is way more important than this. I would have punted it down the road and instrumented to estimate cache hit rates before building this out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596
Performance,optimiz,optimization,"> The thinking behind this is that there's a huge amount of re-use of code for an individual from one Hail analysis to the next. So I'm not sure I buy this. Hail is a exploratory data analysis platform, it isn't a SQL engine running nightly billing reports. In particular, as we do whole stage optimization and code generation. It isn't clear to me how you do code-reuse when we're specializing each operation into the global context (happy to hear the plan). For example, tables that have many fields, you will need to load different ones for different queries and in general it is infeasible (exponential) to generate them all. Also, to get sharing you need to break the code up and now you're running the compiler multiple times which also seems bad. Given our focus on large-scale analysis, introducing optimization boundaries for code reuse seems like a bad trade off to me. A significant amount of analysis happens in the cloud where $HOME is ephemeral so you won't get savings between sessions. Finally, there are pipelines that are more standardized but it is my impression they are run on extremely large datasets (hours, overnight) in which case compilation speed isn't important. Finally, there's the complexity around locking that isn't easy and have real technical risk. A compiler cache potentially becomes more appealing in the context of an always-on service. There's no locking issue, and you can start to do things like speculative compilation (e.g. immediately start compiling the decoder (the full decoder? Hmm.) when a user opens the dataset.). I would say getting in a 3x decoder improvement is way more important than this. I would have punted it down the road and instrumented to estimate cache hit rates before building this out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596
Safety,risk,risk,"> The thinking behind this is that there's a huge amount of re-use of code for an individual from one Hail analysis to the next. So I'm not sure I buy this. Hail is a exploratory data analysis platform, it isn't a SQL engine running nightly billing reports. In particular, as we do whole stage optimization and code generation. It isn't clear to me how you do code-reuse when we're specializing each operation into the global context (happy to hear the plan). For example, tables that have many fields, you will need to load different ones for different queries and in general it is infeasible (exponential) to generate them all. Also, to get sharing you need to break the code up and now you're running the compiler multiple times which also seems bad. Given our focus on large-scale analysis, introducing optimization boundaries for code reuse seems like a bad trade off to me. A significant amount of analysis happens in the cloud where $HOME is ephemeral so you won't get savings between sessions. Finally, there are pipelines that are more standardized but it is my impression they are run on extremely large datasets (hours, overnight) in which case compilation speed isn't important. Finally, there's the complexity around locking that isn't easy and have real technical risk. A compiler cache potentially becomes more appealing in the context of an always-on service. There's no locking issue, and you can start to do things like speculative compilation (e.g. immediately start compiling the decoder (the full decoder? Hmm.) when a user opens the dataset.). I would say getting in a 3x decoder improvement is way more important than this. I would have punted it down the road and instrumented to estimate cache hit rates before building this out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596
Usability,clear,clear,"> The thinking behind this is that there's a huge amount of re-use of code for an individual from one Hail analysis to the next. So I'm not sure I buy this. Hail is a exploratory data analysis platform, it isn't a SQL engine running nightly billing reports. In particular, as we do whole stage optimization and code generation. It isn't clear to me how you do code-reuse when we're specializing each operation into the global context (happy to hear the plan). For example, tables that have many fields, you will need to load different ones for different queries and in general it is infeasible (exponential) to generate them all. Also, to get sharing you need to break the code up and now you're running the compiler multiple times which also seems bad. Given our focus on large-scale analysis, introducing optimization boundaries for code reuse seems like a bad trade off to me. A significant amount of analysis happens in the cloud where $HOME is ephemeral so you won't get savings between sessions. Finally, there are pipelines that are more standardized but it is my impression they are run on extremely large datasets (hours, overnight) in which case compilation speed isn't important. Finally, there's the complexity around locking that isn't easy and have real technical risk. A compiler cache potentially becomes more appealing in the context of an always-on service. There's no locking issue, and you can start to do things like speculative compilation (e.g. immediately start compiling the decoder (the full decoder? Hmm.) when a user opens the dataset.). I would say getting in a 3x decoder improvement is way more important than this. I would have punted it down the road and instrumented to estimate cache hit rates before building this out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596
Integrability,depend,depend,"iltering in various; ways, then that may not be changed at all by changes to what happen in the real analysis; after the filtering. And this is also influenced by the medium-term goal of having a Hail service; which (amongst other things) can do simple analyses on small data in under ten seconds - in; that realm compilation time could become a critical factor as a serial bottleneck. [The place where persistent cacheing of compiled files helps most of all is in testing,; where you really are running the exact same queries over and over again on the same; small datasets, and in many cases after making small changes which only affect a few; of the queries]. b) We may actually have some version of the locking problem even if we don't try to reuse the files -; since we have several workers on a node, and possibly a master as well, all needing to put code; into a file (or wait for someone else to populate the file) so that they can load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could manage it with nothing but atomic file-create; and atomic-rename, but that didn't quite pan out). As for writing LLVM IR, it can definitely be done, because that's what Endeca/Oracle did. But there; was such a huge learning curve that only 3 people ever did it successfully (I wasn't one of them),; and debugging seemed very unpleasant and slow. [It was also a masterful achievement in ; job-security-through-obscurity, because no-one in management was going to mess with the; two people who wrote it - u",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385
Performance,cache,cacheing,"Replying to cseed on code-reuse and cacheing/locking. a) Even with whole-stage codegen, there's a possibility that during development a user; will be tweaking a query in ways which don't change all the stages. And in that case the re-use; would give hits on some stages. The plan is not really to aim at structuring things to get a; high level of re-use, but just to opportunistically exploit re-use which happens to occur -; e.g. if the early stages of an analysis involve reading an existing file and filtering in various; ways, then that may not be changed at all by changes to what happen in the real analysis; after the filtering. And this is also influenced by the medium-term goal of having a Hail service; which (amongst other things) can do simple analyses on small data in under ten seconds - in; that realm compilation time could become a critical factor as a serial bottleneck. [The place where persistent cacheing of compiled files helps most of all is in testing,; where you really are running the exact same queries over and over again on the same; small datasets, and in many cases after making small changes which only affect a few; of the queries]. b) We may actually have some version of the locking problem even if we don't try to reuse the files -; since we have several workers on a node, and possibly a master as well, all needing to put code; into a file (or wait for someone else to populate the file) so that they can load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385
Security,secur,security-through-obscurity,"n load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could manage it with nothing but atomic file-create; and atomic-rename, but that didn't quite pan out). As for writing LLVM IR, it can definitely be done, because that's what Endeca/Oracle did. But there; was such a huge learning curve that only 3 people ever did it successfully (I wasn't one of them),; and debugging seemed very unpleasant and slow. [It was also a masterful achievement in ; job-security-through-obscurity, because no-one in management was going to mess with the; two people who wrote it - until the whole project got canned]. ... and in the time I was there, the Endeca/Oracle stuff wasn't distributed, which could be another; place where the generate-LLVM-IR needs some kind of extra glue for distributing compiled code,; whereas the conventional DLL's are trivial to ship around. Not claiming that part of it is difficult,; just that it didn't happen at Oracle until long after I left. In contrast, at PhysicsSpeed it was fairly smooth to implement nice abstractions (dense-join-table,; hash-join-table, tuple-with-order) as template classes which could be tested and debugged; in a standalone environment, and then have simpler codegen using those abstractions. At least,; that's a good way to get a lot of functionality with modest effort - and it doesn't preclude migrating; towards a more complex codegen later. It's nice to be able to have templates as low-runtime-cost; abstractions, but you don't have to use them if yo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385
Testability,test,testing,"euse and cacheing/locking. a) Even with whole-stage codegen, there's a possibility that during development a user; will be tweaking a query in ways which don't change all the stages. And in that case the re-use; would give hits on some stages. The plan is not really to aim at structuring things to get a; high level of re-use, but just to opportunistically exploit re-use which happens to occur -; e.g. if the early stages of an analysis involve reading an existing file and filtering in various; ways, then that may not be changed at all by changes to what happen in the real analysis; after the filtering. And this is also influenced by the medium-term goal of having a Hail service; which (amongst other things) can do simple analyses on small data in under ten seconds - in; that realm compilation time could become a critical factor as a serial bottleneck. [The place where persistent cacheing of compiled files helps most of all is in testing,; where you really are running the exact same queries over and over again on the same; small datasets, and in many cases after making small changes which only affect a few; of the queries]. b) We may actually have some version of the locking problem even if we don't try to reuse the files -; since we have several workers on a node, and possibly a master as well, all needing to put code; into a file (or wait for someone else to populate the file) so that they can load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could manage it with nothing but a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385
Usability,simpl,simple,"Replying to cseed on code-reuse and cacheing/locking. a) Even with whole-stage codegen, there's a possibility that during development a user; will be tweaking a query in ways which don't change all the stages. And in that case the re-use; would give hits on some stages. The plan is not really to aim at structuring things to get a; high level of re-use, but just to opportunistically exploit re-use which happens to occur -; e.g. if the early stages of an analysis involve reading an existing file and filtering in various; ways, then that may not be changed at all by changes to what happen in the real analysis; after the filtering. And this is also influenced by the medium-term goal of having a Hail service; which (amongst other things) can do simple analyses on small data in under ten seconds - in; that realm compilation time could become a critical factor as a serial bottleneck. [The place where persistent cacheing of compiled files helps most of all is in testing,; where you really are running the exact same queries over and over again on the same; small datasets, and in many cases after making small changes which only affect a few; of the queries]. b) We may actually have some version of the locking problem even if we don't try to reuse the files -; since we have several workers on a node, and possibly a master as well, all needing to put code; into a file (or wait for someone else to populate the file) so that they can load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385
Safety,safe,safe,"On my rowstore1 branch I now have it working with careful use of flock() (to steer clear of the cases; where NFS behavior diverges from local filesystems, using perl's rename command (which should; be safe assuming it uses the POSIX rename() syscall) to avoid having to lock/unlock from a Makefile. Also a bunch of Makefile changes to use commands from /bin or /usr/bin when they exist, but; otherwise to give a warning and pick up whatever might be found on $PATH. That seems a suitable; compromise between avoid-mysterious-behavior and give-best-effort-on-nonstandard-platform.; [In doing so, I noticed that I actually was picking up /Users/rcownie/anaconda2/envs/py36/bin/curl; rather than /usr/bin/curl - and I don't know whether there's any difference]. But current consensus is that we should figure out how to ship with a known-good compiler; and libraries, so I'm looking into that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413647683
Usability,clear,clear,"On my rowstore1 branch I now have it working with careful use of flock() (to steer clear of the cases; where NFS behavior diverges from local filesystems, using perl's rename command (which should; be safe assuming it uses the POSIX rename() syscall) to avoid having to lock/unlock from a Makefile. Also a bunch of Makefile changes to use commands from /bin or /usr/bin when they exist, but; otherwise to give a warning and pick up whatever might be found on $PATH. That seems a suitable; compromise between avoid-mysterious-behavior and give-best-effort-on-nonstandard-platform.; [In doing so, I noticed that I actually was picking up /Users/rcownie/anaconda2/envs/py36/bin/curl; rather than /usr/bin/curl - and I don't know whether there's any difference]. But current consensus is that we should figure out how to ship with a known-good compiler; and libraries, so I'm looking into that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413647683
Availability,error,error," RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getting this committed, I have not made changes relating to logging and; error messages. The DLL's are still in the jar, and I think it has to stay that way because; all nodes need to see libhail.so. The header files are also in the jar, and have to be; unpacked in a convoluted way, and that could probably be simplified if/when we change; the approach to packaging. Once this goes in, I can follow it with a PR which adds the NativePackDecoder in RowStore.scala,; controlled by whether environment variable ""HAIL_ENABLE_CPP_CODEGEN"" is defined; (so defaulting to using the JVM bytecode CompiledPackDecoder).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863
Deployability,upgrade,upgrade,"Various changes:. 1. Locking is now done with a carefully restricted use of flock(), and the makefiles use; perl's rename command to get atomic rename, so they don't need to take locks. 2. The makefile conforms to the customary use-whatever-is-on-$PATH, with the slight wrinkle that; the full pathnames of the commands used will be visible in the build log - so if someone; picks up something weird we'll at least see it. 3. There is a cache of NativeModule objects, so that we won't do enormous numbers of; calls to dlopen/dlclose. This may help in shuffle code, which creates a new PackDecoder; for each RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863
Integrability,interface,interfaces," RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getting this committed, I have not made changes relating to logging and; error messages. The DLL's are still in the jar, and I think it has to stay that way because; all nodes need to see libhail.so. The header files are also in the jar, and have to be; unpacked in a convoluted way, and that could probably be simplified if/when we change; the approach to packaging. Once this goes in, I can follow it with a PR which adds the NativePackDecoder in RowStore.scala,; controlled by whether environment variable ""HAIL_ENABLE_CPP_CODEGEN"" is defined; (so defaulting to using the JVM bytecode CompiledPackDecoder).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863
Modifiability,variab,variable," RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getting this committed, I have not made changes relating to logging and; error messages. The DLL's are still in the jar, and I think it has to stay that way because; all nodes need to see libhail.so. The header files are also in the jar, and have to be; unpacked in a convoluted way, and that could probably be simplified if/when we change; the approach to packaging. Once this goes in, I can follow it with a PR which adds the NativePackDecoder in RowStore.scala,; controlled by whether environment variable ""HAIL_ENABLE_CPP_CODEGEN"" is defined; (so defaulting to using the JVM bytecode CompiledPackDecoder).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863
Performance,cache,cache,"Various changes:. 1. Locking is now done with a carefully restricted use of flock(), and the makefiles use; perl's rename command to get atomic rename, so they don't need to take locks. 2. The makefile conforms to the customary use-whatever-is-on-$PATH, with the slight wrinkle that; the full pathnames of the commands used will be visible in the build log - so if someone; picks up something weird we'll at least see it. 3. There is a cache of NativeModule objects, so that we won't do enormous numbers of; calls to dlopen/dlclose. This may help in shuffle code, which creates a new PackDecoder; for each RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863
Security,hash,hash,"Various changes:. 1. Locking is now done with a carefully restricted use of flock(), and the makefiles use; perl's rename command to get atomic rename, so they don't need to take locks. 2. The makefile conforms to the customary use-whatever-is-on-$PATH, with the slight wrinkle that; the full pathnames of the commands used will be visible in the build log - so if someone; picks up something weird we'll at least see it. 3. There is a cache of NativeModule objects, so that we won't do enormous numbers of; calls to dlopen/dlclose. This may help in shuffle code, which creates a new PackDecoder; for each RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863
Testability,log,log,"Various changes:. 1. Locking is now done with a carefully restricted use of flock(), and the makefiles use; perl's rename command to get atomic rename, so they don't need to take locks. 2. The makefile conforms to the customary use-whatever-is-on-$PATH, with the slight wrinkle that; the full pathnames of the commands used will be visible in the build log - so if someone; picks up something weird we'll at least see it. 3. There is a cache of NativeModule objects, so that we won't do enormous numbers of; calls to dlopen/dlclose. This may help in shuffle code, which creates a new PackDecoder; for each RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863
Usability,simpl,simplified," RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getting this committed, I have not made changes relating to logging and; error messages. The DLL's are still in the jar, and I think it has to stay that way because; all nodes need to see libhail.so. The header files are also in the jar, and have to be; unpacked in a convoluted way, and that could probably be simplified if/when we change; the approach to packaging. Once this goes in, I can follow it with a PR which adds the NativePackDecoder in RowStore.scala,; controlled by whether environment variable ""HAIL_ENABLE_CPP_CODEGEN"" is defined; (so defaulting to using the JVM bytecode CompiledPackDecoder).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863
Usability,clear,clear,"Just to be clear, as much as possible we aim to keep the main source code free of historical inconsistencies. That applies doubly to pull requests, which should be single, semantically coherent units.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-415089165
Integrability,interface,interface,"@rcownie I think this design is a reasonable compromise on interface, at least until linear algebra is in the compiler and we can ""cancel"" conversion from ndarray to BlockMatrix and back. The optimal `complexity_bound` will be hugely dependent on cluster setup. To aid user intuition, I made the unit in terms of a single dimension rather than dimension-cubed. I've found the divide-and-conquer eigh method (with memory proportional to elements) to be 2.5-3x faster than the RRR eigh method (with memory proportional to dimension) when run on laptop and GCP; it takes greater advantage of vectorized BLAS3 ops. Since we're CPU rather than RAM limited on a high-core GCP machine, I've set this up to use divide-and-conquer whenever it won't result in an overflow on `lwork` which is still an int32 in the Python stack (boo). @cseed please let me know if you have any high-level feedback",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3977#issuecomment-407268618
Usability,intuit,intuition,"@rcownie I think this design is a reasonable compromise on interface, at least until linear algebra is in the compiler and we can ""cancel"" conversion from ndarray to BlockMatrix and back. The optimal `complexity_bound` will be hugely dependent on cluster setup. To aid user intuition, I made the unit in terms of a single dimension rather than dimension-cubed. I've found the divide-and-conquer eigh method (with memory proportional to elements) to be 2.5-3x faster than the RRR eigh method (with memory proportional to dimension) when run on laptop and GCP; it takes greater advantage of vectorized BLAS3 ops. Since we're CPU rather than RAM limited on a high-core GCP machine, I've set this up to use divide-and-conquer whenever it won't result in an overflow on `lwork` which is still an int32 in the Python stack (boo). @cseed please let me know if you have any high-level feedback",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3977#issuecomment-407268618
Performance,optimiz,optimized,"Currently this is optimized to:. ```; (TableMapRows (idx) 1; (TableRange 5 5); (MakeStruct; (idx; (GetField idx; (Ref Struct{idx:Int32} row))); (x; (I32 5)))); ```. But clearly it should be an InsertFields, not a MakeStruct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4001#issuecomment-408232868
Usability,clear,clearly,"Currently this is optimized to:. ```; (TableMapRows (idx) 1; (TableRange 5 5); (MakeStruct; (idx; (GetField idx; (Ref Struct{idx:Int32} row))); (x; (I32 5)))); ```. But clearly it should be an InsertFields, not a MakeStruct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4001#issuecomment-408232868
Usability,feedback,feedback,"> I'm not sure people actually use globals, though. I not sure either. I'm pretty sure @konradjk, does but it may be to work around other issues that can be fixed separately. We're going to set up a regular (quarterly?) meeting with the data group for Hail feedback. We could ask this there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4027#issuecomment-408648344
Usability,feedback,feedback,@tpoterba I'm assigning you since I've incorporated your feedback from when you reviewed these name changes before I separated them from the IR PR. I'll add a discuss post on breaking changes once this goes in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4032#issuecomment-408850197
Usability,clear,clear,"I'm going to close this if no one objects, I think the service obviates this issue and there is no clear win to be had with images.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4038#issuecomment-433529930
Usability,clear,clear,"Yes, absolutely! I was confused between filename and method name when I first added them. I think things are clear in my head now -- but maybe some more explanations about what is required to add a method to experimental (beyond adding the method code) would be neat!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4052#issuecomment-409731138
Usability,feedback,feedback,"Needs a bit more documentation, but looking to get feedback on the structure of the scala code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4058#issuecomment-409578166
Usability,learn,learning,"I've been thinking about this for a while now, and I think that what's _extremely_ helpful for learning is seeing a lot of short code examples for a lot of different applications. I think what we want is a bunch of examples formatted something like this:. ```; TITLE: what the code does; --------------; TAGS: comma-delimited set of search terms. DESC [optional] one- or two-sentence (max) clarification of what is being done. >>> CODE. --OR--. if there are multiple ways to do something, patterns like:. Method 1, if (condition 1):; >>> CODE 1. Method 2 (if condition 1 is not true):; >>> CODE 2. USES: clickable links to functions used in the code above. Probably not required for basic things like annotate / select. but definitely good to have for ld_prune or trio_matrix or their ilk. UNDERSTANDING [optional] An understanding section with click-to-expand styling. This shouldn't be required for a contribution to the cookbook, but could really help in some cases.; ```. This format has the advantage of keeping everything very visually tight, which will help people who learn through examples just scan through and look for patterns (this is a lot of people, I think). It also lets us embed understanding sections, which I do see the value of.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-411863621
Usability,guid,guides,"These are good thoughts. I agree that a visually clean format will make the guides easier for people to use. I like the idea of hiding the explanation underneath a clickable thing. I'd be hesitant to completely remove the explanatory text, because I think people will be more likely to use the how-to guides than to go through an entire tutorial, but making it optional is a good middle-ground. I can experiment on one of the files, before I do them all, so we can settle on a good format.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-411940112
Deployability,toggle,toggleable,"So I did some simple formatting on the ""Filter loci by a list of locus intervals"" example. . The cloud sphinx theme you mentioned on zulip has toggleable sections that look a bit nicer. I could emulate that formatting by writing a sphinx extension if we wanted to get fancier, but what do you think of this layout?. IMAGE 1. <img width=""720"" alt=""screen shot 2018-08-22 at 11 23 34 am"" src=""https://user-images.githubusercontent.com/35241112/44473344-1eb11c80-a5fe-11e8-954d-41440a031d24.png"">. IMAGE 2; clicking on `show` would expose more content:. <img width=""699"" alt=""screen shot 2018-08-22 at 11 23 46 am"" src=""https://user-images.githubusercontent.com/35241112/44473350-2375d080-a5fe-11e8-98e9-31f1c3bb825c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-415074240
Security,expose,expose,"So I did some simple formatting on the ""Filter loci by a list of locus intervals"" example. . The cloud sphinx theme you mentioned on zulip has toggleable sections that look a bit nicer. I could emulate that formatting by writing a sphinx extension if we wanted to get fancier, but what do you think of this layout?. IMAGE 1. <img width=""720"" alt=""screen shot 2018-08-22 at 11 23 34 am"" src=""https://user-images.githubusercontent.com/35241112/44473344-1eb11c80-a5fe-11e8-954d-41440a031d24.png"">. IMAGE 2; clicking on `show` would expose more content:. <img width=""699"" alt=""screen shot 2018-08-22 at 11 23 46 am"" src=""https://user-images.githubusercontent.com/35241112/44473350-2375d080-a5fe-11e8-98e9-31f1c3bb825c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-415074240
Usability,simpl,simple,"So I did some simple formatting on the ""Filter loci by a list of locus intervals"" example. . The cloud sphinx theme you mentioned on zulip has toggleable sections that look a bit nicer. I could emulate that formatting by writing a sphinx extension if we wanted to get fancier, but what do you think of this layout?. IMAGE 1. <img width=""720"" alt=""screen shot 2018-08-22 at 11 23 34 am"" src=""https://user-images.githubusercontent.com/35241112/44473344-1eb11c80-a5fe-11e8-954d-41440a031d24.png"">. IMAGE 2; clicking on `show` would expose more content:. <img width=""699"" alt=""screen shot 2018-08-22 at 11 23 46 am"" src=""https://user-images.githubusercontent.com/35241112/44473350-2375d080-a5fe-11e8-98e9-31f1c3bb825c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-415074240
Performance,cache,cached,"Yeah, I misjudged, clearly the determinism issue. I cached for my test case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4096#issuecomment-410890171
Testability,test,test,"Yeah, I misjudged, clearly the determinism issue. I cached for my test case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4096#issuecomment-410890171
Usability,clear,clearly,"Yeah, I misjudged, clearly the determinism issue. I cached for my test case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4096#issuecomment-410890171
Performance,optimiz,optimization,"@cseed I think this is probably ready for review. There's a couple of things that I'm going to look at/do more that aren't in this PR:; - add better docs for how random functions behave; - add more tests for randomness in various IR nodes; - add more tests and make sure that the optimization rules preserve the context that random functions are going to be evaluated in. @tpoterba and I talked briefly about the last one and it might involve restructuring simplify, since some rules are fine in isolation but will return different results when performed in combination (and sometimes one of the rules in question doesn't even involve the random node, which is kind of difficult to account for currently)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411878967
Testability,test,tests,"@cseed I think this is probably ready for review. There's a couple of things that I'm going to look at/do more that aren't in this PR:; - add better docs for how random functions behave; - add more tests for randomness in various IR nodes; - add more tests and make sure that the optimization rules preserve the context that random functions are going to be evaluated in. @tpoterba and I talked briefly about the last one and it might involve restructuring simplify, since some rules are fine in isolation but will return different results when performed in combination (and sometimes one of the rules in question doesn't even involve the random node, which is kind of difficult to account for currently)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411878967
Usability,simpl,simplify,"@cseed I think this is probably ready for review. There's a couple of things that I'm going to look at/do more that aren't in this PR:; - add better docs for how random functions behave; - add more tests for randomness in various IR nodes; - add more tests and make sure that the optimization rules preserve the context that random functions are going to be evaluated in. @tpoterba and I talked briefly about the last one and it might involve restructuring simplify, since some rules are fine in isolation but will return different results when performed in combination (and sometimes one of the rules in question doesn't even involve the random node, which is kind of difficult to account for currently)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411878967
Usability,clear,clear,"So my reading now of `seed` is that it uniquely identifies a sequence of numbers. I read the docs now, and I'm not sure I agree with these sentences:; > The values are seeded when the function is called, so calling a random Hail function and then using it several times in the same expression will yield the same result each time.; > ; > Evaluating the same expression will yield the same value every time, but multiple calls of the same function will have different results. I think the trouble is with the meaning of ""called"", ""using"", and ""evaluating"". I see now that you mean calling the python function by ""called"", but that wasn't clear to me on first reading. ""Using"" I think just means appearing in the source code, which feels right. The last sentence, I think, is not true, given the below:. ```python; In [31]: z = hl.rand_unif(0, 1, seed=0); ...: ; ...: t = hl.utils.range_table(2); ...: t = t.annotate(; ...: x = hl.literal([1,2,3]).map(lambda i: hl.rand_unif(0,1,seed=0)),; ...: y = hl.literal([1,2,3]).map(lambda i: hl.rand_unif(0,1,seed=0)),; ...: z = hl.literal([1,2,3]).map(lambda i: z)); ...: ; ...: t.show(); ```. I think I really prefer the `[z,z]` means two draws. If you want one draw, you gotta use a let or `annotate_globals` or something.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4166#issuecomment-415835792
Integrability,interface,interface,"@danking I've made all of the changes except the one about the interface for `af_dist`. (I agree with the point you're making about it being confusing that both functions are seeded separately and need to be kept the same for the same dataset to be produced, but making it take a function might just add a lot of visual noise. Honestly, I'm really tempted to take all of the `seed` stuff out of `balding_nichols_model` and put a note in to the effect of ""for reproducible results, use `hl.set_global_seed()` just before creating a dataset."" How would you feel about that?). re: how seeding functions happens more generally---I don't think anything I've said in the docs is actually incorrect (I would consider array transformations a context, kind of like the axes of a table or matrix table, so the table should end up with `x = y = z` but different values for the elements in the array, and different values between rows), but I'll work on making the language clearer so that they actually say what I mean. w.r.t. `[z, z]` being two draws---I have sometimes waffled on this but in general I think I tend to disagree, in part because I have struggled to come up with a consistent definition of what that would mean. Happy to talk more about this, but I don't know if it's super relevant to balding nichols since all of the distributions are once-per-element draws.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4166#issuecomment-415848091
Usability,clear,clearer,"@danking I've made all of the changes except the one about the interface for `af_dist`. (I agree with the point you're making about it being confusing that both functions are seeded separately and need to be kept the same for the same dataset to be produced, but making it take a function might just add a lot of visual noise. Honestly, I'm really tempted to take all of the `seed` stuff out of `balding_nichols_model` and put a note in to the effect of ""for reproducible results, use `hl.set_global_seed()` just before creating a dataset."" How would you feel about that?). re: how seeding functions happens more generally---I don't think anything I've said in the docs is actually incorrect (I would consider array transformations a context, kind of like the axes of a table or matrix table, so the table should end up with `x = y = z` but different values for the elements in the array, and different values between rows), but I'll work on making the language clearer so that they actually say what I mean. w.r.t. `[z, z]` being two draws---I have sometimes waffled on this but in general I think I tend to disagree, in part because I have struggled to come up with a consistent definition of what that would mean. Happy to talk more about this, but I don't know if it's super relevant to balding nichols since all of the distributions are once-per-element draws.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4166#issuecomment-415848091
Performance,load,loading,"The latest changes adopt the ""happy medium"" of having at all times either 0 or 1 NativeModule; objects corresponding to each lib, and a worker may get a shared ptr to what was constructed as; a master NativeModule. . In either case, the constructor is responsible for checking that lib already exists, or populating; it if it didn't exist. The big_mutex is held during constructors, in a way which a) protects the; module_table, and b) makes the transition from ""no lib file"" to ""complete and immutable lib file""; appear atomic, whether that occurs by invoking the makefile, or by writing binary data. Consequently, the makefile is simplified to just build $(MODULE).so without worrying about; atomicity, and the perl rename goes away. The loading is now done eagerly in the worker constructor, but still lazily for master-constructed; NativeModule's, since the most common lifecycle for a master is to construct it, do getKey, getBinary,; then throw it away without needing to load it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4211#issuecomment-417031457
Usability,simpl,simplified,"The latest changes adopt the ""happy medium"" of having at all times either 0 or 1 NativeModule; objects corresponding to each lib, and a worker may get a shared ptr to what was constructed as; a master NativeModule. . In either case, the constructor is responsible for checking that lib already exists, or populating; it if it didn't exist. The big_mutex is held during constructors, in a way which a) protects the; module_table, and b) makes the transition from ""no lib file"" to ""complete and immutable lib file""; appear atomic, whether that occurs by invoking the makefile, or by writing binary data. Consequently, the makefile is simplified to just build $(MODULE).so without worrying about; atomicity, and the perl rename goes away. The loading is now done eagerly in the worker constructor, but still lazily for master-constructed; NativeModule's, since the most common lifecycle for a master is to construct it, do getKey, getBinary,; then throw it away without needing to load it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4211#issuecomment-417031457
Availability,avail,available,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154
Deployability,install,installing,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154
Usability,simpl,simple,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154
Availability,error,error,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
Deployability,deploy,deployment,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
Integrability,interface,interfaces,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
Performance,queue,queue,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
Usability,guid,guide,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588
Deployability,upgrade,upgrade,"I have no explanation for the behavior of `pip`, it simply refuses to upgrade to the latest cloud tools. ```; + pip search cloudtools; cloudtools (1.1.16) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools. real	0m0.867s; user	0m0.649s; sys	0m0.084s; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/46/78/966c9af5b88a01af73bb56486e853c00ff4865de0bf380282aa54fdec43a/cloudtools-1.1.15-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.15. real	0m1.718s; user	0m1.378s; sys	0m0.158s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700
Usability,simpl,simply,"I have no explanation for the behavior of `pip`, it simply refuses to upgrade to the latest cloud tools. ```; + pip search cloudtools; cloudtools (1.1.16) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools. real	0m0.867s; user	0m0.649s; sys	0m0.084s; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/46/78/966c9af5b88a01af73bb56486e853c00ff4865de0bf380282aa54fdec43a/cloudtools-1.1.15-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.15. real	0m1.718s; user	0m1.378s; sys	0m0.158s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700
Usability,clear,clear,Is there something in particular you had in mind? The example that exists now seems clear to me.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4259#issuecomment-424833285
Usability,simpl,simple,"that one is simple:; ```; def get_old_mu_data() -> hl.Table:; old_mu_data = hl.import_table('gs://gnomad-resources/constraint/source/fordist_1KG_mutation_rate_table.txt',; delimiter=' ', impute=True); return old_mu_data.transmute(context=old_mu_data['from'], ref=old_mu_data['from'][1],; alt=old_mu_data.to[1]).key_by('context', 'ref', 'alt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4314#issuecomment-420405426
Usability,clear,clear,"ugh. just to be clear, this is just removing the `:`s right? (to force converting to tables most of the time). but you are planning on keeping `[]` generally (maybe as shorthand if you suggest generally using `index_*`)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4383#issuecomment-423410409
Safety,predict,predicting,"Agreed, this is good for useability. note it’s also a special case of simple linreg predicting y from x (we return the square r_sq. The sign is that of beta).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4479#issuecomment-425769239
Usability,simpl,simple,"Agreed, this is good for useability. note it’s also a special case of simple linreg predicting y from x (we return the square r_sq. The sign is that of beta).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4479#issuecomment-425769239
Usability,clear,clearer,"It's much clearer, awesome",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4494#issuecomment-427159094
Usability,simpl,simpler,"@catoverdrive I see now there is an unwrap that should have had this same effect, so I don't quite understand why this was necessary (although it seemed to be). Anyway, this still seems simpler/more correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4499#issuecomment-428398568
Usability,simpl,simpler,"Great, this is way simpler. Is the key_by/drop necessary? What about just making nested a string?. Does this also fail?. ```; t = hl.utils.range_table(1); t = t.annotate(x = hl.bind(lambda s, nested: s.contains(nested), hl.set({'foo'}), hl.null(...))); t._force_count(); ```. I'm going to guess no and that it has something to do with writing the row (which is why isolating it in the IRSuite isn't working).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-430009988
Usability,simpl,simpler,"simpler:; ```; In [12]: import hail as hl; ...: ; ...: t1kg = hl.utils.range_matrix_table(1,1); ...: t1kg = t1kg.key_rows_by(locus=hl.locus(hl.str(t1kg.row_idx), t1kg.row_idx), alleles=['A','T']); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429050886
Availability,error,error,"simpler:; ```; In [16]: import hail as hl; ...: ; ...: t1kg = hl.utils.range_matrix_table(1,1); ...: t1kg = t1kg.key_rows_by(locus=hl.locus(hl.str(t1kg.row_idx+1), t1kg.row_idx+1), alleles=['A','T']); ...: t1kg.write('/tmp/foo.mt', overwrite=True); 2018-10-11 13:42:55 Hail: INFO: Coerced sorted dataset; 2018-10-11 13:42:55 Hail: INFO: wrote 1 items in 1 partitions to /tmp/foo.mt; ^[[A; In [17]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt'); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; error:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; ```; describe:; ```; In [18]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt').describe(); ...: ; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'col_idx': int32 ; ----------------------------------------; Row fields:; 'row_idx': int32 ; 'locus': locus<GRCh37> ; 'alleles': array<str> ; ----------------------------------------; Entry fields:; None; ----------------------------------------; Column key: ['col_idx']; Row key: ['locus', 'alleles']; ----------------------------------------; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397
Performance,optimiz,optimization,"simpler:; ```; In [16]: import hail as hl; ...: ; ...: t1kg = hl.utils.range_matrix_table(1,1); ...: t1kg = t1kg.key_rows_by(locus=hl.locus(hl.str(t1kg.row_idx+1), t1kg.row_idx+1), alleles=['A','T']); ...: t1kg.write('/tmp/foo.mt', overwrite=True); 2018-10-11 13:42:55 Hail: INFO: Coerced sorted dataset; 2018-10-11 13:42:55 Hail: INFO: wrote 1 items in 1 partitions to /tmp/foo.mt; ^[[A; In [17]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt'); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; error:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; ```; describe:; ```; In [18]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt').describe(); ...: ; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'col_idx': int32 ; ----------------------------------------; Row fields:; 'row_idx': int32 ; 'locus': locus<GRCh37> ; 'alleles': array<str> ; ----------------------------------------; Entry fields:; None; ----------------------------------------; Column key: ['col_idx']; Row key: ['locus', 'alleles']; ----------------------------------------; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397
Usability,simpl,simpler,"simpler:; ```; In [16]: import hail as hl; ...: ; ...: t1kg = hl.utils.range_matrix_table(1,1); ...: t1kg = t1kg.key_rows_by(locus=hl.locus(hl.str(t1kg.row_idx+1), t1kg.row_idx+1), alleles=['A','T']); ...: t1kg.write('/tmp/foo.mt', overwrite=True); 2018-10-11 13:42:55 Hail: INFO: Coerced sorted dataset; 2018-10-11 13:42:55 Hail: INFO: wrote 1 items in 1 partitions to /tmp/foo.mt; ^[[A; In [17]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt'); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; error:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; ```; describe:; ```; In [18]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt').describe(); ...: ; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'col_idx': int32 ; ----------------------------------------; Row fields:; 'row_idx': int32 ; 'locus': locus<GRCh37> ; 'alleles': array<str> ; ----------------------------------------; Entry fields:; None; ----------------------------------------; Column key: ['col_idx']; Row key: ['locus', 'alleles']; ----------------------------------------; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397
Availability,error,error,"the error in #4529 was:; ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515
Safety,timeout,timeout," to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--bucket=hail-ci-0-1-dataproc-staging-bucket', '--max-idle=40m']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515
Testability,test,test-,"the error in #4529 was:; ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515
Usability,feedback,feedback,"the error in #4529 was:; ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515
Energy Efficiency,monitor,monitoring,"ed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; gcloud command:; gcloud beta dataproc clusters create \; ci-test-6boype3d \; --image-version=1.2-deb9 \; --metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-boot-disk-size=40GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-1 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --bucket=hail-ci-0-1-dataproc-staging-bucket \; --max-idle=10m; Starting cluster 'ci-test-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
Performance,perform,performance,"2de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip; Copying file://build/distributions/hail-python.zip [Content-Type=application/zip]...; / [0 files][ 0.0 B/ 1.4 MiB] ; / [1 files][ 1.4 MiB/ 1.4 MiB] ; Operation completed over 1 objects/1.4 MiB. . real	0m2.852s; user	0m1.179s; sys	0m0.429s; + cluster start ci-test-6boype3d --master-machine-type n1-standard-1 --master-boot-disk-size 40 --worker-machine-type n1-standard-1 --worker-boot-disk-size 40 --version 0.2 --spark 2.2.0 --max-idle 10m --bucket=hail-ci-0-1-dataproc-staging-bucket --jar gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar --zip gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip --vep; Waiting on operation [projects/broad-ctsa/regions/global/operations/2b6b5772-e45f-3873-be2f-0e04327d29d7].; Waiting for cluster creation operation...; WARNING: For PD-Standard, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; .....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
Safety,timeout,timeout,"42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-boot-disk-size=40GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-1 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --bucket=hail-ci-0-1-dataproc-staging-bucket \; --max-idle=10m; Starting cluster 'ci-test-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 210, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-6boype3d', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
Testability,test,test-,"```; + gsutil cp build/libs/hail-all-spark.jar gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar; Copying file://build/libs/hail-all-spark.jar [Content-Type=application/java-archive]...; / [0 files][ 0.0 B/ 27.2 MiB] ; / [1 files][ 27.2 MiB/ 27.2 MiB] ; -; Operation completed over 1 objects/27.2 MiB. . real	0m3.308s; user	0m1.212s; sys	0m0.548s; + gsutil cp build/distributions/hail-python.zip gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip; Copying file://build/distributions/hail-python.zip [Content-Type=application/zip]...; / [0 files][ 0.0 B/ 1.4 MiB] ; / [1 files][ 1.4 MiB/ 1.4 MiB] ; Operation completed over 1 objects/1.4 MiB. . real	0m2.852s; user	0m1.179s; sys	0m0.429s; + cluster start ci-test-6boype3d --master-machine-type n1-standard-1 --master-boot-disk-size 40 --worker-machine-type n1-standard-1 --worker-boot-disk-size 40 --version 0.2 --spark 2.2.0 --max-idle 10m --bucket=hail-ci-0-1-dataproc-staging-bucket --jar gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar --zip gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip --vep; Waiting on operation [projects/broad-ctsa/regions/global/operations/2b6b5772-e45f-3873-be2f-0e04327d29d7].; Waiting for cluster creation operation...; WARNING: For PD-Standard, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; ...................................................................................................................................................................................................................................................................................................................................",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
Usability,feedback,feedback,"..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; gcloud command:; gcloud beta dataproc clusters create \; ci-test-6boype3d \; --image-version=1.2-deb9 \; --metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518
Availability,down,down,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
Deployability,update,update,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
Energy Efficiency,schedul,schedule,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
Integrability,rout,routes,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
Performance,optimiz,optimize,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
Safety,timeout,timeouts,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
Security,password,password,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
Usability,simpl,simple,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869
Availability,down,down,"it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
Deployability,update,update,"information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
Energy Efficiency,schedul,scheduled,"it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
Integrability,rout,routes,"hink for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I thought we decided we; > preferred that. I thought it would take less time to get a subdirectory working than figure out; how to add a new domain and a cert and deal with DNS. Long term a subdomain makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
Performance,optimiz,optimize,"information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
Safety,avoid,avoid,"> Am I strange in that I want to name something what it is (ci, batch, etc.); > rather than give everything codenames? The purpose of codenames is to hide and; > obscure, you know. Good point. > I think this should be called tutorial. And when it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
Security,password,password,"> Am I strange in that I want to name something what it is (ci, batch, etc.); > rather than give everything codenames? The purpose of codenames is to hide and; > obscure, you know. Good point. > I think this should be called tutorial. And when it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
Usability,simpl,simple,"hink for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I thought we decided we; > preferred that. I thought it would take less time to get a subdirectory working than figure out; how to add a new domain and a cert and deal with DNS. Long term a subdomain makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878
Deployability,update,updates-in-kubernetes-,"'t be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't need to change the DNS to add a domain, and I'll write ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
Energy Efficiency,schedul,scheduled,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
Integrability,rout,routinely,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
Performance,cache,cache,"r both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't ne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
Safety,timeout,timeouts,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
Security,password,password,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
Usability,responsiv,responsive,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659
Availability,down,down,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868
Deployability,rolling,rolling,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868
Modifiability,config,config,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868
Security,encrypt,encrypt,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868
Usability,simpl,simpler,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868
Usability,clear,cleared,"Ok, I added a caution and cleared up the note. I chose 50 MB as the biggest recommended file size since that will take ~5 seconds to write.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4644#issuecomment-433548391
Deployability,pipeline,pipeline,"Added support for a few more nodes, including {Insert, Select}Fields and Array{Range, Map, Filter} (deforested). I'm going to do ordering next. That should give us enough interesting stuff to play with when simple Table pipeline start working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4663#issuecomment-433775154
Usability,simpl,simple,"Added support for a few more nodes, including {Insert, Select}Fields and Array{Range, Map, Filter} (deforested). I'm going to do ordering next. That should give us enough interesting stuff to play with when simple Table pipeline start working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4663#issuecomment-433775154
Usability,simpl,simple,@tpoterba why would these not go on the artifacts index page? It seems odd to couple the CI directly to the artifact structure of `hail-is/hail` when there's an equally simple alternative.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4667#issuecomment-433943049
Availability,down,down,t org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
Deployability,configurat,configuration,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
Energy Efficiency,allocate,allocate," o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11:23 Client: INFO: Setting up container launch context for our AM; 2019-01-22 13:11:23 Client: INFO: Setting up the launch environment for our AM container; 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-01-22 13:11:24 HadoopFSCredentialProvider: INFO: getting token for: hdfs://scc/user/farrell; 2019-01-22 13:11:24 DFSClient: INFO: Created HDFS_DELEGATION_TOKEN token 11364 for farrell on ha-hdfs:scc; 2019-01-22 13:11:26 Client: WARN: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 2019-01-22 13:11:29 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_libs__5184408978318087972.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_libs__5184408978318087972.zip; 2019-01-22 13:11:30 Client: INFO: Uploading resource file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/hail-all-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
Modifiability,config,configuration,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
Performance,load,load,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
Safety,abort,aborting,"p.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLau",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
Security,authenticat,authentication,onda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
Testability,log,log,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
Usability,clear,cleared,"gatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Interrupting monitor thread; 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Shutting down all executors; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asking each executor to shut down; 2019-01-22 13:12:06 SchedulerExtensionServices: INFO: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Stopped; 2019-01-22 13:12:06 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!; 2019-01-22 13:12:06 MemoryStore: INFO: MemoryStore cleared; 2019-01-22 13:12:06 BlockManager: INFO: BlockManager stopped; 2019-01-22 13:12:06 BlockManagerMaster: INFO: BlockManagerMaster stopped; 2019-01-22 13:12:06 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!; 2019-01-22 13:12:06 TransportResponseHandler: ERROR: Still have 1 requests outstanding when connection from /192.168.18.203:44844 is closed; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 14 at RPC address 192.168.18.189:50356, but got no response. Marking as slave lost.; java.io.IOException: Connection from /192.168.18.203:44844 closed; at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(Transpo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587
Usability,guid,guides,"s/random.rst:27: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; docs/functions/random.rst:30: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; docs/functions/random.rst:33: >>> hl.eval(hl.array([x, x, x])) # doctest: +NOTEST; docs/functions/random.rst:42: >>> hl.eval(hl.array([a, b, c])) # doctest: +NOTEST; docs/functions/random.rst:50: >>> table.show() # doctest: +NOTEST; docs/functions/random.rst:72: >>> hl.eval(hl.rand_unif(0, 1, seed=0)) # doctest: +NOTEST; docs/functions/random.rst:75: >>> hl.eval(hl.rand_unif(0, 1, seed=0)) # doctest: +NOTEST; docs/functions/random.rst:82: >>> table.x.collect() # doctest: +NOTEST; docs/functions/random.rst:90: >>> table.x.collect() # doctest: +NOTEST; docs/functions/random.rst:98: >>> table.x.collect() # doctest: +NOTEST; docs/functions/random.rst:110: >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) # doctest: +NOTEST; docs/functions/random.rst:114: >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) # doctest: +NOTEST; docs/guides/basics.rst:95: >>> mt.describe() # doctest: +NOTEST; docs/guides/basics.rst:141: >>> ht.describe() # doctest: +NOTEST; docs/guides/basics.rst:164: >>> mt.s.describe() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:68: >>> mt # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:71: >>> mt.locus # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:83: >>> mt.DP.describe() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:107: >>> mt.describe() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:266: >>> mt_new.replicate_num.show() # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:299: >>> mt.aggregate_entries(hl.agg.mean(mt.GQ)) # doctest: +NOTEST; docs/hailpedia/matrix_table.rst:307: >>> mt.aggregate_entries((agg.stats(mt.DP), agg.stats(mt.GQ))) # doctest: +NOTEST; docs/hailpedia/table.rst:63: >>> ht.describe() # doctest: +NOTEST; docs/hailpedia/table.rst:102: >>> ht # doctest: +NOTEST; docs/hailpedia/table.rst:105: >>> ht.ID # doctest: +NOTEST; experimental",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4817#issuecomment-451506878
Usability,guid,guides,"python/hail/expr/expressions/typed_expressions.py:934: >>> hl.eval(s1.add(10)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1296: >>> hl.eval(d.key_set()) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1312: >>> hl.eval(d.keys()) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1329: >>> hl.eval(d.map_values(lambda x: x * 10)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:1366: >>> hl.eval(d.values()) # doctest: +NOTEST; Binary file /Users/dking/projects/hail/hail/python/hail/expr/expressions/__pycache__/typed_expressions.cpython-37.pyc matches; Binary file /Users/dking/projects/hail/hail/python/hail/__pycache__/table.cpython-37.pyc matches; Binary file /Users/dking/projects/hail/hail/python/hail/__pycache__/matrixtable.cpython-37.pyc matches; /Users/dking/projects/hail/hail/python/hail/docs/guides/basics.rst:95: >>> mt.describe() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/guides/basics.rst:141: >>> ht.describe() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/guides/basics.rst:164: >>> mt.s.describe() # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:21: >>> hl.eval(x) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:24: >>> hl.eval(x) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:27: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:30: >>> hl.eval(hl.rand_unif(0, 1)) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:33: >>> hl.eval(hl.array([x, x, x])) # doctest: +NOTEST; /Users/dking/projects/hail/hail/python/hail/docs/functions/random.rst:42: >>> hl.eval(hl.array([a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4817#issuecomment-506359198
Usability,feedback,feedback,"That's what I expected. Your name just came up in the roulette. I'll also want @cseed to give feedback, since I made these from his original work on https://github.com/cseed/hail/tree/partitioned-combiner.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4824#issuecomment-441771321
Integrability,protocol,protocol,"Oh, shit, I approved. Do we have a working protocol for multi-user reviews? This was clearly not it. I guess the rule should be to dismiss your review if someone else has reviewed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4921#issuecomment-445296624
Usability,clear,clearly,"Oh, shit, I approved. Do we have a working protocol for multi-user reviews? This was clearly not it. I guess the rule should be to dismiss your review if someone else has reviewed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4921#issuecomment-445296624
Availability,avail,available,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
Integrability,rout,route-matching,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
Performance,perform,performance,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
Safety,avoid,avoid,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
Security,access,access,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
Testability,benchmark,benchmarker,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
Usability,usab,usable,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569
Energy Efficiency,efficient,efficient,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
Integrability,depend,dependencies,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
Performance,optimiz,optimizations,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
Safety,avoid,avoiding,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
Security,authenticat,authentication,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
Testability,log,logic,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
Usability,simpl,simplifies,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812
Availability,down,down,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
Integrability,depend,dependency,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
Modifiability,extend,extends,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
Usability,simpl,simplify,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682
Availability,error,error,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
Deployability,integrat,integration,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
Integrability,integrat,integration,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
Performance,cache,cache,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
Security,hash,hash-based,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
Testability,test,tests,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
Usability,simpl,simple,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665
Usability,UX,UX,"Research on UX/UI, and impact on customer acceptance. Is there a reason to invest in surface credibility (beyond functionality)?. 1) http://credibility.stanford.edu/pdf/p80-fogg.pdf",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-450537877
Availability,down,downward," secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need to maintain their own state look a lot like HTML wrapped in a function:. ```js; export default function() { <div>Hello World </div> }; ```. or in JS ES6 form:; ```js; export default () => <div>Hello World</div>; ``` . Performance is excellent. SSR should run about as fast as jinja2 (will be getting faster in 2019). Client side interactions are obviously far more performant. Bundle sizes are on a downward trajectory; react + react-dom is about as big as jQuery today, and reducing that is a focus on facebook in 2019. There are alternatives to react-dom that are under 10kb, but in practice 20kb is nothing to worry about, especially when initial load doesn't require it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
Deployability,install,install,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
Integrability,rout,routing,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
Modifiability,variab,variables,"B .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
Performance,optimiz,optimizations,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
Testability,test,testing,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
Usability,simpl,simply,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935
Performance,perform,performance-tests,"Also, interesting to note: . Home page (with menu bar, dark icon, not logged in): 1.5KB .gz . Logged in: 3.1KB. Bundle size: on order of 100KB. However, 30% of this is the auth0 client library; we can modify it to save space. I've commented on an issue with some light guidance on how to save 5.5KB of that. Effectively 70KB for React + React-Dom + Webpack tooling + all page js compares quite favorably with a jquery-only solution, while being faster than jQuery (https://github.com/jonmiles/react-performance-tests, https://medium.com/thothzocial-engineering/rendering-speed-performance-challenge-with-famous-front-end-framework-196c876a68af), far easier to manage, and with a much large ecosystem (and jquery-only solution would do nothing for universal rendering). The React side should drop this year substantially. They are also interested in writing a compiler to completely remove the vdom, compiling to optimized javascript or maybe web assembly. That may be something interesting to us as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454608700
Testability,log,logged,"Also, interesting to note: . Home page (with menu bar, dark icon, not logged in): 1.5KB .gz . Logged in: 3.1KB. Bundle size: on order of 100KB. However, 30% of this is the auth0 client library; we can modify it to save space. I've commented on an issue with some light guidance on how to save 5.5KB of that. Effectively 70KB for React + React-Dom + Webpack tooling + all page js compares quite favorably with a jquery-only solution, while being faster than jQuery (https://github.com/jonmiles/react-performance-tests, https://medium.com/thothzocial-engineering/rendering-speed-performance-challenge-with-famous-front-end-framework-196c876a68af), far easier to manage, and with a much large ecosystem (and jquery-only solution would do nothing for universal rendering). The React side should drop this year substantially. They are also interested in writing a compiler to completely remove the vdom, compiling to optimized javascript or maybe web assembly. That may be something interesting to us as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454608700
Usability,guid,guidance,"Also, interesting to note: . Home page (with menu bar, dark icon, not logged in): 1.5KB .gz . Logged in: 3.1KB. Bundle size: on order of 100KB. However, 30% of this is the auth0 client library; we can modify it to save space. I've commented on an issue with some light guidance on how to save 5.5KB of that. Effectively 70KB for React + React-Dom + Webpack tooling + all page js compares quite favorably with a jquery-only solution, while being faster than jQuery (https://github.com/jonmiles/react-performance-tests, https://medium.com/thothzocial-engineering/rendering-speed-performance-challenge-with-famous-front-end-framework-196c876a68af), far easier to manage, and with a much large ecosystem (and jquery-only solution would do nothing for universal rendering). The React side should drop this year substantially. They are also interested in writing a compiler to completely remove the vdom, compiling to optimized javascript or maybe web assembly. That may be something interesting to us as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454608700
Integrability,rout,routing,"Regarding SSR-only mode. This is the default behavior. SSR is mostly a function of routing. If we allow the client to handle routes, we save the roundtrip in reconciling current app state (current DOM) with the next state (next page's DOM). To ""enable"" this functionality, instead of using `<Link>` use `<a>`. Nextjs has excellent documentation and a responsive maintainer base: https://github.com/zeit/next.js/issues/575",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454796376
Usability,responsiv,responsive,"Regarding SSR-only mode. This is the default behavior. SSR is mostly a function of routing. If we allow the client to handle routes, we save the roundtrip in reconciling current app state (current DOM) with the next state (next page's DOM). To ""enable"" this functionality, instead of using `<Link>` use `<a>`. Nextjs has excellent documentation and a responsive maintainer base: https://github.com/zeit/next.js/issues/575",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454796376
Availability,error,errors,"Notes: ; #### 1st & 3rd set of errors. 1st and 3rd set identical, except in 3rd handler:200 (another response.post) hangs first... `ConnectionResetError` errorNum=104.; * May be related: https://github.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
Integrability,depend,depending,"b.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
Safety,timeout,timeout,"except in 3rd handler:200 (another response.post) hangs first... `ConnectionResetError` errorNum=104.; * May be related: https://github.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
Testability,log,log,"Notes: ; #### 1st & 3rd set of errors. 1st and 3rd set identical, except in 3rd handler:200 (another response.post) hangs first... `ConnectionResetError` errorNum=104.; * May be related: https://github.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
Usability,simpl,simplest,"b.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389
Usability,clear,clear,"This is more clear to me, thanks Tim! Would it be worth explaining why it's called as `hail.init()` vs say `hail.context.init()` or `HailContext()`? Knowing the chain of custody makes this indirection feel less magical to me. ```python; // Instantiates the HailContext class, unless called with dempotent == True; // Calls def __init__ ; // Imported in hail/__init__.py for use as hail.init() ; ```. Also, wondering if it makes sense to uses/benefits of treating as singleton, though that may be for a different pr.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4987#issuecomment-448319952
Usability,feedback,feedback,"Dan, should be resolved. Thanks for all of the feedback!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/4994#issuecomment-449272584
Usability,simpl,simplifies,"> Commutativity is a nice property for all aggregators to have. In particular, why add restrictive semantics when nobody is asking for them?. Commutativity isn't a property you can add. It either has it or is doesn't. And it doesn't. Same for take. ; ""adding"" it gets you collectAsSet. My proposal only simplifies. We have two semantics for collect, and I want only one. What do I need to do besides delete some key_by simplifier rules (if they are there)? Aggregators should be sequential anyway. > currently agg.collect is non-deterministic. It is? How's that?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448848528
Availability,down,down,"1. I really don't understand what this code is trying to do. Can you give me a short explanation in English?. 2. I think you just need single-use Let forwarding to optimize this. It looks like:. Push down the TableCount. (TableCount (Paralellelize ...)) should turn into (ArrayLen (GetField rows ...)). Push the ArrayLen/GetField into the Let. (Is this what your ""MaximizeLets"" is doing? I think that would traditionally be called let lifting.). Now you have (Let __cols_and_globals (ArrayLen (GetField __cols (Ref __cols_and_globals))). Then you forward the single-use Let, and the rest of the code simplifies into (ArrayLen (TableCollect (TableRead ""cols""))), and that should have the static number of rows and be able to be simplified. 3. So how should single-use Let forwarding work? You need two things: to determine there is only one use, and that the single use isn't in a more expensive context, e.g. you don't want to forward a single-use Let into a loop: (Let expensive X (ArrayMap a x <use expensive once>)). Since our control flow is structured, there is a static notion of ""loop nesting depth"", e.g. in the above code, the Let has nesting depth 0, and the use in the ArrayMap body has nesting depth 1. You can only forward into the same nesting depth. 4. Final question is, do you want the let forwarding pass to also delete unused Lets? If you delete a let, that might delete references that make other lets single (or zero) use. In this case, it might be nice to build a ""use-def chain"" data structure, that keeps, for each let, its list of uses (and vice versa). Then, when you delete a Let, you can dynamically delete the references for the right hand side, possibly creating additional optimization opportunities.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-452863737
Performance,optimiz,optimize,"1. I really don't understand what this code is trying to do. Can you give me a short explanation in English?. 2. I think you just need single-use Let forwarding to optimize this. It looks like:. Push down the TableCount. (TableCount (Paralellelize ...)) should turn into (ArrayLen (GetField rows ...)). Push the ArrayLen/GetField into the Let. (Is this what your ""MaximizeLets"" is doing? I think that would traditionally be called let lifting.). Now you have (Let __cols_and_globals (ArrayLen (GetField __cols (Ref __cols_and_globals))). Then you forward the single-use Let, and the rest of the code simplifies into (ArrayLen (TableCollect (TableRead ""cols""))), and that should have the static number of rows and be able to be simplified. 3. So how should single-use Let forwarding work? You need two things: to determine there is only one use, and that the single use isn't in a more expensive context, e.g. you don't want to forward a single-use Let into a loop: (Let expensive X (ArrayMap a x <use expensive once>)). Since our control flow is structured, there is a static notion of ""loop nesting depth"", e.g. in the above code, the Let has nesting depth 0, and the use in the ArrayMap body has nesting depth 1. You can only forward into the same nesting depth. 4. Final question is, do you want the let forwarding pass to also delete unused Lets? If you delete a let, that might delete references that make other lets single (or zero) use. In this case, it might be nice to build a ""use-def chain"" data structure, that keeps, for each let, its list of uses (and vice versa). Then, when you delete a Let, you can dynamically delete the references for the right hand side, possibly creating additional optimization opportunities.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-452863737
Usability,simpl,simplifies,"1. I really don't understand what this code is trying to do. Can you give me a short explanation in English?. 2. I think you just need single-use Let forwarding to optimize this. It looks like:. Push down the TableCount. (TableCount (Paralellelize ...)) should turn into (ArrayLen (GetField rows ...)). Push the ArrayLen/GetField into the Let. (Is this what your ""MaximizeLets"" is doing? I think that would traditionally be called let lifting.). Now you have (Let __cols_and_globals (ArrayLen (GetField __cols (Ref __cols_and_globals))). Then you forward the single-use Let, and the rest of the code simplifies into (ArrayLen (TableCollect (TableRead ""cols""))), and that should have the static number of rows and be able to be simplified. 3. So how should single-use Let forwarding work? You need two things: to determine there is only one use, and that the single use isn't in a more expensive context, e.g. you don't want to forward a single-use Let into a loop: (Let expensive X (ArrayMap a x <use expensive once>)). Since our control flow is structured, there is a static notion of ""loop nesting depth"", e.g. in the above code, the Let has nesting depth 0, and the use in the ArrayMap body has nesting depth 1. You can only forward into the same nesting depth. 4. Final question is, do you want the let forwarding pass to also delete unused Lets? If you delete a let, that might delete references that make other lets single (or zero) use. In this case, it might be nice to build a ""use-def chain"" data structure, that keeps, for each let, its list of uses (and vice versa). Then, when you delete a Let, you can dynamically delete the references for the right hand side, possibly creating additional optimization opportunities.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-452863737
Usability,simpl,simple,"also, a note on the comment - Konrad, Cotton, and I discussed a simple function ""write_expr"" in the experimental module that uses a dummy table and writes in globals.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5044#issuecomment-452252677
Deployability,update,update,update: took 160s on profile225 (2.0GB as .vcf.gz). The size input to LD Prune (after filtering and split-multi) is 700MB (as mt). 1KG is 16MB as an mt. There's clearly a lot of overhead for small datasets.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078#issuecomment-452419381
Usability,clear,clearly,update: took 160s on profile225 (2.0GB as .vcf.gz). The size input to LD Prune (after filtering and split-multi) is 700MB (as mt). 1KG is 16MB as an mt. There's clearly a lot of overhead for small datasets.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5078#issuecomment-452419381
Usability,clear,clear,You're also removing Interval.point_type (just to be clear what the breaking changes are).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152#issuecomment-454915985
Usability,clear,clearly,"I'm not either. It's clearly the right thing. We can write them in terms of `hl.eval`, but I don't think they will work quite the same since we don't have the point type and type inference could fail (e.g. the endpoints are None).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152#issuecomment-455711797
Testability,test,tests,"I put point_type back. Should be ready to go now. Also, fixed the close definition, good catch. I also removed an additional use of _convert_to_j in import_bgen to get the tests to pass from this PR (sorry my stacking got a bit mixed up): https://github.com/hail-is/hail/pull/5150/files#diff-36d21c1427efe06a781cd36ef5aa8678R961. You can also wait for that to go in and I'll rebase if you're worried about the change. Finally, the imports are a bit of a mess since I wanted to use hail_type in interval.py which is also imported by the types and expr files. @tpoterba I think we should remove types from expr and remove java from utils (we're confusing user utils like hadoop_* and Interval from internal utils like Env and java stuff which don't seem related) and have a clear ""import"" graph: javautils > types > utils > expr.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152#issuecomment-456684801
Usability,clear,clear,"I put point_type back. Should be ready to go now. Also, fixed the close definition, good catch. I also removed an additional use of _convert_to_j in import_bgen to get the tests to pass from this PR (sorry my stacking got a bit mixed up): https://github.com/hail-is/hail/pull/5150/files#diff-36d21c1427efe06a781cd36ef5aa8678R961. You can also wait for that to go in and I'll rebase if you're worried about the change. Finally, the imports are a bit of a mess since I wanted to use hail_type in interval.py which is also imported by the types and expr files. @tpoterba I think we should remove types from expr and remove java from utils (we're confusing user utils like hadoop_* and Interval from internal utils like Env and java stuff which don't seem related) and have a clear ""import"" graph: javautils > types > utils > expr.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152#issuecomment-456684801
Usability,clear,clear,"`clear ""import"" graph: javautils > types > utils > expr` . Sounds good to me.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5152#issuecomment-456802543
Usability,clear,clear,I'm a bit worried about confusing people with two (almost) identical methods named differently. . What do you think about something like `rbind` for right-bind? Then it's clear they're in the same family,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5154#issuecomment-454854904
Availability,robust,robust,"Made a more robust authentication library. One outstanding issue due to auth0js library, that we can solve by checking for and clearing wildcard auth0-prefixed cookies and startup, but this may have side-effects. Created an issue to track:; https://github.com/auth0/auth0.js/issues/897",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-455907662
Security,authenticat,authentication,"Made a more robust authentication library. One outstanding issue due to auth0js library, that we can solve by checking for and clearing wildcard auth0-prefixed cookies and startup, but this may have side-effects. Created an issue to track:; https://github.com/auth0/auth0.js/issues/897",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-455907662
Usability,clear,clearing,"Made a more robust authentication library. One outstanding issue due to auth0js library, that we can solve by checking for and clearing wildcard auth0-prefixed cookies and startup, but this may have side-effects. Created an issue to track:; https://github.com/auth0/auth0.js/issues/897",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-455907662
Availability,avail,available,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
Integrability,rout,routing,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
Performance,load,loading,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
Safety,avoid,avoid,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
Usability,responsiv,responsive,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996
Usability,simpl,simplifies,"It wasn't scanning the full dataset anymore, but:. table.head().flatten() was generating a TableOrderBy(TableKeyBy(TableHead)). There was no way to remove this node, even if the table was already keyed by the sort fields, so we ended up doing an extra scan and possibly shuffle. This change simplifies the whole thing, and emits the correct IR from the beginning",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5172#issuecomment-455698384
Integrability,interface,interface,"Some initial thoughts:; * I'm not sure we should have implicitly broadcasting operations in the IR. It seems simpler to make broadcast an explicit operation, which we make sure to deforest. In fact, broadcast is a special case of the generic tensor index operation I'll describe below. Implicitly broadcasting operations could be provided in the Python interface, making broadcasts explicit when constructing the IR.; * I'm also not sure how much special treatment we should give to block matrices in the IR. I now like to think of block matrices as just 4-tensors, with matrix operations like matrix multiplication lowering to operations on 4-tensors. If we allow tensors to have some distributed dimensions and some ""small"" dimensions, then at least in the backend we might not need special handling of block structures. It may still be helpful to have a special block matrix/tensor representation at the top level IR, or maybe that should only live in Python—I'm not sure. Here's a proposal for a set of primitive tensor operations. * Outer product: Takes two tensors, T1 and T2, with shapes [n1, ..., ni] and [m1, ..., mj], and entry types t1 and t2, and makes a tensor Out with shape [n1, ..., ni, m1, ..., mj] and entry type (t1, t2). If we want to support sparse tensors, this should take a flag specifying how the sparse structure of the output is determined from those of the inputs. I'll call the possible flags ""and"", ""or"", and ""true"". The ""and"" flag says that Out(n, m) is defined iff T1(n) AND T2(m) are both defined. If we will be multiplying the pairs, or applying any other operation with our default missingness semantics, this is the appropriate setting.; The ""or"" flag says Out(n, m) is defined iff T1(n) OR T2(m) is defined, as is appropriate if we are adding the pairs.; ""true"" just means make Out dense, regardless of the sparsity of the inputs. * Map. I don't think there's much to say here. * Generic index operations (not sure what to call these). I'll first give some example",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
Modifiability,variab,variables,"eneric index operations (not sure what to call these). I'll first give some examples using informal notation, then I'll give a proposal for an IR representation. I'll say ""sum"" everywhere, but that could be any aggregation. Let T be a 1-tensor. O represents the output of the operation. Then. * `T(i) -> O()` is the sum of T. * `T(i) -> O(i)` is identity. * `T(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
Security,inject,injective,"(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation using a union-find structure for I. In other words, an index operation consists of a union-find structure I, and two arrays of points of I, encoding the two functions above. Then the composition of (T, I1, M) and (M, I2, O) is (T, I', O), where I' is computed by taking the union I1+I2, then for each dimension of M, unioning the assigned points of I1 and I2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
Usability,simpl,simpler,"Some initial thoughts:; * I'm not sure we should have implicitly broadcasting operations in the IR. It seems simpler to make broadcast an explicit operation, which we make sure to deforest. In fact, broadcast is a special case of the generic tensor index operation I'll describe below. Implicitly broadcasting operations could be provided in the Python interface, making broadcasts explicit when constructing the IR.; * I'm also not sure how much special treatment we should give to block matrices in the IR. I now like to think of block matrices as just 4-tensors, with matrix operations like matrix multiplication lowering to operations on 4-tensors. If we allow tensors to have some distributed dimensions and some ""small"" dimensions, then at least in the backend we might not need special handling of block structures. It may still be helpful to have a special block matrix/tensor representation at the top level IR, or maybe that should only live in Python—I'm not sure. Here's a proposal for a set of primitive tensor operations. * Outer product: Takes two tensors, T1 and T2, with shapes [n1, ..., ni] and [m1, ..., mj], and entry types t1 and t2, and makes a tensor Out with shape [n1, ..., ni, m1, ..., mj] and entry type (t1, t2). If we want to support sparse tensors, this should take a flag specifying how the sparse structure of the output is determined from those of the inputs. I'll call the possible flags ""and"", ""or"", and ""true"". The ""and"" flag says that Out(n, m) is defined iff T1(n) AND T2(m) are both defined. If we will be multiplying the pairs, or applying any other operation with our default missingness semantics, this is the appropriate setting.; The ""or"" flag says Out(n, m) is defined iff T1(n) OR T2(m) is defined, as is appropriate if we are adding the pairs.; ""true"" just means make Out dense, regardless of the sparsity of the inputs. * Map. I don't think there's much to say here. * Generic index operations (not sure what to call these). I'll first give some example",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772
Usability,feedback,feedback,@cseed thanks for the feedback; I'll try urlsafe_b64encode,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459425712
Usability,clear,clear,It's not clear we should do this instead of just making the service work.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5236#issuecomment-459844874
Modifiability,variab,variables,"> Two comments, and a meta-comment:; > ; > * I had looked over async http libraries and had preferred aiohttp over sanic because (1) ""aiohttp"" is a blessed aio library, (2) performance seemed comparable, (4) aiohttp seemed like a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
Performance,perform,performance,"> Two comments, and a meta-comment:; > ; > * I had looked over async http libraries and had preferred aiohttp over sanic because (1) ""aiohttp"" is a blessed aio library, (2) performance seemed comparable, (4) aiohttp seemed like a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
Testability,benchmark,benchmarks,"a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujson wasn't helping much. I can make a ujson-specific pr, but my goal was to test async library implementations in a simple applications, since we need a long term strategy for python web stuff that isn't Flask (or not just Flask)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
Usability,simpl,simpler,"> Two comments, and a meta-comment:; > ; > * I had looked over async http libraries and had preferred aiohttp over sanic because (1) ""aiohttp"" is a blessed aio library, (2) performance seemed comparable, (4) aiohttp seemed like a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051
Performance,queue,queue,"> > 1. It felt outside of the scope of my PR to change that to some queue solution.; > ; > OK, great, thanks for clarifying!. Sure, this is something I am focusing on improving moving forward. Thanks for your feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461210710
Usability,feedback,feedback,"> > 1. It felt outside of the scope of my PR to change that to some queue solution.; > ; > OK, great, thanks for clarifying!. Sure, this is something I am focusing on improving moving forward. Thanks for your feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461210710
Availability,error,errors,"his pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
Deployability,upgrade,upgraded,"http library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 72",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
Energy Efficiency,power,power,"d about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 con",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
Performance,perform,performs,"> > 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis; > ; > Can you point me to the benchmarks? The only head-to-head one I found was this:; > ; > https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto; > ; > where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db. Sanic was chosen because it's a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
Safety,timeout,timeouts,"his pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
Testability,benchmark,benchmarks,"> > 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis; > ; > Can you point me to the benchmarks? The only head-to-head one I found was this:; > ; > https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto; > ; > where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db. Sanic was chosen because it's a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
Usability,user experience,user experience," a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I don’t think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i don’t do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference stand",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030
Usability,clear,clear,"To be clear, I'm working on *aiohttp* project, not *Sanic*.; Regarding TechEmpower -- I did not investigate.; Maybe the problem is trivial, maybe it is fixed on master. ; IIRC Sanic has a partial Flow-Control/HTTP-Pipelining implementation now but I'm not 100% sure.; I have many points to apply my spare time, Sanic problems are not in my TOP-10 personal list. I hope you understand my position.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461451746
Integrability,rout,router,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
Performance,perform,performance,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
Usability,simpl,simple,"Gotcha, thanks, the trouble with reading your reply on the go. I had assumed you contributed to both from your reply. Glad to hear you're taking aiohttp performance seriously. Regarding flow-control, yep, that pr was merged https://github.com/huge-success/sanic/pull/1179. I haven't seen any further conversations from you or Sanic devs on the issue.; * It's not just on master, it's in 0.8. Part of my concerns over Sanic came from reading your post @ https://www.reddit.com/r/Python/comments/876msl/sanic_python_web_server_thats_written_to_die_fast/ ; this now seems outdated, and it would be interesting to hear the reply of a Sanic contributor. Re: ""The last cherry: Sanic has super fast URL router because it caches matching results. The feature is extremely useful for getting awesome numbers with `wrk` tool but in real life URL paths for server usually not constant. URLs like `/users/{userid}` don't fit in cache well :)""; - Surely simple (typical-use, i.e /path or /path/<param>) pattern matching isn't a large performance constraint. I would be surprised if this were a bottleneck in either Sanic or aiohttp.; - If aiohttp is bottlenecked by this, and isn't caching matches, why not? The most common route is say /. Edit: To be clear, the bench mentioned above used 1 worker for Sanic and aiohttp, both using uvloop. Bench attached. [bench.zip](https://github.com/hail-is/hail/files/2841473/bench.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461455096
Availability,down,down,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942
Deployability,update,update,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942
Performance,load,loaded,) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?access_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6Ik16YzNRekpFUXpWRk5VSXdPRE0yTmpJMFF6VkZPVVk1TkRZME9UZzJOa00xUkRBek1ERTJOZyJ9.eyJpc3MiOiJodHRwczovL2hhaWwuYXV0aDAuY29tLyIsInN1YiI6Imdvb2dsZS1vYXV0aDJ8MTEwNzI2NTIxOTIxMjQ5NDQzNzYwIiwiYXVkIjpbImhhaWwiLCJodHRwczovL2hhaWwuYXV0aDAuY29tL3VzZXJpbmZvIl0sImlhdCI6MTU0OTIzMDI2MSwiZXhwIjoxNTQ5MjM3NDYxLCJhenAiOiJURDc4azI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942
Testability,log,logs,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942
Usability,clear,clearly,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942
Usability,guid,guide,"> what's the argument for a Notes. [Numpy style guide](https://numpydoc.readthedocs.io/en/latest/format.html) indicates that the notes section is the appropriate place for detail on the algorithm. In particular, the types of the column and entry fields produced aren't directly related to their names, and having them in the parameter description is unintuitive. This style is also inconsistent with the rest of our documentation. In reading the above link, I've realized that all our docs sections are out of order - the Parameters section should come first, then Returns, then Notes, then Examples. We were hoodwinked by the [Sphinx example page](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_numpy.html). I also strongly reject a warning about a possible future scaling limitation of the function that might never even exist in the life of 0.2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5247#issuecomment-461285927
Usability,clear,clear,"I agree completely. I certainly don't think we can hide or replace Bokeh (I hope the explicit emphasis on Bokeh in the documentation makes this clear), but I think we should continue to add common-case utilities to `hl.plot` life easier for users (and ourselves).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5251#issuecomment-461245241
Performance,load,load,"Sounds good Dan, and agreed it's a long term issue. Regarding point 2, I also don't really like the idea of non-preemtible nodes from a resource utilization standpoint. I think we could probably write our own peak load predictor, or use one of the existing tools, outside of the kube ecosystem. There has been some interesting work using some relatively simple learning models to predict load. It would be interesting to use an RNN for this, but linear regression seems to work pretty well. This could be an interesting topic to investigate. https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-part-2-bb9c4f9b9385",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683
Safety,predict,predictor,"Sounds good Dan, and agreed it's a long term issue. Regarding point 2, I also don't really like the idea of non-preemtible nodes from a resource utilization standpoint. I think we could probably write our own peak load predictor, or use one of the existing tools, outside of the kube ecosystem. There has been some interesting work using some relatively simple learning models to predict load. It would be interesting to use an RNN for this, but linear regression seems to work pretty well. This could be an interesting topic to investigate. https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-part-2-bb9c4f9b9385",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683
Usability,simpl,simple,"Sounds good Dan, and agreed it's a long term issue. Regarding point 2, I also don't really like the idea of non-preemtible nodes from a resource utilization standpoint. I think we could probably write our own peak load predictor, or use one of the existing tools, outside of the kube ecosystem. There has been some interesting work using some relatively simple learning models to predict load. It would be interesting to use an RNN for this, but linear regression seems to work pretty well. This could be an interesting topic to investigate. https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-part-2-bb9c4f9b9385",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683
Usability,feedback,feedback,"I assigned @catoverdrive, but @cseed @tpoterba @chrisvittal may also be interested and/or have useful feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5331#issuecomment-462992065
Availability,error,error,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
Performance,perform,performance,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
Safety,sanity check,sanity check,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
Testability,benchmark,benchmarking,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
Usability,simpl,simple,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410
Deployability,install,install,Ok clearly I need to install g++.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464188941
Usability,clear,clearly,Ok clearly I need to install g++.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464188941
Usability,simpl,simple,Ok I think this works and is simple. @chrisvittal let me know what you think,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464203094
Usability,clear,clear,"@danking this should be good to go. Works. In future PR, should we place move gateway to last line of projects.txt? Not clear to me if CI is enforcing gateway-last in a different way",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5412#issuecomment-466476158
Performance,load,loading,"Should be good to go. There were two problems:. I needed to make the encoder/decoder `@transient lazy`. The encoder/decoder call generated code but can't be serialized. The make functions handle serialization and loading of the generated code. Also, RegionValueAggregators used in scans can have result called multiple times, so I needed to add a MemoryBuffer.clearPos.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5414#issuecomment-466625216
Usability,clear,clearPos,"Should be good to go. There were two problems:. I needed to make the encoder/decoder `@transient lazy`. The encoder/decoder call generated code but can't be serialized. The make functions handle serialization and loading of the generated code. Also, RegionValueAggregators used in scans can have result called multiple times, so I needed to add a MemoryBuffer.clearPos.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5414#issuecomment-466625216
Usability,clear,clearer,"> I realize I wanted `satisfiesAllowedOverlap(key.length - 1)`, not `satisfiesAllowedOverlap(0)`. Because `allowedOverlap` is tricky to think about, I also gave constructor overloads that take `partitionKey` like in the old style. So this case is equivalent to setting `partitionKey == key` in the constructor `def this(partitionKey: Array[String], kType: TStruct, rangeBounds: IndexedSeq[Interval])`, if you think that's clearer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-467085463
Usability,learn,learned,I feel that I've learned more about RVDs and partitioners. The front end of this change looks correct. :+1: from me.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-467119054
Usability,learn,learned,> I feel that I've learned more about RVDs and partitioners. Same here! Good discussion!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-467130254
Usability,clear,clearly,"@danking I think this is set barring change to libsass compilation. If possible, I would like to keep the scss compilation in notebook.py, and create an issue to make a better solution as a step 2. I recognize what you want in broad terms, and am happy to do it, and at the same time the proposed alternative appears more complex, requires me to spend time on research (how to implement auto-reload, not having to retype `make scss` for every style change), and doesn't clearly add value compared to the remaining user-facing work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5430#issuecomment-467256972
Availability,checkpoint,checkpoint,"sorry, wasn't clear. I don't think it's trivial to figure out what a no-args checkpoint should do, but it IS trivial to make that change back-compatibly when we do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469468966
Usability,clear,clear,"sorry, wasn't clear. I don't think it's trivial to figure out what a no-args checkpoint should do, but it IS trivial to make that change back-compatibly when we do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469468966
Testability,test,tested,"(on restart, CI doesn't know that feature branches have been previously tested against master, so it tries to get at least one status finished, for developer feedback purposes)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5550#issuecomment-471026373
Usability,feedback,feedback,"(on restart, CI doesn't know that feature branches have been previously tested against master, so it tries to get at least one status finished, for developer feedback purposes)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5550#issuecomment-471026373
Usability,clear,clear,"# EDIT: DO NOT USE THIS APPROACH. To be clear, a broad user can execute this command on the cluster to fix their environment:. ```; cat >>~/.my.bash.rc <<EOF; mkdir ~/lib; ln -s /broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so ~/lib/libblas.so.3; export LD_LIBARRY_PATH=~/lib:$LD_LIBRARY_PATH; EOF; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5559#issuecomment-472956348
Availability,down,downsampling,"OK, I've moved it and made the interface as close as I could to the previous `scatter`. One thing is the default value for `n_divisions`. It was 500 before, now I've set it to `None` (i.e. no downsampling). I'm fine either way, but it seems somewhat more intuitive to me for the default to be no downsampling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696
Integrability,interface,interface,"OK, I've moved it and made the interface as close as I could to the previous `scatter`. One thing is the default value for `n_divisions`. It was 500 before, now I've set it to `None` (i.e. no downsampling). I'm fine either way, but it seems somewhat more intuitive to me for the default to be no downsampling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696
Usability,intuit,intuitive,"OK, I've moved it and made the interface as close as I could to the previous `scatter`. One thing is the default value for `n_divisions`. It was 500 before, now I've set it to `None` (i.e. no downsampling). I'm fine either way, but it seems somewhat more intuitive to me for the default to be no downsampling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696
Availability,down,down,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
Deployability,integrat,integrating,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
Integrability,integrat,integrating,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
Security,access,access,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
Testability,log,login,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
Usability,simpl,simplifies,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731
Availability,error,error,"@jigold test are fixed, properly clean up; I think the test should be improved to check that deletion properly cleans up expected resources (rather than simply doesn't throw an error, which will happen if deletion fails for any reason other than 404), but I think that could wait for a subsequent PR, because as written, the only way they will fail to do so is if the wrong name or namespace are supplied (else they will throw an error and the test will fail).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478175367
Testability,test,test,"@jigold test are fixed, properly clean up; I think the test should be improved to check that deletion properly cleans up expected resources (rather than simply doesn't throw an error, which will happen if deletion fails for any reason other than 404), but I think that could wait for a subsequent PR, because as written, the only way they will fail to do so is if the wrong name or namespace are supplied (else they will throw an error and the test will fail).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478175367
Usability,simpl,simply,"@jigold test are fixed, properly clean up; I think the test should be improved to check that deletion properly cleans up expected resources (rather than simply doesn't throw an error, which will happen if deletion fails for any reason other than 404), but I think that could wait for a subsequent PR, because as written, the only way they will fail to do so is if the wrong name or namespace are supplied (else they will throw an error and the test will fail).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478175367
Energy Efficiency,efficient,efficient,"@danking img: https://github.com/genuinetools/img. ""Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. img is more cache-efficient than Docker and can also execute multiple build stages concurrently, as it internally uses BuildKit's DAG solver. The commands/UX are the same as docker {build,tag,push,pull,login,logout,save} so all you have to do is replace docker with img in your scripts, command line, and/or life."". Oops, seems it doesn't quite work unprivileged yet, see: https://github.com/genuinetools/img#running-with-docker. Waiting on an upstream docker change, no movement in two months. Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290
Performance,cache,cache-efficient,"@danking img: https://github.com/genuinetools/img. ""Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. img is more cache-efficient than Docker and can also execute multiple build stages concurrently, as it internally uses BuildKit's DAG solver. The commands/UX are the same as docker {build,tag,push,pull,login,logout,save} so all you have to do is replace docker with img in your scripts, command line, and/or life."". Oops, seems it doesn't quite work unprivileged yet, see: https://github.com/genuinetools/img#running-with-docker. Waiting on an upstream docker change, no movement in two months. Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290
Testability,log,login,"@danking img: https://github.com/genuinetools/img. ""Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. img is more cache-efficient than Docker and can also execute multiple build stages concurrently, as it internally uses BuildKit's DAG solver. The commands/UX are the same as docker {build,tag,push,pull,login,logout,save} so all you have to do is replace docker with img in your scripts, command line, and/or life."". Oops, seems it doesn't quite work unprivileged yet, see: https://github.com/genuinetools/img#running-with-docker. Waiting on an upstream docker change, no movement in two months. Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290
Usability,UX,UX,"@danking img: https://github.com/genuinetools/img. ""Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. img is more cache-efficient than Docker and can also execute multiple build stages concurrently, as it internally uses BuildKit's DAG solver. The commands/UX are the same as docker {build,tag,push,pull,login,logout,save} so all you have to do is replace docker with img in your scripts, command line, and/or life."". Oops, seems it doesn't quite work unprivileged yet, see: https://github.com/genuinetools/img#running-with-docker. Waiting on an upstream docker change, no movement in two months. Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290
Deployability,configurat,configuration,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584
Modifiability,config,config,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584
Usability,guid,guide,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584
Availability,error,errors,"Whoa, it worked. I included one change that might have warranted re-review. I was getting errors becomes some Jobs, on which delete had been called, were still being used. I tracked it down to a recent cancel => delete change in `PR.update_from_completed_batch_job`. If look at that function, it is clear delete is not OK because in several cases the build object keep a handle to the job. I reverted it, and now clear all the fields of Job when it is deleted. https://github.com/hail-is/hail/pull/5655/files#diff-433f83d97fa8a526a3f8cff52590e422R479; https://github.com/hail-is/hail/pull/5655/files#diff-0c1f876ad25335b076837f768f727566R59",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475474180
Usability,clear,clear,"Whoa, it worked. I included one change that might have warranted re-review. I was getting errors becomes some Jobs, on which delete had been called, were still being used. I tracked it down to a recent cancel => delete change in `PR.update_from_completed_batch_job`. If look at that function, it is clear delete is not OK because in several cases the build object keep a handle to the job. I reverted it, and now clear all the fields of Job when it is deleted. https://github.com/hail-is/hail/pull/5655/files#diff-433f83d97fa8a526a3f8cff52590e422R479; https://github.com/hail-is/hail/pull/5655/files#diff-0c1f876ad25335b076837f768f727566R59",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475474180
Usability,learn,learned,I'm worried that we're back into a bad state where we'll keep seeing the same job over and over again and not realize we've already learned from it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475664792
Usability,responsiv,responsive,"If this is still an issue, please make a post on the forum, OK? We're more responsive there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/5657#issuecomment-476543562
Usability,clear,clear,"to be clear, there is an option to display row fields, but it defaults to `False`. There's a lot of visual noise if you show both.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5677#issuecomment-476797101
Testability,test,test,Ya I really need to set up running clusters locally. It's a frustrating feedback loop for a single cluster test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5686#issuecomment-477170521
Usability,feedback,feedback,Ya I really need to set up running clusters locally. It's a frustrating feedback loop for a single cluster test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5686#issuecomment-477170521
Modifiability,extend,extending,"nats are now not types and align more with the ReferenceGenome structure, making for better function signatures. There's a `NatBase` with `Nat` and `NatVariable` as extending classes. For `NatVariable`, instances mutate a single class variable `_nat`. Type variables behave similarly using using a map of name -> box, but I didn't really see the point of ever needing more than one `nat` variable in the same context. Either way it seems a little weird so glad to take feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5737#issuecomment-479528708
Usability,feedback,feedback,"nats are now not types and align more with the ReferenceGenome structure, making for better function signatures. There's a `NatBase` with `Nat` and `NatVariable` as extending classes. For `NatVariable`, instances mutate a single class variable `_nat`. Type variables behave similarly using using a map of name -> box, but I didn't really see the point of ever needing more than one `nat` variable in the same context. Either way it seems a little weird so glad to take feedback.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5737#issuecomment-479528708
Availability,failure,failureThreshold,"; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-nam",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
Energy Efficiency,schedul,schedulerName,"tebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-name; secret:; defaultMode: 420; secretName: gsa-key-j7gwm; - name: user-kmpnh-token-hbdd4; secret:; defaultMode: 420; secretName: user-kmpnh-token-hbdd4. hostIP: 10.128.0.32; phase: Running; podIP: 10.32.19.165; qosClass: Burstable; startTime: ""2019-04-02T19:50:21Z""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
Integrability,protocol,protocol,"; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-nam",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
Safety,timeout,timeoutSeconds,"; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-nam",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
Security,access,access,"In-cluster I have the ability to create a pod, including the secret, which is slightly surprising to me. Does the ability create a pod give ability to mount any secret? Surely not. At the same time, my rbac for notebook clearly defines the only secret it can access:. ```; (base) alex:~/projects/hail/notebook2:$ k get role read-get-user-secret -o json; {; ""apiVersion"": ""rbac.authorization.k8s.io/v1"",; ""kind"": ""Role"",; ""rules"": [; {; ""apiGroups"": [; """"; ],; ""resourceNames"": [; ""get-users""; ],; ""resources"": [; ""secrets""; ],; ""verbs"": [; ""get""; ]; }; ]; }; ```. The other permissions are for service and pod resources. These pods are bound to the user's service account. I also don't appear to need to give that service account that is bound (SA ""B"") permission to read the mounted secret. This makes sense to me: the container should be able to access anything on its file system. The notebook leader defines what that is. cc @cseed, thought you may want to know. The following was from a manual in-cluster test:; <img width=""940"" alt=""Screenshot 2019-04-02 15 55 39"" src=""https://user-images.githubusercontent.com/5543229/55432272-78989e00-5560-11e9-960e-1362d277d759.png"">. Partial description of a recently created pod (sans status); ```sh; (base) alex:~/projects/hail/notebook2:$ k get pod notebook2-worker-d4snh -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b199",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
Testability,test,test,"o create a pod, including the secret, which is slightly surprising to me. Does the ability create a pod give ability to mount any secret? Surely not. At the same time, my rbac for notebook clearly defines the only secret it can access:. ```; (base) alex:~/projects/hail/notebook2:$ k get role read-get-user-secret -o json; {; ""apiVersion"": ""rbac.authorization.k8s.io/v1"",; ""kind"": ""Role"",; ""rules"": [; {; ""apiGroups"": [; """"; ],; ""resourceNames"": [; ""get-users""; ],; ""resources"": [; ""secrets""; ],; ""verbs"": [; ""get""; ]; }; ]; }; ```. The other permissions are for service and pod resources. These pods are bound to the user's service account. I also don't appear to need to give that service account that is bound (SA ""B"") permission to read the mounted secret. This makes sense to me: the container should be able to access anything on its file system. The notebook leader defines what that is. cc @cseed, thought you may want to know. The following was from a manual in-cluster test:; <img width=""940"" alt=""Screenshot 2019-04-02 15 55 39"" src=""https://user-images.githubusercontent.com/5543229/55432272-78989e00-5560-11e9-960e-1362d277d759.png"">. Partial description of a recently created pod (sans status); ```sh; (base) alex:~/projects/hail/notebook2:$ k get pod notebook2-worker-d4snh -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
Usability,clear,clearly,"In-cluster I have the ability to create a pod, including the secret, which is slightly surprising to me. Does the ability create a pod give ability to mount any secret? Surely not. At the same time, my rbac for notebook clearly defines the only secret it can access:. ```; (base) alex:~/projects/hail/notebook2:$ k get role read-get-user-secret -o json; {; ""apiVersion"": ""rbac.authorization.k8s.io/v1"",; ""kind"": ""Role"",; ""rules"": [; {; ""apiGroups"": [; """"; ],; ""resourceNames"": [; ""get-users""; ],; ""resources"": [; ""secrets""; ],; ""verbs"": [; ""get""; ]; }; ]; }; ```. The other permissions are for service and pod resources. These pods are bound to the user's service account. I also don't appear to need to give that service account that is bound (SA ""B"") permission to read the mounted secret. This makes sense to me: the container should be able to access anything on its file system. The notebook leader defines what that is. cc @cseed, thought you may want to know. The following was from a manual in-cluster test:; <img width=""940"" alt=""Screenshot 2019-04-02 15 55 39"" src=""https://user-images.githubusercontent.com/5543229/55432272-78989e00-5560-11e9-960e-1362d277d759.png"">. Partial description of a recently created pod (sans status); ```sh; (base) alex:~/projects/hail/notebook2:$ k get pod notebook2-worker-d4snh -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b199",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611
Availability,checkpoint,checkpoint,"> The missing permission is `storage.buckets.get` though? It seems reasonable for a user to [be able to read metadata](https://cloud.google.com/storage/docs/access-control/iam-permissions) about their own bucket. I'd wager that jgscm was designed for use with the `roles/storage.legacyBucketWriter` role granted on their bucket. What role are we currently granting?. The problem I believe is that they would need project-wide read/list permissions. The blob (folder) is not being created in their bucket, but as a new bucket in the project. edit: You can clearly see the difference if you click on the checkpoint folder, back up to the folder /bucket_name and try to create a folder. No additional permissions needed (it's being made in their bucket)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480375549
Security,access,access-control,"> The missing permission is `storage.buckets.get` though? It seems reasonable for a user to [be able to read metadata](https://cloud.google.com/storage/docs/access-control/iam-permissions) about their own bucket. I'd wager that jgscm was designed for use with the `roles/storage.legacyBucketWriter` role granted on their bucket. What role are we currently granting?. The problem I believe is that they would need project-wide read/list permissions. The blob (folder) is not being created in their bucket, but as a new bucket in the project. edit: You can clearly see the difference if you click on the checkpoint folder, back up to the folder /bucket_name and try to create a folder. No additional permissions needed (it's being made in their bucket)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480375549
Usability,clear,clearly,"> The missing permission is `storage.buckets.get` though? It seems reasonable for a user to [be able to read metadata](https://cloud.google.com/storage/docs/access-control/iam-permissions) about their own bucket. I'd wager that jgscm was designed for use with the `roles/storage.legacyBucketWriter` role granted on their bucket. What role are we currently granting?. The problem I believe is that they would need project-wide read/list permissions. The blob (folder) is not being created in their bucket, but as a new bucket in the project. edit: You can clearly see the difference if you click on the checkpoint folder, back up to the folder /bucket_name and try to create a folder. No additional permissions needed (it's being made in their bucket)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480375549
Deployability,deploy,deploy,I've removed the `make test-deploy` stuff to simplify this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-481837110
Testability,test,test-deploy,I've removed the `make test-deploy` stuff to simplify this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-481837110
Usability,simpl,simplify,I've removed the `make test-deploy` stuff to simplify this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-481837110
Performance,concurren,concurrent,"@akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected. The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py. It involves three new things:; - [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; - [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; - [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool. Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop. I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/. Probably demands another review on Monday. cc: @cseed, possibly some asyncio engineering best practices in this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482744047
Testability,log,log,"@akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected. The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py. It involves three new things:; - [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; - [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; - [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool. Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop. I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/. Probably demands another review on Monday. cc: @cseed, possibly some asyncio engineering best practices in this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482744047
Usability,learn,learning,"@akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected. The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py. It involves three new things:; - [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; - [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; - [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool. Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop. I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/. Probably demands another review on Monday. cc: @cseed, possibly some asyncio engineering best practices in this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482744047
Performance,concurren,concurrent,"> @akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected.; > ; > The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py.; > ; > It involves three new things:; > ; > * [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; > * [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; > * [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool.; > ; > Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop.; > ; > I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/.; > ; > Probably demands another review on Monday.; > ; > cc: @cseed, possibly some asyncio engineering best practices in this. Awesome work. I think this will make batch much more performant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482761679
Testability,log,log,"> @akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected.; > ; > The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py.; > ; > It involves three new things:; > ; > * [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; > * [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; > * [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool.; > ; > Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop.; > ; > I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/.; > ; > Probably demands another review on Monday.; > ; > cc: @cseed, possibly some asyncio engineering best practices in this. Awesome work. I think this will make batch much more performant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482761679
Usability,learn,learning,"> @akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected.; > ; > The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py.; > ; > It involves three new things:; > ; > * [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; > * [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; > * [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool.; > ; > Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop.; > ; > I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/.; > ; > Probably demands another review on Monday.; > ; > cc: @cseed, possibly some asyncio engineering best practices in this. Awesome work. I think this will make batch much more performant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482761679
Availability,down,down,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
Modifiability,config,config,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
Safety,risk,risk,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
Security,attack,attacker,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
Usability,learn,learn,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146
Availability,error,error,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
Modifiability,config,config,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
Safety,risk,risk,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
Security,attack,attacker,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
Usability,learn,learn,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170
Usability,clear,clear,@danking ready for another review. This PR has expanded because we needed to properly store/clear session-specific stuff in a bunch of places.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5872#issuecomment-484569009
Usability,feedback,feedback,"Great feedback, thanks @danking. I think I address or responded to all the comments. I also changed the behavior of batch not to run any node until all its ancestors have completed running, which could happen if something failed which caused something else to get cancelled that was always_run. It fixes: https://github.com/hail-is/hail/issues/5903.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5891#issuecomment-484330528
Modifiability,extend,extend,"the script may be clearer and easier to extend if written in python, too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5910#issuecomment-484684296
Usability,clear,clearer,"the script may be clearer and easier to extend if written in python, too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/5910#issuecomment-484684296
Usability,simpl,simple,added disjoint interval. Happy to go over the code with you if you want -- it's pretty simple right now!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6109#issuecomment-492688221
Usability,clear,clear,"Took a look, and not clear why this is an issue. They seem to track different information, with WatchedBranch relating to one branch/sha combination and PR relating to two (with also a sha for the PR itself). Also Code doesn't appear to define a concrete `code` implementation, at least as of now. If you can help me understand the issue, I'll attempt to address tonight.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6142#issuecomment-497841118
Modifiability,parameteriz,parameterize,"OK, I added a pair of simple tests and got everything working. There's one wrinkle: I don't want to ship jars around so I need the executors to have the same jar as the client. That means the need the test jar for the tests. I'm not quite sure how to properly parameterize that in the build system yet, so I'm just leaving it with the test jar for the moment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6221#issuecomment-499365037
Testability,test,tests,"OK, I added a pair of simple tests and got everything working. There's one wrinkle: I don't want to ship jars around so I need the executors to have the same jar as the client. That means the need the test jar for the tests. I'm not quite sure how to properly parameterize that in the build system yet, so I'm just leaving it with the test jar for the moment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6221#issuecomment-499365037
Usability,simpl,simple,"OK, I added a pair of simple tests and got everything working. There's one wrinkle: I don't want to ship jars around so I need the executors to have the same jar as the client. That means the need the test jar for the tests. I'm not quite sure how to properly parameterize that in the build system yet, so I'm just leaving it with the test jar for the moment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6221#issuecomment-499365037
Usability,simpl,simply,"> The check for duplicate key is necessary temporarily to make sure someone doesn't do this.; > ; > ```; > b = batch_client.create_batch(); > j = batch.create_job(); > b2 = batch_client.get_batch(b.id); > j2 = b2.create_job(); > ```; > ; > `j` and `j2` will have the same batch id and job id. Checking the job_id in some capacity is totally necessary, since you're no longer using an auto increment id for the job_id (since you're really emulating one job table per batch, by sticking into a denormalized table containing both batch and job information). However, in the time it takes you to check that the job_id doesn't exist, another process could issue that job_id, invalidating your request, and causing you to fail *anyhow*, despite your check. . I would use the table as the origin of truth of the value, and have python simply read that resulting value. We give MySQL back the control for incrementing the id (by using a nested query, checking the max id in a select statement, and then using +1 that as your id). This will guarantee the behavior that you want, because InnoDB will lock the table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498333308
Energy Efficiency,reduce,reduce,It was just a way to try and reduce the duplication in the code. The correct thing to do is to use requests and not have the overhead of an asynchronous library for a simple client. We can have this discussion in #6244.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6243#issuecomment-498329233
Usability,simpl,simple,It was just a way to try and reduce the duplication in the code. The correct thing to do is to use requests and not have the overhead of an asynchronous library for a simple client. We can have this discussion in #6244.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6243#issuecomment-498329233
Usability,clear,clear,"Just to be clear, I'm proposing:. Pending -> Ready; Ready -> Error, Running; Running -> Ready, Error, Failed, Success",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339906
Usability,simpl,simple,"I looked into this a bit and its not simple. There's a [BOMInputStream](https://commons.apache.org/proper/commons-io/javadocs/api-2.5/org/apache/commons/io/input/BOMInputStream.html) and [definitions of the BOMs](https://commons.apache.org/proper/commons-io/javadocs/api-2.5/org/apache/commons/io/ByteOrderMark.html) in commons. After handling compression and if we are looking at the first byte in the file, we need to check if the first 2-4 bytes are one of the magic BOM constants, if yes we should drop that. Unfortunately, the BOMInputStream doesn't propagate position information along. We probably need a PositionedBOMInputStream and I'm not exactly sure what getPosition should return.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6342#issuecomment-1265654206
Usability,simpl,simplified,"Note, @jigold gets the credit for this, ExportBGEN.scala was basically taken from an old PR of hers that never made it in. I just simplified it to only support the 8-bit case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6462#issuecomment-506362677
Deployability,deploy,deploy,@patrick-schultz sounds like an opportunity to learn how to do a deploy from Tim 😉,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509642293
Usability,learn,learn,@patrick-schultz sounds like an opportunity to learn how to do a deploy from Tim 😉,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509642293
Deployability,deploy,deploy,"Ah yes, this is what I meant by deploy. Patrick can learn how to do a release / pip deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509643864
Usability,learn,learn,"Ah yes, this is what I meant by deploy. Patrick can learn how to do a release / pip deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509643864
Modifiability,rewrite,rewrite,"> I'm curious if there's a way to rewrite this to be more clear about the state machine?. Yes, this is quite confusing as written, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6581#issuecomment-509610153
Usability,clear,clear,"> I'm curious if there's a way to rewrite this to be more clear about the state machine?. Yes, this is quite confusing as written, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6581#issuecomment-509610153
Integrability,message,message,"I figured, noticed the double post :). ```; The exception in your above message is coming from the Apply node being inferred as a PVoid by your case _ => PVoid code. Writing the rule for the apply node should fix that.; ```. Right. It's just that I tried to write the rule, and quickly ran across the fact that Seq[IR] would be inferred such that the first IR had a different type from the 2nd or Nth. This is what I had written:. ```scala; case ApplySpecial(name, irs) => {; val it = irs.iterator; val head = it.next(); head.inferSetPType(env). while(it.hasNext) {; val value = it.next(). value.inferSetPType(env); assert(value.pType2 == head.pType2); }. head.pType2; }; ```. With the result in one case that `head.pType2` was bool, `value.pType2` was something else. Without a type union, it wasn't clear to me what to return. One possibility was that I shouldn't handle this node, so I started with that possibility, which I know understand isn't right. The other was that the implementation was wrong, and my first guess there is that one of the IRs dictates the type (say the first), the 2nd is that there is a simple precedence rule, the 3rd is that the type inference procedure has some branching logic over the collection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513007861
Testability,assert,assert,"I figured, noticed the double post :). ```; The exception in your above message is coming from the Apply node being inferred as a PVoid by your case _ => PVoid code. Writing the rule for the apply node should fix that.; ```. Right. It's just that I tried to write the rule, and quickly ran across the fact that Seq[IR] would be inferred such that the first IR had a different type from the 2nd or Nth. This is what I had written:. ```scala; case ApplySpecial(name, irs) => {; val it = irs.iterator; val head = it.next(); head.inferSetPType(env). while(it.hasNext) {; val value = it.next(). value.inferSetPType(env); assert(value.pType2 == head.pType2); }. head.pType2; }; ```. With the result in one case that `head.pType2` was bool, `value.pType2` was something else. Without a type union, it wasn't clear to me what to return. One possibility was that I shouldn't handle this node, so I started with that possibility, which I know understand isn't right. The other was that the implementation was wrong, and my first guess there is that one of the IRs dictates the type (say the first), the 2nd is that there is a simple precedence rule, the 3rd is that the type inference procedure has some branching logic over the collection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513007861
Usability,clear,clear,"I figured, noticed the double post :). ```; The exception in your above message is coming from the Apply node being inferred as a PVoid by your case _ => PVoid code. Writing the rule for the apply node should fix that.; ```. Right. It's just that I tried to write the rule, and quickly ran across the fact that Seq[IR] would be inferred such that the first IR had a different type from the 2nd or Nth. This is what I had written:. ```scala; case ApplySpecial(name, irs) => {; val it = irs.iterator; val head = it.next(); head.inferSetPType(env). while(it.hasNext) {; val value = it.next(). value.inferSetPType(env); assert(value.pType2 == head.pType2); }. head.pType2; }; ```. With the result in one case that `head.pType2` was bool, `value.pType2` was something else. Without a type union, it wasn't clear to me what to return. One possibility was that I shouldn't handle this node, so I started with that possibility, which I know understand isn't right. The other was that the implementation was wrong, and my first guess there is that one of the IRs dictates the type (say the first), the 2nd is that there is a simple precedence rule, the 3rd is that the type inference procedure has some branching logic over the collection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513007861
Availability,error,error,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139
Safety,safe,safe,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139
Testability,test,tests,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139
Usability,clear,cleared,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139
Availability,down,down,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707
Deployability,pipeline,pipeline,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707
Testability,log,log,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707
Usability,clear,clearly,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707
Usability,undo,undocumented,This uses an undocumented feature. It can wait for a little while.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6629#issuecomment-511021715
Modifiability,inherit,inherits,"> the TableIR doesn't define partitionCounts. statically known partition counts are used to optimize `.count()` when we know the partition sizes. Here that doesn't apply, so I don't think you need to define that method (it inherits `def partitionCounts = None`). > perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable"". Yes, it was. No need to write a rule for this. Separately, I think we should delete the checklist. It'll never be correct, since it's not checked against the codebase. To add an IR node, one needs to understand the compiler, and we can't adequately document that in a bullet list right now (over time, things should get simpler). . If a node is missing from somewhere it needs to appear, then tests should catch that case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766
Performance,optimiz,optimize,"> the TableIR doesn't define partitionCounts. statically known partition counts are used to optimize `.count()` when we know the partition sizes. Here that doesn't apply, so I don't think you need to define that method (it inherits `def partitionCounts = None`). > perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable"". Yes, it was. No need to write a rule for this. Separately, I think we should delete the checklist. It'll never be correct, since it's not checked against the codebase. To add an IR node, one needs to understand the compiler, and we can't adequately document that in a bullet list right now (over time, things should get simpler). . If a node is missing from somewhere it needs to appear, then tests should catch that case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766
Testability,test,tests,"> the TableIR doesn't define partitionCounts. statically known partition counts are used to optimize `.count()` when we know the partition sizes. Here that doesn't apply, so I don't think you need to define that method (it inherits `def partitionCounts = None`). > perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable"". Yes, it was. No need to write a rule for this. Separately, I think we should delete the checklist. It'll never be correct, since it's not checked against the codebase. To add an IR node, one needs to understand the compiler, and we can't adequately document that in a bullet list right now (over time, things should get simpler). . If a node is missing from somewhere it needs to appear, then tests should catch that case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766
Usability,simpl,simpler,"> the TableIR doesn't define partitionCounts. statically known partition counts are used to optimize `.count()` when we know the partition sizes. Here that doesn't apply, so I don't think you need to define that method (it inherits `def partitionCounts = None`). > perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable"". Yes, it was. No need to write a rule for this. Separately, I think we should delete the checklist. It'll never be correct, since it's not checked against the codebase. To add an IR node, one needs to understand the compiler, and we can't adequately document that in a bullet list right now (over time, things should get simpler). . If a node is missing from somewhere it needs to appear, then tests should catch that case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766
Testability,log,logic,"So.... this is technically working, but I'm not very happy with how difficult the logic is to parse through. I'm going to work at tidying it up into a more readable format but would love feedback on how to do that. cc @tpoterba @cseed @patrick-schultz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6727#issuecomment-516096236
Usability,feedback,feedback,"So.... this is technically working, but I'm not very happy with how difficult the logic is to parse through. I'm going to work at tidying it up into a more readable format but would love feedback on how to do that. cc @tpoterba @cseed @patrick-schultz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6727#issuecomment-516096236
Testability,test,test,"Alright, I've addressed all of the above, you were right I was able to move things up to PContainer and simplify some of the code there. I didn't do the one decorator to test cxx and java in this PR because the cxx shape test ended up using some things I haven't implemented on jvm side yet so for now I just made a separate test. I'll add that in a separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-523600895
Usability,simpl,simplify,"Alright, I've addressed all of the above, you were right I was able to move things up to PContainer and simplify some of the code there. I didn't do the one decorator to test cxx and java in this PR because the cxx shape test ended up using some things I haven't implemented on jvm side yet so for now I just made a separate test. I'll add that in a separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-523600895
Deployability,configurat,configurations,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
Modifiability,config,configurations,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
Safety,sanity check,sanity checking,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
Testability,test,test,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
Usability,simpl,simpler,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
Usability,simpl,simple,more improvements (though less simple) to come.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6980#issuecomment-527418968
Energy Efficiency,monitor,monitoring,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
Integrability,rout,router,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
Performance,cache,cache,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
Testability,log,logs,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
Usability,clear,clear,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
Energy Efficiency,monitor,monitoring,"> I still generally find nginx and rules about trailing slashes confusing, but as far as I can tell this seems fine. What do you find confusing?. edit:; Not sure if this helps (if not, let me know, I should be able to clearly answer any questions you have):. Normal location blocks (meaning `location /path/to` with no modifiers or regex) match on prefix, which means that Nginx checks whether the specified `/path/to` exists entirely as a word in the longer url, starting from the beginning of that url (after the domain). A `location = /path/to` (with `=` modifier), is similar in that it is checked against the beginning of the url, with the change that the matched prefix must match exactly (there cannot be anything after `/path/to`). The proxy_pass rule with regard to slashes: Say I have `location /path/to/foo { proxy_pass http://127.0.0.1; }`. Upon matching, Nginx will redirect the request to `127.0.0.1/path/to/foo`, because in the absence of a trailing slash, the entire url (after the domain) is appended. If the proxy_pass directive has a trailing slash, the entire matched prefix is dropped, and only the uri after the prefix is appended. Ex: if the url was `http://domain.com/path/to/foo`, the redirect would be `http://127.0.0.1/`. If the url was `http://domain.com/path/to/foo/bar` the redirect would be to `http://127.0.0.1/bar`. In this PR, when no trailing slash is provided to `https://internal.hail.is/monitoring/service`, the exact location block matches, since the inexact match block has a trailing slash, and therefore doesn't match the slash-less url (moot point anyway: if we made both the inexact and exact location blocks have no slashes, the exact one would take precedence). Then we specify that the proxy_pass has a trailing slash for each block, so that the root of the internal website is `/` rather than `/monitoring/{service}`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-555152983
Usability,clear,clearly,"> I still generally find nginx and rules about trailing slashes confusing, but as far as I can tell this seems fine. What do you find confusing?. edit:; Not sure if this helps (if not, let me know, I should be able to clearly answer any questions you have):. Normal location blocks (meaning `location /path/to` with no modifiers or regex) match on prefix, which means that Nginx checks whether the specified `/path/to` exists entirely as a word in the longer url, starting from the beginning of that url (after the domain). A `location = /path/to` (with `=` modifier), is similar in that it is checked against the beginning of the url, with the change that the matched prefix must match exactly (there cannot be anything after `/path/to`). The proxy_pass rule with regard to slashes: Say I have `location /path/to/foo { proxy_pass http://127.0.0.1; }`. Upon matching, Nginx will redirect the request to `127.0.0.1/path/to/foo`, because in the absence of a trailing slash, the entire url (after the domain) is appended. If the proxy_pass directive has a trailing slash, the entire matched prefix is dropped, and only the uri after the prefix is appended. Ex: if the url was `http://domain.com/path/to/foo`, the redirect would be `http://127.0.0.1/`. If the url was `http://domain.com/path/to/foo/bar` the redirect would be to `http://127.0.0.1/bar`. In this PR, when no trailing slash is provided to `https://internal.hail.is/monitoring/service`, the exact location block matches, since the inexact match block has a trailing slash, and therefore doesn't match the slash-less url (moot point anyway: if we made both the inexact and exact location blocks have no slashes, the exact one would take precedence). Then we specify that the proxy_pass has a trailing slash for each block, so that the root of the internal website is `/` rather than `/monitoring/{service}`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-555152983
Usability,clear,clearer,"@konradjk I fixed the instructions. I remembered investigating this a month ago when a user asked how to do this and I didn't know off the top of my head what the semantics were. And I remembered the ""--"" thing that's used to pass arguments to gcloud and got them mixed up. So this is really not a big docs change now, but I think it's a little clearer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7029#issuecomment-530395992
Deployability,deploy,deploy,"I fixed scorecard deploy stuff and now it is working with `dev deploy`. I also pushed some CSS changes:; - body { margin: 0; } that removes the extra header spacing; - but added an 0 8px 8px 8px margin to #content; - simplified the header layout CSS; - fixed the header item clickable area, should be bigger and uniform across header items; - fixed the misalignment on Safari. I'm pretty happy with this for this iteration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531991835
Usability,simpl,simplified,"I fixed scorecard deploy stuff and now it is working with `dev deploy`. I also pushed some CSS changes:; - body { margin: 0; } that removes the extra header spacing; - but added an 0 8px 8px 8px margin to #content; - simplified the header layout CSS; - fixed the header item clickable area, should be bigger and uniform across header items; - fixed the misalignment on Safari. I'm pretty happy with this for this iteration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531991835
Security,validat,validated,I've validated our setup has those requirements and we're just hitting a FatalError from a commit a few weeks ago. https://github.com/hail-is/hail/blob/a0e8eb81e0f4d7ad446723e7cc04d4c6ac4ad066/hail/python/hail/context.py#L59-L67. If I revert this file we're able to pass in the existing SparkContext with the expected `hl.init(sc=sc)`. As general feedback it may be better to warn here than force exit. I may be wrong but I don't see a way around this for people using remote notebooks to talk to Spark via Livy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-537048154
Usability,feedback,feedback,I've validated our setup has those requirements and we're just hitting a FatalError from a commit a few weeks ago. https://github.com/hail-is/hail/blob/a0e8eb81e0f4d7ad446723e7cc04d4c6ac4ad066/hail/python/hail/context.py#L59-L67. If I revert this file we're able to pass in the existing SparkContext with the expected `hl.init(sc=sc)`. As general feedback it may be better to warn here than force exit. I may be wrong but I don't see a way around this for people using remote notebooks to talk to Spark via Livy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-537048154
Modifiability,variab,variable,"Though running: `pprint(dict(os.environ.items()))`, yielded:. ```; {'CLICOLOR': '1',; 'GIT_PAGER': 'cat',; 'HOME': '/root',; 'INVOCATION_ID': '0faec80a970f4cf29ce69112519fe641',; 'JOURNAL_STREAM': '8:38888',; 'JPY_PARENT_PID': '5858',; 'LANG': 'en_US.UTF-8',; 'LOGNAME': 'root',; 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',; 'PAGER': 'cat',; 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',; 'SHELL': '/bin/sh',; 'SPARKMONITOR_KERNEL_PORT': '38853',; 'TERM': 'xterm-color',; 'USER': 'root'}; ```. which does not include the environment variable you added saying to use the new thing, though that's clearly present in `init_notebook.py`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532865823
Usability,clear,clearly,"Though running: `pprint(dict(os.environ.items()))`, yielded:. ```; {'CLICOLOR': '1',; 'GIT_PAGER': 'cat',; 'HOME': '/root',; 'INVOCATION_ID': '0faec80a970f4cf29ce69112519fe641',; 'JOURNAL_STREAM': '8:38888',; 'JPY_PARENT_PID': '5858',; 'LANG': 'en_US.UTF-8',; 'LOGNAME': 'root',; 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',; 'PAGER': 'cat',; 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',; 'SHELL': '/bin/sh',; 'SPARKMONITOR_KERNEL_PORT': '38853',; 'TERM': 'xterm-color',; 'USER': 'root'}; ```. which does not include the environment variable you added saying to use the new thing, though that's clearly present in `init_notebook.py`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532865823
Security,validat,validate,"Obviously, look forward to feedback on the UI and let me know if you run into any UI bugs. Another todo that I've started:; - write a UI testing playbook to enumerate all the UI interactions we want to test (by hand) to validate this code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534267706
Testability,test,testing,"Obviously, look forward to feedback on the UI and let me know if you run into any UI bugs. Another todo that I've started:; - write a UI testing playbook to enumerate all the UI interactions we want to test (by hand) to validate this code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534267706
Usability,feedback,feedback,"Obviously, look forward to feedback on the UI and let me know if you run into any UI bugs. Another todo that I've started:; - write a UI testing playbook to enumerate all the UI interactions we want to test (by hand) to validate this code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534267706
Usability,simpl,simplify,"addressed your comment (good catch). I commented out a simplify rule that is currently invalid, but will be possible to introduce soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7141#issuecomment-536620808
Usability,simpl,simplify,`TableCollect(TableOrderBy(...))` has a simplify rule that uses a local sort,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7144#issuecomment-570306267
Deployability,install,installs,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
Modifiability,config,config,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
Security,password,password,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
Testability,benchmark,benchmarks,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
Usability,simpl,simpler,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
Usability,feedback,feedback,"yes, sure. I'll ask Kumar for more feedback tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7240#issuecomment-540264157
Usability,clear,clear,"What do you mean specifically? Code-wise? If you're asking how I feel about the code, I think the change is clear now, but it's a bit verbose. I think it was more readable without all of the `_mcpu` extensions, but was also more confusing that way. . I had an idea that I tried to implement earlier where we had a class `Cores` that internally represented the cores as mCPU but then printed everything in CPU when referenced. That could have worked but I had to be careful with how I overrode all of the mathematical operations. I thought this was overkill for the problem so I abandoned it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7241#issuecomment-541195142
Usability,clear,clearest,"Yeah, code-wise. I wanted to see how you felt my suggestion(s) were playing out. I agree with you. Having seen the code, I think I still like this best since it is the least confusing (clearest?) A `Cores` class if we used it consistently is an interesting option, although I also like that everyone knows how integers work and there's no conceptual overhead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7241#issuecomment-541225212
Usability,simpl,simpler,"Not needed, simpler js-based solution in https://github.com/hail-is/hail/pull/7334",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7290#issuecomment-545056325
Usability,undo,undocumented,Let me look over the docs again. There were a number of methods with no documentation when this was created. I don’t don’t quite understand what you mean by too broad (could you clarify?) I just mean that anything that doesn’t have an explanation should not be in the online doc. If you’re saying that there are no longer any undocumented methods then yes let’s close.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7292#issuecomment-558279772
Usability,undo,undocumented,"> But I actually think we are in decent shape now. I agree that we’re in decent shape. At the same time, having undocumented methods in our public documentation is not excellent, and we should strive for excellence/complete coverage",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7292#issuecomment-558282593
Usability,learn,learning,"I imagine this is a low priority issue, but is there a workaround for learning what the structure of a grouped MT is in the meantime? Or has there been any progress on this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7410#issuecomment-585212012
Modifiability,variab,variable,"I think the clear default answer is referential transparency. Whether you bind something in a python variable, or you inline that definition, should be semantically equivalent. Unless we come up with a compelling reason to break that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7418#issuecomment-548485790
Usability,clear,clear,"I think the clear default answer is referential transparency. Whether you bind something in a python variable, or you inline that definition, should be semantically equivalent. Unless we come up with a compelling reason to break that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7418#issuecomment-548485790
Availability,error,error,"@cseed Should the behavior of the logs be to not have a link if the job is ready or pending or to report None? Right now, we report None for `status` if it's ready or pending and have a web.HTTPNotFound error for logs when it's pending or ready. I think we should have it be consistent between logs and status and I think having no links in the ready and pending case is clearer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549440769
Testability,log,logs,"@cseed Should the behavior of the logs be to not have a link if the job is ready or pending or to report None? Right now, we report None for `status` if it's ready or pending and have a web.HTTPNotFound error for logs when it's pending or ready. I think we should have it be consistent between logs and status and I think having no links in the ready and pending case is clearer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549440769
Usability,clear,clearer,"@cseed Should the behavior of the logs be to not have a link if the job is ready or pending or to report None? Right now, we report None for `status` if it's ready or pending and have a web.HTTPNotFound error for logs when it's pending or ready. I think we should have it be consistent between logs and status and I think having no links in the ready and pending case is clearer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549440769
Usability,clear,clear,I wanted to remove /batch in a different PR so it was clear what additional changes there were besides `rm /batch`. I can make it a separate commit though and that should fulfill the same purpose.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7474#issuecomment-551147727
Usability,clear,clear,> I wanted to remove /batch in a different PR so it was clear what additional changes there were besides rm /batch. Smart. Disregard my suggestion.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7474#issuecomment-551180556
Usability,simpl,simple,"OK, another idea to make this easier (maintaining this image separate from the build process is going to be painful): buildImage should have an optional script that runs before the docker build call. If you do this, you can just cat > Dockerfile with a very simple docker file to create the bootstrap image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7534#issuecomment-557184077
Testability,log,logic,"I think I'd flip the logic. I'm not sure if this one is wrong:. ```; In [9]: hl.eval((p, hl.range(2).map(lambda x: p))); Out[9]: (0.46124206583236194, [0.46124206583236194, 0.46124206583236194]); ```. But if it's right, clearly this one is wrong:. ```; In [7]: p = 1 - r. In [8]: hl.eval(hl.range(2).map(lambda x: p)); Out[8]: [0.46124206583236194, 0.06052003544873086]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7572#issuecomment-557020739
Usability,clear,clearly,"I think I'd flip the logic. I'm not sure if this one is wrong:. ```; In [9]: hl.eval((p, hl.range(2).map(lambda x: p))); Out[9]: (0.46124206583236194, [0.46124206583236194, 0.46124206583236194]); ```. But if it's right, clearly this one is wrong:. ```; In [7]: p = 1 - r. In [8]: hl.eval(hl.range(2).map(lambda x: p)); Out[8]: [0.46124206583236194, 0.06052003544873086]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7572#issuecomment-557020739
Usability,clear,clearly,"Sorry will fix this, trying to break up the larger PR clearly failed. One min.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7599#issuecomment-557715665
Deployability,update,update,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375
Energy Efficiency,charge,charge,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375
Safety,avoid,avoid,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375
Usability,clear,clearer,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375
Energy Efficiency,power,powerful,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
Integrability,interface,interface,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
Modifiability,variab,variable," add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter for loops, but I've since changed my mind. I think loops will have hard region management no matter what. > What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. As a side note, @iitalics stream emitter can handle streams of multiple values",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
Performance,perform,performance,"h the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter for loops, but I've since changed my mind. I think loops will have hard region management no matter what. > What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. As a side note, @iitalics stream emitter can handle streams of multiple values fine. Effectively, you can make a `Stream[(Code[A], Code[B], Code[C])]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
Safety,safe,safe,"ecur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
Security,expose,expose,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
Testability,log,logic,"h the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter for loops, but I've since changed my mind. I think loops will have hard region management no matter what. > What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. As a side note, @iitalics stream emitter can handle streams of multiple values fine. Effectively, you can make a `Stream[(Code[A], Code[B], Code[C])]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
Usability,simpl,simpler,"ld implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
Integrability,interface,interface,"This proposal mounts to programming with explicit continuations. It doesn't increase the expressiveness of the loop construct that I can see. Our users are reluctant enough to learn functional programming, I think continuations is one step too far for the user interface. Internally, I don't care as much, although personally I would prefer to code up my solution. @catoverdrive's doing the work, so I'll let them decide. > As a side note, @iitalics stream emitter. Ah, I thought @catoverdrive was referring to IR level streams.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559099522
Usability,learn,learn,"This proposal mounts to programming with explicit continuations. It doesn't increase the expressiveness of the loop construct that I can see. Our users are reluctant enough to learn functional programming, I think continuations is one step too far for the user interface. Internally, I don't care as much, although personally I would prefer to code up my solution. @catoverdrive's doing the work, so I'll let them decide. > As a side note, @iitalics stream emitter. Ah, I thought @catoverdrive was referring to IR level streams.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559099522
Availability,down,download,"I conducted tests on my laptop and on the cluster. I made these comments at https://github.com/hail-is/hail/pull/7534 and on [Zulip](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Dev/topic/ci/near/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
Integrability,rout,route,"ar/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
Performance,latency,latency,"in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also clones the repo, so for my repo I wait at least 2 minutes before I even have a chance to get feedback; with this PR and #7534 I should wait like 45 seconds?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
Testability,test,tests,"I conducted tests on my laptop and on the cluster. I made these comments at https://github.com/hail-is/hail/pull/7534 and on [Zulip](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Dev/topic/ci/near/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
Usability,feedback,feedback,"in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also clones the repo, so for my repo I wait at least 2 minutes before I even have a chance to get feedback; with this PR and #7534 I should wait like 45 seconds?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
Availability,toler,toleration,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
Deployability,configurat,configuration,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
Energy Efficiency,schedul,scheduler,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
Modifiability,config,configuration,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
Usability,simpl,simplest,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
Testability,test,tests,"This is at a place where people could look. Currently plan is to implement storeShallow on every PType separately (could keep all in PType for convenience initially). Majority of functionality: https://github.com/hail-is/hail/pull/7639/files#diff-2cba834adc6803ff8b274f8634bb46c0R394; ; # TODO:. - [x] Implement deep copy; - [ ] Implement for things that are not PCanonicalArray or PArrayBackedContainer; - [ ] Non-staged version; - [ ] More tests. cc @patrick-schultz, @catoverdrive, @danking for feedback",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7639#issuecomment-568135072
Usability,feedback,feedback,"This is at a place where people could look. Currently plan is to implement storeShallow on every PType separately (could keep all in PType for convenience initially). Majority of functionality: https://github.com/hail-is/hail/pull/7639/files#diff-2cba834adc6803ff8b274f8634bb46c0R394; ; # TODO:. - [x] Implement deep copy; - [ ] Implement for things that are not PCanonicalArray or PArrayBackedContainer; - [ ] Non-staged version; - [ ] More tests. cc @patrick-schultz, @catoverdrive, @danking for feedback",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7639#issuecomment-568135072
Performance,optimiz,optimized,"Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments. Cotton also had the suggestion of writing this function unstaged using two utility functions:; ```scala; def findFirstNonZeroByte(addr: Long, n: Long): Long; def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448054
Usability,simpl,simplified,"Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments. Cotton also had the suggestion of writing this function unstaged using two utility functions:; ```scala; def findFirstNonZeroByte(addr: Long, n: Long): Long; def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448054
Modifiability,rewrite,rewrite,"Let's rewrite to use this style. I think that will be both simple and performant, to make us all happy!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448218
Performance,perform,performant,"Let's rewrite to use this style. I think that will be both simple and performant, to make us all happy!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448218
Usability,simpl,simple,"Let's rewrite to use this style. I think that will be both simple and performant, to make us all happy!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448218
Energy Efficiency,power,power,"> Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments.; > ; > Cotton also had the suggestion of writing this function unstaged using two utility functions:; > ; > ```scala; > def findFirstNonZeroByte(addr: Long, n: Long): Long; > def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; > ```. How do we convert from address: Code[Long] to Long without going through emit? I suppose you mean creating an unstated version, and then calling it through invokeScalaObject?. I'm happy to write this. edit: In this version, if we don't want to assume alignment, what is an easier way other than checking the alignment of the addr? I think we're assuming int alignment in much of our codebase, but this is also not guaranteed (since we only aim to guarantee power of 2, as discussed last week).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561475978
Performance,optimiz,optimized,"> Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments.; > ; > Cotton also had the suggestion of writing this function unstaged using two utility functions:; > ; > ```scala; > def findFirstNonZeroByte(addr: Long, n: Long): Long; > def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; > ```. How do we convert from address: Code[Long] to Long without going through emit? I suppose you mean creating an unstated version, and then calling it through invokeScalaObject?. I'm happy to write this. edit: In this version, if we don't want to assume alignment, what is an easier way other than checking the alignment of the addr? I think we're assuming int alignment in much of our codebase, but this is also not guaranteed (since we only aim to guarantee power of 2, as discussed last week).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561475978
Usability,simpl,simplified,"> Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments.; > ; > Cotton also had the suggestion of writing this function unstaged using two utility functions:; > ; > ```scala; > def findFirstNonZeroByte(addr: Long, n: Long): Long; > def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; > ```. How do we convert from address: Code[Long] to Long without going through emit? I suppose you mean creating an unstated version, and then calling it through invokeScalaObject?. I'm happy to write this. edit: In this version, if we don't want to assume alignment, what is an easier way other than checking the alignment of the addr? I think we're assuming int alignment in much of our codebase, but this is also not guaranteed (since we only aim to guarantee power of 2, as discussed last week).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561475978
Modifiability,config,config,"The simpler version is slower, but in the python test, not by a large amount (previous version was, in this test 23.8s or so, although Scala benches may show a larger difference). {""config"": {""cores"": 1, ""version"": ""0.2.28-7888aeb97570"", ""timestamp"": ""2019-12-04 02:07:13.182303"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [28.613776744999996, 28.361242108, 28.481231283]}]}. So 20% slower. I would prefer to use longs, because it doesn't feel right to me to leave performance on the table, however I'm ok with this tradeoff if you find it aligns with your goals better. ; - Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. edit: I propose to put the unstaged version for a later time, but can do now as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056
Performance,perform,performance,"The simpler version is slower, but in the python test, not by a large amount (previous version was, in this test 23.8s or so, although Scala benches may show a larger difference). {""config"": {""cores"": 1, ""version"": ""0.2.28-7888aeb97570"", ""timestamp"": ""2019-12-04 02:07:13.182303"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [28.613776744999996, 28.361242108, 28.481231283]}]}. So 20% slower. I would prefer to use longs, because it doesn't feel right to me to leave performance on the table, however I'm ok with this tradeoff if you find it aligns with your goals better. ; - Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. edit: I propose to put the unstaged version for a later time, but can do now as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056
Testability,test,test,"The simpler version is slower, but in the python test, not by a large amount (previous version was, in this test 23.8s or so, although Scala benches may show a larger difference). {""config"": {""cores"": 1, ""version"": ""0.2.28-7888aeb97570"", ""timestamp"": ""2019-12-04 02:07:13.182303"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [28.613776744999996, 28.361242108, 28.481231283]}]}. So 20% slower. I would prefer to use longs, because it doesn't feel right to me to leave performance on the table, however I'm ok with this tradeoff if you find it aligns with your goals better. ; - Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. edit: I propose to put the unstaged version for a later time, but can do now as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056
Usability,simpl,simpler,"The simpler version is slower, but in the python test, not by a large amount (previous version was, in this test 23.8s or so, although Scala benches may show a larger difference). {""config"": {""cores"": 1, ""version"": ""0.2.28-7888aeb97570"", ""timestamp"": ""2019-12-04 02:07:13.182303"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [28.613776744999996, 28.361242108, 28.481231283]}]}. So 20% slower. I would prefer to use longs, because it doesn't feel right to me to leave performance on the table, however I'm ok with this tradeoff if you find it aligns with your goals better. ; - Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. edit: I propose to put the unstaged version for a later time, but can do now as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056
Usability,clear,clear,"Looks good to me. PNDArray is a little unstable at the moment and probably going to get shaken up in January, so I don't have a clear sense of any must have default methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7734#issuecomment-567100923
Usability,clear,clear,"> Looks good to me. PNDArray is a little unstable at the moment and probably going to get shaken up in January, so I don't have a clear sense of any must have default methods. cool, we'll stay on top of it then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7734#issuecomment-567166972
Usability,clear,clear,"Hey, thanks for all the pictures, this is really clear. Looking at this, tho, I have question: what problem is it solving? Maybe asked another way, what does it look like without this that's an issue?. For a narrow window, you get a double scrollbar: one on the table and one on the window. That has always seemed like bad design to me.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7786#issuecomment-580065507
Usability,UX,UX,"The issue it's solving: allows the dropdown be the requested 75% width of the table. In order to have this you need to set width on the parent of the table, and allow the table to stretch. Hard part is not having it stretch too much when it's empty. Easy alternative is to have the dropdown fixed width. For the double scroll: for very narrow views, I don't think you should see that anymore, unless the table is wider than 1024px. ; * For very wide views, if we set max-width: 100% on the parent, you can have a slightly nicer UX than would have had previously, in that the table is scrollable (so 1 scroll bar), but the search bar is no longer off screen (ex below). But now parent is full width. To solve this can add a media query for narrow views. I found another issue, this time in Safari, will PR. Or can revert, up to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7786#issuecomment-580067553
Usability,simpl,simpler,"> Right I understand. If a PType always places its items into one region, that doesn't seem problematic. I'm having trouble explaining why I think this is wrong. I think it basically makes nothing simpler, and makes many things more complicated (InferPTypes as written will not work, since you have no idea about regions at that point).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575732118
Usability,learn,learn,"Thanks guys, something new to learn!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575745357
Usability,clear,clearly,"Tim, if you're willing would like to keep this open (or make another issue), to track progress on Patrick/your proposal. Patrick walked me through it, and I like it. The proposal I had, although clearly not explained well, is similar in nature, and for educational reasons, would like to talk to you about it, to see pro's/cons (maybe next week?). Patrick's proposal diagram attached. I would like to implement this once this becomes a priority. ![IMG_6021](https://user-images.githubusercontent.com/5543229/72645429-a610d580-3941-11ea-8086-85fe1f8618a4.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575791483
Energy Efficiency,schedul,scheduling,"Would it be useful to give people repr() feedback on lazy operations? Like ""scheduling x"" and ""executing x""? cc @tpoterba. Would be a separate PR, but related issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7828#issuecomment-572726743
Usability,feedback,feedback,"Would it be useful to give people repr() feedback on lazy operations? Like ""scheduling x"" and ""executing x""? cc @tpoterba. Would be a separate PR, but related issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7828#issuecomment-572726743
Usability,feedback,feedback,I think the best way to provide feedback is with things like progress bars.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7828#issuecomment-572727108
Usability,progress bar,progress bar,"For a very fast operation that is expected by the user to be immediately executed, the progress bar would suggest that operation completed very quickly, but wouldn't tell them that the operation that was actually completed was generating an IR with that operation's node present.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7828#issuecomment-572729552
Usability,clear,clearMissingBits,"one sec, need to remove the clearMissingBits function",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7831#issuecomment-572767655
Deployability,release,release,"I don't think this keeps too much garbage in memory. Your next method extracts exactly the data it needs from its producer. No garbage there, you asked for only data you absolutely need. You stated (via `addReferenceTo`) that your region references these child regions, so that memory must be accessible at least as long as your region is accessible. Whoever is consuming your data can release all this memory by clearing the region you're using. The only nodes which should be clearing are folks who call `next` multiple times *and don't need that data to have the same lifetime*. This is true for filter, only surviving values must live, other values' lifetimes may end when we discover they fail the filter condition. It's also true for `write` because after one value is dumped into a file, it is no longer needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366
Security,access,accessible,"I don't think this keeps too much garbage in memory. Your next method extracts exactly the data it needs from its producer. No garbage there, you asked for only data you absolutely need. You stated (via `addReferenceTo`) that your region references these child regions, so that memory must be accessible at least as long as your region is accessible. Whoever is consuming your data can release all this memory by clearing the region you're using. The only nodes which should be clearing are folks who call `next` multiple times *and don't need that data to have the same lifetime*. This is true for filter, only surviving values must live, other values' lifetimes may end when we discover they fail the filter condition. It's also true for `write` because after one value is dumped into a file, it is no longer needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366
Usability,clear,clearing,"I don't think this keeps too much garbage in memory. Your next method extracts exactly the data it needs from its producer. No garbage there, you asked for only data you absolutely need. You stated (via `addReferenceTo`) that your region references these child regions, so that memory must be accessible at least as long as your region is accessible. Whoever is consuming your data can release all this memory by clearing the region you're using. The only nodes which should be clearing are folks who call `next` multiple times *and don't need that data to have the same lifetime*. This is true for filter, only surviving values must live, other values' lifetimes may end when we discover they fail the filter condition. It's also true for `write` because after one value is dumped into a file, it is no longer needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366
Testability,test,test,"Tim, bug in the test fixed...but I'm not entirely clear why it should have caused an issue yet. In PBaseStruct.copyFromType, I was calling srcFieldType.storeShallowAtOffset instead of dstFieldType.storeShallowAtOffset, in a case where srcFieldType was +PCArray and dstFieldType was PCArray, aka:. ```scala; srcFieldType: +PCArray[+PInt32], dstFieldType: PCArray[PInt32]; ```. Where the invocation is:. ```scala; srcFieldType.storeShallowAtOffset(; this.fieldOffset(dstStructAddress, dstField.index),; dstFieldType.copyFromType(...); ```. The storeShallowAtOffset function on PCArray is stateless and identical between required and non-required PCArray instantiations:. ```scala; def storeShallowAtOffset(dstAddress: Code[Long], valueAddress: Code[Long]): Code[Unit] =; Region.storeAddress(dstAddress, valueAddress); ```. I don't have a clear idea why this issue occurred. Also, clearly not easily triggered, required PStruct(""bar"" -> PArray(PInt32(true),false) dest and PStruct(""bar"" -> PArray(PInt32(true),true) source, having the ""bar"" field be a primitive wouldn't do it (we had those tests)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958#issuecomment-578204878
Usability,clear,clear,"Tim, bug in the test fixed...but I'm not entirely clear why it should have caused an issue yet. In PBaseStruct.copyFromType, I was calling srcFieldType.storeShallowAtOffset instead of dstFieldType.storeShallowAtOffset, in a case where srcFieldType was +PCArray and dstFieldType was PCArray, aka:. ```scala; srcFieldType: +PCArray[+PInt32], dstFieldType: PCArray[PInt32]; ```. Where the invocation is:. ```scala; srcFieldType.storeShallowAtOffset(; this.fieldOffset(dstStructAddress, dstField.index),; dstFieldType.copyFromType(...); ```. The storeShallowAtOffset function on PCArray is stateless and identical between required and non-required PCArray instantiations:. ```scala; def storeShallowAtOffset(dstAddress: Code[Long], valueAddress: Code[Long]): Code[Unit] =; Region.storeAddress(dstAddress, valueAddress); ```. I don't have a clear idea why this issue occurred. Also, clearly not easily triggered, required PStruct(""bar"" -> PArray(PInt32(true),false) dest and PStruct(""bar"" -> PArray(PInt32(true),true) source, having the ""bar"" field be a primitive wouldn't do it (we had those tests)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958#issuecomment-578204878
Usability,undo,undocumented,"I think Tim is right and that seems better than an undocumented hidden option, though I admit it's about as undocumented and hidden.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7964#issuecomment-578799305
Deployability,update,updated,"Does not work (passes) with PL present in all rows, which surprised me since type should be virtual type should be taken as stated, I believe. Haven't investigated further (stats day) to see what the IR generated is. Also does not work if I edit the VCF file and insert a bogus PL of .,.,. for each sample. An upcast seems to be happening in the mt1 child, because PL is clearly missing in mt2:. <img width=""705"" alt=""Screenshot 2020-01-31 12 40 13"" src=""https://user-images.githubusercontent.com/5543229/73561429-f9e1eb00-4426-11ea-9bb8-0cec77398d92.png"">. code in updated, pushed test. edit, to show that mt1 does have expected entries (though this shouldn't matter unless array_elements_required doesn't loosen requiredeness over the imputed type):. MT1:; <img width=""170"" alt=""Screenshot 2020-01-31 12 47 00"" src=""https://user-images.githubusercontent.com/5543229/73561943-07e43b80-4428-11ea-847e-65f2f3771af8.png"">; MT2:; <img width=""208"" alt=""Screenshot 2020-01-31 12 47 05"" src=""https://user-images.githubusercontent.com/5543229/73561944-087cd200-4428-11ea-8968-6daf53291d83.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580834992
Testability,test,test,"Does not work (passes) with PL present in all rows, which surprised me since type should be virtual type should be taken as stated, I believe. Haven't investigated further (stats day) to see what the IR generated is. Also does not work if I edit the VCF file and insert a bogus PL of .,.,. for each sample. An upcast seems to be happening in the mt1 child, because PL is clearly missing in mt2:. <img width=""705"" alt=""Screenshot 2020-01-31 12 40 13"" src=""https://user-images.githubusercontent.com/5543229/73561429-f9e1eb00-4426-11ea-9bb8-0cec77398d92.png"">. code in updated, pushed test. edit, to show that mt1 does have expected entries (though this shouldn't matter unless array_elements_required doesn't loosen requiredeness over the imputed type):. MT1:; <img width=""170"" alt=""Screenshot 2020-01-31 12 47 00"" src=""https://user-images.githubusercontent.com/5543229/73561943-07e43b80-4428-11ea-847e-65f2f3771af8.png"">; MT2:; <img width=""208"" alt=""Screenshot 2020-01-31 12 47 05"" src=""https://user-images.githubusercontent.com/5543229/73561944-087cd200-4428-11ea-8968-6daf53291d83.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580834992
Usability,clear,clearly,"Does not work (passes) with PL present in all rows, which surprised me since type should be virtual type should be taken as stated, I believe. Haven't investigated further (stats day) to see what the IR generated is. Also does not work if I edit the VCF file and insert a bogus PL of .,.,. for each sample. An upcast seems to be happening in the mt1 child, because PL is clearly missing in mt2:. <img width=""705"" alt=""Screenshot 2020-01-31 12 40 13"" src=""https://user-images.githubusercontent.com/5543229/73561429-f9e1eb00-4426-11ea-9bb8-0cec77398d92.png"">. code in updated, pushed test. edit, to show that mt1 does have expected entries (though this shouldn't matter unless array_elements_required doesn't loosen requiredeness over the imputed type):. MT1:; <img width=""170"" alt=""Screenshot 2020-01-31 12 47 00"" src=""https://user-images.githubusercontent.com/5543229/73561943-07e43b80-4428-11ea-847e-65f2f3771af8.png"">; MT2:; <img width=""208"" alt=""Screenshot 2020-01-31 12 47 05"" src=""https://user-images.githubusercontent.com/5543229/73561944-087cd200-4428-11ea-8968-6daf53291d83.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580834992
Deployability,update,updated,"> you also don't need to add another resource VCF to make this fail. Yeah, I wasn't sure if the array_elements_required would be overridden if no missingness was found in the data. I wanted to test that possibility (updated my comment with the results of that test). > ah, crap, there's a simplify rule that turns a TableCount(TableUnion(...)) into the sum of the TableCounts for each child. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581049805
Testability,test,test,"> you also don't need to add another resource VCF to make this fail. Yeah, I wasn't sure if the array_elements_required would be overridden if no missingness was found in the data. I wanted to test that possibility (updated my comment with the results of that test). > ah, crap, there's a simplify rule that turns a TableCount(TableUnion(...)) into the sum of the TableCounts for each child. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581049805
Usability,simpl,simplify,"> you also don't need to add another resource VCF to make this fail. Yeah, I wasn't sure if the array_elements_required would be overridden if no missingness was found in the data. I wanted to test that possibility (updated my comment with the results of that test). > ah, crap, there's a simplify rule that turns a TableCount(TableUnion(...)) into the sum of the TableCounts for each child. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581049805
Usability,clear,clear,"we're thinking that GitHub issues should just be bug reports / problems with a clear fix that can be addressed with a maximum of a few commits. Feature requests should be in the forum, development discuss in in dev.hail.is or Zulip dev channel.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-613667435
Usability,clear,clear,"> In my proposal, the body would have to have a stream type. It does:; https://github.com/hail-is/hail/pull/8028/files#diff-beb69e35856952c39b4b4929ac5fb987R973. This design is very similar to what you're looking for. The right structure for the node wasn't super clear (we aren't super far along in lowering, so the needs aren't obvious) but I think the core piece that we need in any kind of lowering intermediate is code-generated iterators, which are implemented here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8028#issuecomment-589016449
Deployability,deploy,deploying,"No state. I printed the session in the logs (I was deploying into prod to test while I had a broken cookie). My session had nothing set except that it was marked as created on January 7th. This despite that I saw a log statement from when I hit /login,m that clearly showed me session with all the right values. My guess is that there was some old signing key or somehow the session got corrupted so you can decode it to get an empty session and add new fields but they fail to be written back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388
Testability,log,logs,"No state. I printed the session in the logs (I was deploying into prod to test while I had a broken cookie). My session had nothing set except that it was marked as created on January 7th. This despite that I saw a log statement from when I hit /login,m that clearly showed me session with all the right values. My guess is that there was some old signing key or somehow the session got corrupted so you can decode it to get an empty session and add new fields but they fail to be written back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388
Usability,clear,clearly,"No state. I printed the session in the logs (I was deploying into prod to test while I had a broken cookie). My session had nothing set except that it was marked as created on January 7th. This despite that I saw a log statement from when I hit /login,m that clearly showed me session with all the right values. My guess is that there was some old signing key or somehow the session got corrupted so you can decode it to get an empty session and add new fields but they fail to be written back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388
Availability,redundant,redundant,"Indeed in my streamify, forcing MakeArray to remain a MakeArray fixes the problem. Now to investigate why MakeStream is the wrong solution, and why the new streamify isn't handling this correctly. to be clear, this branch finds the MakeArray inside of the MakeTuple and generates a ToArray(MakeStream), which both seems not super wrong and redundant. But the fact that's it's a value issue, with an array reading garbage, also make it look like a requiredeness/ copy function issue (though this was previously tested)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511
Safety,redund,redundant,"Indeed in my streamify, forcing MakeArray to remain a MakeArray fixes the problem. Now to investigate why MakeStream is the wrong solution, and why the new streamify isn't handling this correctly. to be clear, this branch finds the MakeArray inside of the MakeTuple and generates a ToArray(MakeStream), which both seems not super wrong and redundant. But the fact that's it's a value issue, with an array reading garbage, also make it look like a requiredeness/ copy function issue (though this was previously tested)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511
Testability,test,tested,"Indeed in my streamify, forcing MakeArray to remain a MakeArray fixes the problem. Now to investigate why MakeStream is the wrong solution, and why the new streamify isn't handling this correctly. to be clear, this branch finds the MakeArray inside of the MakeTuple and generates a ToArray(MakeStream), which both seems not super wrong and redundant. But the fact that's it's a value issue, with an array reading garbage, also make it look like a requiredeness/ copy function issue (though this was previously tested)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511
Usability,clear,clear,"Indeed in my streamify, forcing MakeArray to remain a MakeArray fixes the problem. Now to investigate why MakeStream is the wrong solution, and why the new streamify isn't handling this correctly. to be clear, this branch finds the MakeArray inside of the MakeTuple and generates a ToArray(MakeStream), which both seems not super wrong and redundant. But the fact that's it's a value issue, with an array reading garbage, also make it look like a requiredeness/ copy function issue (though this was previously tested)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511
Integrability,wrap,wrap,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
Performance,perform,perform,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
Safety,safe,safe,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
Testability,test,test,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
Usability,simpl,simplified,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
Usability,simpl,simpler,I'm happy with how much simpler this is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586726532
Usability,simpl,simplicity,"@tpoterba I just did some back of the envelope calculations on this, and while I like the simplicity of it I don't think it scales in a tenable way. I'll let you know when I've fixed that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8071#issuecomment-584740001
Deployability,update,update,"Yeah, this was broken in a recent update to the gcloud libraries. For now, following the command to do:; ```; gcloud config set dataproc/region VALUE; ```; will generate the best user experience. We'll fix this for the next version though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078#issuecomment-584868077
Modifiability,config,config,"Yeah, this was broken in a recent update to the gcloud libraries. For now, following the command to do:; ```; gcloud config set dataproc/region VALUE; ```; will generate the best user experience. We'll fix this for the next version though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078#issuecomment-584868077
Usability,user experience,user experience,"Yeah, this was broken in a recent update to the gcloud libraries. For now, following the command to do:; ```; gcloud config set dataproc/region VALUE; ```; will generate the best user experience. We'll fix this for the next version though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078#issuecomment-584868077
Safety,detect,detect,"@danking . The redirect was caused by lack of X-Forwarded-Host + X-Forwarded-Proto; * https://github.com/expressjs/express/blob/b8e50568af9c73ef1ade434e92c60d389868361d/lib/request.js#L429; * remoteAddress is the url from the last hop, not any forwarded address. > I'll revisit why ghost issues redirects with the new changes this afternoon.; Ghost doesn't read this header. They use X-Forwarded* headers, via Express: https://expressjs.com/en/guide/behind-proxies.html ; * Expresses finds ips using proxy-addr package (the getter is req.ips): https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/lib/request.js#L349; * The test: https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/test/req.ip.js; * proxy-addr does not support x-real-ip: https://github.com/jshttp/proxy-addr/issues/15. Ghost apparently uses X-Forwarded-For to rate limit malicious addresses:. * ""6. Include the X-Forwarded-For header, populated with the remote IP of the original request.; Without this, we aren't able to detect spam traffic patterns and your site risks being rate limited or incorrectly restricted.""; * https://ghost.org/faq/can-i-run-ghost-from-a-subdirectory/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107#issuecomment-587687066
Testability,test,test,"@danking . The redirect was caused by lack of X-Forwarded-Host + X-Forwarded-Proto; * https://github.com/expressjs/express/blob/b8e50568af9c73ef1ade434e92c60d389868361d/lib/request.js#L429; * remoteAddress is the url from the last hop, not any forwarded address. > I'll revisit why ghost issues redirects with the new changes this afternoon.; Ghost doesn't read this header. They use X-Forwarded* headers, via Express: https://expressjs.com/en/guide/behind-proxies.html ; * Expresses finds ips using proxy-addr package (the getter is req.ips): https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/lib/request.js#L349; * The test: https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/test/req.ip.js; * proxy-addr does not support x-real-ip: https://github.com/jshttp/proxy-addr/issues/15. Ghost apparently uses X-Forwarded-For to rate limit malicious addresses:. * ""6. Include the X-Forwarded-For header, populated with the remote IP of the original request.; Without this, we aren't able to detect spam traffic patterns and your site risks being rate limited or incorrectly restricted.""; * https://ghost.org/faq/can-i-run-ghost-from-a-subdirectory/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107#issuecomment-587687066
Usability,guid,guide,"@danking . The redirect was caused by lack of X-Forwarded-Host + X-Forwarded-Proto; * https://github.com/expressjs/express/blob/b8e50568af9c73ef1ade434e92c60d389868361d/lib/request.js#L429; * remoteAddress is the url from the last hop, not any forwarded address. > I'll revisit why ghost issues redirects with the new changes this afternoon.; Ghost doesn't read this header. They use X-Forwarded* headers, via Express: https://expressjs.com/en/guide/behind-proxies.html ; * Expresses finds ips using proxy-addr package (the getter is req.ips): https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/lib/request.js#L349; * The test: https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/test/req.ip.js; * proxy-addr does not support x-real-ip: https://github.com/jshttp/proxy-addr/issues/15. Ghost apparently uses X-Forwarded-For to rate limit malicious addresses:. * ""6. Include the X-Forwarded-For header, populated with the remote IP of the original request.; Without this, we aren't able to detect spam traffic patterns and your site risks being rate limited or incorrectly restricted.""; * https://ghost.org/faq/can-i-run-ghost-from-a-subdirectory/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107#issuecomment-587687066
Usability,simpl,simpler,"I don't like the proliferation of `.copy` methods -- I find it extremely hard to reason about what they're actually doing and where they should be used. I think it would be reasonable for each PType to implement a `setRequired` method, which seems to require the same amount of total code, but makes things a bit simpler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8127#issuecomment-589025030
Usability,simpl,simpler,"> I don't like the proliferation of `.copy` methods -- I find it extremely hard to reason about what they're actually doing and where they should be used. I think it would be reasonable for each PType to implement a `setRequired` method, which seems to require the same amount of total code, but makes things a bit simpler. Will do this. At the same time, would you mind helping me understand why you don't like .copy? This has come up before, and wasn't fully explained. The semantics seem straightforward to me, and for case classes require no new code (since case classes define this copy method): ""construct a new instance of this class, using constructor arguments whose defaults are the object's constructor arguments"". I'd like to understand how you see .copy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8127#issuecomment-589146282
Usability,simpl,simpler,"Tim would you prefer RVD.unify to be separate from RVD.union, such that the caller controls upcast? Interface seems much simpler if RVD.union calls RVD.unify, but may result in unify being called too many times (if the caller unifies rvds, and doesn't realize that they also transitively call RVD.unify because some function the caller directly calls calls RVD.union)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8132#issuecomment-589323266
Energy Efficiency,efficient,efficient,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391
Integrability,wrap,wrap,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391
Performance,perform,performance,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391
Safety,avoid,avoid,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391
Usability,simpl,simplify,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391
Usability,simpl,simpler,"Here are some slightly scattered thoughts:. You're pointing out that sometimes, it is better to unroll a stream, meaning the consumer code is repeated once per stream element. There is a more general version of this, where you want to concatenate several streams, and you might want to repeat the consumer code once per source stream. My instinct in cases like that is that both options (unroll the stream or not) should be explicitly representable in the IR, and the choice should be made before code generation. I think the fundamental issue you're pointing out is that MakeStream is most naturally a push stream (it wants to be the one driving the outer loop), and that because we don't support push streams we're forced to generate suboptimal code. But zip doesn't work for push streams. I think the only general solution is to support both push streams and pull streams explicitly, but that would add a significant amount of complexity. That may be worth doing at some point, but I definitely think we should get the simpler version working first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590972329
Usability,simpl,simplify,I have a second PR coming that will simplify the makefiles but is not a functional change.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-590890096
Energy Efficiency,schedul,scheduler,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
Integrability,message,messages,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
Modifiability,config,config,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
Performance,perform,performance,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
Usability,clear,clear,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
Availability,failure,failures,"Looks like this has failures and needs a rebase. Your PR stack is getting pretty high so let's keep the bottom moving. Also, I rebased my lir branch on Value[T] and now I'm passing the asm4s tests and most other tests are failing on joinpoint which I didn't port. So this stack is now a blocker for me to resume that thread.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594495172
Testability,test,tests,"Looks like this has failures and needs a rebase. Your PR stack is getting pretty high so let's keep the bottom moving. Also, I rebased my lir branch on Value[T] and now I'm passing the asm4s tests and most other tests are failing on joinpoint which I didn't port. So this stack is now a blocker for me to resume that thread.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594495172
Usability,resume,resume,"Looks like this has failures and needs a rebase. Your PR stack is getting pretty high so let's keep the bottom moving. Also, I rebased my lir branch on Value[T] and now I'm passing the asm4s tests and most other tests are failing on joinpoint which I didn't port. So this stack is now a blocker for me to resume that thread.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594495172
Usability,clear,clear,"to be clear, the comment on default is a conversation starter, not a change request",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8164#issuecomment-591670074
Performance,optimiz,optimizer,"> I think we can also get rid of TIterable by refining the type casting in ToSet/ToDict/ToArray/ToStream nodes to reflect the new semantics. Yes, I will consider that in a later pass. > For python---the idea is that we're not really introducing any user-visible changes, right?. Correct. And yes, the optimizer should strip out any glue. > I'm a little nervous about the ToArray(ToStream(dict/set) idiom we're using here. OK, this is fixed. I added a CastToArray which takes a TContainer and does a no-op cast to array type. I use it wherever I can and I added a simplify rule to introduce it when possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8171#issuecomment-592219318
Usability,simpl,simplify,"> I think we can also get rid of TIterable by refining the type casting in ToSet/ToDict/ToArray/ToStream nodes to reflect the new semantics. Yes, I will consider that in a later pass. > For python---the idea is that we're not really introducing any user-visible changes, right?. Correct. And yes, the optimizer should strip out any glue. > I'm a little nervous about the ToArray(ToStream(dict/set) idiom we're using here. OK, this is fixed. I added a CastToArray which takes a TContainer and does a no-op cast to array type. I use it wherever I can and I added a simplify rule to introduce it when possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8171#issuecomment-592219318
Integrability,wrap,wrapper,"> old world a MethodBuilder had a FunctionBuilder as a field, and now instead a FunctionBuilder has a ClassBuilder and also a way to make a MethodBuilder. Uh, so there are three pictures, the old way, this PR, and where I'm headed (the PR comment); - the old way, method builder had a function builder, because function builder was really also the class builder (for an AsmFunctionN); - now I added class builder and function builder (which was really building a class) has one; - in the final picture, function builder will be a simpler wrapper to class and method builders. Method builders will point back to their class.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8179#issuecomment-592278110
Usability,simpl,simpler,"> old world a MethodBuilder had a FunctionBuilder as a field, and now instead a FunctionBuilder has a ClassBuilder and also a way to make a MethodBuilder. Uh, so there are three pictures, the old way, this PR, and where I'm headed (the PR comment); - the old way, method builder had a function builder, because function builder was really also the class builder (for an AsmFunctionN); - now I added class builder and function builder (which was really building a class) has one; - in the final picture, function builder will be a simpler wrapper to class and method builders. Method builders will point back to their class.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8179#issuecomment-592278110
Availability,down,downsampled,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
Security,access,accessing,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
Usability,simpl,simpler,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
Availability,error,errorTransformer,"This PR implements the beginning of the EmitCode and CodeBuilder change discussed here: https://dev.hail.is/t/on-the-subject-of-emittriplet/183/7. The codegen rule for ArrayRef is now the delightfully clear and concise:. ```; EmitCode.fromI(mb) { cb =>; emit(a).toI(cb).flatMap(cb) { (ac) =>; emit(i).toI(cb).flatMap(cb) { (ic) =>; val av = ac.asIndexable.memoize(cb, ""aref_a""); val iv = cb.memoize(ic.tcode[Int], ""aref_i""). cb.ifx(iv < 0 || iv >= av.loadLength(), {; cb._fatal(errorTransformer(; const(""array index out of bounds: index=""); .concat(iv.toS); .concat("", length=""); .concat(av.loadLength().toS))); }); av.loadElement(cb, iv); }; }; }; ```. Summary of changes:. Introduce CodeBuilder. CodeBuilder allows for the imperative, sequential construction of code. The idea is that it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413
Integrability,rout,routines,"it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the imperative style (yet), I wrote routines to convert them back and forth: `EmitCode.fromI { cb => ... }` provides a CodeBuilder and converts a resulting IEmitCode back to an EmitCode, and EmitCode.toI(cb) does the opposite. There is also `(Emit)CodeBuilder.scoped` that will run a code builder function and collect the code as a Code[Unit]. A second idea introduced in this PR is that some PType operations may only be available on a PSettable/PValue, rather than on PCode. The motivation is the canonical array ref implementation, which involves a lot of duplicate code generation. In the current setup, we can have a compound PValue, and this introduces a PCanonicalIndexableValue that has three fields: the base array address, the length and the elements addresss. FYI @patrick-schultz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413
Performance,load,loadLength,"This PR implements the beginning of the EmitCode and CodeBuilder change discussed here: https://dev.hail.is/t/on-the-subject-of-emittriplet/183/7. The codegen rule for ArrayRef is now the delightfully clear and concise:. ```; EmitCode.fromI(mb) { cb =>; emit(a).toI(cb).flatMap(cb) { (ac) =>; emit(i).toI(cb).flatMap(cb) { (ic) =>; val av = ac.asIndexable.memoize(cb, ""aref_a""); val iv = cb.memoize(ic.tcode[Int], ""aref_i""). cb.ifx(iv < 0 || iv >= av.loadLength(), {; cb._fatal(errorTransformer(; const(""array index out of bounds: index=""); .concat(iv.toS); .concat("", length=""); .concat(av.loadLength().toS))); }); av.loadElement(cb, iv); }; }; }; ```. Summary of changes:. Introduce CodeBuilder. CodeBuilder allows for the imperative, sequential construction of code. The idea is that it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413
Usability,clear,clear,"This PR implements the beginning of the EmitCode and CodeBuilder change discussed here: https://dev.hail.is/t/on-the-subject-of-emittriplet/183/7. The codegen rule for ArrayRef is now the delightfully clear and concise:. ```; EmitCode.fromI(mb) { cb =>; emit(a).toI(cb).flatMap(cb) { (ac) =>; emit(i).toI(cb).flatMap(cb) { (ic) =>; val av = ac.asIndexable.memoize(cb, ""aref_a""); val iv = cb.memoize(ic.tcode[Int], ""aref_i""). cb.ifx(iv < 0 || iv >= av.loadLength(), {; cb._fatal(errorTransformer(; const(""array index out of bounds: index=""); .concat(iv.toS); .concat("", length=""); .concat(av.loadLength().toS))); }); av.loadElement(cb, iv); }; }; }; ```. Summary of changes:. Introduce CodeBuilder. CodeBuilder allows for the imperative, sequential construction of code. The idea is that it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413
Usability,guid,guidelines,"Sorry I didn't get to this, Nik! I've been trying to think if we should revisit our experimental module guidelines in light of the questions we're getting on Zulip, but I think things are probably fine.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-600357758
Usability,clear,clear,"RegionPool does have a finalize method, so presumably that will be called on the driver if needed (I don't really know if or when java calls finalizers). I implemented a `clear()` method that reclaims all RegionMemory owned by the pool, but leaves the pool ready to be used by the next task.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8287#issuecomment-597830451
Availability,error,error,"Just to be clear, this pipeline was what I wrote when trying to replicate the bug Duncan was seeing, but it hit a different assertion error than the one he was hitting. He was hitting ""local in the wrong method builder"" problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150
Deployability,pipeline,pipeline,"Just to be clear, this pipeline was what I wrote when trying to replicate the bug Duncan was seeing, but it hit a different assertion error than the one he was hitting. He was hitting ""local in the wrong method builder"" problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150
Testability,assert,assertion,"Just to be clear, this pipeline was what I wrote when trying to replicate the bug Duncan was seeing, but it hit a different assertion error than the one he was hitting. He was hitting ""local in the wrong method builder"" problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150
Usability,clear,clear,"Just to be clear, this pipeline was what I wrote when trying to replicate the bug Duncan was seeing, but it hit a different assertion error than the one he was hitting. He was hitting ""local in the wrong method builder"" problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150
Usability,clear,clear,"I thought about it a bit, and I think it is clear what to do (ultimately) for registered functions: we need a variant that takes PCode instead of Code[_], and then we can begin to migrate the functions incrementally. A separate thread of work, obviously.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8351#issuecomment-603808832
Usability,simpl,simpler,As I started to get attempt_ids working in the DB it seemed simpler to go with your approach. I implemented that instead.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8398#issuecomment-607436509
Deployability,release,release,"Nice work. Let's block release on this, clearly it was bugged before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8436#issuecomment-609932494
Usability,clear,clearly,"Nice work. Let's block release on this, clearly it was bugged before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8436#issuecomment-609932494
Deployability,deploy,deploy,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
Integrability,interface,interface,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
Modifiability,config,config,"such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDelaySeconds: 5; periodSeconds: 5; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
Security,secur,secure,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
Usability,simpl,simpler,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
Usability,clear,clear,"Also, thanks for that detailed write up. That was incredibly clear and instructive.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615210085
Availability,error,error," for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. crea",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
Deployability,deploy,deployment,"wever, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a n",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
Integrability,rout,router-resolver,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
Modifiability,rewrite,rewrite,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
Performance,load,load,"ion strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
Security,secur,security,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
Testability,test,tests,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
Usability,simpl,simplest,"icate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
Modifiability,plugin,plugin,Somewhat surprising that no one has written a shade plugin that renames SO symbols 🤷 . That's clearly the correct answer here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-616208715
Usability,clear,clearly,Somewhat surprising that no one has written a shade plugin that renames SO symbols 🤷 . That's clearly the correct answer here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-616208715
Availability,down,down,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
Deployability,continuous,continuous,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
Integrability,contract,contract,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
Security,expose,exposed,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
Testability,assert,assert,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
Usability,simpl,simple,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
Testability,assert,assertion,"I was never clear on what `sign == 0` meant, but it appears not to be used: the tightened assertion passes everything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8655#issuecomment-620651570
Usability,clear,clear,"I was never clear on what `sign == 0` meant, but it appears not to be used: the tightened assertion passes everything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8655#issuecomment-620651570
Usability,simpl,simplicity,"Eh, as you point out in the billing changes it's 0.02208 USD per core per hour, that's 2 USD per day. It's probably a bit more if we have a full SSD for a 4 core, but still, this is extremely far away from our biggest cost center. I vote for simplicity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8850#issuecomment-635382758
Testability,test,test,"I backed off the support for treating deep NAs as nonequal. That makes the change simpler, and also easier to test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8859#issuecomment-634159069
Usability,simpl,simpler,"I backed off the support for treating deep NAs as nonequal. That makes the change simpler, and also easier to test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8859#issuecomment-634159069
Testability,test,test,"Btw, I actually don't understand how one would use this function (not clear to me from the docs nor the test)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-641489296
Usability,clear,clear,"Btw, I actually don't understand how one would use this function (not clear to me from the docs nor the test)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-641489296
Usability,clear,clearer,"I tried adding clearer names, but I thought that talking about ""new"" vs ""old"" was relatively clear. I may be too deep in it to judge these days.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8867#issuecomment-634763312
Usability,clear,clear,"Alright, think names are clear now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8867#issuecomment-634788030
Usability,clear,clear,"I don't think we have a clear policy on this. When I'm making stacked changes, I use one-commit per PR so that they can be reviewed independently. You're welcome to take either approach.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638969261
Usability,clear,clear,"> I don't think we have a clear policy on this. When I'm making stacked changes, I use one-commit per PR so that they can be reviewed independently. You're welcome to take either approach. I think the policy we've been roughly sticking to on the compilers team is that people don't need to make one commit per PR, but also shouldn't expect that stacked PRs are reviewed until the parent goes in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638971130
Usability,clear,clear,"To be clear, not the link you provided, what the page (https://internal.hail.is/dking/site/index.html) pulls.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639057471
Availability,error,error,"Haven't figured it out yet, but reproduced the error with a simpler pipeline that just uses one annotate instead of `sample_qc`:. ```; P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = mt.annotate_cols(n_called = hl.agg.filter(hl.is_defined(mt.GT), hl.agg.count())); mt = mt.filter_cols(mt.n_called > 0); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception as e:; print(""\n[FAIL] with "", N, ""partitions""); raise e; break; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734
Deployability,pipeline,pipeline,"Haven't figured it out yet, but reproduced the error with a simpler pipeline that just uses one annotate instead of `sample_qc`:. ```; P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = mt.annotate_cols(n_called = hl.agg.filter(hl.is_defined(mt.GT), hl.agg.count())); mt = mt.filter_cols(mt.n_called > 0); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception as e:; print(""\n[FAIL] with "", N, ""partitions""); raise e; break; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734
Usability,simpl,simpler,"Haven't figured it out yet, but reproduced the error with a simpler pipeline that just uses one annotate instead of `sample_qc`:. ```; P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = mt.annotate_cols(n_called = hl.agg.filter(hl.is_defined(mt.GT), hl.agg.count())); mt = mt.filter_cols(mt.n_called > 0); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception as e:; print(""\n[FAIL] with "", N, ""partitions""); raise e; break; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734
Safety,safe,safest,"Actually, a slightly longer answer:. The parent relationship is essentially a representation of the JVM stack. Each X corresponds to 1 JVM bytecode (except NewInstanceX which is fused). During emit, children are pushed on the stack left-to-right by necessity. Slightly more generally, there are two distinct questions here: what the implementation does, and what it guarantees. For what it guarantees, there are two options: fixed (like Java, which evaluates things left-to-right), and undefined (like C). In general, I think it is safest not to rely on the order of evaluation. I'm not aware of where we do. ; Our backend is simple enough and the mapping to the JVM concrete enough that I don't see any reason why we'd have reason to deviate from left-to-right. So I guess I fall somewhere in between (don't rely on it, but violate it).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645533486
Usability,simpl,simple,"Actually, a slightly longer answer:. The parent relationship is essentially a representation of the JVM stack. Each X corresponds to 1 JVM bytecode (except NewInstanceX which is fused). During emit, children are pushed on the stack left-to-right by necessity. Slightly more generally, there are two distinct questions here: what the implementation does, and what it guarantees. For what it guarantees, there are two options: fixed (like Java, which evaluates things left-to-right), and undefined (like C). In general, I think it is safest not to rely on the order of evaluation. I'm not aware of where we do. ; Our backend is simple enough and the mapping to the JVM concrete enough that I don't see any reason why we'd have reason to deviate from left-to-right. So I guess I fall somewhere in between (don't rely on it, but violate it).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645533486
Safety,safe,safest,"> In general, I think it is safest not to rely on the order of evaluation. I'm not aware of where we do. Unless I'm missing something, this code splitter does rely on the evaluation order, which is why I asked about it. If a statement `x` has large children, it moves them to execute before `x`, evaluated in the `children` array order (more precisely, in the post-order traversal order), storing their results in locals. I think it's possible to avoid relying on order of evaluation, if instead of only splitting out sequences of statements, we can also split out a `ValueX` directly, replacing it by a call to new method which returns the result. I don't have strong feelings about whether the order should be guaranteed, just want to be clear which it is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645546246
Usability,clear,clear,"> In general, I think it is safest not to rely on the order of evaluation. I'm not aware of where we do. Unless I'm missing something, this code splitter does rely on the evaluation order, which is why I asked about it. If a statement `x` has large children, it moves them to execute before `x`, evaluated in the `children` array order (more precisely, in the post-order traversal order), storing their results in locals. I think it's possible to avoid relying on order of evaluation, if instead of only splitting out sequences of statements, we can also split out a `ValueX` directly, replacing it by a call to new method which returns the result. I don't have strong feelings about whether the order should be guaranteed, just want to be clear which it is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645546246
Usability,clear,clear,"> Unless I'm missing something, this code splitter does rely on the evaluation order. I don't understand. I think I must be missing something. Maybe there is a bug I don't see. I'm particularly confused by your use of ""rely"". I think splitter, by lifting things out into separate statements (where Block has a clear order of evaluation) implements an order of evaluation. Now, you could ask if the order implemented by splitting is the same as the order of evaluation of the children of the parent. What I'm saying is the the order of evaluation of the children should be undefined, so any choice of splitting out large children is valid (but all orderings being valid, we should prefer left-to-right). Not, I'm just talking about the order of the evaluation of the children. Children are always evaluated before parents (as necessitated by the data flow), and blocks are executed linearly in order.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645561070
Usability,simpl,simple,"There may also be a way of handling the intended effect using alpha blending, but this works and is a simple fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-644687247
Usability,clear,clear,"Better experiment below, you can see in the right hand side the computed values. Note: I don't see any visual difference between .25 and any value < 1 in safari (therefore I did not see clear evidence that safari affected line blending in a way that called for 1 / devicePixelRatio), so for the sake of not complicating this further, I want to keep .25 unless there is evidence this causes issues. Otherwise I think this issue is solved. All on low dpi device (1920*1080 tv):; ![Screenshot 2020-06-16 06 58 11](https://user-images.githubusercontent.com/5543229/84766569-095d6d00-af9f-11ea-8102-6d79eeed2aba.png); ![Screenshot 2020-06-16 06 58 28](https://user-images.githubusercontent.com/5543229/84766571-09f60380-af9f-11ea-9fe6-2fb9ae4bbe43.png); ![Screenshot 2020-06-16 06 58 45](https://user-images.githubusercontent.com/5543229/84766572-09f60380-af9f-11ea-9789-242bc3693598.png); ![Screenshot 2020-06-16 06 59 03](https://user-images.githubusercontent.com/5543229/84766573-09f60380-af9f-11ea-8a11-21c74cc0409d.png). All on high dpi display (pixel ratio 2):; <img width=""1920"" alt=""Screenshot 2020-06-16 07 04 28"" src=""https://user-images.githubusercontent.com/5543229/84766950-a5877400-af9f-11ea-8e4b-a691a1b5f5b7.png"">; <img width=""1920"" alt=""Screenshot 2020-06-16 07 01 51"" src=""https://user-images.githubusercontent.com/5543229/84766749-55101680-af9f-11ea-9a77-c7bacc79dd16.png"">; <img width=""1920"" alt=""Screenshot 2020-06-16 07 02 03"" src=""https://user-images.githubusercontent.com/5543229/84766751-55a8ad00-af9f-11ea-8e49-3c68f588fb0f.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-644692798
Usability,simpl,simple,"This change achieves the effect you desire. What I additionally need from this PR is a mental model of why this change works. This change specifies a pixel ratio of two when the device's physical:css pixel ratio is one, but uses the device's pixel ratio otherwise. This breaks my conceptual model. It's important for us all to share compatible models of what the code does so that we all are able to manipulate the code in the future. I think there's a few moving pieces and if we can get a handle on them all, we'll all agree on what the right fix is. AFAICT, three js is built on WebGL. The MDN recommends not using a WebGL `lineWidth` other than one because of inconsistent (or lack of) support for line widths other than one. You observe that `pixelRatio` affects the visual behavior, at least in Safari, when the lineWidth is set to 0.25. In particular, when the pixel ratio is set to `2` and the lineWidth is set to `0.25`, the lines appear thinner. If the pixel ratio is higher than the device pixel ratio, something must be interpolating to device pixels. It seems to me that relying on this interpolation behavior will lead to code that is more difficult to understand and manipulate. If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645569518
Integrability,depend,dependent,"> This change achieves the effect you desire. What I additionally need from this PR is a mental model of why this change works. I think I communicated this above: as far as I understand it, the effective resolution of the graphic is dependent on both the viewport resolution and the pixel ratio. The observed resolution is too low with pixelRatio 1, gives thick lines. . reference: https://github.com/mrdoob/three.js/issues/16747. > You observe that pixelRatio affects the visual behavior, at least in Safari, when the lineWidth is set to 0.25. In particular, when the pixel ratio is set to 2 and the lineWidth is set to 0.25, the lines appear thinner. No, as you see above, regardless of pixel ratio, Safari's minimum line width is somewhere below one. Setting a linewidth below 1 has no drawbacks; it will just cause the browser to use the min. > If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?. Not that I know of. Some people use cylinder geometry to get around line thickness issues. Yes, there may be an alpha-based solution, which I brought up above. This will be more complicated, and further increase the development cycle. > It seems to me that relying on this interpolation behavior will lead to code that is more difficult to understand and manipulate. I don't understand. Is there another fix that seems easier?; ; > If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?. Potentially, but this will obviously be a significantly more complicated fix. The proposed fix is effectively 1 line, and takes care of the observed issue in a wide range of browsers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645572011
Usability,simpl,simple,"> This change achieves the effect you desire. What I additionally need from this PR is a mental model of why this change works. I think I communicated this above: as far as I understand it, the effective resolution of the graphic is dependent on both the viewport resolution and the pixel ratio. The observed resolution is too low with pixelRatio 1, gives thick lines. . reference: https://github.com/mrdoob/three.js/issues/16747. > You observe that pixelRatio affects the visual behavior, at least in Safari, when the lineWidth is set to 0.25. In particular, when the pixel ratio is set to 2 and the lineWidth is set to 0.25, the lines appear thinner. No, as you see above, regardless of pixel ratio, Safari's minimum line width is somewhere below one. Setting a linewidth below 1 has no drawbacks; it will just cause the browser to use the min. > If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?. Not that I know of. Some people use cylinder geometry to get around line thickness issues. Yes, there may be an alpha-based solution, which I brought up above. This will be more complicated, and further increase the development cycle. > It seems to me that relying on this interpolation behavior will lead to code that is more difficult to understand and manipulate. I don't understand. Is there another fix that seems easier?; ; > If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?. Potentially, but this will obviously be a significantly more complicated fix. The proposed fix is effectively 1 line, and takes care of the observed issue in a wide range of browsers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645572011
Usability,clear,clear,"If you're actually seeing some inconsistent results in some browser, I agree the solution isn't sufficient. Else, why not get the easy solution in, and do more work when it's needed. I see a potential problem statement, no clear reason why the present solution is problematic, and a desire to move to a different solution. . I looked into the relationship between fractional line widths and alpha a few days ago, when I wrote the comment suggesting that alpha blending could be an alternative solution. Antialiased fractional line width (which is also what you would get if the effective resolution is larger than the viewport resolution and then objects scaled), will act like an alpha-blended 1px line width. reference: https://community.khronos.org/t/how-to-draw-a-line-with-width-less-than-1-0/42022",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645574948
Testability,test,test,"> I believe you're referencing Chrome Bug 675308 which suggests that lines thicker than 1 are broken in some versions of Chrome. My change uses line width 1. My point was that there are browser inconsistencies. > The screenshots you shared seem to suggest that Brave is making the lines somewhat fainter than Safari. I can't explain that. . I also can't explain it, and haven't found a Safari bug that would suggest why this is the case, and yet it is clear from that screenshot that there is a difference. > It feels brittle to fiddle with device pixel ratio and line width to try and simulate line widths smaller than a pixel. My current mental model is that we're just hitting a resolution issue. Internally threejs I believe sets the webgl buffer to pixelRatio * width/height. With resolution 1, resolution seems too low, and this causes fuzzy lines. I don't know why it doesn't work better. To test this hypothesis, I've tried setting pixelRatio to 1 manually on a hidpi display, and it gave a similar fuzzy/thick result, which of course doesn't make any sense if linewidth actually did what it seems it should, and so I agree that not relying on linewidth would be nicer, but would also be additional work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-646227792
Usability,clear,clear,"> I believe you're referencing Chrome Bug 675308 which suggests that lines thicker than 1 are broken in some versions of Chrome. My change uses line width 1. My point was that there are browser inconsistencies. > The screenshots you shared seem to suggest that Brave is making the lines somewhat fainter than Safari. I can't explain that. . I also can't explain it, and haven't found a Safari bug that would suggest why this is the case, and yet it is clear from that screenshot that there is a difference. > It feels brittle to fiddle with device pixel ratio and line width to try and simulate line widths smaller than a pixel. My current mental model is that we're just hitting a resolution issue. Internally threejs I believe sets the webgl buffer to pixelRatio * width/height. With resolution 1, resolution seems too low, and this causes fuzzy lines. I don't know why it doesn't work better. To test this hypothesis, I've tried setting pixelRatio to 1 manually on a hidpi display, and it gave a similar fuzzy/thick result, which of course doesn't make any sense if linewidth actually did what it seems it should, and so I agree that not relying on linewidth would be nicer, but would also be additional work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-646227792
Usability,learn,learned,"I'll have to think about the circular thing, but I learned that; ```; from __future__ import annotations; ```; let's you use names directly rather than as strings.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-644774393
Deployability,install,installing,"@jigold this should pass now. I also learned that Mypy checks each command line argument as an independent module. Instead of specifying individual files, we have to tell it to check `batch`. I also added `google_storage.py` in an ill-fated attempt to let batch use that. I think we can only type check modules that depend on one another by installing them first. We'll leave that for future work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-662143859
Integrability,depend,depend,"@jigold this should pass now. I also learned that Mypy checks each command line argument as an independent module. Instead of specifying individual files, we have to tell it to check `batch`. I also added `google_storage.py` in an ill-fated attempt to let batch use that. I think we can only type check modules that depend on one another by installing them first. We'll leave that for future work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-662143859
Usability,learn,learned,"@jigold this should pass now. I also learned that Mypy checks each command line argument as an independent module. Instead of specifying individual files, we have to tell it to check `batch`. I also added `google_storage.py` in an ill-fated attempt to let batch use that. I think we can only type check modules that depend on one another by installing them first. We'll leave that for future work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-662143859
Usability,simpl,simpler,"> I also think this should just be defined inline in the makeNDArray emitter, we shouldn't need to change this file. Good point that's simpler",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9094#issuecomment-660299878
Energy Efficiency,power,powerful,"I think you're right. I tried a number of things, but I need something to key the column by, and a global has no concept a key (which is why it is a global). I found this very confusing. . Let's say mt.C contains phenotypes for samples 1..n. This is, in my mind, a distributed array, with someone fancy (non-integer) indexing support. Great, but I don't care about that, I just want a distributed array. I want to localize_entries, but this creates a hail Table, which drops my phenotypes, because that's now a table and not a matrix table (why! all I wanted was to create a new field in my MT with the result of a column aggregation per row). So the natural thing I reach to is storing my phenotypes elsewhere. I think: ""I want to continue benefitting from Hail query planner), so I try not to materialize the phenotypes in memory. If I say mt.annotate_globals(Y = mt.C) I expect that to just work, because in my mind, I took something that was a a distributed array, but with more powerful indexing support, and converted it to something that is even more array like, that I'm going to need to understand how to index myself (which I'm fine with since I'm moving the thing to globals). Alternatively, I could also expect that globals now contains a reference to a new table, that contains only the column index, and value (phenotype), which seems fine. Neither of these options happens. Instead, I need to realize the array in memory on my master, which seems like a potentially bad idea. The bigger problem though is that I want 1 change (simplify indexing or make a reference to the array), and I seem to need 3 (that + memory + loss of distribution). . In short: I want to be able to choose whether I realize the values in memory, not be forced into it. Let me know if there's something I missed!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121#issuecomment-662693797
Usability,simpl,simplify,"I think you're right. I tried a number of things, but I need something to key the column by, and a global has no concept a key (which is why it is a global). I found this very confusing. . Let's say mt.C contains phenotypes for samples 1..n. This is, in my mind, a distributed array, with someone fancy (non-integer) indexing support. Great, but I don't care about that, I just want a distributed array. I want to localize_entries, but this creates a hail Table, which drops my phenotypes, because that's now a table and not a matrix table (why! all I wanted was to create a new field in my MT with the result of a column aggregation per row). So the natural thing I reach to is storing my phenotypes elsewhere. I think: ""I want to continue benefitting from Hail query planner), so I try not to materialize the phenotypes in memory. If I say mt.annotate_globals(Y = mt.C) I expect that to just work, because in my mind, I took something that was a a distributed array, but with more powerful indexing support, and converted it to something that is even more array like, that I'm going to need to understand how to index myself (which I'm fine with since I'm moving the thing to globals). Alternatively, I could also expect that globals now contains a reference to a new table, that contains only the column index, and value (phenotype), which seems fine. Neither of these options happens. Instead, I need to realize the array in memory on my master, which seems like a potentially bad idea. The bigger problem though is that I want 1 change (simplify indexing or make a reference to the array), and I seem to need 3 (that + memory + loss of distribution). . In short: I want to be able to choose whether I realize the values in memory, not be forced into it. Let me know if there's something I missed!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121#issuecomment-662693797
Usability,clear,clear,"sorry, I think I wasn't clear -- you can put them in globals when doing `mt.localize_entries` by passing both the `entries_field_name` and `columns_array_field_name` params.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121#issuecomment-662700219
Testability,test,testng,"This more or less follows the same strategy as the python splitting. Instead of each job figuring out which split it owns, I create 5 separate testng.xml files in `build_hail`. Each job reads one of those files. testng.xml has a pretty simple XML format and you can explicitly list the classes of interest. I noticed that hail java tests were up to 15 minutes which was really cramping my PR merging style. With this, some of the splits are a minute or two. I think we suffer a bit from programmatically generated tests since I'm splitting at the granularity of classes rather than methods, or, even better, generated methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9139#issuecomment-663248573
Usability,simpl,simple,"This more or less follows the same strategy as the python splitting. Instead of each job figuring out which split it owns, I create 5 separate testng.xml files in `build_hail`. Each job reads one of those files. testng.xml has a pretty simple XML format and you can explicitly list the classes of interest. I noticed that hail java tests were up to 15 minutes which was really cramping my PR merging style. With this, some of the splits are a minute or two. I think we suffer a bit from programmatically generated tests since I'm splitting at the granularity of classes rather than methods, or, even better, generated methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9139#issuecomment-663248573
Deployability,install,installing-editable,"Interesting: I tried in a new session (after checking out master, installing-editable), and this time I could break things with out-of-bounds slices, but `hl.eval(a[0:a.shape[0],0:1]) ` and `hl.eval(a[0:a.shape[0],0:2])` worked ok. So what's going on?. ```python; In [22]: hl.eval(a[0:a.shape[0],0:1]) ; Out[22]: ; array([[1],; [2],; [3],; [4],; [5]], dtype=int32). In [23]: hl.eval(a[0:a.shape[0],0:1]) ; Out[23]: ; array([[1],; [2],; [3],; [4],; [5]], dtype=int32). In [24]: a = a.T . In [25]: hl.eval(a) ; Out[25]: ; array([[ 1, 2, 3, 4, 5],; [ 7, 6, 8, 9, 10]], dtype=int32). In [26]: a = a.T . In [27]: hl.eval(a) ; Out[27]: ; array([[ 1, 7],; [ 2, 6],; [ 3, 8],; [ 4, 9],; [ 5, 10]], dtype=int32). In [28]: hl.eval(a[0:a.shape[0],0:1]) ; Out[28]: ; array([[1],; [7],; [0],; [2],; [0]], dtype=int32). In [32]: hl.eval(a[0:a.shape[0],0:1]) ; Out[32]: ; array([[ 1],; [ 7],; [ 0],; [ 4],; [32749]], dtype=int32); ``` . totally broken. Seems like 2 problems: 1) out of bounds checks not being done on inner dimension. 2) strides get scrambled between transposes, or something isn't being cleared after transposition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9144#issuecomment-663267587
Usability,clear,cleared,"Interesting: I tried in a new session (after checking out master, installing-editable), and this time I could break things with out-of-bounds slices, but `hl.eval(a[0:a.shape[0],0:1]) ` and `hl.eval(a[0:a.shape[0],0:2])` worked ok. So what's going on?. ```python; In [22]: hl.eval(a[0:a.shape[0],0:1]) ; Out[22]: ; array([[1],; [2],; [3],; [4],; [5]], dtype=int32). In [23]: hl.eval(a[0:a.shape[0],0:1]) ; Out[23]: ; array([[1],; [2],; [3],; [4],; [5]], dtype=int32). In [24]: a = a.T . In [25]: hl.eval(a) ; Out[25]: ; array([[ 1, 2, 3, 4, 5],; [ 7, 6, 8, 9, 10]], dtype=int32). In [26]: a = a.T . In [27]: hl.eval(a) ; Out[27]: ; array([[ 1, 7],; [ 2, 6],; [ 3, 8],; [ 4, 9],; [ 5, 10]], dtype=int32). In [28]: hl.eval(a[0:a.shape[0],0:1]) ; Out[28]: ; array([[1],; [7],; [0],; [2],; [0]], dtype=int32). In [32]: hl.eval(a[0:a.shape[0],0:1]) ; Out[32]: ; array([[ 1],; [ 7],; [ 0],; [ 4],; [32749]], dtype=int32); ``` . totally broken. Seems like 2 problems: 1) out of bounds checks not being done on inner dimension. 2) strides get scrambled between transposes, or something isn't being cleared after transposition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9144#issuecomment-663267587
Usability,intuit,intuition,"Ahh, hmm. Pylint agrees with me that this isn't Kosher:; ```; /hailtop/batch/backend.py:44:8: W0107: Unnecessary pass statement (unnecessary-pass); /hailtop/batch/backend.py:47:0: W0223: Method 'close' is abstract in class 'Backend' but is not overridden (abstract-method); ```. We've generally heeded pylint's advice even if it clashes with our intuition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9191#issuecomment-667227984
Availability,down,down,"ppose, ""applications"" that use Batch. Since we plan to support, maintain, and test the Hail Batch regenie implementation, I think it doesn't belong in a ""contrib"" directory. The hail python package has a `genetics` module for genetics-specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BGEN files for testing in the Hail src/test/resources. I already have an example provided. The",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987
Integrability,depend,dependencies,"> Thanks for sharing this detailed plan!; > ; > So, I don't currently have any plan for, I suppose, ""applications"" that use Batch. Since we plan to support, maintain, and test the Hail Batch regenie implementation, I think it doesn't belong in a ""contrib"" directory. The hail python package has a `genetics` module for genetics-specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BG",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987
Modifiability,config,config,"specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BGEN files for testing in the Hail src/test/resources. I already have an example provided. The example folder contains the config for that, and the regenie folder contains the example. We need an example that has a known result, and regenie's c++ repo conveniently provides that. This is what the regenie/regenie folder contains.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987
Testability,test,test,"> Thanks for sharing this detailed plan!; > ; > So, I don't currently have any plan for, I suppose, ""applications"" that use Batch. Since we plan to support, maintain, and test the Hail Batch regenie implementation, I think it doesn't belong in a ""contrib"" directory. The hail python package has a `genetics` module for genetics-specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BG",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987
Usability,clear,clear,"specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BGEN files for testing in the Hail src/test/resources. I already have an example provided. The example folder contains the config for that, and the regenie folder contains the example. We need an example that has a known result, and regenie's c++ repo conveniently provides that. This is what the regenie/regenie folder contains.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987
Deployability,install,installed,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400
Integrability,depend,dependent,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400
Performance,perform,performs,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400
Usability,clear,clear,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400
Deployability,install,installed,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
Integrability,depend,dependent,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
Performance,perform,performs,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
Testability,test,test,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
Usability,clear,clear,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
Performance,optimiz,optimizing,I talked to Cotton about it and he said not to. But it's not clear how much of a difference that makes yet anyway. I think this version is pretty good and an improvement. Plus it'll add a benchmark which we can work on optimizing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668622849
Testability,benchmark,benchmark,I talked to Cotton about it and he said not to. But it's not clear how much of a difference that makes yet anyway. I think this version is pretty good and an improvement. Plus it'll add a benchmark which we can work on optimizing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668622849
Usability,clear,clear,I talked to Cotton about it and he said not to. But it's not clear how much of a difference that makes yet anyway. I think this version is pretty good and an improvement. Plus it'll add a benchmark which we can work on optimizing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668622849
Usability,usab,usability,"This is on the radar, so I vote for closing this issue. We're using issues for bugs, and this seems more in the realm of a feature request / usability improvement.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226#issuecomment-672877232
Usability,simpl,simplification,"I just realized, `refreshBuffer` is generally always accompanied by a (possibly implicit), `bufferCursor = start`, so that's a simplification that I'm going to try.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676511001
Usability,simpl,simplified,"Bah, fixed. There was some weird grouping going on in the svgs that was causing the PNG conversion to do something weird, so I simplified all the svgs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9382#issuecomment-684992756
Usability,clear,clearly,"Looking at our documentation, we document `n` as `maximum number of splits`. That makes this seem like a bug to me, especially in the 0 case. But clearly people use this function and this change is breaking to anyone who uses it. My vote would be to do the deprecation thing I suggested above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9383#issuecomment-684034614
Performance,cache,cache,w on D's values; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.TupleExpression.count:: WARNING: py:class reference target not found: integer -- return number of occurrences of value; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.TupleExpression.index:: WARNING: py:class reference target not found: integer -- return first index of value.; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.annotate_cols:9: WARNING: py:class reference target not found: hail.Table; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.annotate_rows:11: WARNING: py:class reference target not found: TVariant; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.cache:11: WARNING: py:func reference target not found: hail.MatrixTable.persist; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:15: WARNING: py:data reference target not found: int64; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:17: WARNING: py:data reference target not found: int64; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:19: WARNING: py:data reference target not found: float32; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.index_cols:17: WARNING: py:meth reference target not found: index_cols(exprs); /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.index_rows:17: WARNING: py:meth reference target not found: index_rows(exprs); /Users/dking/projects/hail/hail/pyt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-685996666
Usability,guid,guides,r.functions.qnorm:1: WARNING: py:meth reference target not found: pnorm; /Users/dking/projects/hail/hail/python/hail/expr/functions.py:docstring of hail.expr.functions.qpois:1: WARNING: py:meth reference target not found: ppois; /Users/dking/projects/hail/hail/python/hail/expr/functions.py:docstring of hail.expr.functions.qpois:15: WARNING: py:meth reference target not found: ppois; /Users/dking/projects/hail/hail/python/hail/genetics/pedigree.py:docstring of hail.genetics.pedigree.Pedigree.filter_to:: WARNING: py:class reference target not found: list of str; /Users/dking/projects/hail/hail/python/hail/genetics/pedigree.py:docstring of hail.genetics.pedigree.Pedigree.write:14: WARNING: py:meth reference target not found: hail.KeyTable.import_fam; /Users/dking/projects/hail/hail/python/hail/genetics/reference_genome.py:docstring of hail.genetics.reference_genome.ReferenceGenome.write:10: WARNING: py:meth reference target not found: hail.ReferenceGenome.read; /Users/dking/projects/hail/hail/python/hail/docs/guides/api.rst:19: WARNING: py:func reference target not found: Table.show; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.CallExpression.one_hot_alleles:25: WARNING: py:obj reference target not found: tint32; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.IntervalExpression.overlaps:11: WARNING: py:data reference target not found: tinterval; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.NDArrayExpression.T:5: WARNING: py:func reference target not found: transpose; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.NDArrayNumericExpression.T:5: WARNING: py:func reference target not found: transpose; /Users/dking/projects/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-685996666
Usability,simpl,simple,"cause is that newer versions of autodoc include, [as an experimental addition](https://www.sphinx-doc.org/en/2.0/usage/extensions/autodoc.html#generating-documents-from-type-annotations), sphinx-autodoc-typehints. This addition (which we use in batch) only works when a class is documented using its true name (i.e. where it is defined, not re-exported). A quick fix is to [disable this functionality](https://www.sphinx-doc.org/en/2.0/usage/extensions/autodoc.html#confval-autodoc_typehints):; ```; autodoc_typehints = 'none'; ```. - autodoc issue about this https://github.com/agronholm/sphinx-autodoc-typehints/issues/38; - another autodoc issue with a fix for the particular use case https://github.com/agronholm/sphinx-autodoc-typehints/issues/124; - root sphinx issue wrt fully qualified names versus the documented name: https://github.com/sphinx-doc/sphinx/issues/4826. The autodoc-typehints maintainer seems to have gotten stuck when trying to fix this. It appears that someone went and figured out enough of Sphinx to [fix this](https://git-cral.univ-lyon1.fr/MUSE/mpdaf/blob/23d52ba059fe76df5e1655542b17a28a7137cf20/doc/ext/smart_resolver.py). When a doc string is processed, they record a mapping from the documented name to the fully-resolved name. The code that catches missing references and fixes them is kinda big and complicated. I'm uncomfortable dropping it into our project. There's some good documentation about how autodoc_typehints works at [scanpydocs' docs](https://icb-scanpydoc.readthedocs-hosted.com/en/latest/scanpydoc.elegant_typehints.html). This [flying sheep](https://github.com/flying-sheep) seems pretty competent. I think they fixed it for scanpydocs [here](https://github.com/theislab/scanpydoc/pull/19/files) but it's a rather complex looking solution. It's frankly pretty shocking that such a simple operation (have a mapping from all the names of an object to its documented name) results in a two years of back and forth that still hasn't reached a solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-703776111
Availability,error,error,"> Could you explain why you think the boundary is invalid?. Boundary can only be used by consumers who generate an iterator with a new context. Here we were just inserting a clear before each next(), no matter who was consuming the iterator. The particular pipeline that triggered this error was a TableMapPartitions with a ToArray(Ref rows) after an IR with a repartitionedOrderedRDD2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9425#issuecomment-689597263
Deployability,pipeline,pipeline,"> Could you explain why you think the boundary is invalid?. Boundary can only be used by consumers who generate an iterator with a new context. Here we were just inserting a clear before each next(), no matter who was consuming the iterator. The particular pipeline that triggered this error was a TableMapPartitions with a ToArray(Ref rows) after an IR with a repartitionedOrderedRDD2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9425#issuecomment-689597263
Usability,clear,clear,"> Could you explain why you think the boundary is invalid?. Boundary can only be used by consumers who generate an iterator with a new context. Here we were just inserting a clear before each next(), no matter who was consuming the iterator. The particular pipeline that triggered this error was a TableMapPartitions with a ToArray(Ref rows) after an IR with a repartitionedOrderedRDD2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9425#issuecomment-689597263
Testability,benchmark,benchmark,"So, something weird is going on that I don't understand, and we should probably hold off on merging this for now. When I emit the stream code directly, bypassing the compiler, the new reducible version is clearly faster: running; ```scala; val f = compile1[Int, Unit] { (mb, n) =>; val outer = Stream.range(mb, 0, 1, n); val flatMap = outer.flatMap(i => Stream.range(mb, 0, 1, i)); flatMap.forEach(mb, i => Code._empty); }; val n = 50000; var t = System.nanoTime(); f(n); println(s""first run: ${(System.nanoTime() - t) / 1000000} ms""); for (i <- 1 to 10) { f(n) }; t = System.nanoTime(); for (i <- 1 to 50) { f(n) }; println(s""warmed up mean: ${(System.nanoTime() - t) / (1000000 * 50)} ms""); ```; on main prints; ```; first run: 2088 ms; warmed up mean: 1972 ms; ```; and on this PR; ```; first run: 867 ms; warmed up mean: 937 ms; ```; (As an aside, the lack of burn in is interesting. I think it means either the function is never getting jit compiled, or OSR kicks in on the first run and is as effective as full compilation.). On the other hand, running the benchmark; ```scala; ht = hl.utils.range_table(30); ht = ht.annotate(sum=hl.sum(hl.range(5_000).flatmap(lambda x: hl.range(x)))); ht._force_count(); ```; I get; ```; > hail-bench compare main-bench.json branch-bench.json; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; table_annotate_flatMap 371.8% 0.795 2.958; ```. I'm currently at a loss for theories to explain this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9429#issuecomment-690420227
Usability,clear,clearly,"So, something weird is going on that I don't understand, and we should probably hold off on merging this for now. When I emit the stream code directly, bypassing the compiler, the new reducible version is clearly faster: running; ```scala; val f = compile1[Int, Unit] { (mb, n) =>; val outer = Stream.range(mb, 0, 1, n); val flatMap = outer.flatMap(i => Stream.range(mb, 0, 1, i)); flatMap.forEach(mb, i => Code._empty); }; val n = 50000; var t = System.nanoTime(); f(n); println(s""first run: ${(System.nanoTime() - t) / 1000000} ms""); for (i <- 1 to 10) { f(n) }; t = System.nanoTime(); for (i <- 1 to 50) { f(n) }; println(s""warmed up mean: ${(System.nanoTime() - t) / (1000000 * 50)} ms""); ```; on main prints; ```; first run: 2088 ms; warmed up mean: 1972 ms; ```; and on this PR; ```; first run: 867 ms; warmed up mean: 937 ms; ```; (As an aside, the lack of burn in is interesting. I think it means either the function is never getting jit compiled, or OSR kicks in on the first run and is as effective as full compilation.). On the other hand, running the benchmark; ```scala; ht = hl.utils.range_table(30); ht = ht.annotate(sum=hl.sum(hl.range(5_000).flatmap(lambda x: hl.range(x)))); ht._force_count(); ```; I get; ```; > hail-bench compare main-bench.json branch-bench.json; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; table_annotate_flatMap 371.8% 0.795 2.958; ```. I'm currently at a loss for theories to explain this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9429#issuecomment-690420227
Usability,simpl,simpler,"I can't remember why I did it the other way, but your way seems simpler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701544404
Performance,tune,tuned,"@patrick-schultz, you may have tuned out the long thread about all the issues with the first Python Chained Linear Regression and pruning, but this one is now clear of all of that and is purely a Python implementation of Scala's `LinearRegressionChained`: https://github.com/hail-is/hail/blob/main/hail/src/main/scala/is/hail/methods/LinearRegression.scala#L175",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9634#issuecomment-718124825
Usability,clear,clear,"@patrick-schultz, you may have tuned out the long thread about all the issues with the first Python Chained Linear Regression and pruning, but this one is now clear of all of that and is purely a Python implementation of Scala's `LinearRegressionChained`: https://github.com/hail-is/hail/blob/main/hail/src/main/scala/is/hail/methods/LinearRegression.scala#L175",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9634#issuecomment-718124825
Deployability,deploy,deploy,"The ATGU intranet service is home for some tools we're going to build help support ATGU operations. After talking to the administrators, we started a simple tool for the admins to curate resources for members of the group. We've also talked about things related to personnel and financial and grant management. Although this will likely be a slightly different resource, we're talking to the PMs about tools we can build to help pulling together large-scale datasets like gnomAD. The complexity of doing this is becoming a blocker for producing such datasets. We're Hail. Whatever we build is part of Hail. Things that we own and operate and deploy together live in our monorepo.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-723166642
Usability,simpl,simple,"The ATGU intranet service is home for some tools we're going to build help support ATGU operations. After talking to the administrators, we started a simple tool for the admins to curate resources for members of the group. We've also talked about things related to personnel and financial and grant management. Although this will likely be a slightly different resource, we're talking to the PMs about tools we can build to help pulling together large-scale datasets like gnomAD. The complexity of doing this is becoming a blocker for producing such datasets. We're Hail. Whatever we build is part of Hail. Things that we own and operate and deploy together live in our monorepo.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-723166642
Testability,test,test,"I've made the changes you suggested patrick, but now we have a failing test. I'm getting a situation where Vt is clearly orthonormal, and U@S@Vt multiplies back to the input matrix, but U@U.t is not particularly close to the identity matrix. It's not clear why this is. The test is:. ```; np_rank_2_wide_rectangle = np.arange(12).reshape((4, 3)); rank_2_wide_rectangle = hl.nd.array(np_rank_2_wide_rectangle). ......... assert_evals_to_same_svd(rank_2_wide_rectangle, np_rank_2_wide_rectangle, full_matrices=False); ```. (The 4th test in the `test_svd` method)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-733190588
Usability,clear,clearly,"I've made the changes you suggested patrick, but now we have a failing test. I'm getting a situation where Vt is clearly orthonormal, and U@S@Vt multiplies back to the input matrix, but U@U.t is not particularly close to the identity matrix. It's not clear why this is. The test is:. ```; np_rank_2_wide_rectangle = np.arange(12).reshape((4, 3)); rank_2_wide_rectangle = hl.nd.array(np_rank_2_wide_rectangle). ......... assert_evals_to_same_svd(rank_2_wide_rectangle, np_rank_2_wide_rectangle, full_matrices=False); ```. (The 4th test in the `test_svd` method)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-733190588
Deployability,release,release,"> The one thing I'd say is just that when we make a user facing change, we try to make one of the commits look like:; >; > CHANGELOG: Added or_error method to SwitchBuilder; >; > This ends up being helpful when I have to go through and generate the change log for a new version release. Good to know, updated the commit message. It would be nice to document conventions like this in the [docs for software developers](https://hail.is/docs/0.2/getting_started_developing.html#contributing) and/or a [contributing file](https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/setting-guidelines-for-repository-contributors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468
Integrability,message,message,"> The one thing I'd say is just that when we make a user facing change, we try to make one of the commits look like:; >; > CHANGELOG: Added or_error method to SwitchBuilder; >; > This ends up being helpful when I have to go through and generate the change log for a new version release. Good to know, updated the commit message. It would be nice to document conventions like this in the [docs for software developers](https://hail.is/docs/0.2/getting_started_developing.html#contributing) and/or a [contributing file](https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/setting-guidelines-for-repository-contributors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468
Testability,log,log,"> The one thing I'd say is just that when we make a user facing change, we try to make one of the commits look like:; >; > CHANGELOG: Added or_error method to SwitchBuilder; >; > This ends up being helpful when I have to go through and generate the change log for a new version release. Good to know, updated the commit message. It would be nice to document conventions like this in the [docs for software developers](https://hail.is/docs/0.2/getting_started_developing.html#contributing) and/or a [contributing file](https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/setting-guidelines-for-repository-contributors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468
Usability,guid,guidelines-for-repository-contributors,"> The one thing I'd say is just that when we make a user facing change, we try to make one of the commits look like:; >; > CHANGELOG: Added or_error method to SwitchBuilder; >; > This ends up being helpful when I have to go through and generate the change log for a new version release. Good to know, updated the commit message. It would be nice to document conventions like this in the [docs for software developers](https://hail.is/docs/0.2/getting_started_developing.html#contributing) and/or a [contributing file](https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/setting-guidelines-for-repository-contributors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468
Usability,guid,guidelines,"Added the ""CHANGELOG"" note to contribution guidelines in #9752.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-736676863
Availability,down,down,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
Energy Efficiency,monitor,monitor,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
Integrability,rout,route,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
Modifiability,layers,layers,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
Safety,avoid,avoids,"nformation. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried about that. I think it's fine and it helps simplify the architecture. It avoids entangling the monitor with the pools and the JPIM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
Usability,clear,clear,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
Usability,simpl,simplest,"This is caused by the circularity in the import chain. Imports need to be a directed acyclic graph. The simplest fix seems to be to define `schedule_job` in `pool.py`, the only place it is used.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9832#issuecomment-758162781
Energy Efficiency,monitor,monitoring,"Hi, sorry we missed this -- clearly we're not monitoring issues well. We do support on the forum: https://discuss.hail.is. If this is still an open question, please make a post there!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9837#issuecomment-827850982
Usability,clear,clearly,"Hi, sorry we missed this -- clearly we're not monitoring issues well. We do support on the forum: https://discuss.hail.is. If this is still an open question, please make a post there!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9837#issuecomment-827850982
Deployability,configurat,configuration,"> I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. Do you mean remove all options that are simple pass throughs (such as `--num-worker-local-ssds`) or all options that are also gcloud options (such as `--project`)? The latter could be difficult, since there are some gcloud options that hailctl also needs to read, like `hailctl dataproc start` using `--project` to set requester pays configuration, extending `--initialization-actions` with notebook/VEP init scripts, setting a higher default disk size when `--vep` is specified, etc. or `hailctl dataproc submit` automatically zipping `--py-files`. > I think this also addresses the issue hailctl dataproc submit not supporting --, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters. Nice. That would solve the problem. I would guess submitting script arguments is more common than using gcloud options here, so it would be nice for the gcloud arguments group to be optional, so that `hailctl dataproc submit cluster -- --script-options` would work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758100479
Modifiability,config,configuration,"> I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. Do you mean remove all options that are simple pass throughs (such as `--num-worker-local-ssds`) or all options that are also gcloud options (such as `--project`)? The latter could be difficult, since there are some gcloud options that hailctl also needs to read, like `hailctl dataproc start` using `--project` to set requester pays configuration, extending `--initialization-actions` with notebook/VEP init scripts, setting a higher default disk size when `--vep` is specified, etc. or `hailctl dataproc submit` automatically zipping `--py-files`. > I think this also addresses the issue hailctl dataproc submit not supporting --, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters. Nice. That would solve the problem. I would guess submitting script arguments is more common than using gcloud options here, so it would be nice for the gcloud arguments group to be optional, so that `hailctl dataproc submit cluster -- --script-options` would work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758100479
Usability,simpl,simple,"> I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. Do you mean remove all options that are simple pass throughs (such as `--num-worker-local-ssds`) or all options that are also gcloud options (such as `--project`)? The latter could be difficult, since there are some gcloud options that hailctl also needs to read, like `hailctl dataproc start` using `--project` to set requester pays configuration, extending `--initialization-actions` with notebook/VEP init scripts, setting a higher default disk size when `--vep` is specified, etc. or `hailctl dataproc submit` automatically zipping `--py-files`. > I think this also addresses the issue hailctl dataproc submit not supporting --, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters. Nice. That would solve the problem. I would guess submitting script arguments is more common than using gcloud options here, so it would be nice for the gcloud arguments group to be optional, so that `hailctl dataproc submit cluster -- --script-options` would work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758100479
Availability,error,error,"> all options that are also gcloud options (such as --project)? [That] could be difficult. Yes, this is what I was thinking. `hailctl` could parse (as much as is needed) the `gcloud` options to find options (like `--project`) and modify others (like `--initialization-actions`). The latter is somewhat surprising since one expects everything after the `--` to pass through unchanged. OK, summarizing our options so far:. - hailctl has no options that are also gcloud options. gcloud options go after the `--`, and get modified as needed by hailctl (with a message).; - hailctl has no gcloud options that are simply pass through. gcloud options that are needed by hailctl commands are hailctl options (like `--project`). When a gcloud option is needed by some hailctl command, all hailctl commands take that option (when it makes sense), even if in some cases that makes them simply pass through. This fixes the inconsistency issues, but the user still needs to keep track of which gcloud options needs to be passed to hailctl and which are passed to gcloud directly. If you specify an option twice, once to hailctl and once to gcloud, we invoke gcloud with the option duplicated. Pros and cons:; - The first option has the most consistent interface.; - The first option modifies options after the --, which is surprising.; - The first option involves replication (some of) the gcloud option parsing semantics, which is annoying.; - The second option requires the user to know which gcloud options need to be passed to hailctl instead (but globally, not per-command).; - With the second option, if we want to warn (or error) on duplicate options, we're back to duplicating the gcloud option parsing logic. I think I'm coming around to the second option. > so that hailctl dataproc submit cluster -- --script-options would work. I see, so if there is only one `--` it refers to script options, and if there are two, the first one corresponds to gcloud options? I think that should be doable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554
Integrability,message,message,"> all options that are also gcloud options (such as --project)? [That] could be difficult. Yes, this is what I was thinking. `hailctl` could parse (as much as is needed) the `gcloud` options to find options (like `--project`) and modify others (like `--initialization-actions`). The latter is somewhat surprising since one expects everything after the `--` to pass through unchanged. OK, summarizing our options so far:. - hailctl has no options that are also gcloud options. gcloud options go after the `--`, and get modified as needed by hailctl (with a message).; - hailctl has no gcloud options that are simply pass through. gcloud options that are needed by hailctl commands are hailctl options (like `--project`). When a gcloud option is needed by some hailctl command, all hailctl commands take that option (when it makes sense), even if in some cases that makes them simply pass through. This fixes the inconsistency issues, but the user still needs to keep track of which gcloud options needs to be passed to hailctl and which are passed to gcloud directly. If you specify an option twice, once to hailctl and once to gcloud, we invoke gcloud with the option duplicated. Pros and cons:; - The first option has the most consistent interface.; - The first option modifies options after the --, which is surprising.; - The first option involves replication (some of) the gcloud option parsing semantics, which is annoying.; - The second option requires the user to know which gcloud options need to be passed to hailctl instead (but globally, not per-command).; - With the second option, if we want to warn (or error) on duplicate options, we're back to duplicating the gcloud option parsing logic. I think I'm coming around to the second option. > so that hailctl dataproc submit cluster -- --script-options would work. I see, so if there is only one `--` it refers to script options, and if there are two, the first one corresponds to gcloud options? I think that should be doable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554
Testability,log,logic,"> all options that are also gcloud options (such as --project)? [That] could be difficult. Yes, this is what I was thinking. `hailctl` could parse (as much as is needed) the `gcloud` options to find options (like `--project`) and modify others (like `--initialization-actions`). The latter is somewhat surprising since one expects everything after the `--` to pass through unchanged. OK, summarizing our options so far:. - hailctl has no options that are also gcloud options. gcloud options go after the `--`, and get modified as needed by hailctl (with a message).; - hailctl has no gcloud options that are simply pass through. gcloud options that are needed by hailctl commands are hailctl options (like `--project`). When a gcloud option is needed by some hailctl command, all hailctl commands take that option (when it makes sense), even if in some cases that makes them simply pass through. This fixes the inconsistency issues, but the user still needs to keep track of which gcloud options needs to be passed to hailctl and which are passed to gcloud directly. If you specify an option twice, once to hailctl and once to gcloud, we invoke gcloud with the option duplicated. Pros and cons:; - The first option has the most consistent interface.; - The first option modifies options after the --, which is surprising.; - The first option involves replication (some of) the gcloud option parsing semantics, which is annoying.; - The second option requires the user to know which gcloud options need to be passed to hailctl instead (but globally, not per-command).; - With the second option, if we want to warn (or error) on duplicate options, we're back to duplicating the gcloud option parsing logic. I think I'm coming around to the second option. > so that hailctl dataproc submit cluster -- --script-options would work. I see, so if there is only one `--` it refers to script options, and if there are two, the first one corresponds to gcloud options? I think that should be doable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554
Usability,simpl,simply,"> all options that are also gcloud options (such as --project)? [That] could be difficult. Yes, this is what I was thinking. `hailctl` could parse (as much as is needed) the `gcloud` options to find options (like `--project`) and modify others (like `--initialization-actions`). The latter is somewhat surprising since one expects everything after the `--` to pass through unchanged. OK, summarizing our options so far:. - hailctl has no options that are also gcloud options. gcloud options go after the `--`, and get modified as needed by hailctl (with a message).; - hailctl has no gcloud options that are simply pass through. gcloud options that are needed by hailctl commands are hailctl options (like `--project`). When a gcloud option is needed by some hailctl command, all hailctl commands take that option (when it makes sense), even if in some cases that makes them simply pass through. This fixes the inconsistency issues, but the user still needs to keep track of which gcloud options needs to be passed to hailctl and which are passed to gcloud directly. If you specify an option twice, once to hailctl and once to gcloud, we invoke gcloud with the option duplicated. Pros and cons:; - The first option has the most consistent interface.; - The first option modifies options after the --, which is surprising.; - The first option involves replication (some of) the gcloud option parsing semantics, which is annoying.; - The second option requires the user to know which gcloud options need to be passed to hailctl instead (but globally, not per-command).; - With the second option, if we want to warn (or error) on duplicate options, we're back to duplicating the gcloud option parsing logic. I think I'm coming around to the second option. > so that hailctl dataproc submit cluster -- --script-options would work. I see, so if there is only one `--` it refers to script options, and if there are two, the first one corresponds to gcloud options? I think that should be doable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554
Availability,avail,available,"For @tpoterba's benefit (and to get things clear), here's the current proposal:; - Move extra gcloud arguments to `--extra-gcloud-<description>-args=""--arg1 ... --argN""` where there is one such argument for each invocation of gcloud inside a hailctl command. gcloud args no longer go at the end.; - Only `hailctl dataproc submit` supports `--` which is used to separate submit arguments from the script arguments,; - Remove all gcloud arguments that are pass through in all commands, but mention them in the command help so users don't need to look at the gcloud help for commonly used options.; - gcloud options that are needed by some hailctl command should be consistently available among all hailctl commands (where appropriate, and where in some cases they may simply be pass-through).; - The other consistency changes @nawatts highlighted. (I wouldn't be surprised if I other issues come up when I make the changes, but this is a start.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758255396
Usability,clear,clear,"For @tpoterba's benefit (and to get things clear), here's the current proposal:; - Move extra gcloud arguments to `--extra-gcloud-<description>-args=""--arg1 ... --argN""` where there is one such argument for each invocation of gcloud inside a hailctl command. gcloud args no longer go at the end.; - Only `hailctl dataproc submit` supports `--` which is used to separate submit arguments from the script arguments,; - Remove all gcloud arguments that are pass through in all commands, but mention them in the command help so users don't need to look at the gcloud help for commonly used options.; - gcloud options that are needed by some hailctl command should be consistently available among all hailctl commands (where appropriate, and where in some cases they may simply be pass-through).; - The other consistency changes @nawatts highlighted. (I wouldn't be surprised if I other issues come up when I make the changes, but this is a start.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758255396
Availability,error,error,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
Deployability,configurat,configuration,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
Integrability,message,message,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
Modifiability,config,configuration,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
Safety,timeout,timeout,"ugh.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more informat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
Usability,feedback,feedback,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
Availability,error,error,"> The hailctl dataproc subcommand now has --beta, --configuration=, --dry-run, --project= and --zone=. These apply to all commands. There is a GcloudRunner object that takes these options, is set to the click context user obj field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with click, the subcommand options must go on the subcommand, so hailctl dataproc stop --dry-run is an error. Nice. It's great that these are handled at the `hailctl dataproc` level instead of having to remember to account for them in every `hailctl dataproc` subcommand. That's going to resolve a lot of inconsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
Deployability,configurat,configuration,"> The hailctl dataproc subcommand now has --beta, --configuration=, --dry-run, --project= and --zone=. These apply to all commands. There is a GcloudRunner object that takes these options, is set to the click context user obj field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with click, the subcommand options must go on the subcommand, so hailctl dataproc stop --dry-run is an error. Nice. It's great that these are handled at the `hailctl dataproc` level instead of having to remember to account for them in every `hailctl dataproc` subcommand. That's going to resolve a lot of inconsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
Modifiability,config,configuration,"> The hailctl dataproc subcommand now has --beta, --configuration=, --dry-run, --project= and --zone=. These apply to all commands. There is a GcloudRunner object that takes these options, is set to the click context user obj field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with click, the subcommand options must go on the subcommand, so hailctl dataproc stop --dry-run is an error. Nice. It's great that these are handled at the `hailctl dataproc` level instead of having to remember to account for them in every `hailctl dataproc` subcommand. That's going to resolve a lot of inconsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
Safety,avoid,avoid,"e the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing the number of simple pass through parameters was mainly directed toward `hailctl dataproc start`, which has several options than can be specified separately for master node, worker nodes, and secondary worker nodes. I wanted to avoid cases where, for example, `--worker-boot-disk-size` was a `hailctl` option, but `--secondary-worker-boot-disk-size` had to be specified after a `--` or with `--extra-gcloud-start-args`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
Usability,simpl,simple,"e the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing the number of simple pass through parameters was mainly directed toward `hailctl dataproc start`, which has several options than can be specified separately for master node, worker nodes, and secondary worker nodes. I wanted to avoid cases where, for example, `--worker-boot-disk-size` was a `hailctl` option, but `--secondary-worker-boot-disk-size` had to be specified after a `--` or with `--extra-gcloud-start-args`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
Deployability,release,released,"While we're making breaking changes to the `hailctl` interface... there is a beta feature to start/stop Dataproc clusters. Once this is released, it could create some confusion that `hailctl dataproc start` runs `gcloud dataproc clusters create` instead of `gcloud dataproc clusters start`. Likewise for `hailctl dataproc stop` and `gcloud dataproc clusters delete`. Should we rename `hailctl dataproc` start/stop to create/delete?. https://cloud.google.com/dataproc/docs/guides/dataproc-start-stop",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171070
Integrability,interface,interface,"While we're making breaking changes to the `hailctl` interface... there is a beta feature to start/stop Dataproc clusters. Once this is released, it could create some confusion that `hailctl dataproc start` runs `gcloud dataproc clusters create` instead of `gcloud dataproc clusters start`. Likewise for `hailctl dataproc stop` and `gcloud dataproc clusters delete`. Should we rename `hailctl dataproc` start/stop to create/delete?. https://cloud.google.com/dataproc/docs/guides/dataproc-start-stop",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171070
Usability,guid,guides,"While we're making breaking changes to the `hailctl` interface... there is a beta feature to start/stop Dataproc clusters. Once this is released, it could create some confusion that `hailctl dataproc start` runs `gcloud dataproc clusters create` instead of `gcloud dataproc clusters start`. Likewise for `hailctl dataproc stop` and `gcloud dataproc clusters delete`. Should we rename `hailctl dataproc` start/stop to create/delete?. https://cloud.google.com/dataproc/docs/guides/dataproc-start-stop",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171070
Integrability,wrap,wrapper,"It also fails on this simpler example:; ```; In [1]: import hail as hl ; ...: ; ...: temp = hl.utils.range_table(100) ; ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-c548354b6e81; LOGGING: writing to /Users/dking/projects/hail/hail-20210107-1038-0.2.61-c548354b6e81.log; Traceback (most recent call last):; File ""<ipython-input-1-a2e56feaf799>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1092>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementExce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
Testability,test,test,"It also fails on this simpler example:; ```; In [1]: import hail as hl ; ...: ; ...: temp = hl.utils.range_table(100) ; ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-c548354b6e81; LOGGING: writing to /Users/dking/projects/hail/hail-20210107-1038-0.2.61-c548354b6e81.log; Traceback (most recent call last):; File ""<ipython-input-1-a2e56feaf799>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1092>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementExce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
Usability,simpl,simpler,"It also fails on this simpler example:; ```; In [1]: import hail as hl ; ...: ; ...: temp = hl.utils.range_table(100) ; ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-c548354b6e81; LOGGING: writing to /Users/dking/projects/hail/hail-20210107-1038-0.2.61-c548354b6e81.log; Traceback (most recent call last):; File ""<ipython-input-1-a2e56feaf799>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1092>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementExce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
Usability,clear,clear,"I wasn't very clear initially, I think if `apt` hasn't shown reason to be retried then we can leave it without a retry. If it ever seriously acts up there's one less thing in the way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-767083626
Availability,down,downloading,"At some point, we should think about how to improve the discoverability and machine-verifiability of our APIs. Currently the tightest type of job log is rather complex. If the performance is OK, I think we should move towards classes that define the request and response types of each call. ---. The main difference is `hail-pip-install` having `retry`. If pip exits with a non-zero exit code, we'll just rerun the command exactly, at most four more times. This mitigates missing retry logic in `pip` itself. For example, [this job](https://ci.hail.is/batches/167314/jobs/27) failed because pip encountered a connection reset while downloading a file. Ideally, pip would simply retry the download. Since we don't control the pip source code, I use a retry that treats all of pip as a black box. There's definitely a failure mode: if you specify a package that doesn't exist, pip will error five times in a row and take ~30 seconds before the retry logic gives up. I'm OK with this because pip should basically never fail for legitimate reasons.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-775241278
Deployability,install,install,"At some point, we should think about how to improve the discoverability and machine-verifiability of our APIs. Currently the tightest type of job log is rather complex. If the performance is OK, I think we should move towards classes that define the request and response types of each call. ---. The main difference is `hail-pip-install` having `retry`. If pip exits with a non-zero exit code, we'll just rerun the command exactly, at most four more times. This mitigates missing retry logic in `pip` itself. For example, [this job](https://ci.hail.is/batches/167314/jobs/27) failed because pip encountered a connection reset while downloading a file. Ideally, pip would simply retry the download. Since we don't control the pip source code, I use a retry that treats all of pip as a black box. There's definitely a failure mode: if you specify a package that doesn't exist, pip will error five times in a row and take ~30 seconds before the retry logic gives up. I'm OK with this because pip should basically never fail for legitimate reasons.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-775241278
Performance,perform,performance,"At some point, we should think about how to improve the discoverability and machine-verifiability of our APIs. Currently the tightest type of job log is rather complex. If the performance is OK, I think we should move towards classes that define the request and response types of each call. ---. The main difference is `hail-pip-install` having `retry`. If pip exits with a non-zero exit code, we'll just rerun the command exactly, at most four more times. This mitigates missing retry logic in `pip` itself. For example, [this job](https://ci.hail.is/batches/167314/jobs/27) failed because pip encountered a connection reset while downloading a file. Ideally, pip would simply retry the download. Since we don't control the pip source code, I use a retry that treats all of pip as a black box. There's definitely a failure mode: if you specify a package that doesn't exist, pip will error five times in a row and take ~30 seconds before the retry logic gives up. I'm OK with this because pip should basically never fail for legitimate reasons.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-775241278
Testability,log,log,"At some point, we should think about how to improve the discoverability and machine-verifiability of our APIs. Currently the tightest type of job log is rather complex. If the performance is OK, I think we should move towards classes that define the request and response types of each call. ---. The main difference is `hail-pip-install` having `retry`. If pip exits with a non-zero exit code, we'll just rerun the command exactly, at most four more times. This mitigates missing retry logic in `pip` itself. For example, [this job](https://ci.hail.is/batches/167314/jobs/27) failed because pip encountered a connection reset while downloading a file. Ideally, pip would simply retry the download. Since we don't control the pip source code, I use a retry that treats all of pip as a black box. There's definitely a failure mode: if you specify a package that doesn't exist, pip will error five times in a row and take ~30 seconds before the retry logic gives up. I'm OK with this because pip should basically never fail for legitimate reasons.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-775241278
Usability,simpl,simply,"At some point, we should think about how to improve the discoverability and machine-verifiability of our APIs. Currently the tightest type of job log is rather complex. If the performance is OK, I think we should move towards classes that define the request and response types of each call. ---. The main difference is `hail-pip-install` having `retry`. If pip exits with a non-zero exit code, we'll just rerun the command exactly, at most four more times. This mitigates missing retry logic in `pip` itself. For example, [this job](https://ci.hail.is/batches/167314/jobs/27) failed because pip encountered a connection reset while downloading a file. Ideally, pip would simply retry the download. Since we don't control the pip source code, I use a retry that treats all of pip as a black box. There's definitely a failure mode: if you specify a package that doesn't exist, pip will error five times in a row and take ~30 seconds before the retry logic gives up. I'm OK with this because pip should basically never fail for legitimate reasons.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-775241278
Modifiability,refactor,refactoring,"Hi,. I'm just curious if you tried black on a code that uses Hail query. As far as I see, PEP8 conflicts with the code style adopted in the Hail docs, e.g. black would remove spaces in named function arguments:. ```; - mt = mt.annotate_entries(GT = lgt_to_gt(mt.LGT, mt.LA)); + mt = mt.annotate_entries(GT=lgt_to_gt(mt.LGT, mt.LA)); ```. On the other hand, query can be seen as a DSL on top of Python, so the same guidelines probably don't need to be applied to it. Wondering if you had thoughts about lining the query code? We will be writing a lot of that in the nearest future in the Centre for Population Genomics, and would love to set up some style checks, or even automate that with a tool like black. And on black - are you considering automating code refactoring with black as part of the CI? Or you wanted to just do checks, alongside with pylint?. Vlad",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9931#issuecomment-768677633
Usability,guid,guidelines,"Hi,. I'm just curious if you tried black on a code that uses Hail query. As far as I see, PEP8 conflicts with the code style adopted in the Hail docs, e.g. black would remove spaces in named function arguments:. ```; - mt = mt.annotate_entries(GT = lgt_to_gt(mt.LGT, mt.LA)); + mt = mt.annotate_entries(GT=lgt_to_gt(mt.LGT, mt.LA)); ```. On the other hand, query can be seen as a DSL on top of Python, so the same guidelines probably don't need to be applied to it. Wondering if you had thoughts about lining the query code? We will be writing a lot of that in the nearest future in the Centre for Population Genomics, and would love to set up some style checks, or even automate that with a tool like black. And on black - are you considering automating code refactoring with black as part of the CI? Or you wanted to just do checks, alongside with pylint?. Vlad",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9931#issuecomment-768677633
Usability,guid,guidelines,"> On the other hand, query can be seen as a DSL on top of Python, so the same guidelines probably don't need to be applied to it. You've hit exactly on the center of a longstanding discussion on the team with no definitive resolution 😄",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9931#issuecomment-768742121
Usability,simpl,simplify,"OK, I think this is ready for review again. I rebased, was able to simplify the online bounded gather a bit, and parallelized the GCS rmtree more, so that cleanup can happen in parallel with the merge operations as much as possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9934#issuecomment-785209202
Usability,clear,clear,"For the multiple JPICs, I was thinking that some day there could be a JPIC per cloud. I can try and convert it back so it's clear there is one instance of the class if that is clearer for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9972#issuecomment-773536457
Usability,clear,clearer,I think it's clearer with one instance of the JPIC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9972#issuecomment-773722261
Usability,clear,clearer,"This is very nice! I didn't know some of things about the 0.1 days. I agree with John's comments, though I think it's worth making explicit whether a given Tour of Hail Query is for Scientists or Software Engineers. I think you're proposing the latter? Also, a stylistic nitpick but I find bullet-delimited sentences quite difficult to read, and think it might be clearer to drop the ands and commas at the end of the bullets.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10020#issuecomment-776811671
Integrability,message,messages,"I intend the Tour of Hail Query to assume no genetics knowledge. Indeed, it probably won't ever mention genetics. I removed the sentence-as-bulleted list. I like them, but I'm not the reader here 😉. I intend for each of these documents to be separate files, and people with git experience can skip the git doc. I agree that we use git in one of the typical ways. However, I've learned that Atom's GitHub support doesn't consider the possibility of PRs from one remote to another. We also have some conventions around git messages and stacked PRs that I think are worth getting in writing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10020#issuecomment-776868568
Usability,learn,learned,"I intend the Tour of Hail Query to assume no genetics knowledge. Indeed, it probably won't ever mention genetics. I removed the sentence-as-bulleted list. I like them, but I'm not the reader here 😉. I intend for each of these documents to be separate files, and people with git experience can skip the git doc. I agree that we use git in one of the typical ways. However, I've learned that Atom's GitHub support doesn't consider the possibility of PRs from one remote to another. We also have some conventions around git messages and stacked PRs that I think are worth getting in writing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10020#issuecomment-776868568
Usability,feedback,feedback,"OK, I pushed some docs and clarifying comments based on your feedback, but not functional code changes. Ready for another look.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10023#issuecomment-776843634
Availability,failure,failures,"I don't think so. The change is clearly fixes an issue and is an improvement. That said, write failures are rare and I just want to flush out any other rare errors so the tests are reliable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10023#issuecomment-776868666
Testability,test,tests,"I don't think so. The change is clearly fixes an issue and is an improvement. That said, write failures are rare and I just want to flush out any other rare errors so the tests are reliable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10023#issuecomment-776868666
Usability,clear,clearly,"I don't think so. The change is clearly fixes an issue and is an improvement. That said, write failures are rare and I just want to flush out any other rare errors so the tests are reliable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10023#issuecomment-776868666
Performance,optimiz,optimized,"We run this query to get fair share values per user:. ```; SELECT user, CAST(COALESCE(SUM(n_cancelled_running_jobs), 0) AS SIGNED) AS n_cancelled_running_jobs; FROM user_inst_coll_resources; GROUP BY user; HAVING n_cancelled_running_jobs > 0;; ```. This query will return 0 even though there could be attempts still running. Plus these queries only look at running batches. ```; async for batch in self.db.select_and_fetchall(; '''; SELECT id; FROM batches; WHERE user = %s AND `state` = 'running' AND cancelled = 1;; ''',; (user,),; timer_description=f'in cancel_cancelled_running_jobs: get {user} cancelled batches'):; async for record in self.db.select_and_fetchall(; '''; SELECT jobs.job_id, attempts.attempt_id, attempts.instance_name; FROM jobs FORCE INDEX(jobs_batch_id_state_always_run_cancelled); STRAIGHT_JOIN attempts; ON attempts.batch_id = jobs.batch_id AND attempts.job_id = jobs.job_id; WHERE jobs.batch_id = %s AND state = 'Running' AND always_run = 0 AND cancelled = 0; LIMIT %s;; ''',; (batch['id'], remaining.value),; timer_description=f'in cancel_cancelled_running_jobs: get {user} batch {batch[""id""]} running cancelled jobs'):; record['batch_id'] = batch['id']; yield record; ``` . I'll think about whether I can combine these queries over the attempts. However, it seemed clearer to me to look for two different things as the other queries are optimized by looking at the batch and job state where the orphaned query doesn't care about fair share or the batch / job state..",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10071#issuecomment-783583573
Usability,clear,clearer,"We run this query to get fair share values per user:. ```; SELECT user, CAST(COALESCE(SUM(n_cancelled_running_jobs), 0) AS SIGNED) AS n_cancelled_running_jobs; FROM user_inst_coll_resources; GROUP BY user; HAVING n_cancelled_running_jobs > 0;; ```. This query will return 0 even though there could be attempts still running. Plus these queries only look at running batches. ```; async for batch in self.db.select_and_fetchall(; '''; SELECT id; FROM batches; WHERE user = %s AND `state` = 'running' AND cancelled = 1;; ''',; (user,),; timer_description=f'in cancel_cancelled_running_jobs: get {user} cancelled batches'):; async for record in self.db.select_and_fetchall(; '''; SELECT jobs.job_id, attempts.attempt_id, attempts.instance_name; FROM jobs FORCE INDEX(jobs_batch_id_state_always_run_cancelled); STRAIGHT_JOIN attempts; ON attempts.batch_id = jobs.batch_id AND attempts.job_id = jobs.job_id; WHERE jobs.batch_id = %s AND state = 'Running' AND always_run = 0 AND cancelled = 0; LIMIT %s;; ''',; (batch['id'], remaining.value),; timer_description=f'in cancel_cancelled_running_jobs: get {user} batch {batch[""id""]} running cancelled jobs'):; record['batch_id'] = batch['id']; yield record; ``` . I'll think about whether I can combine these queries over the attempts. However, it seemed clearer to me to look for two different things as the other queries are optimized by looking at the batch and job state where the orphaned query doesn't care about fair share or the batch / job state..",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10071#issuecomment-783583573
Usability,simpl,simpler,"Meant to assign this, just forgot to. Curious if you see any other ways to make this code simpler/faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10075#issuecomment-784322347
Availability,error,errors,"1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci-test, which I updated to copy the hail_version file just like service-base. https://github.com/hail-is/hail/compare/main...danking:add-version-endpoint. Do you want to cherry-pick that change onto your add-version-endpoint branch?. One more change request: can we remove the try-catch? I don't expect any errors in that call and I tend to avoid revealing anything about errors to our users. Aiohttp will log the error and the stack trace if you let it rise all the way. Sorry for all the complication! Our build system is the second service we built and is clearly showing its age.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401
Deployability,update,update,"om the `copy_files` build step. Because I wanted `service_base_image` to depend on `copy_files`, I had to move the whole step after `copy_files`. This is a limitation in build.yaml: a step must appear *after* steps on which it depends. I also had to move `check_services` for the same reason: it depends on `service_base_image`. File dependencies in build.yaml work like this:; 1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci-test, which I updated to copy the hail_version file just like service-base. https://github.com/hail-is/hail/compare/main...danking:add-version-endpoint. Do you want to cherry-pick that change onto your a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401
Integrability,depend,depend,"Hey @illusional ! Sorry for the massive latency. OK, so, big apologies are in order, I totally lead you astray by mentioning the makefile. The Makefile *is* the source of hail version truth, but invoking the makefile inside an image build step feels wrong to me. Each step creates a layer which inflates the image sizes. Hail's images are already too big!. I took your commits and added one of my own that snags the version from the `copy_files` build step. Because I wanted `service_base_image` to depend on `copy_files`, I had to move the whole step after `copy_files`. This is a limitation in build.yaml: a step must appear *after* steps on which it depends. I also had to move `check_services` for the same reason: it depends on `service_base_image`. File dependencies in build.yaml work like this:; 1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401
Performance,latency,latency,"Hey @illusional ! Sorry for the massive latency. OK, so, big apologies are in order, I totally lead you astray by mentioning the makefile. The Makefile *is* the source of hail version truth, but invoking the makefile inside an image build step feels wrong to me. Each step creates a layer which inflates the image sizes. Hail's images are already too big!. I took your commits and added one of my own that snags the version from the `copy_files` build step. Because I wanted `service_base_image` to depend on `copy_files`, I had to move the whole step after `copy_files`. This is a limitation in build.yaml: a step must appear *after* steps on which it depends. I also had to move `check_services` for the same reason: it depends on `service_base_image`. File dependencies in build.yaml work like this:; 1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401
Safety,avoid,avoid,"1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci-test, which I updated to copy the hail_version file just like service-base. https://github.com/hail-is/hail/compare/main...danking:add-version-endpoint. Do you want to cherry-pick that change onto your add-version-endpoint branch?. One more change request: can we remove the try-catch? I don't expect any errors in that call and I tend to avoid revealing anything about errors to our users. Aiohttp will log the error and the stack trace if you let it rise all the way. Sorry for all the complication! Our build system is the second service we built and is clearly showing its age.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401
Security,access,access,"gies are in order, I totally lead you astray by mentioning the makefile. The Makefile *is* the source of hail version truth, but invoking the makefile inside an image build step feels wrong to me. Each step creates a layer which inflates the image sizes. Hail's images are already too big!. I took your commits and added one of my own that snags the version from the `copy_files` build step. Because I wanted `service_base_image` to depend on `copy_files`, I had to move the whole step after `copy_files`. This is a limitation in build.yaml: a step must appear *after* steps on which it depends. I also had to move `check_services` for the same reason: it depends on `service_base_image`. File dependencies in build.yaml work like this:; 1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401
Testability,test,test,"1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci-test, which I updated to copy the hail_version file just like service-base. https://github.com/hail-is/hail/compare/main...danking:add-version-endpoint. Do you want to cherry-pick that change onto your add-version-endpoint branch?. One more change request: can we remove the try-catch? I don't expect any errors in that call and I tend to avoid revealing anything about errors to our users. Aiohttp will log the error and the stack trace if you let it rise all the way. Sorry for all the complication! Our build system is the second service we built and is clearly showing its age.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401
Usability,learn,learn,"1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci-test, which I updated to copy the hail_version file just like service-base. https://github.com/hail-is/hail/compare/main...danking:add-version-endpoint. Do you want to cherry-pick that change onto your add-version-endpoint branch?. One more change request: can we remove the try-catch? I don't expect any errors in that call and I tend to avoid revealing anything about errors to our users. Aiohttp will log the error and the stack trace if you let it rise all the way. Sorry for all the complication! Our build system is the second service we built and is clearly showing its age.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401
Usability,clear,clearly,"Sorry to drop another big one on you, Chris, but you have the context for this. . The big diff doesn't reflect the size of the conceptual change here. This PR does as follows:; * Breaks up and moves CodeOrdering to its own package under `ir`. ; * Changes all CodeOrdering factories to take STypes instead of PTypes; * Removes some factory methods, like `pType.codeOrdering`.; * There are now two places to construct orderings: `EmitClassBuilder.getOrderingFunction` (which takes stypes and an `Op`), and `CodeOrdering.makeOrdering`.; * I slightly changed the method layout to memoize within each ordering. We check the types on each method call.; * There are only a couple meaningful changes in the typed ordering generators. I changed CallOrdering and StringOrdering a bit to more clearly describe what they're doing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10091#issuecomment-784737260
Usability,clear,clear,Could we just change the Python to generate `ArrayRef`? Not clear to me why we use `indexArray` instead.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10119#issuecomment-786807963
Modifiability,sandbox,sandbox,"Drat, it appears that pyspark doesn't work with Python 3.8. https://stackoverflow.com/a/58849063/342839. A simpler reproduction to demonstrate that this is a pyspark issues:. ```; snafu$ python -m pyspark.cloudpickle; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 185, in _run_module_as_main; mod_name, mod_spec, code = _get_module_details(mod_name, _Error); File ""/usr/lib/python3.8/runpy.py"", line 111, in _get_module_details; __import__(pkg_name); File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py"", line 51, in <module>; from pyspark.context import SparkContext; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py"", line 31, in <module>; from pyspark import accumulators; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py"", line 97, in <module>; from pyspark.serializers import read_int, PickleSerializer; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py"", line 71, in <module>; from pyspark import cloudpickle; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py"", line 145, in <module>; _cell_set_template_code = _make_cell_set_template_code(); File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py"", line 126, in _make_cell_set_template_code; return types.CodeType(; TypeError: an integer is required (got type bytes); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197#issuecomment-800647452
Testability,sandbox,sandbox,"Drat, it appears that pyspark doesn't work with Python 3.8. https://stackoverflow.com/a/58849063/342839. A simpler reproduction to demonstrate that this is a pyspark issues:. ```; snafu$ python -m pyspark.cloudpickle; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 185, in _run_module_as_main; mod_name, mod_spec, code = _get_module_details(mod_name, _Error); File ""/usr/lib/python3.8/runpy.py"", line 111, in _get_module_details; __import__(pkg_name); File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py"", line 51, in <module>; from pyspark.context import SparkContext; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py"", line 31, in <module>; from pyspark import accumulators; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py"", line 97, in <module>; from pyspark.serializers import read_int, PickleSerializer; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py"", line 71, in <module>; from pyspark import cloudpickle; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py"", line 145, in <module>; _cell_set_template_code = _make_cell_set_template_code(); File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py"", line 126, in _make_cell_set_template_code; return types.CodeType(; TypeError: an integer is required (got type bytes); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197#issuecomment-800647452
Usability,simpl,simpler,"Drat, it appears that pyspark doesn't work with Python 3.8. https://stackoverflow.com/a/58849063/342839. A simpler reproduction to demonstrate that this is a pyspark issues:. ```; snafu$ python -m pyspark.cloudpickle; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 185, in _run_module_as_main; mod_name, mod_spec, code = _get_module_details(mod_name, _Error); File ""/usr/lib/python3.8/runpy.py"", line 111, in _get_module_details; __import__(pkg_name); File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py"", line 51, in <module>; from pyspark.context import SparkContext; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py"", line 31, in <module>; from pyspark import accumulators; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py"", line 97, in <module>; from pyspark.serializers import read_int, PickleSerializer; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py"", line 71, in <module>; from pyspark import cloudpickle; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py"", line 145, in <module>; _cell_set_template_code = _make_cell_set_template_code(); File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py"", line 126, in _make_cell_set_template_code; return types.CodeType(; TypeError: an integer is required (got type bytes); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/10197#issuecomment-800647452
Performance,cache,cache,"I think that's right, though we serialize other potentially private information. I think we ought to have a per-organization (Hail billing project?) cache, but also not very high priority. I'd be pretty chuffed to learn we're running important enough stuff that people are attempting timing attacks on our cache to learn what queries other people are executing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10309#issuecomment-821250865
Security,attack,attacks,"I think that's right, though we serialize other potentially private information. I think we ought to have a per-organization (Hail billing project?) cache, but also not very high priority. I'd be pretty chuffed to learn we're running important enough stuff that people are attempting timing attacks on our cache to learn what queries other people are executing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10309#issuecomment-821250865
Usability,learn,learn,"I think that's right, though we serialize other potentially private information. I think we ought to have a per-organization (Hail billing project?) cache, but also not very high priority. I'd be pretty chuffed to learn we're running important enough stuff that people are attempting timing attacks on our cache to learn what queries other people are executing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10309#issuecomment-821250865
Availability,avail,available,"Thanks so much Daniel!! This is awesome. I can't seem to be able to merge though, probably due to permissions?; <img width=""516"" alt=""Screen Shot 2021-04-22 at 9 58 00 am"" src=""https://user-images.githubusercontent.com/1575412/115636406-3ada5e00-a351-11eb-887d-3882271f6369.png"">. There are also conflicts, but I'm resolving them right now :). UPD: Ah, learned from Leo that the merge button will be available after the tests pass :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10347#issuecomment-824439697
Testability,test,tests,"Thanks so much Daniel!! This is awesome. I can't seem to be able to merge though, probably due to permissions?; <img width=""516"" alt=""Screen Shot 2021-04-22 at 9 58 00 am"" src=""https://user-images.githubusercontent.com/1575412/115636406-3ada5e00-a351-11eb-887d-3882271f6369.png"">. There are also conflicts, but I'm resolving them right now :). UPD: Ah, learned from Leo that the merge button will be available after the tests pass :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10347#issuecomment-824439697
Usability,learn,learned,"Thanks so much Daniel!! This is awesome. I can't seem to be able to merge though, probably due to permissions?; <img width=""516"" alt=""Screen Shot 2021-04-22 at 9 58 00 am"" src=""https://user-images.githubusercontent.com/1575412/115636406-3ada5e00-a351-11eb-887d-3882271f6369.png"">. There are also conflicts, but I'm resolving them right now :). UPD: Ah, learned from Leo that the merge button will be available after the tests pass :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10347#issuecomment-824439697
Performance,perform,performance,"Yes, definitely! I'll assign you as a reviewer, though I'd like to also do a quick design review with Cotton to make sure I'm not doing anything crazy. I don't think there are any major redesigns I have in mind, barring feedback from review. I think I'll make it possible to control the default split size using a flag / env var, but other than that it works and seems to improve performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10512#issuecomment-852084745
Usability,feedback,feedback,"Yes, definitely! I'll assign you as a reviewer, though I'd like to also do a quick design review with Cotton to make sure I'm not doing anything crazy. I don't think there are any major redesigns I have in mind, barring feedback from review. I think I'll make it possible to control the default split size using a flag / env var, but other than that it works and seems to improve performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10512#issuecomment-852084745
Usability,simpl,simpler,"I don't have any specific ideas, no. Just noticing that this seems to implement a stronger abstraction: a type for default values. I don't know of any cases where we use default values, other than unreachable values, but if there were, it looks like this could be used. Mostly, that seems simpler conceptually than ""unreachable values"", since it doesn't involve control flow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10539#issuecomment-870779483
Integrability,wrap,wrapping,"Oof, good catch! The thing we're trying to avoid is `e^x` overflowing for large positive `x`. In double precision, the smallest `x` that overflows is 710. So to test that we handle overflow correctly, you can check `sigmoid(710) == 1.0` and `sigmoid(-710) == 0.0` (using approximate equality). Actually, after playing with this, if you just use the simple definition `sigmoid(x) = 1 / (1 + np.exp(-x))`, then `sigmoid(-710)` does overflow, but it returns the right answer since `np.exp(710)` returns `inf`, and `1 / inf == 0.0`. But `math.exp(710)` throws an exception. `hl.exp` seems to have the numpy behavior, so I think the simple version actually works. But we should add the above test. I think wrapping this in an exposed function is a good idea. I agree it should be called `expit`, both for consistency with scipy, and because as you say, `sigmoid` really just means an S shaped function. And if we do expose `expit`, we should probably expose its inverse `logit` too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10606#issuecomment-866034244
Safety,avoid,avoid,"Oof, good catch! The thing we're trying to avoid is `e^x` overflowing for large positive `x`. In double precision, the smallest `x` that overflows is 710. So to test that we handle overflow correctly, you can check `sigmoid(710) == 1.0` and `sigmoid(-710) == 0.0` (using approximate equality). Actually, after playing with this, if you just use the simple definition `sigmoid(x) = 1 / (1 + np.exp(-x))`, then `sigmoid(-710)` does overflow, but it returns the right answer since `np.exp(710)` returns `inf`, and `1 / inf == 0.0`. But `math.exp(710)` throws an exception. `hl.exp` seems to have the numpy behavior, so I think the simple version actually works. But we should add the above test. I think wrapping this in an exposed function is a good idea. I agree it should be called `expit`, both for consistency with scipy, and because as you say, `sigmoid` really just means an S shaped function. And if we do expose `expit`, we should probably expose its inverse `logit` too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10606#issuecomment-866034244
Security,expose,exposed,"Oof, good catch! The thing we're trying to avoid is `e^x` overflowing for large positive `x`. In double precision, the smallest `x` that overflows is 710. So to test that we handle overflow correctly, you can check `sigmoid(710) == 1.0` and `sigmoid(-710) == 0.0` (using approximate equality). Actually, after playing with this, if you just use the simple definition `sigmoid(x) = 1 / (1 + np.exp(-x))`, then `sigmoid(-710)` does overflow, but it returns the right answer since `np.exp(710)` returns `inf`, and `1 / inf == 0.0`. But `math.exp(710)` throws an exception. `hl.exp` seems to have the numpy behavior, so I think the simple version actually works. But we should add the above test. I think wrapping this in an exposed function is a good idea. I agree it should be called `expit`, both for consistency with scipy, and because as you say, `sigmoid` really just means an S shaped function. And if we do expose `expit`, we should probably expose its inverse `logit` too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10606#issuecomment-866034244
Testability,test,test,"Oof, good catch! The thing we're trying to avoid is `e^x` overflowing for large positive `x`. In double precision, the smallest `x` that overflows is 710. So to test that we handle overflow correctly, you can check `sigmoid(710) == 1.0` and `sigmoid(-710) == 0.0` (using approximate equality). Actually, after playing with this, if you just use the simple definition `sigmoid(x) = 1 / (1 + np.exp(-x))`, then `sigmoid(-710)` does overflow, but it returns the right answer since `np.exp(710)` returns `inf`, and `1 / inf == 0.0`. But `math.exp(710)` throws an exception. `hl.exp` seems to have the numpy behavior, so I think the simple version actually works. But we should add the above test. I think wrapping this in an exposed function is a good idea. I agree it should be called `expit`, both for consistency with scipy, and because as you say, `sigmoid` really just means an S shaped function. And if we do expose `expit`, we should probably expose its inverse `logit` too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10606#issuecomment-866034244
Usability,simpl,simple,"Oof, good catch! The thing we're trying to avoid is `e^x` overflowing for large positive `x`. In double precision, the smallest `x` that overflows is 710. So to test that we handle overflow correctly, you can check `sigmoid(710) == 1.0` and `sigmoid(-710) == 0.0` (using approximate equality). Actually, after playing with this, if you just use the simple definition `sigmoid(x) = 1 / (1 + np.exp(-x))`, then `sigmoid(-710)` does overflow, but it returns the right answer since `np.exp(710)` returns `inf`, and `1 / inf == 0.0`. But `math.exp(710)` throws an exception. `hl.exp` seems to have the numpy behavior, so I think the simple version actually works. But we should add the above test. I think wrapping this in an exposed function is a good idea. I agree it should be called `expit`, both for consistency with scipy, and because as you say, `sigmoid` really just means an S shaped function. And if we do expose `expit`, we should probably expose its inverse `logit` too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10606#issuecomment-866034244
Usability,clear,clear,"Should be pretty clear what I'm doing, but let me know if you want me to walk through the design process here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10628#issuecomment-880902195
Usability,clear,clearly,My hacky attempt to allow setting a path to an alternate lapack library clearly doesn't work on the cluster: it's trying to look up a flag on the HailContext on workers. Open to suggestions on the right design here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10770#issuecomment-897005443
Usability,simpl,simpler,"That would work, though I think it's probably simpler to have min and max be the first and last elements of the pivots array (which should be required to be sorted).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10816#issuecomment-906667906
Availability,error,error,I commented on Zulip about how to make this error the same for every backend. I think it should be a simple change to use `parallelizeAndComputeWithIndex`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10819#issuecomment-906833242
Usability,simpl,simple,I commented on Zulip about how to make this error the same for every backend. I think it should be a simple change to use `parallelizeAndComputeWithIndex`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10819#issuecomment-906833242
Security,hash,hash,"The Google docs aren't clear about whether the hash needs to be a suffix or prefix: . > add the hash of the sequence number as part of the object name to make it non-sequential. I'm somewhat hesitant to make this change since it means our part outputs are no longer sorted lexicographically, and this property has been very useful in the past.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10836#issuecomment-914241609
Usability,clear,clear,"The Google docs aren't clear about whether the hash needs to be a suffix or prefix: . > add the hash of the sequence number as part of the object name to make it non-sequential. I'm somewhat hesitant to make this change since it means our part outputs are no longer sorted lexicographically, and this property has been very useful in the past.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10836#issuecomment-914241609
Usability,feedback,feedback,"FYI @danking. If you don't like how this is structured, then please give me high level feedback on what you would do differently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10898#issuecomment-928209199
Usability,feedback,feedback,"I didn't quite everything done in worker.py or managing the different secrets. However, I'd still appreciate feedback on the structure before I work on this more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-934758488
Testability,test,test,"I ran the stress test. It finished in 8 minutes which makes for a paltry 2 jobs per second. That was largely driven by three jobs that took 5 minutes (!!!) to upload their logs to GCS. No idea what's going on there, but clearly unrelated to these DB changes. If you ignore those jobs and the private jobs, which required VM spin up, it only took 3 minutes, which is still an unfortunate 6 jobs per second, but I have more speed coming.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10985#issuecomment-954864994
Usability,clear,clearly,"I ran the stress test. It finished in 8 minutes which makes for a paltry 2 jobs per second. That was largely driven by three jobs that took 5 minutes (!!!) to upload their logs to GCS. No idea what's going on there, but clearly unrelated to these DB changes. If you ignore those jobs and the private jobs, which required VM spin up, it only took 3 minutes, which is still an unfortunate 6 jobs per second, but I have more speed coming.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10985#issuecomment-954864994
Usability,feedback,feedback,"I'd appreciate feedback, but I realize there's more ways to abstract the code and make it more reusable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/10986#issuecomment-947023708
Testability,test,test,Could we add a specific test for compatibilty on JDK 8 and move the default forward? We're on a crash course of incompatibility with the current python3.6 / ubuntu:18.04 / Java 8 restrictions. From Oracle's pages it feels like Java 8 will simply never die...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11046#issuecomment-965471326
Usability,simpl,simply,Could we add a specific test for compatibilty on JDK 8 and move the default forward? We're on a crash course of incompatibility with the current python3.6 / ubuntu:18.04 / Java 8 restrictions. From Oracle's pages it feels like Java 8 will simply never die...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11046#issuecomment-965471326
Usability,clear,clearer,"@patrick-schultz I addressed your comments, would be happy to take more steps to make code clearer if you see some. Otherwise I'd like if you could check that you're now happy with how `oversamplingNum` is being used.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11050#issuecomment-992692981
Deployability,deploy,deploys,"I'll think about this more, but making the CI version explicit rather than implicit would at least provide a clear progression of necessary deploys. We'd want to tag the CI versions in git so that folks know which commits are necessary to achieve the step-wise transition. Let's find some time to chat next next week.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11122#issuecomment-985759470
Usability,clear,clear,"I'll think about this more, but making the CI version explicit rather than implicit would at least provide a clear progression of necessary deploys. We'd want to tag the CI versions in git so that folks know which commits are necessary to achieve the step-wise transition. Let's find some time to chat next next week.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11122#issuecomment-985759470
Deployability,integrat,integrates,"Thank you all for another round of detailed critique!. OK, I think the only remaining critical fix is to hard-code a mainclass. This is a wee bit complicated because I need to multiplex the ServiceBackendSocketAPI2 and the Worker. I hope to do this tomorrow AM. I'll then dismiss reviews. I also have a list of todos generated by this process which will feedback into some master QoB doc that integrates the two teams necessary todos.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11194#issuecomment-1033173934
Integrability,integrat,integrates,"Thank you all for another round of detailed critique!. OK, I think the only remaining critical fix is to hard-code a mainclass. This is a wee bit complicated because I need to multiplex the ServiceBackendSocketAPI2 and the Worker. I hope to do this tomorrow AM. I'll then dismiss reviews. I also have a list of todos generated by this process which will feedback into some master QoB doc that integrates the two teams necessary todos.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11194#issuecomment-1033173934
Usability,feedback,feedback,"Thank you all for another round of detailed critique!. OK, I think the only remaining critical fix is to hard-code a mainclass. This is a wee bit complicated because I need to multiplex the ServiceBackendSocketAPI2 and the Worker. I hope to do this tomorrow AM. I'll then dismiss reviews. I also have a list of todos generated by this process which will feedback into some master QoB doc that integrates the two teams necessary todos.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11194#issuecomment-1033173934
Usability,simpl,simplify,cc: @jigold some changes to memory to simplify it and address some weird behavior. sent over to Daniel since I feel like I've been hitting you with a bunch of PRs lately 😉 !,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11202#issuecomment-1006964286
Testability,test,test,"AFAIK, we don't test credential discovery. A little tricky because you want to modify the file system, which isn't kosher on a developer's laptop. Also not clear what to do about, e.g., the metadata server. You could mock it, but I've always felt pretty negative about the value of mocking. You might try adding some build.yaml steps that use docker to replicate some common environments?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11207#issuecomment-1007489641
Usability,clear,clear,"AFAIK, we don't test credential discovery. A little tricky because you want to modify the file system, which isn't kosher on a developer's laptop. Also not clear what to do about, e.g., the metadata server. You could mock it, but I've always felt pretty negative about the value of mocking. You might try adding some build.yaml steps that use docker to replicate some common environments?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11207#issuecomment-1007489641
Usability,feedback,feedback,"@illusional @lgruen Thanks for the feedback. I think the new changes address your comments, but let me know if you see something else.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11249#issuecomment-1020325698
Availability,down,down,"Now that we have the SQL query monitoring, I would love to also see just the simple comparison of the total number of queries we perform over a 1-minute period under high load. We have the tools now to see just what chunk of overall database communication we are cutting down on, which is an achievement in itself. Just need to run the test!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520
Energy Efficiency,monitor,monitoring,"Now that we have the SQL query monitoring, I would love to also see just the simple comparison of the total number of queries we perform over a 1-minute period under high load. We have the tools now to see just what chunk of overall database communication we are cutting down on, which is an achievement in itself. Just need to run the test!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520
Performance,perform,perform,"Now that we have the SQL query monitoring, I would love to also see just the simple comparison of the total number of queries we perform over a 1-minute period under high load. We have the tools now to see just what chunk of overall database communication we are cutting down on, which is an achievement in itself. Just need to run the test!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520
Testability,test,test,"Now that we have the SQL query monitoring, I would love to also see just the simple comparison of the total number of queries we perform over a 1-minute period under high load. We have the tools now to see just what chunk of overall database communication we are cutting down on, which is an achievement in itself. Just need to run the test!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520
Usability,simpl,simple,"Now that we have the SQL query monitoring, I would love to also see just the simple comparison of the total number of queries we perform over a 1-minute period under high load. We have the tools now to see just what chunk of overall database communication we are cutting down on, which is an achievement in itself. Just need to run the test!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520
Deployability,update,updated,"Here's the deadlock that I'm observing now on this branch. I don't have a great understanding of what's happening. I believe Transaction 1 to be [here](https://github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L1186-L1188) in MJC, and Transaction 2 to be [here](https://github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L449-L453) in the jobs_after_update trigger. Looking at the second transaction in context now, it looks like that is probably another MJC transaction toward the end of its run after it updated the jobs table. I think it would make sense then that T2 would still hold the lock for `instances_free_cores_mcpu` but I'm not sure where the contention for `batch_inst_coll_cancellable_resources` is coming from, as I don't see how T1 could be holding any form of lock on it. Either way it seems like how we use these tables is similarly a mess. ```; *** (1) TRANSACTION:; TRANSACTION 644409381, ACTIVE 0 sec starting index read; mysql tables in use 1, locked 1; LOCK WAIT 39 lock struct(s), heap size 3520, 50 row lock(s), undo log entries 28; MySQL thread id 1941960, OS thread handle 140297909716736, query id 1869168359 10.32.3.8 dgoldste updating; UPDATE instances_free_cores_mcpu; SET free_cores_mcpu = free_cores_mcpu + cur_cores_mcpu; WHERE instances_free_cores_mcpu.name = in_instance_name; *** (1) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409381 lock_mode X locks rec but not gap waiting; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) TRANSACTION:; TRANSACTI",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116
Testability,log,log," happening. I believe Transaction 1 to be [here](https://github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L1186-L1188) in MJC, and Transaction 2 to be [here](https://github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L449-L453) in the jobs_after_update trigger. Looking at the second transaction in context now, it looks like that is probably another MJC transaction toward the end of its run after it updated the jobs table. I think it would make sense then that T2 would still hold the lock for `instances_free_cores_mcpu` but I'm not sure where the contention for `batch_inst_coll_cancellable_resources` is coming from, as I don't see how T1 could be holding any form of lock on it. Either way it seems like how we use these tables is similarly a mess. ```; *** (1) TRANSACTION:; TRANSACTION 644409381, ACTIVE 0 sec starting index read; mysql tables in use 1, locked 1; LOCK WAIT 39 lock struct(s), heap size 3520, 50 row lock(s), undo log entries 28; MySQL thread id 1941960, OS thread handle 140297909716736, query id 1869168359 10.32.3.8 dgoldste updating; UPDATE instances_free_cores_mcpu; SET free_cores_mcpu = free_cores_mcpu + cur_cores_mcpu; WHERE instances_free_cores_mcpu.name = in_instance_name; *** (1) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409381 lock_mode X locks rec but not gap waiting; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) TRANSACTION:; TRANSACTION 644409370, ACTIVE 0 sec inserting; mysql tables in use 6, locked 6; 39 lock struct(s), heap size 35",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116
Usability,undo,undo," happening. I believe Transaction 1 to be [here](https://github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L1186-L1188) in MJC, and Transaction 2 to be [here](https://github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L449-L453) in the jobs_after_update trigger. Looking at the second transaction in context now, it looks like that is probably another MJC transaction toward the end of its run after it updated the jobs table. I think it would make sense then that T2 would still hold the lock for `instances_free_cores_mcpu` but I'm not sure where the contention for `batch_inst_coll_cancellable_resources` is coming from, as I don't see how T1 could be holding any form of lock on it. Either way it seems like how we use these tables is similarly a mess. ```; *** (1) TRANSACTION:; TRANSACTION 644409381, ACTIVE 0 sec starting index read; mysql tables in use 1, locked 1; LOCK WAIT 39 lock struct(s), heap size 3520, 50 row lock(s), undo log entries 28; MySQL thread id 1941960, OS thread handle 140297909716736, query id 1869168359 10.32.3.8 dgoldste updating; UPDATE instances_free_cores_mcpu; SET free_cores_mcpu = free_cores_mcpu + cur_cores_mcpu; WHERE instances_free_cores_mcpu.name = in_instance_name; *** (1) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409381 lock_mode X locks rec but not gap waiting; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) TRANSACTION:; TRANSACTION 644409370, ACTIVE 0 sec inserting; mysql tables in use 6, locked 6; 39 lock struct(s), heap size 35",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116
Deployability,update,updated,"It is mighty fishy that both azure and google failed the callback test. What are we missing? If MJC returns, then the database was clearly updated. Subsequent DB queries should see those changes. total_jobs_in_batch won't change during the lifetime of the batch, so that should be correct (though we should probably LOCK IN SHARE MODE anyway). Assuming I'm reading the [reference manual](https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html) correctly, that select should see the result of the UPDATE *or a later state*. The updates to a single row are serial. So there must exist a transaction that takes it from n_jobs-1 to n_jobs. That transaction thus must see n_jobs for new_n_completed. That transaction thus ought to update batches. Once that transaction is committed the subsequent query for notification should see the changes...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1040809121
Testability,test,test,"It is mighty fishy that both azure and google failed the callback test. What are we missing? If MJC returns, then the database was clearly updated. Subsequent DB queries should see those changes. total_jobs_in_batch won't change during the lifetime of the batch, so that should be correct (though we should probably LOCK IN SHARE MODE anyway). Assuming I'm reading the [reference manual](https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html) correctly, that select should see the result of the UPDATE *or a later state*. The updates to a single row are serial. So there must exist a transaction that takes it from n_jobs-1 to n_jobs. That transaction thus must see n_jobs for new_n_completed. That transaction thus ought to update batches. Once that transaction is committed the subsequent query for notification should see the changes...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1040809121
Usability,clear,clearly,"It is mighty fishy that both azure and google failed the callback test. What are we missing? If MJC returns, then the database was clearly updated. Subsequent DB queries should see those changes. total_jobs_in_batch won't change during the lifetime of the batch, so that should be correct (though we should probably LOCK IN SHARE MODE anyway). Assuming I'm reading the [reference manual](https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html) correctly, that select should see the result of the UPDATE *or a later state*. The updates to a single row are serial. So there must exist a transaction that takes it from n_jobs-1 to n_jobs. That transaction thus must see n_jobs for new_n_completed. That transaction thus ought to update batches. Once that transaction is committed the subsequent query for notification should see the changes...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1040809121
Usability,simpl,simpler,The other issue is I'm getting a small negative value for the first time point on CPU. Not sure how to fix that. Maybe we just ignore it for now? Or revert back to the simpler code where we omit the first timepoint? Although I don't know how to represent NaN in binary data if we keep the first memory timepoint.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11368#issuecomment-1058213280
Deployability,deploy,deployed,"Hi Vlad, thanks for the PR! I'm afraid there are some internal migrations we're making that are probably not clear from just looking at the codebase. Are you up to date on our `main`? We've found working with `config.mk` cumbersome because it can be stale if you switch between different instances of Batch (e.g. one deployed in azure and the other in GCP). > DOCKER_ROOT_IMAGE used to build batch workers and benchmark. I've recently updated the scripts for building the batch worker VM image to query kubernetes directly and we should probably do the same for benchmark. > HAIL_TEST_GCS_BUCKET used to build query; KUBERNETES_SERVER_URL used to build amundsen. These services are both currently deleted in our `main`. > PROJECT, ZONE, REGION are probably not need, but might make sense to add for consistency. These will fail in an Azure deployment, and while we want to move away from `config.mk` entirely, we would at least want it to contain configurations that are valid across clouds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11371#issuecomment-1041941055
Modifiability,config,config,"Hi Vlad, thanks for the PR! I'm afraid there are some internal migrations we're making that are probably not clear from just looking at the codebase. Are you up to date on our `main`? We've found working with `config.mk` cumbersome because it can be stale if you switch between different instances of Batch (e.g. one deployed in azure and the other in GCP). > DOCKER_ROOT_IMAGE used to build batch workers and benchmark. I've recently updated the scripts for building the batch worker VM image to query kubernetes directly and we should probably do the same for benchmark. > HAIL_TEST_GCS_BUCKET used to build query; KUBERNETES_SERVER_URL used to build amundsen. These services are both currently deleted in our `main`. > PROJECT, ZONE, REGION are probably not need, but might make sense to add for consistency. These will fail in an Azure deployment, and while we want to move away from `config.mk` entirely, we would at least want it to contain configurations that are valid across clouds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11371#issuecomment-1041941055
Testability,benchmark,benchmark,"Hi Vlad, thanks for the PR! I'm afraid there are some internal migrations we're making that are probably not clear from just looking at the codebase. Are you up to date on our `main`? We've found working with `config.mk` cumbersome because it can be stale if you switch between different instances of Batch (e.g. one deployed in azure and the other in GCP). > DOCKER_ROOT_IMAGE used to build batch workers and benchmark. I've recently updated the scripts for building the batch worker VM image to query kubernetes directly and we should probably do the same for benchmark. > HAIL_TEST_GCS_BUCKET used to build query; KUBERNETES_SERVER_URL used to build amundsen. These services are both currently deleted in our `main`. > PROJECT, ZONE, REGION are probably not need, but might make sense to add for consistency. These will fail in an Azure deployment, and while we want to move away from `config.mk` entirely, we would at least want it to contain configurations that are valid across clouds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11371#issuecomment-1041941055
Usability,clear,clear,"Hi Vlad, thanks for the PR! I'm afraid there are some internal migrations we're making that are probably not clear from just looking at the codebase. Are you up to date on our `main`? We've found working with `config.mk` cumbersome because it can be stale if you switch between different instances of Batch (e.g. one deployed in azure and the other in GCP). > DOCKER_ROOT_IMAGE used to build batch workers and benchmark. I've recently updated the scripts for building the batch worker VM image to query kubernetes directly and we should probably do the same for benchmark. > HAIL_TEST_GCS_BUCKET used to build query; KUBERNETES_SERVER_URL used to build amundsen. These services are both currently deleted in our `main`. > PROJECT, ZONE, REGION are probably not need, but might make sense to add for consistency. These will fail in an Azure deployment, and while we want to move away from `config.mk` entirely, we would at least want it to contain configurations that are valid across clouds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11371#issuecomment-1041941055
Availability,error,error,"Working backwards, we need to not return a 500 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078
Energy Efficiency,schedul,scheduled,"00 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_statuses': {k: None for k in tasks},; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078
Integrability,message,message,"Working backwards, we need to not return a 500 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078
Modifiability,config,config,"00 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_statuses': {k: None for k in tasks},; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078
Safety,timeout,timeout,"00 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_statuses': {k: None for k in tasks},; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078
Testability,log,log,"00 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_statuses': {k: None for k in tasks},; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078
Usability,clear,clearer,"Working backwards, we need to not return a 500 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078
Availability,error,error,"yeah this was a simple error, forgot to do one upstream processing step",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/11413#issuecomment-1050268863
Usability,simpl,simple,"yeah this was a simple error, forgot to do one upstream processing step",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/11413#issuecomment-1050268863
Availability,checkpoint,checkpointing,"Huh, checkpointing `sample.vcf` after the filter clears up the issue. This may be something to do with the result of import_vcf being used directly with `_same`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11450#issuecomment-1061867500
Usability,clear,clears,"Huh, checkpointing `sample.vcf` after the filter clears up the issue. This may be something to do with the result of import_vcf being used directly with `_same`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11450#issuecomment-1061867500
Deployability,update,update,"with the new mypy update, mypy complains if we don't use the type stubs for our dependencies. This is fixed in main (we add the type stubs) but not in the previously released pip hail, because well it's already released. One option is we decide we don't like this requirement and disable that for mypy (though I do enjoy having the type hints). The problem remains that we lint the released version with the `setup.cfg` on main, so this will fail if we ever tighten our linting. It's not clear to me why we want to lint already-released hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1061947409
Integrability,depend,dependencies,"with the new mypy update, mypy complains if we don't use the type stubs for our dependencies. This is fixed in main (we add the type stubs) but not in the previously released pip hail, because well it's already released. One option is we decide we don't like this requirement and disable that for mypy (though I do enjoy having the type hints). The problem remains that we lint the released version with the `setup.cfg` on main, so this will fail if we ever tighten our linting. It's not clear to me why we want to lint already-released hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1061947409
Testability,stub,stubs,"with the new mypy update, mypy complains if we don't use the type stubs for our dependencies. This is fixed in main (we add the type stubs) but not in the previously released pip hail, because well it's already released. One option is we decide we don't like this requirement and disable that for mypy (though I do enjoy having the type hints). The problem remains that we lint the released version with the `setup.cfg` on main, so this will fail if we ever tighten our linting. It's not clear to me why we want to lint already-released hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1061947409
Usability,clear,clear,"with the new mypy update, mypy complains if we don't use the type stubs for our dependencies. This is fixed in main (we add the type stubs) but not in the previously released pip hail, because well it's already released. One option is we decide we don't like this requirement and disable that for mypy (though I do enjoy having the type hints). The problem remains that we lint the released version with the `setup.cfg` on main, so this will fail if we ever tighten our linting. It's not clear to me why we want to lint already-released hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1061947409
Deployability,install,installed,"Oh grrr, the current build is extra borked because a minor version bump in the azure blob storage library broke. I'll back out that change. Oh I think I totally misunderstood the ""pip-installed images"". [here](https://ci.hail.is/batches/774665/jobs/63) is the job I was initially confused about. Not clear why in that batch only the pip-installed hail failed the lint and not the `check_hail` step",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1062078939
Usability,clear,clear,"Oh grrr, the current build is extra borked because a minor version bump in the azure blob storage library broke. I'll back out that change. Oh I think I totally misunderstood the ""pip-installed images"". [here](https://ci.hail.is/batches/774665/jobs/63) is the job I was initially confused about. Not clear why in that batch only the pip-installed hail failed the lint and not the `check_hail` step",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1062078939
Usability,pause,pause,"I'm still working out the best workaround for the library version. I tried a mix of gradle sub-project-ing and shading with John and they all didn't work out for various different reasons. Azure library was doing some weird classLoader stuff that didn't like being bulk-relocated, and force-upgrading jackson didn't work because pyspark bundles in the buggy version of jackson without shading it. We can try to work out the former but I decided to pause on that front and try instead to just not use the buggy Azure method from the compatible library version and hit the REST endpoint directly, which involves some XML parsing. Currently in the middle of that. Happy to discuss the various approaches at our 1:1 tomorrow or earlier. In terms of hail-side implementation, it's all here under the assumption that the underlying library doesn't cause classpath conflicts (which it currently does).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11617#issuecomment-1095033418
Usability,clear,clearly,"`test_rectangles_to_numpy` shouldn't pass on local, that's one of the weird ones where we don't understand why it's passing, it's clearly unlowered. In fact, in this PR I added the `fail_local` annotation to it. I'll try what you suggested.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11637#issuecomment-1078198872
Usability,feedback,feedback,"Excellent feedback, I've improved the makefile substantially. It should now be a seamless experience for AUS et al.!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11645#issuecomment-1076594568
Usability,clear,clear,"Potentially relevant: https://community.plotly.com/t/graphing-storage-in-units-of-1024-kilobytes-megabytes-etc/14305/2. I think it's a little bit of work to get to a place where units show up nicely for memory. Going to want to find the max value in the column in bytes, figure out what scale we are at (bytes, kilobytes, megabytes, gigabytes, etc.) then set custom tick range and tickvals accordingly. Also should probably make it clear whether we are in ""megabytes"" or ""mebibytes"" or whatever they're called.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11690#issuecomment-1080637072
Usability,clear,clear,"I think if it's not too hard of a change, we should add the files with encoded secrets to something like `infra/gcp/data/...`. This makes it clear that these files have a different purpose and gives some indication that they're specific to your repo. If you want to also add prefixing the file name with the repo, then that would make it even clearer. But if it's too much work, don't bother. Maybe something like `infra/gcp/data/hail-is/` etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1171321194
Deployability,deploy,deployment-specific,"> I think if it's not too hard of a change, we should add the files with encoded secrets to something like `infra/gcp/data/...`. This makes it clear that these files have a different purpose and gives some indication that they're specific to your repo. If you want to also add prefixing the file name with the repo, then that would make it even clearer. But if it's too much work, don't bother. Maybe something like `infra/gcp/data/hail-is/` etc. I've put all deployment-specific configs in an `$ORGANIZATION_DOMAIN` subfolder now, which hopefully should avoid any collisions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1173211731
Modifiability,config,configs,"> I think if it's not too hard of a change, we should add the files with encoded secrets to something like `infra/gcp/data/...`. This makes it clear that these files have a different purpose and gives some indication that they're specific to your repo. If you want to also add prefixing the file name with the repo, then that would make it even clearer. But if it's too much work, don't bother. Maybe something like `infra/gcp/data/hail-is/` etc. I've put all deployment-specific configs in an `$ORGANIZATION_DOMAIN` subfolder now, which hopefully should avoid any collisions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1173211731
Safety,avoid,avoid,"> I think if it's not too hard of a change, we should add the files with encoded secrets to something like `infra/gcp/data/...`. This makes it clear that these files have a different purpose and gives some indication that they're specific to your repo. If you want to also add prefixing the file name with the repo, then that would make it even clearer. But if it's too much work, don't bother. Maybe something like `infra/gcp/data/hail-is/` etc. I've put all deployment-specific configs in an `$ORGANIZATION_DOMAIN` subfolder now, which hopefully should avoid any collisions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1173211731
Usability,clear,clear,"> I think if it's not too hard of a change, we should add the files with encoded secrets to something like `infra/gcp/data/...`. This makes it clear that these files have a different purpose and gives some indication that they're specific to your repo. If you want to also add prefixing the file name with the repo, then that would make it even clearer. But if it's too much work, don't bother. Maybe something like `infra/gcp/data/hail-is/` etc. I've put all deployment-specific configs in an `$ORGANIZATION_DOMAIN` subfolder now, which hopefully should avoid any collisions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1173211731
Availability,error,error,"So what I did to address the comments:; 1. I encoded the task names and the offsets of the data into the response from the worker to the front end and got rid of the MultiPart Reader/Writer.; 2. I reorganized the front end code so it's hopefully clearer what's going on.; 3. I got rid of the periodically_call changes and just did what I wanted directly in the `measure` function in the Resource Manager. Now it should be guaranteed that we do not call the measure function with the write more than once every 5 seconds. I do not want to retry calling this function at all on any error including transient errors. Otherwise, I can't figure out how the set of changes in this PR would use up all the disk space.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11750#issuecomment-1137707785
Usability,clear,clearer,"So what I did to address the comments:; 1. I encoded the task names and the offsets of the data into the response from the worker to the front end and got rid of the MultiPart Reader/Writer.; 2. I reorganized the front end code so it's hopefully clearer what's going on.; 3. I got rid of the periodically_call changes and just did what I wanted directly in the `measure` function in the Resource Manager. Now it should be guaranteed that we do not call the measure function with the write more than once every 5 seconds. I do not want to retry calling this function at all on any error including transient errors. Otherwise, I can't figure out how the set of changes in this PR would use up all the disk space.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11750#issuecomment-1137707785
Usability,simpl,simple,"BTW, this is an extremely awesome change. I didn't think it would be so simple!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11771#issuecomment-1100420864
Usability,simpl,simple,"> BTW, this is an extremely awesome change. I didn't think it would be so simple!. I am simply here for the negative LoC points",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11771#issuecomment-1100517115
Usability,feedback,feedback,"When I run copy inside a batch job to copy some files, it's really annoying to get no feedback at all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11776#issuecomment-1101494934
Testability,test,test,@jigold this should be simple. Last thing Dan wanted was that I fix the test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11777#issuecomment-1170350346
Usability,simpl,simple,@jigold this should be simple. Last thing Dan wanted was that I fix the test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11777#issuecomment-1170350346
Usability,simpl,simpler,This has diverged significantly and contains parts of some changes to handling hailgenetics images that was handled in separate PRs. Closing in favor of #12446 which is much simpler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11797#issuecomment-1314533333
Deployability,continuous,continuous,"Hey @williambrandler ! Thanks for your contribution to Hail. We endeavor to keep our docs always accurate and up-to-date. Our continuous deployment system verifies the correctness of our Google Dataproc and Azure HDInsight instructions before releasing a new version of Hail to PyPI. Does Databricks have an open source program that would provide us with free credits to incorporate the Databricks platform into our continuous deployment process? Alternatively, I'm comfortable accepting these new instructions with a disclaimer that clearly identifies these instructions as contributed by Databricks and not maintained by the Hail team. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11800#issuecomment-1112572464
Usability,clear,clearly,"Hey @williambrandler ! Thanks for your contribution to Hail. We endeavor to keep our docs always accurate and up-to-date. Our continuous deployment system verifies the correctness of our Google Dataproc and Azure HDInsight instructions before releasing a new version of Hail to PyPI. Does Databricks have an open source program that would provide us with free credits to incorporate the Databricks platform into our continuous deployment process? Alternatively, I'm comfortable accepting these new instructions with a disclaimer that clearly identifies these instructions as contributed by Databricks and not maintained by the Hail team. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11800#issuecomment-1112572464
Availability,error,error,Maybe we should update the WARNING message to be clear that this is a transient error and we've automatically retried it and there's nothing to be worried about?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877
Deployability,update,update,Maybe we should update the WARNING message to be clear that this is a transient error and we've automatically retried it and there's nothing to be worried about?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877
Integrability,message,message,Maybe we should update the WARNING message to be clear that this is a transient error and we've automatically retried it and there's nothing to be worried about?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877
Usability,clear,clear,Maybe we should update the WARNING message to be clear that this is a transient error and we've automatically retried it and there's nothing to be worried about?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877
Availability,error,error,"cc: @daniel-goldstein is it possible to make the error more clear? The CI test job is a bit hard to read. Maybe the last line of the output should be something like ""XXXX file is out of date, run YYYY""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1325464620
Testability,test,test,"cc: @daniel-goldstein is it possible to make the error more clear? The CI test job is a bit hard to read. Maybe the last line of the output should be something like ""XXXX file is out of date, run YYYY""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1325464620
Usability,clear,clear,"cc: @daniel-goldstein is it possible to make the error more clear? The CI test job is a bit hard to read. Maybe the last line of the output should be something like ""XXXX file is out of date, run YYYY""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1325464620
Availability,redundant,redundant,"Here's the terraform configurations in GCP and Azure:. - GCP: Batch has admin storage permissions, as granted here https://github.com/hail-is/hail/blob/1f5e1540c04abfde58ead1084841fec5aa6e0ed3/infra/gcp/main.tf#L415-L424. We also grant it a Viewer role on the query bucket after that which seems redundant. We should really not grant it global storage admin and instead give it admin for just the query bucket and other associated batch buckets. I checked in hail-vdc and batch does not have the global storage admin role, and it has the Viewer role on the query bucket. I've changed that role now to admin on the query bucket. - Azure: Story is simpler. The `query` storage container is part of the `batch` storage account. The batch SP has ownership over the `batch` storage account and by extension all of the containers inside it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011
Deployability,configurat,configurations,"Here's the terraform configurations in GCP and Azure:. - GCP: Batch has admin storage permissions, as granted here https://github.com/hail-is/hail/blob/1f5e1540c04abfde58ead1084841fec5aa6e0ed3/infra/gcp/main.tf#L415-L424. We also grant it a Viewer role on the query bucket after that which seems redundant. We should really not grant it global storage admin and instead give it admin for just the query bucket and other associated batch buckets. I checked in hail-vdc and batch does not have the global storage admin role, and it has the Viewer role on the query bucket. I've changed that role now to admin on the query bucket. - Azure: Story is simpler. The `query` storage container is part of the `batch` storage account. The batch SP has ownership over the `batch` storage account and by extension all of the containers inside it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011
Modifiability,config,configurations,"Here's the terraform configurations in GCP and Azure:. - GCP: Batch has admin storage permissions, as granted here https://github.com/hail-is/hail/blob/1f5e1540c04abfde58ead1084841fec5aa6e0ed3/infra/gcp/main.tf#L415-L424. We also grant it a Viewer role on the query bucket after that which seems redundant. We should really not grant it global storage admin and instead give it admin for just the query bucket and other associated batch buckets. I checked in hail-vdc and batch does not have the global storage admin role, and it has the Viewer role on the query bucket. I've changed that role now to admin on the query bucket. - Azure: Story is simpler. The `query` storage container is part of the `batch` storage account. The batch SP has ownership over the `batch` storage account and by extension all of the containers inside it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011
Safety,redund,redundant,"Here's the terraform configurations in GCP and Azure:. - GCP: Batch has admin storage permissions, as granted here https://github.com/hail-is/hail/blob/1f5e1540c04abfde58ead1084841fec5aa6e0ed3/infra/gcp/main.tf#L415-L424. We also grant it a Viewer role on the query bucket after that which seems redundant. We should really not grant it global storage admin and instead give it admin for just the query bucket and other associated batch buckets. I checked in hail-vdc and batch does not have the global storage admin role, and it has the Viewer role on the query bucket. I've changed that role now to admin on the query bucket. - Azure: Story is simpler. The `query` storage container is part of the `batch` storage account. The batch SP has ownership over the `batch` storage account and by extension all of the containers inside it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011
Usability,simpl,simpler,"Here's the terraform configurations in GCP and Azure:. - GCP: Batch has admin storage permissions, as granted here https://github.com/hail-is/hail/blob/1f5e1540c04abfde58ead1084841fec5aa6e0ed3/infra/gcp/main.tf#L415-L424. We also grant it a Viewer role on the query bucket after that which seems redundant. We should really not grant it global storage admin and instead give it admin for just the query bucket and other associated batch buckets. I checked in hail-vdc and batch does not have the global storage admin role, and it has the Viewer role on the query bucket. I've changed that role now to admin on the query bucket. - Azure: Story is simpler. The `query` storage container is part of the `batch` storage account. The batch SP has ownership over the `batch` storage account and by extension all of the containers inside it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011
Usability,learn,learned,Let's take what I learned about worker_processes into account when making a decision here. You might just be seeing clashing between 8 processes with 1 core limits.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11876#issuecomment-1168824908
Usability,learn,learned,Closing this in light of what we've learned about keepalive,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11876#issuecomment-1170569730
Deployability,integrat,integrated,"Ok. I thought about this some more. What you've implemented is essentially a ""taint"" in Kubernetes. I ultimately want both taints and something more complicated that will have to be integrated into the scheduler. I think for now your change is self contained and it will be easy to transform later on without too much complexity or breaking changes for users. I think if you want to rename ""label"" to ""taint"" then that might make it clearer what's going on. cc: @daniel-goldstein",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11879#issuecomment-1145343746
Energy Efficiency,schedul,scheduler,"Ok. I thought about this some more. What you've implemented is essentially a ""taint"" in Kubernetes. I ultimately want both taints and something more complicated that will have to be integrated into the scheduler. I think for now your change is self contained and it will be easy to transform later on without too much complexity or breaking changes for users. I think if you want to rename ""label"" to ""taint"" then that might make it clearer what's going on. cc: @daniel-goldstein",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11879#issuecomment-1145343746
Integrability,integrat,integrated,"Ok. I thought about this some more. What you've implemented is essentially a ""taint"" in Kubernetes. I ultimately want both taints and something more complicated that will have to be integrated into the scheduler. I think for now your change is self contained and it will be easy to transform later on without too much complexity or breaking changes for users. I think if you want to rename ""label"" to ""taint"" then that might make it clearer what's going on. cc: @daniel-goldstein",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11879#issuecomment-1145343746
Usability,clear,clearer,"Ok. I thought about this some more. What you've implemented is essentially a ""taint"" in Kubernetes. I ultimately want both taints and something more complicated that will have to be integrated into the scheduler. I think for now your change is self contained and it will be easy to transform later on without too much complexity or breaking changes for users. I think if you want to rename ""label"" to ""taint"" then that might make it clearer what's going on. cc: @daniel-goldstein",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11879#issuecomment-1145343746
Deployability,release,release,"I think the proposed new default and the option to change it is much more intuitive than the current behavior and worth a change. Though, I think it would be most polite to announce it on zulip/email list a week or two in advance of the release (which you may already planned to do).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11884#issuecomment-1145290949
Usability,intuit,intuitive,"I think the proposed new default and the option to change it is much more intuitive than the current behavior and worth a change. Though, I think it would be most polite to announce it on zulip/email list a week or two in advance of the release (which you may already planned to do).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11884#issuecomment-1145290949
Deployability,deploy,deploy,"=registry,ref=gcr.io/hail-vdc/foo; ```; Will use as a cache source the `latest` tag in the `gcr.io/hail-vdc/foo` repository. It is *not* sufficient for an image to be present in the repository and untagged or with a different tag from `latest`. In particular, every push to the `cache` tag prevents us from using other images even though they are in the registry! For example, I pushed two images to `cache`:. ```; (base) # gcloud container images list-tags gcr.io/hail-vdc/dktest; DIGEST TAGS TIMESTAMP; fb551d9bdb94 2022-06-10T14:16:39; afb4c5ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little bugs.; 2. The most recent common commit with main should avoid rebuilds unless the packages changed.; 3. I suspect the current main is actually not helpful (either 2 will work or 3 wouldn't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800
Performance,cache,cache,"Ah, I understand. You're correct that pushing to the `cache` tag disassociates that tag with any image that was previously associated with it. That image is still in the registry though. I thought that was sufficient for the cache to work. (I was wrong, see below!). AFAICT, this change doesn't prevent PR tests from pushing to the `cache` tag. This change just makes the tests run by the CI-in-the-PR not overwrite the cache. Every image build for a PR (which is tested by default namespace CI) will still overwrite the cache tag. AFAICT, this; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/foo; ```; Will use as a cache source the `latest` tag in the `gcr.io/hail-vdc/foo` repository. It is *not* sufficient for an image to be present in the repository and untagged or with a different tag from `latest`. In particular, every push to the `cache` tag prevents us from using other images even though they are in the registry! For example, I pushed two images to `cache`:. ```; (base) # gcloud container images list-tags gcr.io/hail-vdc/dktest; DIGEST TAGS TIMESTAMP; fb551d9bdb94 2022-06-10T14:16:39; afb4c5ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800
Safety,avoid,avoid,"e! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little bugs.; 2. The most recent common commit with main should avoid rebuilds unless the packages changed.; 3. I suspect the current main is actually not helpful (either 2 will work or 3 wouldn't help).; 4. Pushing to something like `cache-11907` would allow force pushes to still access the last build's images. What do you think of the #3 proposal? . ---. [1]: I had two files:; ```; # cat sleep/Dockerfile; FROM ubuntu:18.04; RUN sleep 10; # cat touch/Dockerfile; FROM ubuntu:18.04; RUN touch foo; ```; To build I use this command (slightly different syntax from the buildctl syntax, but, AFAIK, uses the same backend):; ```; docker buildx \ ; build \; DIRECTORY_NAME_HERE \; --output 'type=image,""name=gcr.io/hail-vdc/dktest,gcr.io/hail-vdc/dktest:cache"",push=true' \; --cache-to type=inline \; --cache-from type=registry,ref=gcr.io/hail-vdc/dktest; ```; Before every build I clear the _local_ cache with:; ```; docker system prune -a; ```; I can clear the remote cache with:; ```; gcloud container images list-tags gcr.io/hail-vdc/dktest --format=""get(dige",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800
Security,access,access," push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little bugs.; 2. The most recent common commit with main should avoid rebuilds unless the packages changed.; 3. I suspect the current main is actually not helpful (either 2 will work or 3 wouldn't help).; 4. Pushing to something like `cache-11907` would allow force pushes to still access the last build's images. What do you think of the #3 proposal? . ---. [1]: I had two files:; ```; # cat sleep/Dockerfile; FROM ubuntu:18.04; RUN sleep 10; # cat touch/Dockerfile; FROM ubuntu:18.04; RUN touch foo; ```; To build I use this command (slightly different syntax from the buildctl syntax, but, AFAIK, uses the same backend):; ```; docker buildx \ ; build \; DIRECTORY_NAME_HERE \; --output 'type=image,""name=gcr.io/hail-vdc/dktest,gcr.io/hail-vdc/dktest:cache"",push=true' \; --cache-to type=inline \; --cache-from type=registry,ref=gcr.io/hail-vdc/dktest; ```; Before every build I clear the _local_ cache with:; ```; docker system prune -a; ```; I can clear the remote cache with:; ```; gcloud container images list-tags gcr.io/hail-vdc/dktest --format=""get(digest)"" \; | awk '{print ""gcr.io/hail-vdc/dktest@"" $1}' \; | xargs gcloud container images delete --force-delete-tags; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800
Testability,test,tests,"Ah, I understand. You're correct that pushing to the `cache` tag disassociates that tag with any image that was previously associated with it. That image is still in the registry though. I thought that was sufficient for the cache to work. (I was wrong, see below!). AFAICT, this change doesn't prevent PR tests from pushing to the `cache` tag. This change just makes the tests run by the CI-in-the-PR not overwrite the cache. Every image build for a PR (which is tested by default namespace CI) will still overwrite the cache tag. AFAICT, this; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/foo; ```; Will use as a cache source the `latest` tag in the `gcr.io/hail-vdc/foo` repository. It is *not* sufficient for an image to be present in the repository and untagged or with a different tag from `latest`. In particular, every push to the `cache` tag prevents us from using other images even though they are in the registry! For example, I pushed two images to `cache`:. ```; (base) # gcloud container images list-tags gcr.io/hail-vdc/dktest; DIGEST TAGS TIMESTAMP; fb551d9bdb94 2022-06-10T14:16:39; afb4c5ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800
Usability,clear,clear," push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little bugs.; 2. The most recent common commit with main should avoid rebuilds unless the packages changed.; 3. I suspect the current main is actually not helpful (either 2 will work or 3 wouldn't help).; 4. Pushing to something like `cache-11907` would allow force pushes to still access the last build's images. What do you think of the #3 proposal? . ---. [1]: I had two files:; ```; # cat sleep/Dockerfile; FROM ubuntu:18.04; RUN sleep 10; # cat touch/Dockerfile; FROM ubuntu:18.04; RUN touch foo; ```; To build I use this command (slightly different syntax from the buildctl syntax, but, AFAIK, uses the same backend):; ```; docker buildx \ ; build \; DIRECTORY_NAME_HERE \; --output 'type=image,""name=gcr.io/hail-vdc/dktest,gcr.io/hail-vdc/dktest:cache"",push=true' \; --cache-to type=inline \; --cache-from type=registry,ref=gcr.io/hail-vdc/dktest; ```; Before every build I clear the _local_ cache with:; ```; docker system prune -a; ```; I can clear the remote cache with:; ```; gcloud container images list-tags gcr.io/hail-vdc/dktest --format=""get(digest)"" \; | awk '{print ""gcr.io/hail-vdc/dktest@"" $1}' \; | xargs gcloud container images delete --force-delete-tags; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800
Deployability,deploy,deploys,"@jigold Ok I changed this in a way that will hopefully be more clear. The rules are as follows:. 1. We only use `ci-intermediate` for anonymous images. Images are named auth, batch, etc. even when they are in tests or dev deploys.; 1. Every image draws from the main branch cache tag, named `<DOCKER_PREFIX>/<image_name>:cache`; 2. Every image has an additional cache tag that it draws from and pushes to. For deploys, that is the same as the main branch cache, for PRs, it is `cache-pr-<pr_number>`, for dev deploys it is `cache-<dev_username>`, and for deploys conducted by CIs in a non-default namespace, it is `cache-<namespace-CI-is-in>-deploy`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11999#issuecomment-1177641450
Performance,cache,cache,"@jigold Ok I changed this in a way that will hopefully be more clear. The rules are as follows:. 1. We only use `ci-intermediate` for anonymous images. Images are named auth, batch, etc. even when they are in tests or dev deploys.; 1. Every image draws from the main branch cache tag, named `<DOCKER_PREFIX>/<image_name>:cache`; 2. Every image has an additional cache tag that it draws from and pushes to. For deploys, that is the same as the main branch cache, for PRs, it is `cache-pr-<pr_number>`, for dev deploys it is `cache-<dev_username>`, and for deploys conducted by CIs in a non-default namespace, it is `cache-<namespace-CI-is-in>-deploy`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11999#issuecomment-1177641450
Testability,test,tests,"@jigold Ok I changed this in a way that will hopefully be more clear. The rules are as follows:. 1. We only use `ci-intermediate` for anonymous images. Images are named auth, batch, etc. even when they are in tests or dev deploys.; 1. Every image draws from the main branch cache tag, named `<DOCKER_PREFIX>/<image_name>:cache`; 2. Every image has an additional cache tag that it draws from and pushes to. For deploys, that is the same as the main branch cache, for PRs, it is `cache-pr-<pr_number>`, for dev deploys it is `cache-<dev_username>`, and for deploys conducted by CIs in a non-default namespace, it is `cache-<namespace-CI-is-in>-deploy`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11999#issuecomment-1177641450
Usability,clear,clear,"@jigold Ok I changed this in a way that will hopefully be more clear. The rules are as follows:. 1. We only use `ci-intermediate` for anonymous images. Images are named auth, batch, etc. even when they are in tests or dev deploys.; 1. Every image draws from the main branch cache tag, named `<DOCKER_PREFIX>/<image_name>:cache`; 2. Every image has an additional cache tag that it draws from and pushes to. For deploys, that is the same as the main branch cache, for PRs, it is `cache-pr-<pr_number>`, for dev deploys it is `cache-<dev_username>`, and for deploys conducted by CIs in a non-default namespace, it is `cache-<namespace-CI-is-in>-deploy`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/11999#issuecomment-1177641450
Usability,feedback,feedback,It's probably not going to pass CI on the first try after the rebase. I'm mainly looking for feedback on the API first.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215231720
Deployability,update,update,"I think we should find a time to discuss this in person if the following explanation doesn't make sense. . Right now, for small batches, we send one REST request to the server to both create the batch and create the jobs. However, if we want one REST request for an update (ideal for the query service and low latency jobs?), we have to use relative job ids because (1) we don't know the absolute start index of the jobs until we've gotten the start id of the update back from the server and (2) the job dependencies can be a mix of known job ids that have already been previously submitted in a previous creation/update. The negative job IDs are a way to deal with a mix of relative ids within an update and known, submitted job ids. We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd have to make sure this will actually simplify things because I also ran into a bifurcation in how the job IDs are handled in `BatchBuilder.create_job()`. We currently populate the spec with a job id before we've made any requests to the server. We need to know how many total jobs there are before we can figure out the job ids because the API for creating a new update requires reserving a block of job IDs which then returns the start id. This complexity is because we allow multiple updates to occur simultaneously to a batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215919856
Integrability,depend,dependencies,"I think we should find a time to discuss this in person if the following explanation doesn't make sense. . Right now, for small batches, we send one REST request to the server to both create the batch and create the jobs. However, if we want one REST request for an update (ideal for the query service and low latency jobs?), we have to use relative job ids because (1) we don't know the absolute start index of the jobs until we've gotten the start id of the update back from the server and (2) the job dependencies can be a mix of known job ids that have already been previously submitted in a previous creation/update. The negative job IDs are a way to deal with a mix of relative ids within an update and known, submitted job ids. We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd have to make sure this will actually simplify things because I also ran into a bifurcation in how the job IDs are handled in `BatchBuilder.create_job()`. We currently populate the spec with a job id before we've made any requests to the server. We need to know how many total jobs there are before we can figure out the job ids because the API for creating a new update requires reserving a block of job IDs which then returns the start id. This complexity is because we allow multiple updates to occur simultaneously to a batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215919856
Performance,latency,latency,"I think we should find a time to discuss this in person if the following explanation doesn't make sense. . Right now, for small batches, we send one REST request to the server to both create the batch and create the jobs. However, if we want one REST request for an update (ideal for the query service and low latency jobs?), we have to use relative job ids because (1) we don't know the absolute start index of the jobs until we've gotten the start id of the update back from the server and (2) the job dependencies can be a mix of known job ids that have already been previously submitted in a previous creation/update. The negative job IDs are a way to deal with a mix of relative ids within an update and known, submitted job ids. We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd have to make sure this will actually simplify things because I also ran into a bifurcation in how the job IDs are handled in `BatchBuilder.create_job()`. We currently populate the spec with a job id before we've made any requests to the server. We need to know how many total jobs there are before we can figure out the job ids because the API for creating a new update requires reserving a block of job IDs which then returns the start id. This complexity is because we allow multiple updates to occur simultaneously to a batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215919856
Usability,simpl,simplify,"I think we should find a time to discuss this in person if the following explanation doesn't make sense. . Right now, for small batches, we send one REST request to the server to both create the batch and create the jobs. However, if we want one REST request for an update (ideal for the query service and low latency jobs?), we have to use relative job ids because (1) we don't know the absolute start index of the jobs until we've gotten the start id of the update back from the server and (2) the job dependencies can be a mix of known job ids that have already been previously submitted in a previous creation/update. The negative job IDs are a way to deal with a mix of relative ids within an update and known, submitted job ids. We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd have to make sure this will actually simplify things because I also ran into a bifurcation in how the job IDs are handled in `BatchBuilder.create_job()`. We currently populate the spec with a job id before we've made any requests to the server. We need to know how many total jobs there are before we can figure out the job ids because the API for creating a new update requires reserving a block of job IDs which then returns the start id. This complexity is because we allow multiple updates to occur simultaneously to a batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215919856
Deployability,update,updates,"This is indeed tricky, but I think there are other options here. For example, we could expand the spec so that a job specifies *either* a job_id or update_relative_job_id, and same for parents. This way the relative vs absolute job id is baked into the schema of the spec, and not inferred from the sign of the job id. Though similar in concept, I think that would be much less confusing. However,. > We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd like to try this first. I feel like if we get a really solid API and it has a couple of superfluous requests in some edge cases, we will be able to come up with good performance shortcuts that don't muddle the normal path. Since the Query Driver currently lives the full life of the batch and is likely to stay that way for a while, it will satisfy these conditions without making any extra requests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215951685
Performance,perform,performance,"This is indeed tricky, but I think there are other options here. For example, we could expand the spec so that a job specifies *either* a job_id or update_relative_job_id, and same for parents. This way the relative vs absolute job id is baked into the schema of the spec, and not inferred from the sign of the job id. Though similar in concept, I think that would be much less confusing. However,. > We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd like to try this first. I feel like if we get a really solid API and it has a couple of superfluous requests in some edge cases, we will be able to come up with good performance shortcuts that don't muddle the normal path. Since the Query Driver currently lives the full life of the batch and is likely to stay that way for a while, it will satisfy these conditions without making any extra requests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215951685
Usability,simpl,simplify,"This is indeed tricky, but I think there are other options here. For example, we could expand the spec so that a job specifies *either* a job_id or update_relative_job_id, and same for parents. This way the relative vs absolute job id is baked into the schema of the spec, and not inferred from the sign of the job id. Though similar in concept, I think that would be much less confusing. However,. > We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd like to try this first. I feel like if we get a really solid API and it has a couple of superfluous requests in some edge cases, we will be able to come up with good performance shortcuts that don't muddle the normal path. Since the Query Driver currently lives the full life of the batch and is likely to stay that way for a while, it will satisfy these conditions without making any extra requests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215951685
Availability,down,down,"I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488
Deployability,update,updates,"I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488
Performance,perform,performance,"I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488
Safety,avoid,avoid,"I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488
Usability,simpl,simplifying,"I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488
Availability,down,down,"> I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first. Can you elaborate more? I'm not sure which code paths you are referring to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494
Deployability,update,updates,"> I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first. Can you elaborate more? I'm not sure which code paths you are referring to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494
Performance,perform,performance,"> I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first. Can you elaborate more? I'm not sure which code paths you are referring to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494
Safety,avoid,avoid,"> I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first. Can you elaborate more? I'm not sure which code paths you are referring to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494
Usability,simpl,simplifying,"> I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first. Can you elaborate more? I'm not sure which code paths you are referring to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494
Availability,down,down,"> > I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.; > ; > Can you elaborate more? I'm not sure which code paths you are referring to. Mainly that the commit procedure branches on whether the start id is 1 and that we sometimes grab the update id from the batch token and sometimes from the update table. Not very different code paths but slightly different, which could lead to some confusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403
Deployability,update,updates,"> > I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.; > ; > Can you elaborate more? I'm not sure which code paths you are referring to. Mainly that the commit procedure branches on whether the start id is 1 and that we sometimes grab the update id from the batch token and sometimes from the update table. Not very different code paths but slightly different, which could lead to some confusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403
Performance,perform,performance,"> > I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.; > ; > Can you elaborate more? I'm not sure which code paths you are referring to. Mainly that the commit procedure branches on whether the start id is 1 and that we sometimes grab the update id from the batch token and sometimes from the update table. Not very different code paths but slightly different, which could lead to some confusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403
Safety,avoid,avoid,"> > I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.; > ; > Can you elaborate more? I'm not sure which code paths you are referring to. Mainly that the commit procedure branches on whether the start id is 1 and that we sometimes grab the update id from the batch token and sometimes from the update table. Not very different code paths but slightly different, which could lead to some confusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403
Usability,simpl,simplifying,"> > I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.; > ; > Can you elaborate more? I'm not sure which code paths you are referring to. Mainly that the commit procedure branches on whether the start id is 1 and that we sometimes grab the update id from the batch token and sometimes from the update table. Not very different code paths but slightly different, which could lead to some confusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403
Energy Efficiency,allocate,allocated,"I'm not seeing the leak. `MemoryBuffer.clear` only zeroes the `pos` and `end` variables, and all the allocated memory is in the java heap. If anything, maybe you want to do `cb.assign(lazyBuffer, Code._null)`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12040#issuecomment-1191668397
Modifiability,variab,variables,"I'm not seeing the leak. `MemoryBuffer.clear` only zeroes the `pos` and `end` variables, and all the allocated memory is in the java heap. If anything, maybe you want to do `cb.assign(lazyBuffer, Code._null)`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12040#issuecomment-1191668397
Usability,clear,clear,"I'm not seeing the leak. `MemoryBuffer.clear` only zeroes the `pos` and `end` variables, and all the allocated memory is in the java heap. If anything, maybe you want to do `cb.assign(lazyBuffer, Code._null)`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12040#issuecomment-1191668397
Usability,feedback,feedback,Apologies for the delay in addressing the feedback. Had some urgent projects to attend to. Here are the changes you've suggested. Let me know if there's anything else I can do to help with this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12065#issuecomment-1243163758
Energy Efficiency,reduce,reduce,"I made some edits that I think helped to reduce the number of places that are aware of the notion of a default_region. It's really now just isolated to the `InstanceCollectionManager`, since that's the piece of code that's making the decision ""use the default region when the cluster is small"". I didn't quite like the pattern of retrieving a default from a `LocationMonitor` just to give it right back to the location monitor in the next line. I think this way the `LocationMonitor` API is much simpler, and we can actually remove its `default_location` method entirely as I believe it is no longer used. I can do that in a follow-up PR if you like this approach. One other thing is I wanted to articulate the distinction between the ""region CI needs its jobs in"" and the ""default region that batch will spin up workers in for small clusters"". While they are in practice the same, I found that treating them both as the ""default_region"" tied the logic around jobs for CI closely with internal Batch decisions and made it more confusing for me to reason about. I tried to separate out these two concepts so that in the future when jobs support region-specific scheduling it will be easier to excise the CI-specific code from the Batch scheduler. Another thing that I realized during this process is that Azure has regions and zones just like GCP, though they may differ slightly since we don't need to specify a zone for a VM and such. I am fine with using the term ""location"" to mean ""where we scheduled the VM, either zone or region depending on the cloud provider"", but I would also like to follow up with a sweep that makes this language more precise where possible. For example, the `possible_cloud_locations` function is really just `possible_cloud_regions`, and we could even go so far as mandating a `region` field in the global config instead of having `azure_location` and `gcp_region`, which are synonymous even though named differently. It also leads me to wonder why we only schedule in a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240
Integrability,depend,depending,"think helped to reduce the number of places that are aware of the notion of a default_region. It's really now just isolated to the `InstanceCollectionManager`, since that's the piece of code that's making the decision ""use the default region when the cluster is small"". I didn't quite like the pattern of retrieving a default from a `LocationMonitor` just to give it right back to the location monitor in the next line. I think this way the `LocationMonitor` API is much simpler, and we can actually remove its `default_location` method entirely as I believe it is no longer used. I can do that in a follow-up PR if you like this approach. One other thing is I wanted to articulate the distinction between the ""region CI needs its jobs in"" and the ""default region that batch will spin up workers in for small clusters"". While they are in practice the same, I found that treating them both as the ""default_region"" tied the logic around jobs for CI closely with internal Batch decisions and made it more confusing for me to reason about. I tried to separate out these two concepts so that in the future when jobs support region-specific scheduling it will be easier to excise the CI-specific code from the Batch scheduler. Another thing that I realized during this process is that Azure has regions and zones just like GCP, though they may differ slightly since we don't need to specify a zone for a VM and such. I am fine with using the term ""location"" to mean ""where we scheduled the VM, either zone or region depending on the cloud provider"", but I would also like to follow up with a sweep that makes this language more precise where possible. For example, the `possible_cloud_locations` function is really just `possible_cloud_regions`, and we could even go so far as mandating a `region` field in the global config instead of having `azure_location` and `gcp_region`, which are synonymous even though named differently. It also leads me to wonder why we only schedule in a single region in Azure?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240
Modifiability,config,config,"think helped to reduce the number of places that are aware of the notion of a default_region. It's really now just isolated to the `InstanceCollectionManager`, since that's the piece of code that's making the decision ""use the default region when the cluster is small"". I didn't quite like the pattern of retrieving a default from a `LocationMonitor` just to give it right back to the location monitor in the next line. I think this way the `LocationMonitor` API is much simpler, and we can actually remove its `default_location` method entirely as I believe it is no longer used. I can do that in a follow-up PR if you like this approach. One other thing is I wanted to articulate the distinction between the ""region CI needs its jobs in"" and the ""default region that batch will spin up workers in for small clusters"". While they are in practice the same, I found that treating them both as the ""default_region"" tied the logic around jobs for CI closely with internal Batch decisions and made it more confusing for me to reason about. I tried to separate out these two concepts so that in the future when jobs support region-specific scheduling it will be easier to excise the CI-specific code from the Batch scheduler. Another thing that I realized during this process is that Azure has regions and zones just like GCP, though they may differ slightly since we don't need to specify a zone for a VM and such. I am fine with using the term ""location"" to mean ""where we scheduled the VM, either zone or region depending on the cloud provider"", but I would also like to follow up with a sweep that makes this language more precise where possible. For example, the `possible_cloud_locations` function is really just `possible_cloud_regions`, and we could even go so far as mandating a `region` field in the global config instead of having `azure_location` and `gcp_region`, which are synonymous even though named differently. It also leads me to wonder why we only schedule in a single region in Azure?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240
Testability,log,logic,"I made some edits that I think helped to reduce the number of places that are aware of the notion of a default_region. It's really now just isolated to the `InstanceCollectionManager`, since that's the piece of code that's making the decision ""use the default region when the cluster is small"". I didn't quite like the pattern of retrieving a default from a `LocationMonitor` just to give it right back to the location monitor in the next line. I think this way the `LocationMonitor` API is much simpler, and we can actually remove its `default_location` method entirely as I believe it is no longer used. I can do that in a follow-up PR if you like this approach. One other thing is I wanted to articulate the distinction between the ""region CI needs its jobs in"" and the ""default region that batch will spin up workers in for small clusters"". While they are in practice the same, I found that treating them both as the ""default_region"" tied the logic around jobs for CI closely with internal Batch decisions and made it more confusing for me to reason about. I tried to separate out these two concepts so that in the future when jobs support region-specific scheduling it will be easier to excise the CI-specific code from the Batch scheduler. Another thing that I realized during this process is that Azure has regions and zones just like GCP, though they may differ slightly since we don't need to specify a zone for a VM and such. I am fine with using the term ""location"" to mean ""where we scheduled the VM, either zone or region depending on the cloud provider"", but I would also like to follow up with a sweep that makes this language more precise where possible. For example, the `possible_cloud_locations` function is really just `possible_cloud_regions`, and we could even go so far as mandating a `region` field in the global config instead of having `azure_location` and `gcp_region`, which are synonymous even though named differently. It also leads me to wonder why we only schedule in a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240
Usability,simpl,simpler,"I made some edits that I think helped to reduce the number of places that are aware of the notion of a default_region. It's really now just isolated to the `InstanceCollectionManager`, since that's the piece of code that's making the decision ""use the default region when the cluster is small"". I didn't quite like the pattern of retrieving a default from a `LocationMonitor` just to give it right back to the location monitor in the next line. I think this way the `LocationMonitor` API is much simpler, and we can actually remove its `default_location` method entirely as I believe it is no longer used. I can do that in a follow-up PR if you like this approach. One other thing is I wanted to articulate the distinction between the ""region CI needs its jobs in"" and the ""default region that batch will spin up workers in for small clusters"". While they are in practice the same, I found that treating them both as the ""default_region"" tied the logic around jobs for CI closely with internal Batch decisions and made it more confusing for me to reason about. I tried to separate out these two concepts so that in the future when jobs support region-specific scheduling it will be easier to excise the CI-specific code from the Batch scheduler. Another thing that I realized during this process is that Azure has regions and zones just like GCP, though they may differ slightly since we don't need to specify a zone for a VM and such. I am fine with using the term ""location"" to mean ""where we scheduled the VM, either zone or region depending on the cloud provider"", but I would also like to follow up with a sweep that makes this language more precise where possible. For example, the `possible_cloud_locations` function is really just `possible_cloud_regions`, and we could even go so far as mandating a `region` field in the global config instead of having `azure_location` and `gcp_region`, which are synonymous even though named differently. It also leads me to wonder why we only schedule in a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12078#issuecomment-1207095240
Usability,clear,clearer,"sure, seems clearer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12213#issuecomment-1254048690
Usability,simpl,simplification,Subquery simplification is in progress!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1270347717
Energy Efficiency,schedul,schedule,"I think maybe I'm overcomplicating the regions thing. Just not specifying regions clearly means you can schedule anywhere. `regions(None)` is confusing, but users should never do that directly. It will only happen when folks are programmatically generating jobs. People doing that are experts who will understand that `None` is just a stand in for ""any region"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1270359051
Usability,clear,clearly,"I think maybe I'm overcomplicating the regions thing. Just not specifying regions clearly means you can schedule anywhere. `regions(None)` is confusing, but users should never do that directly. It will only happen when folks are programmatically generating jobs. People doing that are experts who will understand that `None` is just a stand in for ""any region"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1270359051
Energy Efficiency,schedul,scheduler,"Can you take another look now?. I added two new fields to the jobs table to help with indexing and order bys. This should make the queries simpler and allow us to revert back to the old scheduler that Cotton wrote that was optimitzed. The regions_bits_rep is just a 0/1 for each region. So [us-east1, us-central1] could be ""1100000"". I also realized that I could aggregate the ready cores per user and then order them after unioning each user. I think this will perform better. From small tests, the autoscaler query seemed much better, but I'll want to do one last load test once you're okay with this approach.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1274632733
Performance,perform,perform,"Can you take another look now?. I added two new fields to the jobs table to help with indexing and order bys. This should make the queries simpler and allow us to revert back to the old scheduler that Cotton wrote that was optimitzed. The regions_bits_rep is just a 0/1 for each region. So [us-east1, us-central1] could be ""1100000"". I also realized that I could aggregate the ready cores per user and then order them after unioning each user. I think this will perform better. From small tests, the autoscaler query seemed much better, but I'll want to do one last load test once you're okay with this approach.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1274632733
Testability,test,tests,"Can you take another look now?. I added two new fields to the jobs table to help with indexing and order bys. This should make the queries simpler and allow us to revert back to the old scheduler that Cotton wrote that was optimitzed. The regions_bits_rep is just a 0/1 for each region. So [us-east1, us-central1] could be ""1100000"". I also realized that I could aggregate the ready cores per user and then order them after unioning each user. I think this will perform better. From small tests, the autoscaler query seemed much better, but I'll want to do one last load test once you're okay with this approach.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1274632733
Usability,simpl,simpler,"Can you take another look now?. I added two new fields to the jobs table to help with indexing and order bys. This should make the queries simpler and allow us to revert back to the old scheduler that Cotton wrote that was optimitzed. The regions_bits_rep is just a 0/1 for each region. So [us-east1, us-central1] could be ""1100000"". I also realized that I could aggregate the ready cores per user and then order them after unioning each user. I think this will perform better. From small tests, the autoscaler query seemed much better, but I'll want to do one last load test once you're okay with this approach.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1274632733
Usability,clear,clearer,What do you think of setting the auto increment to 0 for the region id? I think that will make the bit shift operations clearer and less vulnerable to mistakes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1277915822
Deployability,deploy,deployed,"Made this change backwards compatible. Note that I have not made any changes to worker.py in this PR anymore, so there's no danger of incompatibility. I tested the JAR from this PR against default and ran a simple hail query to see that it behaved as usual. Separately, I made #12246, dev deployed it, then ran this same JAR against my dev namespace to see that it added all worker jobs to the same batch as the driver job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12222#issuecomment-1262670715
Testability,test,tested,"Made this change backwards compatible. Note that I have not made any changes to worker.py in this PR anymore, so there's no danger of incompatibility. I tested the JAR from this PR against default and ran a simple hail query to see that it behaved as usual. Separately, I made #12246, dev deployed it, then ran this same JAR against my dev namespace to see that it added all worker jobs to the same batch as the driver job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12222#issuecomment-1262670715
Usability,simpl,simple,"Made this change backwards compatible. Note that I have not made any changes to worker.py in this PR anymore, so there's no danger of incompatibility. I tested the JAR from this PR against default and ran a simple hail query to see that it behaved as usual. Separately, I made #12246, dev deployed it, then ran this same JAR against my dev namespace to see that it added all worker jobs to the same batch as the driver job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12222#issuecomment-1262670715
Usability,guid,guidance,Waiting for guidance from htsjdk team on how to replace the deprecated method `Allele.acceptableAlleleBases`. https://github.com/samtools/htsjdk/issues/1623,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12229#issuecomment-1266123134
Safety,avoid,avoid,"OK, here's my understanding of prometheus and our use of it after chatting with @daniel-goldstein :; 1. The Python client treats Summary as just a pair of a Gauge and Counter. It's only useful if you need to track the number of times you set/increments/decrement a value and the value itself. For example, the total number of visits to a web page. Some clients for other languages treat Summaries as histogram-like things, but the Python client does not.; 2. A Histogram, with a domain-relevant array of buckets, is the right tool for visualizing a distribution of values. In this PR, we treat percent-of-cores-in-use-on-instance as a Histogram. This should let us see the distribution of instances according to what percent of the cores are revenue-generating versus not.; 3. A Gauge is the right tool for visualizing any other value. In this PR, we use a gauge to measure the total jobs, the used cores, the total free cores, the total cores, the total cost per hour (ignoring disk), and the total revenue per hour (ignoring disk). ; 4. It is important to use `remove` for metrics whose label set changes over time. For example, if all the jobs owned by a particular user finish, then the metrics labelled with that user ought to become `0`. The database query will elide such records; therefore, it is important to `remove` such labels from the `USER_CORES` and `USER_JOBS`. We prefer `remove` to `clear` so as to avoid the case where prometheus collects metrics in between a call to `clear` and a call to `set` which restores the value for a still valid label.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12253#issuecomment-1273927865
Usability,clear,clear,"OK, here's my understanding of prometheus and our use of it after chatting with @daniel-goldstein :; 1. The Python client treats Summary as just a pair of a Gauge and Counter. It's only useful if you need to track the number of times you set/increments/decrement a value and the value itself. For example, the total number of visits to a web page. Some clients for other languages treat Summaries as histogram-like things, but the Python client does not.; 2. A Histogram, with a domain-relevant array of buckets, is the right tool for visualizing a distribution of values. In this PR, we treat percent-of-cores-in-use-on-instance as a Histogram. This should let us see the distribution of instances according to what percent of the cores are revenue-generating versus not.; 3. A Gauge is the right tool for visualizing any other value. In this PR, we use a gauge to measure the total jobs, the used cores, the total free cores, the total cores, the total cost per hour (ignoring disk), and the total revenue per hour (ignoring disk). ; 4. It is important to use `remove` for metrics whose label set changes over time. For example, if all the jobs owned by a particular user finish, then the metrics labelled with that user ought to become `0`. The database query will elide such records; therefore, it is important to `remove` such labels from the `USER_CORES` and `USER_JOBS`. We prefer `remove` to `clear` so as to avoid the case where prometheus collects metrics in between a call to `clear` and a call to `set` which restores the value for a still valid label.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12253#issuecomment-1273927865
Deployability,deploy,deploy,"Are you working with another branch at the same time in dev? If not, I feel like `hailctl dev deploy -b jigold:region-job-queue-fast-ci -s test_batch,test_hailtop_batch -e deploy_batch` would be a faster feedback loop and not take up a spot in the PR queue",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12263#issuecomment-1267535507
Performance,queue,queue-fast-ci,"Are you working with another branch at the same time in dev? If not, I feel like `hailctl dev deploy -b jigold:region-job-queue-fast-ci -s test_batch,test_hailtop_batch -e deploy_batch` would be a faster feedback loop and not take up a spot in the PR queue",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12263#issuecomment-1267535507
Usability,feedback,feedback,"Are you working with another branch at the same time in dev? If not, I feel like `hailctl dev deploy -b jigold:region-job-queue-fast-ci -s test_batch,test_hailtop_batch -e deploy_batch` would be a faster feedback loop and not take up a spot in the PR queue",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12263#issuecomment-1267535507
Usability,clear,cleared,This should now be cleared to merge. I might need to go dismiss all of these alerts on `main` once that merges the first time. Let's leave it on for a bit and assess how much of a headache it is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12269#issuecomment-1272065133
Usability,simpl,simple,"I need help with the emit rule. I have what I believe to be an [XY problem](https://en.wikipedia.org/wiki/XY_problem). . The X: Implement maximal independent set in generated code.; The Y: Compile an IR as a function such that it can be passed _in generated code_ to a helper method akin to [this current implementation](https://github.com/chrisvittal/hail/blob/b286ba4a1463a81ec157e6add6d6d56c00de1138/hail/src/main/scala/is/hail/utils/Graph.scala#L50) of maximal independent set. At this point it becomes a simple method call that takes an `UnsafeIndexedSeq`, unpacks it to an array of tuples, and calls one of the other versions of maximal independent set (returning an array) that is then converted so it can be the return type of this `emitI` match arm. Thoughts, advice?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12295#issuecomment-1272085468
Availability,error,error,"Also when there is a binding outside the tiebreaker used in the tiebreaker it fails in python. The error is very inscrutable, but it simply does not work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12295#issuecomment-1431843216
Usability,simpl,simply,"Also when there is a binding outside the tiebreaker used in the tiebreaker it fails in python. The error is very inscrutable, but it simply does not work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12295#issuecomment-1431843216
Usability,simpl,simpler,This need substantial work. I will create a new PR which revamps import_matrix_table to be simpler and to correctly handle glob patterns.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12299#issuecomment-1304287294
Testability,test,testing,"Re: testing, I wanted to wait on the spawned batch and ensure that it passed, but I had trouble doing that because it looks like the new rich progress bars are printed to stdout so I can't make use of json output and `jq`. Can we print the progress bars to stderr instead?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324141580
Usability,progress bar,progress bars,"Re: testing, I wanted to wait on the spawned batch and ensure that it passed, but I had trouble doing that because it looks like the new rich progress bars are printed to stdout so I can't make use of json output and `jq`. Can we print the progress bars to stderr instead?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324141580
Testability,test,testing,"> Re: testing, I wanted to wait on the spawned batch and ensure that it passed, but I had trouble doing that because it looks like the new rich progress bars are printed to stdout so I can't make use of json output and `jq`. Can we print the progress bars to stderr instead?. I think this is possible but I haven't looked into it. I'm opposed to stderr in Notebooks because I don't want it to appear with red background. I've recently realized that a lot of users find it concerning that we print so much red background text. You might also have a quiet mode? If you don't `wait` on the batch, do you still get a progress bar?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324145967
Usability,progress bar,progress bars,"> Re: testing, I wanted to wait on the spawned batch and ensure that it passed, but I had trouble doing that because it looks like the new rich progress bars are printed to stdout so I can't make use of json output and `jq`. Can we print the progress bars to stderr instead?. I think this is possible but I haven't looked into it. I'm opposed to stderr in Notebooks because I don't want it to appear with red background. I've recently realized that a lot of users find it concerning that we print so much red background text. You might also have a quiet mode? If you don't `wait` on the batch, do you still get a progress bar?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324145967
Usability,progress bar,progress bars,Ya the progress bars I mentioned were the file upload summary and the batch submission. I can try to make a quiet mode so it can cleanly print just the json. My intention was not to wait it in `submit.py` but to print out a json and then use `hailctl batch wait`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324152841
Usability,simpl,simple,"Hail doesn't have this built in. I think your best bet would be to generate a dot file and then run [DOT](https://en.wikipedia.org/wiki/DOT_(graph_description_language)) on it. You could maybe use [pydot](https://github.com/pydot/pydot). It should be as simple as grabbing the samples from your MT and creating a bunch of nodes, then grabbing the edges from the pc relate table and generating edges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12489#issuecomment-1335445679
Performance,cache,cache,"I understand what is going on now. The issue is that the temp directory is getting removed after the first batch.run(), but we're assuming those input files are still there. I think we should just clear the files and definitions cache after submission.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1426386771
Usability,clear,clear,"I understand what is going on now. The issue is that the temp directory is getting removed after the first batch.run(), but we're assuming those input files are still there. I think we should just clear the files and definitions cache after submission.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1426386771
Performance,cache,cache,"> I understand what is going on now. The issue is that the temp directory is getting removed after the first batch.run(), but we're assuming those input files are still there.; > ; > I think we should just clear the files and definitions cache after submission. Ah I see, ya let's not go that far. In that case we don't need any of the changes in this PR that deal with the python input files right? Other than to clear those dictionaries after submit?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1428760971
Usability,clear,clear,"> I understand what is going on now. The issue is that the temp directory is getting removed after the first batch.run(), but we're assuming those input files are still there.; > ; > I think we should just clear the files and definitions cache after submission. Ah I see, ya let's not go that far. In that case we don't need any of the changes in this PR that deal with the python input files right? Other than to clear those dictionaries after submit?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1428760971
Testability,stub,stub,"ugh, this PR is quite mislabeled. This was meant to be the secondary indexing stub -- the last commit here -- but was stacked on the keyed_intersection stuff (which has merged to main in another PR) and I didn't fix up the title. I'll rebase to make this clearer as an example for us to poke at indexing for seqr, and make the warning fix in another PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12539#issuecomment-1349720433
Usability,clear,clearer,"ugh, this PR is quite mislabeled. This was meant to be the secondary indexing stub -- the last commit here -- but was stacked on the keyed_intersection stuff (which has merged to main in another PR) and I didn't fix up the title. I'll rebase to make this clearer as an example for us to poke at indexing for seqr, and make the warning fix in another PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12539#issuecomment-1349720433
Availability,down,down,"There are a few small cosmetic changes in here that were a result of an updated pylint, but I put those in a separate commit to hopefully make that less confusing. There are a few follow-ups after this that I want to tackle; - simplifying the images for testing query (Dockerfile.hail-build, Dockerfile.hail-base, Dockerfile.hail-run). I think these are the only things that use `base_image` so we might be able to collapse a bunch of these; - updating to python 3.8 to avoid accidentally installing that in some of our images; - trying to produce eStargz images so that buildkit can lazily pull the base image when building new images. I hope that can bring some image build times down even further by not having to localize the installed pip dependencies when making changes to our python code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548
Deployability,update,updated,"There are a few small cosmetic changes in here that were a result of an updated pylint, but I put those in a separate commit to hopefully make that less confusing. There are a few follow-ups after this that I want to tackle; - simplifying the images for testing query (Dockerfile.hail-build, Dockerfile.hail-base, Dockerfile.hail-run). I think these are the only things that use `base_image` so we might be able to collapse a bunch of these; - updating to python 3.8 to avoid accidentally installing that in some of our images; - trying to produce eStargz images so that buildkit can lazily pull the base image when building new images. I hope that can bring some image build times down even further by not having to localize the installed pip dependencies when making changes to our python code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548
Integrability,depend,dependencies,"There are a few small cosmetic changes in here that were a result of an updated pylint, but I put those in a separate commit to hopefully make that less confusing. There are a few follow-ups after this that I want to tackle; - simplifying the images for testing query (Dockerfile.hail-build, Dockerfile.hail-base, Dockerfile.hail-run). I think these are the only things that use `base_image` so we might be able to collapse a bunch of these; - updating to python 3.8 to avoid accidentally installing that in some of our images; - trying to produce eStargz images so that buildkit can lazily pull the base image when building new images. I hope that can bring some image build times down even further by not having to localize the installed pip dependencies when making changes to our python code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548
Safety,avoid,avoid,"There are a few small cosmetic changes in here that were a result of an updated pylint, but I put those in a separate commit to hopefully make that less confusing. There are a few follow-ups after this that I want to tackle; - simplifying the images for testing query (Dockerfile.hail-build, Dockerfile.hail-base, Dockerfile.hail-run). I think these are the only things that use `base_image` so we might be able to collapse a bunch of these; - updating to python 3.8 to avoid accidentally installing that in some of our images; - trying to produce eStargz images so that buildkit can lazily pull the base image when building new images. I hope that can bring some image build times down even further by not having to localize the installed pip dependencies when making changes to our python code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548
Testability,test,testing,"There are a few small cosmetic changes in here that were a result of an updated pylint, but I put those in a separate commit to hopefully make that less confusing. There are a few follow-ups after this that I want to tackle; - simplifying the images for testing query (Dockerfile.hail-build, Dockerfile.hail-base, Dockerfile.hail-run). I think these are the only things that use `base_image` so we might be able to collapse a bunch of these; - updating to python 3.8 to avoid accidentally installing that in some of our images; - trying to produce eStargz images so that buildkit can lazily pull the base image when building new images. I hope that can bring some image build times down even further by not having to localize the installed pip dependencies when making changes to our python code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548
Usability,simpl,simplifying,"There are a few small cosmetic changes in here that were a result of an updated pylint, but I put those in a separate commit to hopefully make that less confusing. There are a few follow-ups after this that I want to tackle; - simplifying the images for testing query (Dockerfile.hail-build, Dockerfile.hail-base, Dockerfile.hail-run). I think these are the only things that use `base_image` so we might be able to collapse a bunch of these; - updating to python 3.8 to avoid accidentally installing that in some of our images; - trying to produce eStargz images so that buildkit can lazily pull the base image when building new images. I hope that can bring some image build times down even further by not having to localize the installed pip dependencies when making changes to our python code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548
Integrability,depend,depend,"> How do I use a single non-optional allowedOverlap to express this versus a TMP function that doesn't care about keying/sorting at all?. Ah, I see. I think the answer is: you can't. You would need another integer parameter to say ""I actually depend on this prefix of the key being sorted"". It seems like allowedOverlap and requiredSortedPrefix are completely independent. In the single key case (for simplicity), you may or may not care if keys are localized in one partition, and you may or may not care if they're sorted. I don't see any connection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12587#issuecomment-1407126299
Usability,simpl,simplicity,"> How do I use a single non-optional allowedOverlap to express this versus a TMP function that doesn't care about keying/sorting at all?. Ah, I see. I think the answer is: you can't. You would need another integer parameter to say ""I actually depend on this prefix of the key being sorted"". It seems like allowedOverlap and requiredSortedPrefix are completely independent. In the single key case (for simplicity), you may or may not care if keys are localized in one partition, and you may or may not care if they're sorted. I don't see any connection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12587#issuecomment-1407126299
Usability,clear,clearly,Comments addressed. Still gotta work on the verbiage to clearly explain the present of P_0.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12637#issuecomment-1411233669
Deployability,update,updates,"I want cancel_after_n_failures to be on a job group. The things a job group doesn't have which maybe it should is:; - callback; - attributes; - updates. I think updates should be on a batch and not part of a job group. An update can add jobs to multiple job groups. Otherwise, the batches table should only have static fields that apply to the entire batch. I think we can do callbacks and attributes on a job group. I added a PATCH endpoint to be able to update a job group's cancel_after_n_attributes as the hailtop.batch interface was going to automatically generate job groups without any configuration settings. As for the full text search, I think prefix searches are faster with full text search than with a regular index, but I could be wrong. We'd have to benchmark it. > If we made batches simpler, does that ease complexity and decrease code duplication? In particular, what if batches didn't contain jobs at all? Instead, a batch contains exactly one job group. That job group contains zero or more job groups. Job groups manage: resource aggregation, cancellation, etc. I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163
Integrability,interface,interface,"I want cancel_after_n_failures to be on a job group. The things a job group doesn't have which maybe it should is:; - callback; - attributes; - updates. I think updates should be on a batch and not part of a job group. An update can add jobs to multiple job groups. Otherwise, the batches table should only have static fields that apply to the entire batch. I think we can do callbacks and attributes on a job group. I added a PATCH endpoint to be able to update a job group's cancel_after_n_attributes as the hailtop.batch interface was going to automatically generate job groups without any configuration settings. As for the full text search, I think prefix searches are faster with full text search than with a regular index, but I could be wrong. We'd have to benchmark it. > If we made batches simpler, does that ease complexity and decrease code duplication? In particular, what if batches didn't contain jobs at all? Instead, a batch contains exactly one job group. That job group contains zero or more job groups. Job groups manage: resource aggregation, cancellation, etc. I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163
Modifiability,config,configuration,"I want cancel_after_n_failures to be on a job group. The things a job group doesn't have which maybe it should is:; - callback; - attributes; - updates. I think updates should be on a batch and not part of a job group. An update can add jobs to multiple job groups. Otherwise, the batches table should only have static fields that apply to the entire batch. I think we can do callbacks and attributes on a job group. I added a PATCH endpoint to be able to update a job group's cancel_after_n_attributes as the hailtop.batch interface was going to automatically generate job groups without any configuration settings. As for the full text search, I think prefix searches are faster with full text search than with a regular index, but I could be wrong. We'd have to benchmark it. > If we made batches simpler, does that ease complexity and decrease code duplication? In particular, what if batches didn't contain jobs at all? Instead, a batch contains exactly one job group. That job group contains zero or more job groups. Job groups manage: resource aggregation, cancellation, etc. I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163
Testability,benchmark,benchmark,"I want cancel_after_n_failures to be on a job group. The things a job group doesn't have which maybe it should is:; - callback; - attributes; - updates. I think updates should be on a batch and not part of a job group. An update can add jobs to multiple job groups. Otherwise, the batches table should only have static fields that apply to the entire batch. I think we can do callbacks and attributes on a job group. I added a PATCH endpoint to be able to update a job group's cancel_after_n_attributes as the hailtop.batch interface was going to automatically generate job groups without any configuration settings. As for the full text search, I think prefix searches are faster with full text search than with a regular index, but I could be wrong. We'd have to benchmark it. > If we made batches simpler, does that ease complexity and decrease code duplication? In particular, what if batches didn't contain jobs at all? Instead, a batch contains exactly one job group. That job group contains zero or more job groups. Job groups manage: resource aggregation, cancellation, etc. I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163
Usability,simpl,simpler,"I want cancel_after_n_failures to be on a job group. The things a job group doesn't have which maybe it should is:; - callback; - attributes; - updates. I think updates should be on a batch and not part of a job group. An update can add jobs to multiple job groups. Otherwise, the batches table should only have static fields that apply to the entire batch. I think we can do callbacks and attributes on a job group. I added a PATCH endpoint to be able to update a job group's cancel_after_n_attributes as the hailtop.batch interface was going to automatically generate job groups without any configuration settings. As for the full text search, I think prefix searches are faster with full text search than with a regular index, but I could be wrong. We'd have to benchmark it. > If we made batches simpler, does that ease complexity and decrease code duplication? In particular, what if batches didn't contain jobs at all? Instead, a batch contains exactly one job group. That job group contains zero or more job groups. Job groups manage: resource aggregation, cancellation, etc. I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163
Usability,feedback,feedback,Closing for now while I incorporate the feedback and make a working prototype.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450841695
Deployability,update,updates,"I agree cancel_after_n_failures should be on the group. That lets us match Spark semantics for QoB. 1. I agree, callback per group seems valuable.; 2. I agree attributes seem useful on groups.; 3. I agree, not much value in updates being at the job-group level. . Depends what you mean by prefix search, if that means `LIKE ""X%""`, I think that'll be quite fast on a normal index because you can jump directly to the first record whose prefix is X. I don't see how a fulltext index could do any better in that case. On the other hand, if you mean `LIKE ""%X""` then I agree, a normal index is useless and MySQL will do a table scan. In that case, I expect a fulltext index to be a substantial improvement. > I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1. Ah, that sounds good. So the plan would be to drop, for example, `aggregated_batch_resources_v2` and the other tables which are now replaced with the job group ones? That's exactly what I had in mind.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450945048
Usability,clear,clear,"I agree cancel_after_n_failures should be on the group. That lets us match Spark semantics for QoB. 1. I agree, callback per group seems valuable.; 2. I agree attributes seem useful on groups.; 3. I agree, not much value in updates being at the job-group level. . Depends what you mean by prefix search, if that means `LIKE ""X%""`, I think that'll be quite fast on a normal index because you can jump directly to the first record whose prefix is X. I don't see how a fulltext index could do any better in that case. On the other hand, if you mean `LIKE ""%X""` then I agree, a normal index is useless and MySQL will do a table scan. In that case, I expect a fulltext index to be a substantial improvement. > I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1. Ah, that sounds good. So the plan would be to drop, for example, `aggregated_batch_resources_v2` and the other tables which are now replaced with the job group ones? That's exactly what I had in mind.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450945048
Usability,guid,guidance,"NOTE:. There are a few places yet that I have not plumbed through the `HailStateManager`:. hail/src/main/scala/is/hail/rvd/RVDContext.scala; 42: private[this] val theRvb = new RegionValueBuilder(HailStateManager(Map.empty), r). hail/src/main/scala/is/hail/linalg/BlockMatrix.scala; 2136: val rvb = new RegionValueBuilder(HailStateManager(Map.empty), region). hail/src/main/scala/is/hail/expr/ir/MatrixValue.scala; 34: val rvb = new RegionValueBuilder(HailStateManager(Map.empty), prevGlobals.value.region). hail/src/main/scala/is/hail/expr/ir/agg/ApproxCDFStateManager.scala; 649: val rvb = new RegionValueBuilder(HailStateManager(Map.empty), r). hail/src/main/scala/is/hail/expr/ir/agg/LinearRegressionAggregator.scala; 34: val rvb = new RegionValueBuilder(HailStateManager(Map.empty), region). These places looked like either worthless effort to plumb through or difficult (the RVB in MatrixValue) so would appreciate some guidance on how to handle these use cases. ""Just do the additional plumbing"" is valid advice or a workaround if it's not desirable to add that plumbing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12699#issuecomment-1442055878
Usability,simpl,simple,"Tested deletion in a separate database. 0 rows is instantaneous. 100,000,000 rows is 10 seconds. So at worst, it's probably on order of 10 minutes to drop a table since the table I had was very simple with not a lot of columns. Will rebase and then get this merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12710#issuecomment-1446634047
Deployability,install,install,"Hi @alanmejiamaza ,. Just to be clear, you did `pip install hail` and then you opened a notebook and ran something like:; ```; import hail as hl; hl.init(); from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(); ht = hl.utils.range_table(1000); ht = mt.annotate(DP = hl.rand_unif(0, 100)); p = hl.plot.histogram(ht.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); show(p); ```; And the plot didn't appear? Did you get a message saying ""BokehJS 1.4.0 successfully loaded.""? What version of Jupyter are you using? What web browser are you using?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12717#issuecomment-1452599951
Integrability,message,message,"Hi @alanmejiamaza ,. Just to be clear, you did `pip install hail` and then you opened a notebook and ran something like:; ```; import hail as hl; hl.init(); from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(); ht = hl.utils.range_table(1000); ht = mt.annotate(DP = hl.rand_unif(0, 100)); p = hl.plot.histogram(ht.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); show(p); ```; And the plot didn't appear? Did you get a message saying ""BokehJS 1.4.0 successfully loaded.""? What version of Jupyter are you using? What web browser are you using?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12717#issuecomment-1452599951
Performance,load,loaded,"Hi @alanmejiamaza ,. Just to be clear, you did `pip install hail` and then you opened a notebook and ran something like:; ```; import hail as hl; hl.init(); from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(); ht = hl.utils.range_table(1000); ht = mt.annotate(DP = hl.rand_unif(0, 100)); p = hl.plot.histogram(ht.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); show(p); ```; And the plot didn't appear? Did you get a message saying ""BokehJS 1.4.0 successfully loaded.""? What version of Jupyter are you using? What web browser are you using?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12717#issuecomment-1452599951
Usability,clear,clear,"Hi @alanmejiamaza ,. Just to be clear, you did `pip install hail` and then you opened a notebook and ran something like:; ```; import hail as hl; hl.init(); from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(); ht = hl.utils.range_table(1000); ht = mt.annotate(DP = hl.rand_unif(0, 100)); p = hl.plot.histogram(ht.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); show(p); ```; And the plot didn't appear? Did you get a message saying ""BokehJS 1.4.0 successfully loaded.""? What version of Jupyter are you using? What web browser are you using?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12717#issuecomment-1452599951
Testability,test,testing,"So I think I'd appreciate a review on this. Would especially appreciate feedback about the question I wrote in the PR body as well as what to do about documentation and testing:. - We have pretty expansive FS testing, but not for these new shim functions. Should we convert some of our tests to use these functions instead of the FS objects themselves?; - We don't have `hailtop` docs, and afaik this is the first module outside of `hailtop.batch` that would be public. Where should its docs go?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12731#issuecomment-1499652542
Usability,feedback,feedback,"So I think I'd appreciate a review on this. Would especially appreciate feedback about the question I wrote in the PR body as well as what to do about documentation and testing:. - We have pretty expansive FS testing, but not for these new shim functions. Should we convert some of our tests to use these functions instead of the FS objects themselves?; - We don't have `hailtop` docs, and afaik this is the first module outside of `hailtop.batch` that would be public. Where should its docs go?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12731#issuecomment-1499652542
Usability,learn,learn,"With the binding changes I can't even find the regression code. We're totally swamped by the compiler. After the JIT is warm, the compiler is ~3s. The regression code is probably equally as fast as scikit learn on this tiny example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12793#issuecomment-1477072386
Availability,down,download,"In the short term, a fix which makes the UI usable again for these kinds of jobs is to check blob size, if it's over some threshold, show no log and instruct the user to download it. Then fix the download to use aiohttp's StreamResponse. We should maybe split this issue into a frontend-side and worker-side.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12852#issuecomment-1653991936
Testability,log,log,"In the short term, a fix which makes the UI usable again for these kinds of jobs is to check blob size, if it's over some threshold, show no log and instruct the user to download it. Then fix the download to use aiohttp's StreamResponse. We should maybe split this issue into a frontend-side and worker-side.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12852#issuecomment-1653991936
Usability,usab,usable,"In the short term, a fix which makes the UI usable again for these kinds of jobs is to check blob size, if it's over some threshold, show no log and instruct the user to download it. Then fix the download to use aiohttp's StreamResponse. We should maybe split this issue into a frontend-side and worker-side.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12852#issuecomment-1653991936
Usability,clear,clearer,"> Love this! I think this graph would be easier to reason about if you made all the bars the positive percentage runtime (i.e. runtime_i / mean(runtime)). Then add a gray line at 100% to indicate whether you're faster or slower. In this representation, the relative sizes of the bar are easy to see and meaningful (if bar 1 is half the size of bar 2, then option 1 is 2x as fast). I've taken a different approach. I've made the first argument the baseline against which all other runs are compared. the baseline result isn't plotted anymore, instead differences to the baseline are plotted. I think the plot is a lot clearer now. I also think negative values make more sense as we're comparing runs directly against a baseline. Let me know what you think!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12856#issuecomment-1503596842
Availability,robust,robust,"> Drive by comment here as I review PRs, but this is The Python Way, for better or worse. People call it ""easer to ask forgiveness than permission"". I don't personally have strong feelings one way or the other but we do this in many places in our codebase. Be that as it may, throwing if something does not exist is clearer and more robust that catching an error because some operation failed for what is arguably unexceptional, and just hoping that the `KeyError` that that operation threw is because the item did not exist, rather than another `KeyError` risen in any subsequent work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12864#issuecomment-1516489544
Usability,clear,clearer,"> Drive by comment here as I review PRs, but this is The Python Way, for better or worse. People call it ""easer to ask forgiveness than permission"". I don't personally have strong feelings one way or the other but we do this in many places in our codebase. Be that as it may, throwing if something does not exist is clearer and more robust that catching an error because some operation failed for what is arguably unexceptional, and just hoping that the `KeyError` that that operation threw is because the item did not exist, rather than another `KeyError` risen in any subsequent work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12864#issuecomment-1516489544
Usability,intuit,intuitive,"Hi @jmarshall, you're very right about the memoization aspect. I ended up just scrapping the data structure entirely in #12918 and doing this in a breadth-first way that seemed more intuitive anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12915#issuecomment-1518367237
Usability,simpl,simple,"Banning versions completely is a little tricky because the user can specify a JAR url directly instead of a version. JARs don't currently have a simple way to report pip version to the worker, though we could cook something up. We could also just delete the old JARs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1528389159
Availability,error,error,"> Banning versions completely is a little tricky because the user can specify a JAR url directly instead of a version. JARs don't currently have a simple way to report pip version to the worker, though we could cook something up. We could also just delete the old JARs. I feel like we should make a (cached) request to ensure that the JAR exists in the front-end upon job submission and return a 400 if it doesn't exist instead of waiting for the worker to error. It would:. - Allow us to remove support by deleting old jars; - Fail fast (I know I have accidentally messed up deploying a dev jar and had to wait until a worker came online to find out); - Avoid alerts from workers that can't find dev jars due to mistakes like I mention above",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664
Deployability,deploy,deploying,"> Banning versions completely is a little tricky because the user can specify a JAR url directly instead of a version. JARs don't currently have a simple way to report pip version to the worker, though we could cook something up. We could also just delete the old JARs. I feel like we should make a (cached) request to ensure that the JAR exists in the front-end upon job submission and return a 400 if it doesn't exist instead of waiting for the worker to error. It would:. - Allow us to remove support by deleting old jars; - Fail fast (I know I have accidentally messed up deploying a dev jar and had to wait until a worker came online to find out); - Avoid alerts from workers that can't find dev jars due to mistakes like I mention above",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664
Performance,cache,cached,"> Banning versions completely is a little tricky because the user can specify a JAR url directly instead of a version. JARs don't currently have a simple way to report pip version to the worker, though we could cook something up. We could also just delete the old JARs. I feel like we should make a (cached) request to ensure that the JAR exists in the front-end upon job submission and return a 400 if it doesn't exist instead of waiting for the worker to error. It would:. - Allow us to remove support by deleting old jars; - Fail fast (I know I have accidentally messed up deploying a dev jar and had to wait until a worker came online to find out); - Avoid alerts from workers that can't find dev jars due to mistakes like I mention above",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664
Usability,simpl,simple,"> Banning versions completely is a little tricky because the user can specify a JAR url directly instead of a version. JARs don't currently have a simple way to report pip version to the worker, though we could cook something up. We could also just delete the old JARs. I feel like we should make a (cached) request to ensure that the JAR exists in the front-end upon job submission and return a 400 if it doesn't exist instead of waiting for the worker to error. It would:. - Allow us to remove support by deleting old jars; - Fail fast (I know I have accidentally messed up deploying a dev jar and had to wait until a worker came online to find out); - Avoid alerts from workers that can't find dev jars due to mistakes like I mention above",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664
Availability,error,errors,"Yeah, I think I agree that we have a unique UUID. What I'm more skeptical of is: what if we throw an exception in a `write`? Do we clean up all the open resources? If not, that could totally leave a writer open that will conflict when we retry (even if we retried on a different VM!). ---. `retryTransientErrors` expects partition code to be safe to execute twice, but otherwise it's quite simple:. ```scala; def retryTransientErrors[T](f: => T): T = {; var delay = 0.1; var errors = 0; while (true) {; try {; return f; } catch {; case e: Exception =>; errors += 1; if (errors == 1 && isRetryOnceError(e)); return f; if (!isTransientError(e)); throw e; if (errors % 10 == 0); log.warn(s""encountered $errors transient errors, most recent one was $e""); }; delay = sleepAndBackoff(delay); }. throw new AssertionError(""unreachable""); }; ```; and this is the call site:; ```scala; val htc = new ServiceTaskContext(i); var result: Array[Byte] = null; var userError: HailException = null; try {; retryTransientErrors {; result = f(context, htc, theHailClassLoader, fs); }; } catch {; case err: HailException => userError = err; }; htc.close(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1531592331
Safety,safe,safe,"Yeah, I think I agree that we have a unique UUID. What I'm more skeptical of is: what if we throw an exception in a `write`? Do we clean up all the open resources? If not, that could totally leave a writer open that will conflict when we retry (even if we retried on a different VM!). ---. `retryTransientErrors` expects partition code to be safe to execute twice, but otherwise it's quite simple:. ```scala; def retryTransientErrors[T](f: => T): T = {; var delay = 0.1; var errors = 0; while (true) {; try {; return f; } catch {; case e: Exception =>; errors += 1; if (errors == 1 && isRetryOnceError(e)); return f; if (!isTransientError(e)); throw e; if (errors % 10 == 0); log.warn(s""encountered $errors transient errors, most recent one was $e""); }; delay = sleepAndBackoff(delay); }. throw new AssertionError(""unreachable""); }; ```; and this is the call site:; ```scala; val htc = new ServiceTaskContext(i); var result: Array[Byte] = null; var userError: HailException = null; try {; retryTransientErrors {; result = f(context, htc, theHailClassLoader, fs); }; } catch {; case err: HailException => userError = err; }; htc.close(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1531592331
Testability,log,log,"Yeah, I think I agree that we have a unique UUID. What I'm more skeptical of is: what if we throw an exception in a `write`? Do we clean up all the open resources? If not, that could totally leave a writer open that will conflict when we retry (even if we retried on a different VM!). ---. `retryTransientErrors` expects partition code to be safe to execute twice, but otherwise it's quite simple:. ```scala; def retryTransientErrors[T](f: => T): T = {; var delay = 0.1; var errors = 0; while (true) {; try {; return f; } catch {; case e: Exception =>; errors += 1; if (errors == 1 && isRetryOnceError(e)); return f; if (!isTransientError(e)); throw e; if (errors % 10 == 0); log.warn(s""encountered $errors transient errors, most recent one was $e""); }; delay = sleepAndBackoff(delay); }. throw new AssertionError(""unreachable""); }; ```; and this is the call site:; ```scala; val htc = new ServiceTaskContext(i); var result: Array[Byte] = null; var userError: HailException = null; try {; retryTransientErrors {; result = f(context, htc, theHailClassLoader, fs); }; } catch {; case err: HailException => userError = err; }; htc.close(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1531592331
Usability,simpl,simple,"Yeah, I think I agree that we have a unique UUID. What I'm more skeptical of is: what if we throw an exception in a `write`? Do we clean up all the open resources? If not, that could totally leave a writer open that will conflict when we retry (even if we retried on a different VM!). ---. `retryTransientErrors` expects partition code to be safe to execute twice, but otherwise it's quite simple:. ```scala; def retryTransientErrors[T](f: => T): T = {; var delay = 0.1; var errors = 0; while (true) {; try {; return f; } catch {; case e: Exception =>; errors += 1; if (errors == 1 && isRetryOnceError(e)); return f; if (!isTransientError(e)); throw e; if (errors % 10 == 0); log.warn(s""encountered $errors transient errors, most recent one was $e""); }; delay = sleepAndBackoff(delay); }. throw new AssertionError(""unreachable""); }; ```; and this is the call site:; ```scala; val htc = new ServiceTaskContext(i); var result: Array[Byte] = null; var userError: HailException = null; try {; retryTransientErrors {; result = f(context, htc, theHailClassLoader, fs); }; } catch {; case err: HailException => userError = err; }; htc.close(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1531592331
Availability,error,error,"I saw this again today in a fairly simple and isolated test. I'm beginning to wonder if this is just a new form of transient error. We pick 22 random characters from a 62 character alphabet. Odds of collision are minuscule:; ```; In [2]: (1/62)**22; Out[2]: 3.693029961058969e-40; ```; I verified `SecureRandom` with no constructor uses a randomly chosen seed. There's three exceptions there (all the same one). The deepest one came during a write. The next two came during closes. The outermost exception is from the `using` cleaning up. I'm not sure where the middle exception comes from, I can't imagine who would try to `close` the stream other than `using`. Regardless, it appears that the upload fails in some unrecoverable way. We're writing 2GiB in 256 8MiB chunks in this test, so we have more chances for something to go wrong. Maybe we just have to retry the entire partition when this happens?. https://ci.hail.is/batches/7404773/jobs/145; ```; starting test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt...; Exception:; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756
Energy Efficiency,adapt,adapted,upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 0; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 		at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:219); 		at java.io.DataOutputStream.write(DataOutputStream.java:107); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1(FSSuite.scala:329); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1$adapted(FSSuite.scala:323); 		at is.hail.utils.package$.using(package.scala:635); 		... 26 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756
Modifiability,adapt,adapted,upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 0; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 		at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:219); 		at java.io.DataOutputStream.write(DataOutputStream.java:107); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1(FSSuite.scala:329); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1$adapted(FSSuite.scala:323); 		at is.hail.utils.package$.using(package.scala:635); 		... 26 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756
Performance,concurren,concurrent,ail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 	at java.io.DataOutputStream.flush(DataOutputStream.java:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756
Safety,recover,recover," of transient error. We pick 22 random characters from a 62 character alphabet. Odds of collision are minuscule:; ```; In [2]: (1/62)**22; Out[2]: 3.693029961058969e-40; ```; I verified `SecureRandom` with no constructor uses a randomly chosen seed. There's three exceptions there (all the same one). The deepest one came during a write. The next two came during closes. The outermost exception is from the `using` cleaning up. I'm not sure where the middle exception comes from, I can't imagine who would try to `close` the stream other than `using`. Regardless, it appears that the upload fails in some unrecoverable way. We're writing 2GiB in 256 8MiB chunks in this test, so we have more chances for something to go wrong. Maybe we just have to retry the entire partition when this happens?. https://ci.hail.is/batches/7404773/jobs/145; ```; starting test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt...; Exception:; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756
Security,access,access,https://ci.hail.is/batches/7404773/jobs/145; ```; starting test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt...; Exception:; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297);,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756
Testability,test,test,"I saw this again today in a fairly simple and isolated test. I'm beginning to wonder if this is just a new form of transient error. We pick 22 random characters from a 62 character alphabet. Odds of collision are minuscule:; ```; In [2]: (1/62)**22; Out[2]: 3.693029961058969e-40; ```; I verified `SecureRandom` with no constructor uses a randomly chosen seed. There's three exceptions there (all the same one). The deepest one came during a write. The next two came during closes. The outermost exception is from the `using` cleaning up. I'm not sure where the middle exception comes from, I can't imagine who would try to `close` the stream other than `using`. Regardless, it appears that the upload fails in some unrecoverable way. We're writing 2GiB in 256 8MiB chunks in this test, so we have more chances for something to go wrong. Maybe we just have to retry the entire partition when this happens?. https://ci.hail.is/batches/7404773/jobs/145; ```; starting test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt...; Exception:; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756
Usability,simpl,simple,"I saw this again today in a fairly simple and isolated test. I'm beginning to wonder if this is just a new form of transient error. We pick 22 random characters from a 62 character alphabet. Odds of collision are minuscule:; ```; In [2]: (1/62)**22; Out[2]: 3.693029961058969e-40; ```; I verified `SecureRandom` with no constructor uses a randomly chosen seed. There's three exceptions there (all the same one). The deepest one came during a write. The next two came during closes. The outermost exception is from the `using` cleaning up. I'm not sure where the middle exception comes from, I can't imagine who would try to `close` the stream other than `using`. Regardless, it appears that the upload fails in some unrecoverable way. We're writing 2GiB in 256 8MiB chunks in this test, so we have more chances for something to go wrong. Maybe we just have to retry the entire partition when this happens?. https://ci.hail.is/batches/7404773/jobs/145; ```; starting test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt...; Exception:; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756
Availability,error,errors,"Hi @danking, thanks for this. On the topic of asserts, there are really two interacting issues:. 1. Asserts are intended to ensure invariants, i.e. conditions that should always be true. In correct code, assertions should never raise so disabling them should have no consequences at runtime. In practice, however, they are often casually used to catch value errors, which can be expected to occur if a user-facing method receives bad/nonsensical inputs (e.g. here: https://github.com/hail-is/hail/blob/1940547d35ddddb084ad52684e36153c1e03a331/hail/python/hailtop/hailctl/dataproc/diagnose.py#L62); 2. Python's language design allows anyone calling your code to disable asserts for optimization purposes, because disabling asserts should never change the semantics of the program. Putting these two features together, you can arrive at a situation where a user thinks they're turning off asserts (which should never raise anyway) and instead stops catching value errors (whose absence can never be guaranteed). All that said, if the final answer is: ""if you invoke `-O` you deserve what's coming"", I'm happy to drop it :). Thanks for taking a look at the example. If I understand you correctly, it sounds like I passed the wrong inputs to the function, in which case it might be clearer to raise a ValueError instead of an AssertionError in the end. On a closer look, it seems like most of the instances of `assert(x, y)` are actually in scala code-- my mistake. Thanks again for looking into this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12952#issuecomment-1531675665
Performance,optimiz,optimization,"Hi @danking, thanks for this. On the topic of asserts, there are really two interacting issues:. 1. Asserts are intended to ensure invariants, i.e. conditions that should always be true. In correct code, assertions should never raise so disabling them should have no consequences at runtime. In practice, however, they are often casually used to catch value errors, which can be expected to occur if a user-facing method receives bad/nonsensical inputs (e.g. here: https://github.com/hail-is/hail/blob/1940547d35ddddb084ad52684e36153c1e03a331/hail/python/hailtop/hailctl/dataproc/diagnose.py#L62); 2. Python's language design allows anyone calling your code to disable asserts for optimization purposes, because disabling asserts should never change the semantics of the program. Putting these two features together, you can arrive at a situation where a user thinks they're turning off asserts (which should never raise anyway) and instead stops catching value errors (whose absence can never be guaranteed). All that said, if the final answer is: ""if you invoke `-O` you deserve what's coming"", I'm happy to drop it :). Thanks for taking a look at the example. If I understand you correctly, it sounds like I passed the wrong inputs to the function, in which case it might be clearer to raise a ValueError instead of an AssertionError in the end. On a closer look, it seems like most of the instances of `assert(x, y)` are actually in scala code-- my mistake. Thanks again for looking into this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12952#issuecomment-1531675665
Testability,assert,asserts,"Hi @danking, thanks for this. On the topic of asserts, there are really two interacting issues:. 1. Asserts are intended to ensure invariants, i.e. conditions that should always be true. In correct code, assertions should never raise so disabling them should have no consequences at runtime. In practice, however, they are often casually used to catch value errors, which can be expected to occur if a user-facing method receives bad/nonsensical inputs (e.g. here: https://github.com/hail-is/hail/blob/1940547d35ddddb084ad52684e36153c1e03a331/hail/python/hailtop/hailctl/dataproc/diagnose.py#L62); 2. Python's language design allows anyone calling your code to disable asserts for optimization purposes, because disabling asserts should never change the semantics of the program. Putting these two features together, you can arrive at a situation where a user thinks they're turning off asserts (which should never raise anyway) and instead stops catching value errors (whose absence can never be guaranteed). All that said, if the final answer is: ""if you invoke `-O` you deserve what's coming"", I'm happy to drop it :). Thanks for taking a look at the example. If I understand you correctly, it sounds like I passed the wrong inputs to the function, in which case it might be clearer to raise a ValueError instead of an AssertionError in the end. On a closer look, it seems like most of the instances of `assert(x, y)` are actually in scala code-- my mistake. Thanks again for looking into this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12952#issuecomment-1531675665
Usability,clear,clearer,"Hi @danking, thanks for this. On the topic of asserts, there are really two interacting issues:. 1. Asserts are intended to ensure invariants, i.e. conditions that should always be true. In correct code, assertions should never raise so disabling them should have no consequences at runtime. In practice, however, they are often casually used to catch value errors, which can be expected to occur if a user-facing method receives bad/nonsensical inputs (e.g. here: https://github.com/hail-is/hail/blob/1940547d35ddddb084ad52684e36153c1e03a331/hail/python/hailtop/hailctl/dataproc/diagnose.py#L62); 2. Python's language design allows anyone calling your code to disable asserts for optimization purposes, because disabling asserts should never change the semantics of the program. Putting these two features together, you can arrive at a situation where a user thinks they're turning off asserts (which should never raise anyway) and instead stops catching value errors (whose absence can never be guaranteed). All that said, if the final answer is: ""if you invoke `-O` you deserve what's coming"", I'm happy to drop it :). Thanks for taking a look at the example. If I understand you correctly, it sounds like I passed the wrong inputs to the function, in which case it might be clearer to raise a ValueError instead of an AssertionError in the end. On a closer look, it seems like most of the instances of `assert(x, y)` are actually in scala code-- my mistake. Thanks again for looking into this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/12952#issuecomment-1531675665
Usability,simpl,simple,"> Is it worth making a separate PR that explicitly manages the gcsfuse processes PIDs? If yes, can you do that?. Ya I can do this. Should we try to just keep it simple in this PR then? Like just what you had originally where we return after any calls to unmount fail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12975#issuecomment-1533445556
Modifiability,layers,layers,"Heh. At least in my version of Docker, those are implicitly relative to the root not the WORKDIR:; ```; (base) dking@wm28c-761 /tmp % cat Dockerfile ; FROM ubuntu:20.04; WORKDIR /foo/bar; VOLUME baz; (base) dking@wm28c-761 /tmp % docker build -t foo . ; [+] Building 0.1s (6/6) FINISHED ; => [internal] load build definition from Dockerfile 0.0s; => => transferring dockerfile: 34B 0.0s; => [internal] load .dockerignore 0.0s; => => transferring context: 2B 0.0s; => [internal] load metadata for docker.io/library/ubuntu:20.04 0.0s; => [1/2] FROM docker.io/library/ubuntu:20.04 0.0s; => CACHED [2/2] WORKDIR /foo/bar 0.0s; => exporting to image 0.0s; => => exporting layers 0.0s; => => writing image sha256:217748640e5c53f72b8de9917010e5742fb8bef99a37dcb13ec59a903cb5834c 0.0s; => => naming to docker.io/library/foo 0.0s. Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them; (base) dking@wm28c-761 /tmp % docker run foo /bin/sh -c 'pwd && ls -l . && ls -l / && ls -l /baz'; /foo/bar; total 0; total 56; drwxr-xr-x 2 root root 4096 May 9 15:06 baz; lrwxrwxrwx 1 root root 7 Oct 19 2022 bin -> usr/bin; drwxr-xr-x 2 root root 4096 Apr 15 2020 boot; drwxr-xr-x 5 root root 340 May 9 15:06 dev; drwxr-xr-x 1 root root 4096 May 9 15:06 etc; drwxr-xr-x 3 root root 4096 May 9 15:01 foo; drwxr-xr-x 2 root root 4096 Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 19 2022 lib -> usr/lib; drwxr-xr-x 2 root root 4096 Oct 19 2022 media; drwxr-xr-x 2 root root 4096 Oct 19 2022 mnt; drwxr-xr-x 2 root root 4096 Oct 19 2022 opt; dr-xr-xr-x 238 root root 0 May 9 15:06 proc; drwx------ 2 root root 4096 Oct 19 2022 root; drwxr-xr-x 5 root root 4096 Oct 19 2022 run; lrwxrwxrwx 1 root root 8 Oct 19 2022 sbin -> usr/sbin; drwxr-xr-x 2 root root 4096 Oct 19 2022 srv; dr-xr-xr-x 13 root root 0 May 9 15:06 sys; drwxrwxrwt 2 root root 4096 Oct 19 2022 tmp; drwxr-xr-x 10 root root 4096 Oct 19 2022 usr; drwxr-xr-x 11 root root 4096 Oct 19 2022 var; total 0; (base) dki",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989
Performance,load,load,"Heh. At least in my version of Docker, those are implicitly relative to the root not the WORKDIR:; ```; (base) dking@wm28c-761 /tmp % cat Dockerfile ; FROM ubuntu:20.04; WORKDIR /foo/bar; VOLUME baz; (base) dking@wm28c-761 /tmp % docker build -t foo . ; [+] Building 0.1s (6/6) FINISHED ; => [internal] load build definition from Dockerfile 0.0s; => => transferring dockerfile: 34B 0.0s; => [internal] load .dockerignore 0.0s; => => transferring context: 2B 0.0s; => [internal] load metadata for docker.io/library/ubuntu:20.04 0.0s; => [1/2] FROM docker.io/library/ubuntu:20.04 0.0s; => CACHED [2/2] WORKDIR /foo/bar 0.0s; => exporting to image 0.0s; => => exporting layers 0.0s; => => writing image sha256:217748640e5c53f72b8de9917010e5742fb8bef99a37dcb13ec59a903cb5834c 0.0s; => => naming to docker.io/library/foo 0.0s. Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them; (base) dking@wm28c-761 /tmp % docker run foo /bin/sh -c 'pwd && ls -l . && ls -l / && ls -l /baz'; /foo/bar; total 0; total 56; drwxr-xr-x 2 root root 4096 May 9 15:06 baz; lrwxrwxrwx 1 root root 7 Oct 19 2022 bin -> usr/bin; drwxr-xr-x 2 root root 4096 Apr 15 2020 boot; drwxr-xr-x 5 root root 340 May 9 15:06 dev; drwxr-xr-x 1 root root 4096 May 9 15:06 etc; drwxr-xr-x 3 root root 4096 May 9 15:01 foo; drwxr-xr-x 2 root root 4096 Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 19 2022 lib -> usr/lib; drwxr-xr-x 2 root root 4096 Oct 19 2022 media; drwxr-xr-x 2 root root 4096 Oct 19 2022 mnt; drwxr-xr-x 2 root root 4096 Oct 19 2022 opt; dr-xr-xr-x 238 root root 0 May 9 15:06 proc; drwx------ 2 root root 4096 Oct 19 2022 root; drwxr-xr-x 5 root root 4096 Oct 19 2022 run; lrwxrwxrwx 1 root root 8 Oct 19 2022 sbin -> usr/sbin; drwxr-xr-x 2 root root 4096 Oct 19 2022 srv; dr-xr-xr-x 13 root root 0 May 9 15:06 sys; drwxrwxrwt 2 root root 4096 Oct 19 2022 tmp; drwxr-xr-x 10 root root 4096 Oct 19 2022 usr; drwxr-xr-x 11 root root 4096 Oct 19 2022 var; total 0; (base) dki",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989
Testability,test,tests,"Heh. At least in my version of Docker, those are implicitly relative to the root not the WORKDIR:; ```; (base) dking@wm28c-761 /tmp % cat Dockerfile ; FROM ubuntu:20.04; WORKDIR /foo/bar; VOLUME baz; (base) dking@wm28c-761 /tmp % docker build -t foo . ; [+] Building 0.1s (6/6) FINISHED ; => [internal] load build definition from Dockerfile 0.0s; => => transferring dockerfile: 34B 0.0s; => [internal] load .dockerignore 0.0s; => => transferring context: 2B 0.0s; => [internal] load metadata for docker.io/library/ubuntu:20.04 0.0s; => [1/2] FROM docker.io/library/ubuntu:20.04 0.0s; => CACHED [2/2] WORKDIR /foo/bar 0.0s; => exporting to image 0.0s; => => exporting layers 0.0s; => => writing image sha256:217748640e5c53f72b8de9917010e5742fb8bef99a37dcb13ec59a903cb5834c 0.0s; => => naming to docker.io/library/foo 0.0s. Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them; (base) dking@wm28c-761 /tmp % docker run foo /bin/sh -c 'pwd && ls -l . && ls -l / && ls -l /baz'; /foo/bar; total 0; total 56; drwxr-xr-x 2 root root 4096 May 9 15:06 baz; lrwxrwxrwx 1 root root 7 Oct 19 2022 bin -> usr/bin; drwxr-xr-x 2 root root 4096 Apr 15 2020 boot; drwxr-xr-x 5 root root 340 May 9 15:06 dev; drwxr-xr-x 1 root root 4096 May 9 15:06 etc; drwxr-xr-x 3 root root 4096 May 9 15:01 foo; drwxr-xr-x 2 root root 4096 Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 19 2022 lib -> usr/lib; drwxr-xr-x 2 root root 4096 Oct 19 2022 media; drwxr-xr-x 2 root root 4096 Oct 19 2022 mnt; drwxr-xr-x 2 root root 4096 Oct 19 2022 opt; dr-xr-xr-x 238 root root 0 May 9 15:06 proc; drwx------ 2 root root 4096 Oct 19 2022 root; drwxr-xr-x 5 root root 4096 Oct 19 2022 run; lrwxrwxrwx 1 root root 8 Oct 19 2022 sbin -> usr/sbin; drwxr-xr-x 2 root root 4096 Oct 19 2022 srv; dr-xr-xr-x 13 root root 0 May 9 15:06 sys; drwxrwxrwt 2 root root 4096 Oct 19 2022 tmp; drwxr-xr-x 10 root root 4096 Oct 19 2022 usr; drwxr-xr-x 11 root root 4096 Oct 19 2022 var; total 0; (base) dki",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989
Usability,learn,learn,"Heh. At least in my version of Docker, those are implicitly relative to the root not the WORKDIR:; ```; (base) dking@wm28c-761 /tmp % cat Dockerfile ; FROM ubuntu:20.04; WORKDIR /foo/bar; VOLUME baz; (base) dking@wm28c-761 /tmp % docker build -t foo . ; [+] Building 0.1s (6/6) FINISHED ; => [internal] load build definition from Dockerfile 0.0s; => => transferring dockerfile: 34B 0.0s; => [internal] load .dockerignore 0.0s; => => transferring context: 2B 0.0s; => [internal] load metadata for docker.io/library/ubuntu:20.04 0.0s; => [1/2] FROM docker.io/library/ubuntu:20.04 0.0s; => CACHED [2/2] WORKDIR /foo/bar 0.0s; => exporting to image 0.0s; => => exporting layers 0.0s; => => writing image sha256:217748640e5c53f72b8de9917010e5742fb8bef99a37dcb13ec59a903cb5834c 0.0s; => => naming to docker.io/library/foo 0.0s. Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them; (base) dking@wm28c-761 /tmp % docker run foo /bin/sh -c 'pwd && ls -l . && ls -l / && ls -l /baz'; /foo/bar; total 0; total 56; drwxr-xr-x 2 root root 4096 May 9 15:06 baz; lrwxrwxrwx 1 root root 7 Oct 19 2022 bin -> usr/bin; drwxr-xr-x 2 root root 4096 Apr 15 2020 boot; drwxr-xr-x 5 root root 340 May 9 15:06 dev; drwxr-xr-x 1 root root 4096 May 9 15:06 etc; drwxr-xr-x 3 root root 4096 May 9 15:01 foo; drwxr-xr-x 2 root root 4096 Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 19 2022 lib -> usr/lib; drwxr-xr-x 2 root root 4096 Oct 19 2022 media; drwxr-xr-x 2 root root 4096 Oct 19 2022 mnt; drwxr-xr-x 2 root root 4096 Oct 19 2022 opt; dr-xr-xr-x 238 root root 0 May 9 15:06 proc; drwx------ 2 root root 4096 Oct 19 2022 root; drwxr-xr-x 5 root root 4096 Oct 19 2022 run; lrwxrwxrwx 1 root root 8 Oct 19 2022 sbin -> usr/sbin; drwxr-xr-x 2 root root 4096 Oct 19 2022 srv; dr-xr-xr-x 13 root root 0 May 9 15:06 sys; drwxrwxrwt 2 root root 4096 Oct 19 2022 tmp; drwxr-xr-x 10 root root 4096 Oct 19 2022 usr; drwxr-xr-x 11 root root 4096 Oct 19 2022 var; total 0; (base) dki",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989
Deployability,deploy,deploy,"This is not because we forgot to unfreeze CI, we just have simply never added the dockerhub images to azure automatically. The couple that are there now (only 107 and 112) must have been uploaded manually. Because there are some build.yaml steps that run on deploy that are specific to the broad GCP instance (like maybe making a release), non-hail-vdc instances don't run the whole build.yaml pipeline on deploy, but a subset that are specified through terraform (this is how AUS and MS could decide to only deploy a subset of our services e.g. not monitoring. We somewhat recently added a step (separate from the `deploy` step) called `mirror_hailgenetics_images` that was entirely intended so that other hail deployments (including ourselves on Azure!) could pick up the images that we released to dockerhub. I never added that steps to the Azure CI's config. I have done that now. Somehow I had foreseen this incident happening and when it actually did any prior on it disappeared from my brain entirely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1572657390
Energy Efficiency,monitor,monitoring,"This is not because we forgot to unfreeze CI, we just have simply never added the dockerhub images to azure automatically. The couple that are there now (only 107 and 112) must have been uploaded manually. Because there are some build.yaml steps that run on deploy that are specific to the broad GCP instance (like maybe making a release), non-hail-vdc instances don't run the whole build.yaml pipeline on deploy, but a subset that are specified through terraform (this is how AUS and MS could decide to only deploy a subset of our services e.g. not monitoring. We somewhat recently added a step (separate from the `deploy` step) called `mirror_hailgenetics_images` that was entirely intended so that other hail deployments (including ourselves on Azure!) could pick up the images that we released to dockerhub. I never added that steps to the Azure CI's config. I have done that now. Somehow I had foreseen this incident happening and when it actually did any prior on it disappeared from my brain entirely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1572657390
Modifiability,config,config,"This is not because we forgot to unfreeze CI, we just have simply never added the dockerhub images to azure automatically. The couple that are there now (only 107 and 112) must have been uploaded manually. Because there are some build.yaml steps that run on deploy that are specific to the broad GCP instance (like maybe making a release), non-hail-vdc instances don't run the whole build.yaml pipeline on deploy, but a subset that are specified through terraform (this is how AUS and MS could decide to only deploy a subset of our services e.g. not monitoring. We somewhat recently added a step (separate from the `deploy` step) called `mirror_hailgenetics_images` that was entirely intended so that other hail deployments (including ourselves on Azure!) could pick up the images that we released to dockerhub. I never added that steps to the Azure CI's config. I have done that now. Somehow I had foreseen this incident happening and when it actually did any prior on it disappeared from my brain entirely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1572657390
Usability,simpl,simply,"This is not because we forgot to unfreeze CI, we just have simply never added the dockerhub images to azure automatically. The couple that are there now (only 107 and 112) must have been uploaded manually. Because there are some build.yaml steps that run on deploy that are specific to the broad GCP instance (like maybe making a release), non-hail-vdc instances don't run the whole build.yaml pipeline on deploy, but a subset that are specified through terraform (this is how AUS and MS could decide to only deploy a subset of our services e.g. not monitoring. We somewhat recently added a step (separate from the `deploy` step) called `mirror_hailgenetics_images` that was entirely intended so that other hail deployments (including ourselves on Azure!) could pick up the images that we released to dockerhub. I never added that steps to the Azure CI's config. I have done that now. Somehow I had foreseen this incident happening and when it actually did any prior on it disappeared from my brain entirely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1572657390
Usability,clear,clearly,whoops this clearly broke batch somehow,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13053#issuecomment-1548722663
Usability,progress bar,progress bar,"I think we should provide more information than the progress bar, but I concede that all three panels shown above is probably too much information by default when you're in ipython or python. In contrast, `hailctl batch submit` should probably display lots of information by default with a `--silent` flag to shut it off.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13063#issuecomment-1572344856
Energy Efficiency,monitor,monitoring,"A cluster monitoring terminal command also seems useful since you might forget to start your batch with the detailed information. I think a question for us is why this shouldn't be in the web UI? I think there's a clear benefit to having information directly in the CLI when you're using ipython or python or submit, but if you're starting a new terminal window to monitor a running job, why not start a browser window?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13063#issuecomment-1572346942
Usability,clear,clear,"A cluster monitoring terminal command also seems useful since you might forget to start your batch with the detailed information. I think a question for us is why this shouldn't be in the web UI? I think there's a clear benefit to having information directly in the CLI when you're using ipython or python or submit, but if you're starting a new terminal window to monitor a running job, why not start a browser window?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13063#issuecomment-1572346942
Usability,user experience,user experience,"I think we shouldn't be doing this type of feature without proper planning and a vision for the overall user experience. Perhaps this sort of item should go on a formal road map and not as an ""issue""?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13063#issuecomment-1572357250
Security,authenticat,authenticates,"@danking So would be curious to get your thoughts. I was initially going to make this change such that instead of these activation tokens the batch worker authenticates with a cloud access token. I had a minor pause though because this means that other workers (even from other namespaces) could potentially impersonate each other, whereas they cannot in our current token system. Is that a concern to you? I suppose we are already pretty compromised if someone gets control of the batch worker's identity, considering the buckets that the batch worker has access to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13071#issuecomment-1551928272
Usability,pause,pause,"@danking So would be curious to get your thoughts. I was initially going to make this change such that instead of these activation tokens the batch worker authenticates with a cloud access token. I had a minor pause though because this means that other workers (even from other namespaces) could potentially impersonate each other, whereas they cannot in our current token system. Is that a concern to you? I suppose we are already pretty compromised if someone gets control of the batch worker's identity, considering the buckets that the batch worker has access to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13071#issuecomment-1551928272
Testability,test,tested,"So ya this is what was giving me pause so would appreciate your take. I threw WIP on so I can make sure it's properly tested before it merges. Here are my thoughts:; - I can manually exercise all the commands easily enough. Flexing all the options is harder, but I can compare the help for each command. The fact that these files are all lint-free and typecheck gives me confidence I did have before.; - I'm not worried about breaking dev stuff. Basically the only thing I am worried about breaking is the dataproc group.; - I was planning on running the test dataproc scripts prior to removing the WIP, which is something, but obviously does not test all the options. I think it would be nice to get a close review for dataproc start/submit (the biggest ones that people rely on the most) and everything else can be at whatever granularity you feel comfortable with.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1570824362
Usability,pause,pause,"So ya this is what was giving me pause so would appreciate your take. I threw WIP on so I can make sure it's properly tested before it merges. Here are my thoughts:; - I can manually exercise all the commands easily enough. Flexing all the options is harder, but I can compare the help for each command. The fact that these files are all lint-free and typecheck gives me confidence I did have before.; - I'm not worried about breaking dev stuff. Basically the only thing I am worried about breaking is the dataproc group.; - I was planning on running the test dataproc scripts prior to removing the WIP, which is something, but obviously does not test all the options. I think it would be nice to get a close review for dataproc start/submit (the biggest ones that people rely on the most) and everything else can be at whatever granularity you feel comfortable with.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1570824362
Usability,learn,learn,Neat! Where'd you learn about lateral joins? I had not heard about them until now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13175#issuecomment-1603239327
Availability,error,error,"Because that won't work at all, because the input file name expected by `gatk IndexFeatureFile` won't exist. But perhaps you meant to connect the unrelated filenames via something like. ```; …; bgzip -c {j.tsv_counts} > {j.counts['tsv.gz']}; …; ```. Reasons for not doing that would include:. 1. Filename extensions are significant to GATK, so I would not trust `SubCommand` to write the output file in the right format if the filename did not have the expected extension. (This could be ameliorated via `j.tsv_counts.set_extension`.). 2. Using bgzip with `-c … >` is not its natural mode of operation, so is more likely to encounter bugs than the more typical `bgzip filename` invocation. (For example, plain gzip burns the input filename into the compressed file's header, so the redirection version produces different results from the typical invocation for gzip; bgzip does not embed the filename but the change may have other effects. For example, bgzip's error checking (e.g. in disk full situations) may well be more complete in the typical invocation than when writing to standard output.). It is also less clear than the straightforward invocation, so using this would be a hack. 4. The resulting code is IMHO overall less clear than the original version in which the resource group models the relationship between all three filenames. If Hail Batch is a well-rounded orthogonal API, then that code ought to work too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13191#issuecomment-1599478181
Usability,clear,clear,"Because that won't work at all, because the input file name expected by `gatk IndexFeatureFile` won't exist. But perhaps you meant to connect the unrelated filenames via something like. ```; …; bgzip -c {j.tsv_counts} > {j.counts['tsv.gz']}; …; ```. Reasons for not doing that would include:. 1. Filename extensions are significant to GATK, so I would not trust `SubCommand` to write the output file in the right format if the filename did not have the expected extension. (This could be ameliorated via `j.tsv_counts.set_extension`.). 2. Using bgzip with `-c … >` is not its natural mode of operation, so is more likely to encounter bugs than the more typical `bgzip filename` invocation. (For example, plain gzip burns the input filename into the compressed file's header, so the redirection version produces different results from the typical invocation for gzip; bgzip does not embed the filename but the change may have other effects. For example, bgzip's error checking (e.g. in disk full situations) may well be more complete in the typical invocation than when writing to standard output.). It is also less clear than the straightforward invocation, so using this would be a hack. 4. The resulting code is IMHO overall less clear than the original version in which the resource group models the relationship between all three filenames. If Hail Batch is a well-rounded orthogonal API, then that code ought to work too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13191#issuecomment-1599478181
Modifiability,flexible,flexible,"cc @iris-garden: You might find this discussion interesting. > Is it kosher to use write_output on the individual items within a ResourceGroup like this?. Yes, that is fine. > The resulting code is IMHO overall less clear than the original version in which the resource group models the relationship between all three filenames. The resource group was intended for sets of files that should be **localized** together. The primary motivation was for PLINK files with `.bed, .bim, and .fam` extensions. This is not quite your use case. > If Hail Batch is a well-rounded orthogonal API, then that code ought to work too. It's a fair criticism that the ResourceGroup API isn't very flexible. We can think about ways to improve it. It's unclear from your code what exactly you want to have happen. Do you want to be able to declare not to localize the tsv only file? In this toy example, a string path is assumed to be `ResourceGroupFile(path, localize=True)`. ```python3; j = b.new_job(…). j.declare_resource_group(counts={; 'tsv': ResourceGroupFile('{root}.counts.tsv', localize=False),; 'tsv.gz': '{root}.counts.tsv.gz',; 'tsv.gz.tbi': '{root}.counts.tsv.gz.tbi',; }). j.command(f""""""; gatk SubCommand … --output {j.tsv_counts}; bgzip {j.tsv_counts}; gatk IndexFeatureFile --input {j.counts['tsv.gz']}; """"""). b.write_output(j.counts, output_dir_path); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13191#issuecomment-1599590271
Usability,clear,clear,"cc @iris-garden: You might find this discussion interesting. > Is it kosher to use write_output on the individual items within a ResourceGroup like this?. Yes, that is fine. > The resulting code is IMHO overall less clear than the original version in which the resource group models the relationship between all three filenames. The resource group was intended for sets of files that should be **localized** together. The primary motivation was for PLINK files with `.bed, .bim, and .fam` extensions. This is not quite your use case. > If Hail Batch is a well-rounded orthogonal API, then that code ought to work too. It's a fair criticism that the ResourceGroup API isn't very flexible. We can think about ways to improve it. It's unclear from your code what exactly you want to have happen. Do you want to be able to declare not to localize the tsv only file? In this toy example, a string path is assumed to be `ResourceGroupFile(path, localize=True)`. ```python3; j = b.new_job(…). j.declare_resource_group(counts={; 'tsv': ResourceGroupFile('{root}.counts.tsv', localize=False),; 'tsv.gz': '{root}.counts.tsv.gz',; 'tsv.gz.tbi': '{root}.counts.tsv.gz.tbi',; }). j.command(f""""""; gatk SubCommand … --output {j.tsv_counts}; bgzip {j.tsv_counts}; gatk IndexFeatureFile --input {j.counts['tsv.gz']}; """"""). b.write_output(j.counts, output_dir_path); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13191#issuecomment-1599590271
Usability,simpl,simpler,"For posterity, we decided to make the code / SQL queries simpler here trading off efficiency and speed with having easy to verify correctness and readability. Once the initial compaction is done, this should run pretty quickly and we can scale back how aggressively we run the compaction. We may have to change this approach in the future if there's lots of simultaneous users.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1621923414
Testability,test,testing,"@daniel-goldstein When you get a chance, I'd appreciate your high level opinion on this addition before I start thoroughly testing everything. Goal is to have something before the ATGU Welcome Workshop September 5th. I know the import locations are an issue in `initialize.py`. I mainly want feedback on whether the code passes the high level smell test and whether I've over designed this with too many checks and prompts or am forgetting something that would be nice to check for.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648236180
Usability,feedback,feedback,"@daniel-goldstein When you get a chance, I'd appreciate your high level opinion on this addition before I start thoroughly testing everything. Goal is to have something before the ATGU Welcome Workshop September 5th. I know the import locations are an issue in `initialize.py`. I mainly want feedback on whether the code passes the high level smell test and whether I've over designed this with too many checks and prompts or am forgetting something that would be nice to check for.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648236180
Deployability,configurat,configuration,"This is super cool! I am a big fan of the idea and the overall approach, particularly when it comes to setting up the tmp bucket and getting the permissions on it correct. Here's my high level thoughts. Sorry for the wall of text but I found these a little hard to articulate. ### Regarding number of prompts. I think this is my primary concern. There's a lot of great automation here, but it's a lot right off the bat. I think what this is aiming to do is make it quick and simple to start running batches and every time someone has to stop and ask someone a question as to how they should respond to some prompt that process gets longer and more complicated. I think it's worth considering what the first batch people should run might be and design for a minimal first experience. IMO, a temp bucket is an absolutely crucial piece of configuration before you can do anything interesting and configuring a temp bucket is something that `hailctl` can easily be very opinionated about. Container registry… I feel like there's harder questions there, and you can run a lot of cool batches before having to worry about provisioning your own. It's also not actually a part of the hailctl config (unless something has changed recently) so it feels a little unusual in this flow. I still think that it is helpful to set people up with an AR and keep them from footguns, but maybe that can go in a separate command that the initial init command points to once you're done? Something along the lines of ""if you get to the point where you need to upload custom container images, you can use hailctl to set up a registry""?. Another thing that gives me a little pause is the wording around google projects. I get that you need one to create a bucket, but I think we should just make sure to steer clear of the implication that you are ""selecting a GCP project to use for Hail Batch"", because that implies some link or ownership that isn't there. But I think there's a quick fix here: for a given resource that we",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012
Modifiability,config,configuration,"This is super cool! I am a big fan of the idea and the overall approach, particularly when it comes to setting up the tmp bucket and getting the permissions on it correct. Here's my high level thoughts. Sorry for the wall of text but I found these a little hard to articulate. ### Regarding number of prompts. I think this is my primary concern. There's a lot of great automation here, but it's a lot right off the bat. I think what this is aiming to do is make it quick and simple to start running batches and every time someone has to stop and ask someone a question as to how they should respond to some prompt that process gets longer and more complicated. I think it's worth considering what the first batch people should run might be and design for a minimal first experience. IMO, a temp bucket is an absolutely crucial piece of configuration before you can do anything interesting and configuring a temp bucket is something that `hailctl` can easily be very opinionated about. Container registry… I feel like there's harder questions there, and you can run a lot of cool batches before having to worry about provisioning your own. It's also not actually a part of the hailctl config (unless something has changed recently) so it feels a little unusual in this flow. I still think that it is helpful to set people up with an AR and keep them from footguns, but maybe that can go in a separate command that the initial init command points to once you're done? Something along the lines of ""if you get to the point where you need to upload custom container images, you can use hailctl to set up a registry""?. Another thing that gives me a little pause is the wording around google projects. I get that you need one to create a bucket, but I think we should just make sure to steer clear of the implication that you are ""selecting a GCP project to use for Hail Batch"", because that implies some link or ownership that isn't there. But I think there's a quick fix here: for a given resource that we",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012
Safety,avoid,avoid,"ittle unusual in this flow. I still think that it is helpful to set people up with an AR and keep them from footguns, but maybe that can go in a separate command that the initial init command points to once you're done? Something along the lines of ""if you get to the point where you need to upload custom container images, you can use hailctl to set up a registry""?. Another thing that gives me a little pause is the wording around google projects. I get that you need one to create a bucket, but I think we should just make sure to steer clear of the implication that you are ""selecting a GCP project to use for Hail Batch"", because that implies some link or ownership that isn't there. But I think there's a quick fix here: for a given resource that we *are* creating for hail use, like the temp bucket, ask for the name first and then ask which project it should be created in, using the projects listed in gcloud as choices with the option to write in your own. ### Regarding number of checks; I think it'd be good to avoid warnings when possible. From looking at this I see a pattern of; 1. Ask a leading question; 2. Emit a warning if the user selects the alternative option instead of the suggested option. I think I would prefer instead to ask a leading question and in the prompt explain why the alternative option might be undesirable. Then when they make a decision just move on. On a broader note, I think we should focus on having good documentation and linking to it over having perfectly thorough ; explanations in the CLI. At some point in an interactive setup if it gets longwinded I start spamming enter, but if it was quick and at the end it said something to the effect of: ""Your current configuration could result in excess cloud cost. See the documentation <here> about common pitfalls and how to avoid them"", I might decide to read through that FAQ with a more discerning eye. This is just my opinion though, I would be curious to see if other folks disagree regarding the UX.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012
Usability,simpl,simple,"This is super cool! I am a big fan of the idea and the overall approach, particularly when it comes to setting up the tmp bucket and getting the permissions on it correct. Here's my high level thoughts. Sorry for the wall of text but I found these a little hard to articulate. ### Regarding number of prompts. I think this is my primary concern. There's a lot of great automation here, but it's a lot right off the bat. I think what this is aiming to do is make it quick and simple to start running batches and every time someone has to stop and ask someone a question as to how they should respond to some prompt that process gets longer and more complicated. I think it's worth considering what the first batch people should run might be and design for a minimal first experience. IMO, a temp bucket is an absolutely crucial piece of configuration before you can do anything interesting and configuring a temp bucket is something that `hailctl` can easily be very opinionated about. Container registry… I feel like there's harder questions there, and you can run a lot of cool batches before having to worry about provisioning your own. It's also not actually a part of the hailctl config (unless something has changed recently) so it feels a little unusual in this flow. I still think that it is helpful to set people up with an AR and keep them from footguns, but maybe that can go in a separate command that the initial init command points to once you're done? Something along the lines of ""if you get to the point where you need to upload custom container images, you can use hailctl to set up a registry""?. Another thing that gives me a little pause is the wording around google projects. I get that you need one to create a bucket, but I think we should just make sure to steer clear of the implication that you are ""selecting a GCP project to use for Hail Batch"", because that implies some link or ownership that isn't there. But I think there's a quick fix here: for a given resource that we",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012
Deployability,pipeline,pipelines,"(and to be clear, I'm onboard with the idea of a single command which gets you from zero to simple batch pipelines; jury is out on the Artifact Registry. I think making sure that's configured correctly might be better upstreamed to Sam. Normal users might not have permission to enable/disable things like that.).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1662843157
Modifiability,config,configured,"(and to be clear, I'm onboard with the idea of a single command which gets you from zero to simple batch pipelines; jury is out on the Artifact Registry. I think making sure that's configured correctly might be better upstreamed to Sam. Normal users might not have permission to enable/disable things like that.).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1662843157
Usability,clear,clear,"(and to be clear, I'm onboard with the idea of a single command which gets you from zero to simple batch pipelines; jury is out on the Artifact Registry. I think making sure that's configured correctly might be better upstreamed to Sam. Normal users might not have permission to enable/disable things like that.).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1662843157
Availability,avail,available,"Hey @danking,; Thanks so much for the advice. Your team has been very helpful and responsive. - I made the adjustment to my `create_intervals` function; - Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. I tried your suggestion of writing the data to a table first, but my `cols` field doesn't contain the computed HWE values. These are contained within the `entries` field. Table description is below. I tried modifying the code to what is shown below but I'm still having the same issue. Also tried increasing the RAM to max available per CPU. One thing I noticed is the `mt_hwe_vals` variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'ancestry': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'filters': set<str>; 'variant_qc': struct {; gq_stats: struct {; mean: float64, ; stdev: float64, ; min: float64, ; max: float64; }, ; call_rate: float64, ; n_called: int64, ; n_not_called: int64, ; n_filtered: int64, ; n_het: int64, ; n_non_ref: int64, ; het_freq_hwe: float64, ; p_value_hwe: float64, ; p_value_excess_het: float64; }; 'info': struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>; }; 'a_index': int32; 'was_split': bool; ----------------------------------------; Entry fields:; 'hwe': struct {; het_freq_hwe: float64, ; p_value: float64; }; ----------------------------------------; Column key: ['ancestry']; Row key: ['locus', 'alleles']; ----------------------------------------; ```. ```python; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)). # T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492
Energy Efficiency,efficient,efficient,"Hey @danking,; Thanks so much for the advice. Your team has been very helpful and responsive. - I made the adjustment to my `create_intervals` function; - Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. I tried your suggestion of writing the data to a table first, but my `cols` field doesn't contain the computed HWE values. These are contained within the `entries` field. Table description is below. I tried modifying the code to what is shown below but I'm still having the same issue. Also tried increasing the RAM to max available per CPU. One thing I noticed is the `mt_hwe_vals` variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'ancestry': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'filters': set<str>; 'variant_qc': struct {; gq_stats: struct {; mean: float64, ; stdev: float64, ; min: float64, ; max: float64; }, ; call_rate: float64, ; n_called: int64, ; n_not_called: int64, ; n_filtered: int64, ; n_het: int64, ; n_non_ref: int64, ; het_freq_hwe: float64, ; p_value_hwe: float64, ; p_value_excess_het: float64; }; 'info': struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>; }; 'a_index': int32; 'was_split': bool; ----------------------------------------; Entry fields:; 'hwe': struct {; het_freq_hwe: float64, ; p_value: float64; }; ----------------------------------------; Column key: ['ancestry']; Row key: ['locus', 'alleles']; ----------------------------------------; ```. ```python; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)). # T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492
Modifiability,variab,variable,"Hey @danking,; Thanks so much for the advice. Your team has been very helpful and responsive. - I made the adjustment to my `create_intervals` function; - Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. I tried your suggestion of writing the data to a table first, but my `cols` field doesn't contain the computed HWE values. These are contained within the `entries` field. Table description is below. I tried modifying the code to what is shown below but I'm still having the same issue. Also tried increasing the RAM to max available per CPU. One thing I noticed is the `mt_hwe_vals` variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'ancestry': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'filters': set<str>; 'variant_qc': struct {; gq_stats: struct {; mean: float64, ; stdev: float64, ; min: float64, ; max: float64; }, ; call_rate: float64, ; n_called: int64, ; n_not_called: int64, ; n_filtered: int64, ; n_het: int64, ; n_non_ref: int64, ; het_freq_hwe: float64, ; p_value_hwe: float64, ; p_value_excess_het: float64; }; 'info': struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>; }; 'a_index': int32; 'was_split': bool; ----------------------------------------; Entry fields:; 'hwe': struct {; het_freq_hwe: float64, ; p_value: float64; }; ----------------------------------------; Column key: ['ancestry']; Row key: ['locus', 'alleles']; ----------------------------------------; ```. ```python; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)). # T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492
Usability,responsiv,responsive,"Hey @danking,; Thanks so much for the advice. Your team has been very helpful and responsive. - I made the adjustment to my `create_intervals` function; - Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. I tried your suggestion of writing the data to a table first, but my `cols` field doesn't contain the computed HWE values. These are contained within the `entries` field. Table description is below. I tried modifying the code to what is shown below but I'm still having the same issue. Also tried increasing the RAM to max available per CPU. One thing I noticed is the `mt_hwe_vals` variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'ancestry': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'filters': set<str>; 'variant_qc': struct {; gq_stats: struct {; mean: float64, ; stdev: float64, ; min: float64, ; max: float64; }, ; call_rate: float64, ; n_called: int64, ; n_not_called: int64, ; n_filtered: int64, ; n_het: int64, ; n_non_ref: int64, ; het_freq_hwe: float64, ; p_value_hwe: float64, ; p_value_excess_het: float64; }; 'info': struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>; }; 'a_index': int32; 'was_split': bool; ----------------------------------------; Entry fields:; 'hwe': struct {; het_freq_hwe: float64, ; p_value: float64; }; ----------------------------------------; Column key: ['ancestry']; Row key: ['locus', 'alleles']; ----------------------------------------; ```. ```python; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)). # T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492
Availability,reliab,reliable,"> Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. We take special care to ensure our system is as efficient as possible when reading or writing to this native format. So, it's partly a sociological thing. On the practical end of things, Hail's native formats (for Tables and Matrix Tables) are a partitioned binary format. The partitioned part means Hail can use many cores in parallel to process and write the dataset. The binary part means that Hail need not use unnecessarily large (in terms of bytes) representations of values. These three things together make writing the native formats use less time, use less memory, and be more reliable. ---. > One thing I noticed is the mt_hwe_vals variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. Yes, after you aggregate you get back an MT with a different column key. ---. The `entries` method converts your matrix table from a compact and efficient matrix into a ""long"" and inefficient table. I generally recommend avoiding it if you can. However, if you only have a handful of ancestries, I wouldn't expect this to be *that* bad. You can just write the MT itself:. ```python3; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)); mt_hwe_vals = mt_hwe_vals.select_rows().select_cols() # drop irrelevant row and column fields; mt_hwe_vals.write(bucket + '/hwe.ht'); ```. ---. > I tried modifying the code to what is shown below but I'm still having the same issue. Just to be clear it's the exact same error ""Container exited with a non-zero exit code 137. ""? This makes me think we have an issue with `entries`, because, even though it's not great, it shouldn't be blowing RAM here. Can you share the log file from your previous or next attempt?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636
Energy Efficiency,efficient,efficient,"> Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. We take special care to ensure our system is as efficient as possible when reading or writing to this native format. So, it's partly a sociological thing. On the practical end of things, Hail's native formats (for Tables and Matrix Tables) are a partitioned binary format. The partitioned part means Hail can use many cores in parallel to process and write the dataset. The binary part means that Hail need not use unnecessarily large (in terms of bytes) representations of values. These three things together make writing the native formats use less time, use less memory, and be more reliable. ---. > One thing I noticed is the mt_hwe_vals variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. Yes, after you aggregate you get back an MT with a different column key. ---. The `entries` method converts your matrix table from a compact and efficient matrix into a ""long"" and inefficient table. I generally recommend avoiding it if you can. However, if you only have a handful of ancestries, I wouldn't expect this to be *that* bad. You can just write the MT itself:. ```python3; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)); mt_hwe_vals = mt_hwe_vals.select_rows().select_cols() # drop irrelevant row and column fields; mt_hwe_vals.write(bucket + '/hwe.ht'); ```. ---. > I tried modifying the code to what is shown below but I'm still having the same issue. Just to be clear it's the exact same error ""Container exited with a non-zero exit code 137. ""? This makes me think we have an issue with `entries`, because, even though it's not great, it shouldn't be blowing RAM here. Can you share the log file from your previous or next attempt?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636
Modifiability,variab,variable,"> Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. We take special care to ensure our system is as efficient as possible when reading or writing to this native format. So, it's partly a sociological thing. On the practical end of things, Hail's native formats (for Tables and Matrix Tables) are a partitioned binary format. The partitioned part means Hail can use many cores in parallel to process and write the dataset. The binary part means that Hail need not use unnecessarily large (in terms of bytes) representations of values. These three things together make writing the native formats use less time, use less memory, and be more reliable. ---. > One thing I noticed is the mt_hwe_vals variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. Yes, after you aggregate you get back an MT with a different column key. ---. The `entries` method converts your matrix table from a compact and efficient matrix into a ""long"" and inefficient table. I generally recommend avoiding it if you can. However, if you only have a handful of ancestries, I wouldn't expect this to be *that* bad. You can just write the MT itself:. ```python3; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)); mt_hwe_vals = mt_hwe_vals.select_rows().select_cols() # drop irrelevant row and column fields; mt_hwe_vals.write(bucket + '/hwe.ht'); ```. ---. > I tried modifying the code to what is shown below but I'm still having the same issue. Just to be clear it's the exact same error ""Container exited with a non-zero exit code 137. ""? This makes me think we have an issue with `entries`, because, even though it's not great, it shouldn't be blowing RAM here. Can you share the log file from your previous or next attempt?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636
Safety,avoid,avoiding,"> Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. We take special care to ensure our system is as efficient as possible when reading or writing to this native format. So, it's partly a sociological thing. On the practical end of things, Hail's native formats (for Tables and Matrix Tables) are a partitioned binary format. The partitioned part means Hail can use many cores in parallel to process and write the dataset. The binary part means that Hail need not use unnecessarily large (in terms of bytes) representations of values. These three things together make writing the native formats use less time, use less memory, and be more reliable. ---. > One thing I noticed is the mt_hwe_vals variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. Yes, after you aggregate you get back an MT with a different column key. ---. The `entries` method converts your matrix table from a compact and efficient matrix into a ""long"" and inefficient table. I generally recommend avoiding it if you can. However, if you only have a handful of ancestries, I wouldn't expect this to be *that* bad. You can just write the MT itself:. ```python3; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)); mt_hwe_vals = mt_hwe_vals.select_rows().select_cols() # drop irrelevant row and column fields; mt_hwe_vals.write(bucket + '/hwe.ht'); ```. ---. > I tried modifying the code to what is shown below but I'm still having the same issue. Just to be clear it's the exact same error ""Container exited with a non-zero exit code 137. ""? This makes me think we have an issue with `entries`, because, even though it's not great, it shouldn't be blowing RAM here. Can you share the log file from your previous or next attempt?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636
Testability,log,log,"> Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. We take special care to ensure our system is as efficient as possible when reading or writing to this native format. So, it's partly a sociological thing. On the practical end of things, Hail's native formats (for Tables and Matrix Tables) are a partitioned binary format. The partitioned part means Hail can use many cores in parallel to process and write the dataset. The binary part means that Hail need not use unnecessarily large (in terms of bytes) representations of values. These three things together make writing the native formats use less time, use less memory, and be more reliable. ---. > One thing I noticed is the mt_hwe_vals variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. Yes, after you aggregate you get back an MT with a different column key. ---. The `entries` method converts your matrix table from a compact and efficient matrix into a ""long"" and inefficient table. I generally recommend avoiding it if you can. However, if you only have a handful of ancestries, I wouldn't expect this to be *that* bad. You can just write the MT itself:. ```python3; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)); mt_hwe_vals = mt_hwe_vals.select_rows().select_cols() # drop irrelevant row and column fields; mt_hwe_vals.write(bucket + '/hwe.ht'); ```. ---. > I tried modifying the code to what is shown below but I'm still having the same issue. Just to be clear it's the exact same error ""Container exited with a non-zero exit code 137. ""? This makes me think we have an issue with `entries`, because, even though it's not great, it shouldn't be blowing RAM here. Can you share the log file from your previous or next attempt?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636
Usability,clear,clear,"> Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. We take special care to ensure our system is as efficient as possible when reading or writing to this native format. So, it's partly a sociological thing. On the practical end of things, Hail's native formats (for Tables and Matrix Tables) are a partitioned binary format. The partitioned part means Hail can use many cores in parallel to process and write the dataset. The binary part means that Hail need not use unnecessarily large (in terms of bytes) representations of values. These three things together make writing the native formats use less time, use less memory, and be more reliable. ---. > One thing I noticed is the mt_hwe_vals variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. Yes, after you aggregate you get back an MT with a different column key. ---. The `entries` method converts your matrix table from a compact and efficient matrix into a ""long"" and inefficient table. I generally recommend avoiding it if you can. However, if you only have a handful of ancestries, I wouldn't expect this to be *that* bad. You can just write the MT itself:. ```python3; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)); mt_hwe_vals = mt_hwe_vals.select_rows().select_cols() # drop irrelevant row and column fields; mt_hwe_vals.write(bucket + '/hwe.ht'); ```. ---. > I tried modifying the code to what is shown below but I'm still having the same issue. Just to be clear it's the exact same error ""Container exited with a non-zero exit code 137. ""? This makes me think we have an issue with `entries`, because, even though it's not great, it shouldn't be blowing RAM here. Can you share the log file from your previous or next attempt?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636
Testability,log,logic,"I have an RFC proposal to just handle the ambiguity: https://github.com/hail-is/hail-rfcs/blob/main/rfc/0008-handle-vcf-array-field-ambiguity. I proposed a PR to fix this: https://github.com/hail-is/hail/pull/13465 However, I missed a key issue: many VCF's *elide* fields to indicate missingness. That is not ambiguous: a field that is entirely elided is clearly missing, not an array of one missing value. You can't do this in a FORMAT (aka entry aka genotype) field, but you can do this in an INFO field a la:; ```; ##fileformat=VCFv4.2; ##INFO=<ID=AC,Number=A,Type=Integer,Description=""Allele count in genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=NUMS,Number=*,Type=Float,Description=""some numbers"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT ...; ... AC=1,1;AN=1 ...; ```; the `NUMS` field should be read as missing. My PR considered it unacceptably ambiguous because it thought it had been `NUMS=.`. I don't think we can fix this problem entirely from Python. We need to use Scala-side logic because after we parse in Scala, we lose the knowledge that a field was entirely elided versus a single missing dot.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13346#issuecomment-1773555545
Usability,clear,clearly,"I have an RFC proposal to just handle the ambiguity: https://github.com/hail-is/hail-rfcs/blob/main/rfc/0008-handle-vcf-array-field-ambiguity. I proposed a PR to fix this: https://github.com/hail-is/hail/pull/13465 However, I missed a key issue: many VCF's *elide* fields to indicate missingness. That is not ambiguous: a field that is entirely elided is clearly missing, not an array of one missing value. You can't do this in a FORMAT (aka entry aka genotype) field, but you can do this in an INFO field a la:; ```; ##fileformat=VCFv4.2; ##INFO=<ID=AC,Number=A,Type=Integer,Description=""Allele count in genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=NUMS,Number=*,Type=Float,Description=""some numbers"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT ...; ... AC=1,1;AN=1 ...; ```; the `NUMS` field should be read as missing. My PR considered it unacceptably ambiguous because it thought it had been `NUMS=.`. I don't think we can fix this problem entirely from Python. We need to use Scala-side logic because after we parse in Scala, we lose the knowledge that a field was entirely elided versus a single missing dot.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13346#issuecomment-1773555545
Availability,error,error,Here's a clear instance of buffer corruption after a transient error (in this case an SSLException). https://batch.hail.is/batches/7996481/jobs/182741; ```; 2023-09-13 16:37:36.612 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 2: /batch/1c00c7157d4d41bcbf508f12d75329b1; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 3: /batch/1c00c7157d4d41bcbf508f12d75329b1/log; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 4: gs://hail-query-ger0g/jars/be9d88a80695b04a2a9eb5826361e0897d94c042.jar; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 5: worker; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 6: gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho=; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 7: 38854; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 8: 47960; 2023-09-13 16:37:36.613 JVMEntryway: INFO: Yielding control to the QoB Job.; 2023-09-13 16:37:36.614 Worker$: INFO: is.hail.backend.service.Worker be9d88a80695b04a2a9eb5826361e0897d94c042; 2023-09-13 16:37:36.614 Worker$: INFO: running job 38854/47960 at root gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553
Energy Efficiency,allocate,allocated,"be9d88a80695b04a2a9eb5826361e0897d94c042; 2023-09-13 16:37:36.614 Worker$: INFO: running job 38854/47960 at root gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 : INFO: TaskReport: stage=0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553
Modifiability,adapt,adapted,y$6(BackendUtils.scala:52) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.utils.package$.using(package.scala:635) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$12(Worker.scala:167) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$11(Worker.scala:166) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$11$adapted(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.utils.package$.using(package.scala:635) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.main(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	... 11 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553
Performance,cache,cache,"; 2023-09-13 16:37:36.614 Worker$: INFO: is.hail.backend.service.Worker be9d88a80695b04a2a9eb5826361e0897d94c042; 2023-09-13 16:37:36.614 Worker$: INFO: running job 38854/47960 at root gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 : INFO: TaskReport: stage=0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecuto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553
Testability,log,log,Here's a clear instance of buffer corruption after a transient error (in this case an SSLException). https://batch.hail.is/batches/7996481/jobs/182741; ```; 2023-09-13 16:37:36.612 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 2: /batch/1c00c7157d4d41bcbf508f12d75329b1; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 3: /batch/1c00c7157d4d41bcbf508f12d75329b1/log; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 4: gs://hail-query-ger0g/jars/be9d88a80695b04a2a9eb5826361e0897d94c042.jar; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 5: worker; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 6: gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho=; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 7: 38854; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 8: 47960; 2023-09-13 16:37:36.613 JVMEntryway: INFO: Yielding control to the QoB Job.; 2023-09-13 16:37:36.614 Worker$: INFO: is.hail.backend.service.Worker be9d88a80695b04a2a9eb5826361e0897d94c042; 2023-09-13 16:37:36.614 Worker$: INFO: running job 38854/47960 at root gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553
Usability,clear,clear,Here's a clear instance of buffer corruption after a transient error (in this case an SSLException). https://batch.hail.is/batches/7996481/jobs/182741; ```; 2023-09-13 16:37:36.612 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 2: /batch/1c00c7157d4d41bcbf508f12d75329b1; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 3: /batch/1c00c7157d4d41bcbf508f12d75329b1/log; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 4: gs://hail-query-ger0g/jars/be9d88a80695b04a2a9eb5826361e0897d94c042.jar; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 5: worker; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 6: gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho=; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 7: 38854; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 8: 47960; 2023-09-13 16:37:36.613 JVMEntryway: INFO: Yielding control to the QoB Job.; 2023-09-13 16:37:36.614 Worker$: INFO: is.hail.backend.service.Worker be9d88a80695b04a2a9eb5826361e0897d94c042; 2023-09-13 16:37:36.614 Worker$: INFO: running job 38854/47960 at root gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553
Usability,clear,clear,"I give up, every change I make seems to break something, and it's not clear that any of these changes actually are helping.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13371#issuecomment-1664756542
Usability,simpl,simplify,take a look and tell me if this helps simplify this trigger,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13372#issuecomment-1670091762
Usability,simpl,simplification,This is a great simplification. I'm on board,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13372#issuecomment-1673712560
Availability,error,errors,"> Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path f'{path}/foo, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like '{path}zzzzz/foo or '{path}szzzzz. We need to ignore these as they don't provide any information on whether {path} is a file or directory. This is where isChildOf is needed because we need to make sure the blob is actually a child of the path such as '{path}/file and not {path}zzzzz/file. Ah thanks, this makes more sense to me now. > The other option is to do 2 queries. One to check if it's a file and the next to check if there are any child blobs. This does sound simpler conceptually. I'm not sure off the top of my head what is better performance-wise: the two network requests for a directory or loading a whole page of results in a single request when we only need max 2 results. What do you prefer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464
Deployability,update,update,"> Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path f'{path}/foo, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like '{path}zzzzz/foo or '{path}szzzzz. We need to ignore these as they don't provide any information on whether {path} is a file or directory. This is where isChildOf is needed because we need to make sure the blob is actually a child of the path such as '{path}/file and not {path}zzzzz/file. Ah thanks, this makes more sense to me now. > The other option is to do 2 queries. One to check if it's a file and the next to check if there are any child blobs. This does sound simpler conceptually. I'm not sure off the top of my head what is better performance-wise: the two network requests for a directory or loading a whole page of results in a single request when we only need max 2 results. What do you prefer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464
Integrability,message,message,"> Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path f'{path}/foo, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like '{path}zzzzz/foo or '{path}szzzzz. We need to ignore these as they don't provide any information on whether {path} is a file or directory. This is where isChildOf is needed because we need to make sure the blob is actually a child of the path such as '{path}/file and not {path}zzzzz/file. Ah thanks, this makes more sense to me now. > The other option is to do 2 queries. One to check if it's a file and the next to check if there are any child blobs. This does sound simpler conceptually. I'm not sure off the top of my head what is better performance-wise: the two network requests for a directory or loading a whole page of results in a single request when we only need max 2 results. What do you prefer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464
Performance,optimiz,optimization,"> Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path f'{path}/foo, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like '{path}zzzzz/foo or '{path}szzzzz. We need to ignore these as they don't provide any information on whether {path} is a file or directory. This is where isChildOf is needed because we need to make sure the blob is actually a child of the path such as '{path}/file and not {path}zzzzz/file. Ah thanks, this makes more sense to me now. > The other option is to do 2 queries. One to check if it's a file and the next to check if there are any child blobs. This does sound simpler conceptually. I'm not sure off the top of my head what is better performance-wise: the two network requests for a directory or loading a whole page of results in a single request when we only need max 2 results. What do you prefer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464
Usability,simpl,simpler,"> Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path f'{path}/foo, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like '{path}zzzzz/foo or '{path}szzzzz. We need to ignore these as they don't provide any information on whether {path} is a file or directory. This is where isChildOf is needed because we need to make sure the blob is actually a child of the path such as '{path}/file and not {path}zzzzz/file. Ah thanks, this makes more sense to me now. > The other option is to do 2 queries. One to check if it's a file and the next to check if there are any child blobs. This does sound simpler conceptually. I'm not sure off the top of my head what is better performance-wise: the two network requests for a directory or loading a whole page of results in a single request when we only need max 2 results. What do you prefer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464
Availability,error,error,"@jigold Do I understand correctly that g2-standard-4 can only be created as a job private instance because there is no matching pool? If that's right, it looks like, unlike pool jobs, a JPIM job [will be correctly marked as error](https://github.com/hail-is/hail/blob/main/batch/batch/driver/instance_collection/job_private.py#L457-L467) if there are no available regions. Assuming all that is correct, do I also understand correctly that the only reason to block incoming jobs at the front-end is for a better user experience, not to protect the system from bad data? If yes, then I agree that need not be part of this PR because it merely improves user experience rather than being critical for correct functioning of the system. Are any of my assumptions or inferences wrong?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13430#issuecomment-1725778672
Usability,user experience,user experience,"@jigold Do I understand correctly that g2-standard-4 can only be created as a job private instance because there is no matching pool? If that's right, it looks like, unlike pool jobs, a JPIM job [will be correctly marked as error](https://github.com/hail-is/hail/blob/main/batch/batch/driver/instance_collection/job_private.py#L457-L467) if there are no available regions. Assuming all that is correct, do I also understand correctly that the only reason to block incoming jobs at the front-end is for a better user experience, not to protect the system from bad data? If yes, then I agree that need not be part of this PR because it merely improves user experience rather than being critical for correct functioning of the system. Are any of my assumptions or inferences wrong?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13430#issuecomment-1725778672
Integrability,rout,route,"If we go through route (2), this project can serve as a prototype C or C++ interface to Hail. This interface could take multiple forms. For example, we could actually re-build our memory representation implementations in C++ and compile SAIGE, at Hail-Query-compile-time (i.e. when we are compiling a *user's* query), to use whatever SType/PType that Hail has decided is the ideal. A simpler approach is to implement one canonical implementation of the Hail types in C++, fork & slightly modify SAIGE to accept these memory representations, compile SAIGE at Java compile time (i.e. in CI or when you run `make` on your laptop) against these mem reps, ship the compiled library with the Hail JAR, and expose it, via JNI, into the Hail Query language. This requires that the Query compiler can call a function which only supports arguments using one particular SType/PType.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13442#issuecomment-1679739816
Security,expose,expose,"If we go through route (2), this project can serve as a prototype C or C++ interface to Hail. This interface could take multiple forms. For example, we could actually re-build our memory representation implementations in C++ and compile SAIGE, at Hail-Query-compile-time (i.e. when we are compiling a *user's* query), to use whatever SType/PType that Hail has decided is the ideal. A simpler approach is to implement one canonical implementation of the Hail types in C++, fork & slightly modify SAIGE to accept these memory representations, compile SAIGE at Java compile time (i.e. in CI or when you run `make` on your laptop) against these mem reps, ship the compiled library with the Hail JAR, and expose it, via JNI, into the Hail Query language. This requires that the Query compiler can call a function which only supports arguments using one particular SType/PType.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13442#issuecomment-1679739816
Usability,simpl,simpler,"If we go through route (2), this project can serve as a prototype C or C++ interface to Hail. This interface could take multiple forms. For example, we could actually re-build our memory representation implementations in C++ and compile SAIGE, at Hail-Query-compile-time (i.e. when we are compiling a *user's* query), to use whatever SType/PType that Hail has decided is the ideal. A simpler approach is to implement one canonical implementation of the Hail types in C++, fork & slightly modify SAIGE to accept these memory representations, compile SAIGE at Java compile time (i.e. in CI or when you run `make` on your laptop) against these mem reps, ship the compiled library with the Hail JAR, and expose it, via JNI, into the Hail Query language. This requires that the Query compiler can call a function which only supports arguments using one particular SType/PType.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13442#issuecomment-1679739816
Usability,feedback,feedback,@danking I'm keeping the changes requested state on until I get the full scope of changes that need to be made. Would be good to get your feedback first after my responses so I can do all changes in one pass.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13475#issuecomment-1739219812
Deployability,update,update,"I still dislike job_group_tree because trees are usually represented in terms of their ancestor or parent-child relationships, neither of which are what this is. That said, I don't think we should block this PR on a naming quibble. We can do renames separately. We should update the RFC to be clear about the unique identifiers of the three things groups, jobs, and batches. (In particular, jobs are uniquely identifier by batch id and job id, group id is not part of it).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13475#issuecomment-1760376350
Usability,clear,clear,"I still dislike job_group_tree because trees are usually represented in terms of their ancestor or parent-child relationships, neither of which are what this is. That said, I don't think we should block this PR on a naming quibble. We can do renames separately. We should update the RFC to be clear about the unique identifiers of the three things groups, jobs, and batches. (In particular, jobs are uniquely identifier by batch id and job id, group id is not part of it).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13475#issuecomment-1760376350
Deployability,update,updated,"My issue with both of those names is the same as with `_tree`. It's not clear if you're storing the non-transitive or the transitive relation & its not clear if self-edges are included. I want a name that unambiguously says ""the self-edge and all the ancestor edges are in here"" or a name that is more domain-specific like ""job groups that need to be updated when a job in this job group is changed"". . `job_group_self_and_ancestor` feels like the shortest name so far that satisfies my concerns, but I'm open to other ones that are clear about self-edge-ness and transitive-ness.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13475#issuecomment-1761867398
Usability,clear,clear,"My issue with both of those names is the same as with `_tree`. It's not clear if you're storing the non-transitive or the transitive relation & its not clear if self-edges are included. I want a name that unambiguously says ""the self-edge and all the ancestor edges are in here"" or a name that is more domain-specific like ""job groups that need to be updated when a job in this job group is changed"". . `job_group_self_and_ancestor` feels like the shortest name so far that satisfies my concerns, but I'm open to other ones that are clear about self-edge-ness and transitive-ness.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13475#issuecomment-1761867398
Usability,simpl,simple,"@daniel-goldstein Can you please check the last commit real quick? Should just be a simple rename, but want someone to double check I didn't do something bad.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13475#issuecomment-1766736992
Availability,error,error,"Another very simple pipeline reported https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/zip.3A.20length.20mismatch . We can get access to these files via Sam B. ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.annotate(; uniprot = ensp2uniprot_ht[context_mis_freq_ht.ensp].uniprot); ```. notice that the error is removed if you instead use:; ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.join(ensp2uniprot_ht,'left'). ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13486#issuecomment-1883607858
Deployability,pipeline,pipeline,"Another very simple pipeline reported https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/zip.3A.20length.20mismatch . We can get access to these files via Sam B. ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.annotate(; uniprot = ensp2uniprot_ht[context_mis_freq_ht.ensp].uniprot); ```. notice that the error is removed if you instead use:; ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.join(ensp2uniprot_ht,'left'). ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13486#issuecomment-1883607858
Security,access,access,"Another very simple pipeline reported https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/zip.3A.20length.20mismatch . We can get access to these files via Sam B. ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.annotate(; uniprot = ensp2uniprot_ht[context_mis_freq_ht.ensp].uniprot); ```. notice that the error is removed if you instead use:; ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.join(ensp2uniprot_ht,'left'). ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13486#issuecomment-1883607858
Usability,simpl,simple,"Another very simple pipeline reported https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/zip.3A.20length.20mismatch . We can get access to these files via Sam B. ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.annotate(; uniprot = ensp2uniprot_ht[context_mis_freq_ht.ensp].uniprot); ```. notice that the error is removed if you instead use:; ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.join(ensp2uniprot_ht,'left'). ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13486#issuecomment-1883607858
Usability,clear,clear,"To be clear, all this does is encode in the types what we already guarantee: if version X returns `{'foo': int}`, then all future versions of the server must *at least* return a dict containing the `'foo'` key with an integer. If those servers start returning `{'bar': int}`, that would break all the old clients (and violate the types that we wrote in those clients).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13500#issuecomment-1693583847
Usability,clear,clear,To be clear this is just the *visual* output. The data is fine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13512#issuecomment-1773467913
Usability,simpl,simplest,"MySQL supports looping. It looks like we could [use a cursor with loop](https://stackoverflow.com/a/16350693/6823256) to iterate through the ancestors of the job group. That seems to me like the simplest possible solution here. We should leave any possible speed improvements to future work and not entangle that with job groups. Jackie, my comment during our chat about deadlocks was incorrect. The if statement does prevent deadlocks because it ensures that only one MJC tries to grab an exclusive lock rather than more than one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701854650
Usability,simpl,simple,How long did these jobs run for? I was able to reproduce the OOM with this simple example and the resource usage file was present in GCS albeit with only 8 bytes written (the header). We don't show plots in the UI with just the header and no data. Do you have example Hail Query code that would generate an OOM more slowly?. ```python3; hl.eval(hl.range(1024 * 1024).map(lambda _: hl.range(1024 * 1024))); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13577#issuecomment-1737522562
Deployability,pipeline,pipeline,gnomAD is also exhausting memory (exit code 137) on their frequencies generating pipeline. They’re still exhausting memory after eliminating fork-joins in their pipeline. They were unintentionally invoking densification four times. We’ll need to sort out why even the simple frequencies blows memory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13584#issuecomment-1712493167
Usability,simpl,simple,gnomAD is also exhausting memory (exit code 137) on their frequencies generating pipeline. They’re still exhausting memory after eliminating fork-joins in their pipeline. They were unintentionally invoking densification four times. We’ll need to sort out why even the simple frequencies blows memory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13584#issuecomment-1712493167
Usability,clear,clear,"To be abundantly clear: this is strictly a rename. Originally I had some changes in Python, but I've backed them out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13594#issuecomment-1710921838
Modifiability,variab,variable,"After some reading, I am still not sure what exactly the difference is between dummy coding and one-hot encoding. Suppose there is a categorical variable with $n$ categories. The [referenced Stack Exchange question](https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn) suggests that a one-hot encoding converts the categorical variable to $n$ indicator variables (one for each category) and that a dummy coding converts the categorical variable to $n-1$ indicator variables. With these definitions, the dummy coding is the one-hot encoding without one of the indicator variables. However, from the prototype implementation in this issue, the [scikit-learn one-hot encoder documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), and the [dummy variable Wikipedia article](https://en.m.wikipedia.org/wiki/Dummy_variable_(statistics)), I get the impression that dummy coding and one-hot encoding are synonyms and that there is no real distinction. Anyway, I would like to work on this issue. I will base my implementation on the prototype, and perhaps we can add a parameter to drop one of the indicator variables similar to what the [scikit-learn one-hot encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) has.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13601#issuecomment-1932779413
Usability,learn,learn,"After some reading, I am still not sure what exactly the difference is between dummy coding and one-hot encoding. Suppose there is a categorical variable with $n$ categories. The [referenced Stack Exchange question](https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn) suggests that a one-hot encoding converts the categorical variable to $n$ indicator variables (one for each category) and that a dummy coding converts the categorical variable to $n-1$ indicator variables. With these definitions, the dummy coding is the one-hot encoding without one of the indicator variables. However, from the prototype implementation in this issue, the [scikit-learn one-hot encoder documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), and the [dummy variable Wikipedia article](https://en.m.wikipedia.org/wiki/Dummy_variable_(statistics)), I get the impression that dummy coding and one-hot encoding are synonyms and that there is no real distinction. Anyway, I would like to work on this issue. I will base my implementation on the prototype, and perhaps we can add a parameter to drop one of the indicator variables similar to what the [scikit-learn one-hot encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) has.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13601#issuecomment-1932779413
Deployability,pipeline,pipeline,"FWIW, I finally found a simpler reproducer. It really takes some doing to convince the simplifier to apply this rule. This operation should use a constant ~1GiB of RAM (in reality, in a non-broken pipeline it uses closer to 8GiB, but, still, a constant amount of RAM), but in reality memory use grows with each row processed. ```python3; import hail as hl; ht = hl.utils.range_table(1); ht = ht.key_by(); ht = ht.select(rows = hl.range(10)); ht = ht.explode('rows'); ht = ht.annotate(garbage=hl.range(1024 ** 3)); ht.write('/tmp/foo.ht', overwrite=True); ```; The simplifier cannot simplify the pipeline if the key is still present so this pipeline is sufficient to restore normal memory usage:; ```python3; import hail as hl; ht = hl.utils.range_table(1); ht = ht.select(rows = hl.range(10)); ht = ht.explode('rows'); ht = ht.annotate(garbage=hl.range(1024 ** 3)); ht.write('/tmp/foo.ht', overwrite=True); ```. The ""bad"" `WritePartition` body IR looks like this:; ```; (StreamFlatMap __iruid_447; (StreamRange -1 True; (GetField start (Ref __iruid_446)); (GetField end (Ref __iruid_446)); (I32 1)); (StreamMap __iruid_448; (StreamRange 1 False (I32 0) (I32 10) (I32 1)); (InsertFields; (Literal Struct{} <literal value>); (""rows"" ""garbage""); (rows (Ref __iruid_448)); (garbage; (ToArray; (StreamRange 2 False; (I32 0); (I32 1073741824); (I32 1))))))); ```; The ""good"" IR looks like this:; ```; (StreamFlatMap __iruid_480; (StreamRange -1 True; (GetField start (Ref __iruid_479)); (GetField end (Ref __iruid_479)); (I32 1)); (Let __iruid_481; (MakeStruct; (idx (Ref __iruid_480)); (rows; (ToArray; (StreamRange 1 False (I32 0) (I32 10) (I32 1))))); (StreamMap __iruid_482; (ToStream True (GetField rows (Ref __iruid_481))); (InsertFields; (Ref __iruid_481); (""idx"" ""rows"" ""garbage""); (rows (Ref __iruid_482)); (garbage; (ToArray; (StreamRange 2 False; (I32 0); (I32 1073741824); (I32 1)))))))); ```. Notice, in particular, that the `StreamMap` inside the `StreamFlatMap` uses memory management because",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13619#issuecomment-1720022103
Usability,simpl,simpler,"FWIW, I finally found a simpler reproducer. It really takes some doing to convince the simplifier to apply this rule. This operation should use a constant ~1GiB of RAM (in reality, in a non-broken pipeline it uses closer to 8GiB, but, still, a constant amount of RAM), but in reality memory use grows with each row processed. ```python3; import hail as hl; ht = hl.utils.range_table(1); ht = ht.key_by(); ht = ht.select(rows = hl.range(10)); ht = ht.explode('rows'); ht = ht.annotate(garbage=hl.range(1024 ** 3)); ht.write('/tmp/foo.ht', overwrite=True); ```; The simplifier cannot simplify the pipeline if the key is still present so this pipeline is sufficient to restore normal memory usage:; ```python3; import hail as hl; ht = hl.utils.range_table(1); ht = ht.select(rows = hl.range(10)); ht = ht.explode('rows'); ht = ht.annotate(garbage=hl.range(1024 ** 3)); ht.write('/tmp/foo.ht', overwrite=True); ```. The ""bad"" `WritePartition` body IR looks like this:; ```; (StreamFlatMap __iruid_447; (StreamRange -1 True; (GetField start (Ref __iruid_446)); (GetField end (Ref __iruid_446)); (I32 1)); (StreamMap __iruid_448; (StreamRange 1 False (I32 0) (I32 10) (I32 1)); (InsertFields; (Literal Struct{} <literal value>); (""rows"" ""garbage""); (rows (Ref __iruid_448)); (garbage; (ToArray; (StreamRange 2 False; (I32 0); (I32 1073741824); (I32 1))))))); ```; The ""good"" IR looks like this:; ```; (StreamFlatMap __iruid_480; (StreamRange -1 True; (GetField start (Ref __iruid_479)); (GetField end (Ref __iruid_479)); (I32 1)); (Let __iruid_481; (MakeStruct; (idx (Ref __iruid_480)); (rows; (ToArray; (StreamRange 1 False (I32 0) (I32 10) (I32 1))))); (StreamMap __iruid_482; (ToStream True (GetField rows (Ref __iruid_481))); (InsertFields; (Ref __iruid_481); (""idx"" ""rows"" ""garbage""); (rows (Ref __iruid_482)); (garbage; (ToArray; (StreamRange 2 False; (I32 0); (I32 1073741824); (I32 1)))))))); ```. Notice, in particular, that the `StreamMap` inside the `StreamFlatMap` uses memory management because",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13619#issuecomment-1720022103
Usability,clear,clear,"Good call, done. It would be nice to factor out the common parts of handling refs, whether they're normal children or the only node in a block, but it's not clear how to do that without a pretty big structural change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13627#issuecomment-1724263938
Usability,feedback,feedback,"@daniel-goldstein sorry for the slow review turnaround! the code looks good to me, and i'll try it out tomorrow morning to make sure i don't have any additional feedback. thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13632#issuecomment-1781834758
Usability,clear,clear,Working on it! #12848 will be merged soon and #12849 will be reopened subsequently. The next steps after the billing tables are fully populated are not clear to me and we might want to discuss after stand up once the other two are merged.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13656#issuecomment-1737882692
Performance,optimiz,optimization-guide,Maybe we can use GraalVM Native Image? https://docs.oracle.com/en/graalvm/enterprise/20/docs/reference-manual/native-image/Limitations/#native-image-compatibility-and-optimization-guide,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13675#issuecomment-1728007156
Usability,guid,guide,Maybe we can use GraalVM Native Image? https://docs.oracle.com/en/graalvm/enterprise/20/docs/reference-manual/native-image/Limitations/#native-image-compatibility-and-optimization-guide,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13675#issuecomment-1728007156
Usability,clear,clear,"Adding zulip thread for posterity:. Patrick Schultz (he/him); [12:38 pm](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Query-Dev/topic/FastIndexedSeq/near/392155787); Ah, that's what I thought, originally FastSeq returned a Seq: https://github.com/hail-is/hail/commit/be415a850eb145c69a830e485d3192331799f14f#diff-75be823f33bdb7bc10ab85e3b954c91a4a7bd48176c2a39f2e3d2e7f7e30fab4. daniel king (he/him); [12:40 pm](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Query-Dev/topic/FastIndexedSeq/near/392156010); huh, I wonder when/why that changed. Patrick Schultz (he/him); [12:40 pm](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Query-Dev/topic/FastIndexedSeq/near/392156041); [toFastSeq](https://github.com/hail-is/hail/blob/995994c862b93406bc4b5fc37c7f022f7426cd52/hail/src/main/scala/is/hail/utils/richUtils/RichIterator.scala#L145) still does. If you make this change, you should get rid of toFastIndexedSeq too. [ 12:41 pm](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Query-Dev/topic/FastIndexedSeq/near/392156191); daniel king (he/him) [said](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Query-Dev/topic/FastIndexedSeq/near/392156010):. huh, I wonder when/why that changed. [a month later](https://github.com/hail-is/hail/commit/e2458973ad2bb9b065a56f480e986554b40eed79#diff-75be823f33bdb7bc10ab85e3b954c91a4a7bd48176c2a39f2e3d2e7f7e30fab4), not clear why",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13676#issuecomment-1728108189
Usability,guid,guiding,"@daniel-goldstein I think these changes resolve the problems in hailtop.batch. My guiding philosophy there was sync methods are always shims around async methods. https://github.com/daniel-goldstein/hail/compare/fix-nest-asyncio-apply...danking:hail:fix-nest-asyncio-apply. The issue in Query is deeper. We lazily compute the type of IRs, but to know the type of reading a Table, MatrixTable, or BlockMatrix, we must make a network request to read the metadata from that object. I'm not exactly sure what to do here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13677#issuecomment-1769152362
Usability,progress bar,progress bar,"The change regarding the copy tool progress bar is because on non-verbose mode (like in the local backend) it was generating some weird whitespace by just creating the progress bar not disabled even though none of the tasks were visible. In jupyter notebooks, whitespace rendered as `5l` in the jupyter output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13681#issuecomment-1729901749
Availability,failure,failure,"Another set of eyes on this would be great. My current thoughts on this:. I only looked at the failure in PCA. I was never able to reproduce. My next step to try to reproduce was to run PCA on Lindo's full dataset on dataproc (can't use batch because the error is in spark PCA). I did look carefully through the stack trace, trying to understand what could possibly be happening. The number 177860 from the error isn't either matrix dimension, which is 210234 by 8893. Everything in `org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:106)` is independent of the number of rows, so only the number 8893 of cols should be relevent. I wrote a simple test to execute spark PCA with 8893 rows in scala, so I could step through with a debugger:; ```scala; var mt = rangeMatrix(10000, 8893); mt = MatrixMapEntries(mt, InsertFields(Ref(""g"", mt.typ.entryType), Seq(""a"" -> F64(1)))); val t = MatrixToTableApply(mt, PCA(""a"", 10, false)); val n = TableToValueApply(t, ForceCountTable()); assertEvalsTo(n, 8893L); ```; The array `v` in `symmetricEigs` has length 177860 = 8893*20, and I didn't find anything else with that size. The only line I could find that could generate an exception that looks like this is line 555 of `dev.ludovic.netlib.arpack.AbstractARPACK.dsaupd`; ```scala; public void dsaupd(org.netlib.util.intW ido, String bmat, int n, String which, int nev, org.netlib.util.doubleW tol, double[] resid, int offsetresid, int ncv, double[] v, int offsetv, int ldv, int[] iparam, int offsetiparam, int[] ipntr, int offsetipntr, double[] workd, int offsetworkd, double[] workl, int offsetworkl, int lworkl, org.netlib.util.intW info) {; if (debug) System.err.println(""dsaupd"");; checkArgument(""DSAUPD"", 2, lsame(""I"", bmat) || lsame(""G"", bmat));; checkArgument(""DSAUPD"", 3, n >= 0);; checkArgument(""DSAUPD"", 4, lsame(""LA"", which) || lsame(""SA"", which) || lsame(""LM"", which) || lsame(""SM"", which) || lsame(""BE"", which));; checkArgument(""DSAUPD"", 5, 0 < ne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1760360313
Security,access,access,"nNull(v);; requireNonNull(iparam);; requireNonNull(ipntr);; requireNonNull(workd);; requireNonNull(info);; checkIndex(offsetresid + n - 1, resid.length);; checkIndex(offsetv + n * ncv - 1, v.length); // HERE; checkIndex(offsetiparam + 11 - 1, iparam.length);; checkIndex(offsetipntr + 11 - 1, ipntr.length);; checkIndex(offsetworkd + 3 * n - 1, workd.length);; checkIndex(offsetworkl + lworkl - 1, workl.length);; dsaupdK(ido, bmat, n, which, nev, tol, resid, offsetresid, ncv, v, offsetv, ldv, iparam, offsetiparam, ipntr, offsetipntr, workd, offsetworkd, workl, offsetworkl, lworkl, info);; }; ```; where `v.length = 177860`, `n = 8893`, and `ncv = 20`. `checkIndex` is; ```; private void checkIndex(int index, int length) {; if (index < 0 || index >= length) {; throw new IndexOutOfBoundsException(String.format(""Index %s out of bounds for length %s"", index, length));; }; }; ```; which a) shouldn't throw in this case, b) throws an `IndexOutOfBoundsException`, not an `ArrayIndexOutOfBoundsException`, and c) isn't the root of the stacktrace; ```; 	at org.netlib.blas.Dcopy.dcopy(blas.f); 	at org.netlib.arpack.Dsaitr.dsaitr(arpack.f); 	at org.netlib.arpack.Dsaup2.dsaup2(arpack.f); 	at org.netlib.arpack.Dsaupd.dsaupd(arpack.f); 	at dev.ludovic.netlib.arpack.F2jARPACK.dsaupdK(F2jARPACK.java:189); 	at dev.ludovic.netlib.arpack.AbstractARPACK.dsaupd(AbstractARPACK.java:560); 	at dev.ludovic.netlib.arpack.F2jARPACK.dsaupd(F2jARPACK.java:30); 	at dev.ludovic.netlib.arpack.AbstractARPACK.dsaupd(AbstractARPACK.java:536); 	at dev.ludovic.netlib.arpack.F2jARPACK.dsaupd(F2jARPACK.java:30); 	at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:106); ```; So it looks like somehow bad indices are making it through all these checks and a bad array access is hapenning deep in blas?. So I'm at a complete loss. If anybody else could take a stab at it with a fresh perspective, I'd be happy to discuss, but I'm low on bandwidth to keep digging myself.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1760360313
Testability,test,test,"Another set of eyes on this would be great. My current thoughts on this:. I only looked at the failure in PCA. I was never able to reproduce. My next step to try to reproduce was to run PCA on Lindo's full dataset on dataproc (can't use batch because the error is in spark PCA). I did look carefully through the stack trace, trying to understand what could possibly be happening. The number 177860 from the error isn't either matrix dimension, which is 210234 by 8893. Everything in `org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:106)` is independent of the number of rows, so only the number 8893 of cols should be relevent. I wrote a simple test to execute spark PCA with 8893 rows in scala, so I could step through with a debugger:; ```scala; var mt = rangeMatrix(10000, 8893); mt = MatrixMapEntries(mt, InsertFields(Ref(""g"", mt.typ.entryType), Seq(""a"" -> F64(1)))); val t = MatrixToTableApply(mt, PCA(""a"", 10, false)); val n = TableToValueApply(t, ForceCountTable()); assertEvalsTo(n, 8893L); ```; The array `v` in `symmetricEigs` has length 177860 = 8893*20, and I didn't find anything else with that size. The only line I could find that could generate an exception that looks like this is line 555 of `dev.ludovic.netlib.arpack.AbstractARPACK.dsaupd`; ```scala; public void dsaupd(org.netlib.util.intW ido, String bmat, int n, String which, int nev, org.netlib.util.doubleW tol, double[] resid, int offsetresid, int ncv, double[] v, int offsetv, int ldv, int[] iparam, int offsetiparam, int[] ipntr, int offsetipntr, double[] workd, int offsetworkd, double[] workl, int offsetworkl, int lworkl, org.netlib.util.intW info) {; if (debug) System.err.println(""dsaupd"");; checkArgument(""DSAUPD"", 2, lsame(""I"", bmat) || lsame(""G"", bmat));; checkArgument(""DSAUPD"", 3, n >= 0);; checkArgument(""DSAUPD"", 4, lsame(""LA"", which) || lsame(""SA"", which) || lsame(""LM"", which) || lsame(""SM"", which) || lsame(""BE"", which));; checkArgument(""DSAUPD"", 5, 0 < ne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1760360313
Usability,simpl,simple,"Another set of eyes on this would be great. My current thoughts on this:. I only looked at the failure in PCA. I was never able to reproduce. My next step to try to reproduce was to run PCA on Lindo's full dataset on dataproc (can't use batch because the error is in spark PCA). I did look carefully through the stack trace, trying to understand what could possibly be happening. The number 177860 from the error isn't either matrix dimension, which is 210234 by 8893. Everything in `org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:106)` is independent of the number of rows, so only the number 8893 of cols should be relevent. I wrote a simple test to execute spark PCA with 8893 rows in scala, so I could step through with a debugger:; ```scala; var mt = rangeMatrix(10000, 8893); mt = MatrixMapEntries(mt, InsertFields(Ref(""g"", mt.typ.entryType), Seq(""a"" -> F64(1)))); val t = MatrixToTableApply(mt, PCA(""a"", 10, false)); val n = TableToValueApply(t, ForceCountTable()); assertEvalsTo(n, 8893L); ```; The array `v` in `symmetricEigs` has length 177860 = 8893*20, and I didn't find anything else with that size. The only line I could find that could generate an exception that looks like this is line 555 of `dev.ludovic.netlib.arpack.AbstractARPACK.dsaupd`; ```scala; public void dsaupd(org.netlib.util.intW ido, String bmat, int n, String which, int nev, org.netlib.util.doubleW tol, double[] resid, int offsetresid, int ncv, double[] v, int offsetv, int ldv, int[] iparam, int offsetiparam, int[] ipntr, int offsetipntr, double[] workd, int offsetworkd, double[] workl, int offsetworkl, int lworkl, org.netlib.util.intW info) {; if (debug) System.err.println(""dsaupd"");; checkArgument(""DSAUPD"", 2, lsame(""I"", bmat) || lsame(""G"", bmat));; checkArgument(""DSAUPD"", 3, n >= 0);; checkArgument(""DSAUPD"", 4, lsame(""LA"", which) || lsame(""SA"", which) || lsame(""LM"", which) || lsame(""SM"", which) || lsame(""BE"", which));; checkArgument(""DSAUPD"", 5, 0 < ne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1760360313
Deployability,configurat,configuration,"The [TextInputFormat](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html) class clearly comes from hadoop. It's no longer in the location from which we import it. We must get it from some other dependency. OK. So, before my simplification of build.gradle, we used a configuration called `compile` and another one called `testCompile`. [Neither of those exist in modern gradle, apparently](https://docs.gradle.org/current/userguide/java_library_plugin.html#sec:java_library_configurations_graph). I found a side-note about the `compile` configuration [here](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_dependency_management_overview) (search for ""compile""):. > **Why no compile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a project’s production code. It is now deprecated, and will issue warnings when used, because it doesn’t distinguish between dependencies that impact the public API of a Java library project and those that don’t. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741
Integrability,depend,dependency,"The [TextInputFormat](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html) class clearly comes from hadoop. It's no longer in the location from which we import it. We must get it from some other dependency. OK. So, before my simplification of build.gradle, we used a configuration called `compile` and another one called `testCompile`. [Neither of those exist in modern gradle, apparently](https://docs.gradle.org/current/userguide/java_library_plugin.html#sec:java_library_configurations_graph). I found a side-note about the `compile` configuration [here](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_dependency_management_overview) (search for ""compile""):. > **Why no compile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a project’s production code. It is now deprecated, and will issue warnings when used, because it doesn’t distinguish between dependencies that impact the public API of a Java library project and those that don’t. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741
Modifiability,config,configuration,"The [TextInputFormat](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html) class clearly comes from hadoop. It's no longer in the location from which we import it. We must get it from some other dependency. OK. So, before my simplification of build.gradle, we used a configuration called `compile` and another one called `testCompile`. [Neither of those exist in modern gradle, apparently](https://docs.gradle.org/current/userguide/java_library_plugin.html#sec:java_library_configurations_graph). I found a side-note about the `compile` configuration [here](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_dependency_management_overview) (search for ""compile""):. > **Why no compile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a project’s production code. It is now deprecated, and will issue warnings when used, because it doesn’t distinguish between dependencies that impact the public API of a Java library project and those that don’t. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741
Performance,concurren,concurrent,"pache.commons.math3.distribution.{ChiSquaredDistribution, NormalDistribution}; import org.apache.commons.math3.random.JDKRandomGenerator; import org.apache.commons.math3.util.CombinatoricsUtils.factorialLog; import org.apache.hadoop; import org.apache.log4j.{ConsoleAppender, PatternLayout}; import org.apache.spark.SparkContext; import org.apache.spark.SparkException; import org.apache.spark.mllib.linalg.Vectors; import org.apache.spark.mllib.linalg.distributed.{DistributedMatrix, IndexedRow, IndexedRowMatrix}; import org.apache.spark.rdd.RDD; import org.apache.spark.sql.Row; import org.apache.spark.storage.StorageLevel; import org.apache.{hadoop => hd}; import org.json4s.JValue; import org.json4s.JsonAST._; import org.json4s._; import org.json4s.jackson.JsonMethods; import org.json4s.jackson.JsonMethods._; import org.json4s.jackson.Serialization; import org.json4s.jackson.{JsonMethods, Serialization}; import org.json4s.{DefaultFormats, Formats}; import org.sparkproject.guava.util.concurrent.MoreExecutors; ```. We explicitly depend on; - `htsjdk`; - `breeze`; - `json4s`. That leaves:. ```; import org.apache.avro.SchemaBuilder; import org.apache.avro.file.DataFileWriter; import org.apache.avro.generic.{GenericDatumWriter, GenericRecord, GenericRecordBuilder}; import org.apache.commons.io.IOUtils; import org.apache.commons.math3.distribution.ChiSquaredDistribution; import org.apache.commons.math3.distribution.{ChiSquaredDistribution, NormalDistribution}; import org.apache.commons.math3.random.JDKRandomGenerator; import org.apache.commons.math3.util.CombinatoricsUtils.factorialLog; import org.apache.hadoop; import org.apache.log4j.{ConsoleAppender, PatternLayout}; import org.apache.spark.SparkContext; import org.apache.spark.SparkException; import org.apache.spark.mllib.linalg.Vectors; import org.apache.spark.mllib.linalg.distributed.{DistributedMatrix, IndexedRow, IndexedRowMatrix}; import org.apache.spark.rdd.RDD; import org.apache.spark.sql.Row; import org.apache.spar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741
Testability,test,testCompile,"The [TextInputFormat](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html) class clearly comes from hadoop. It's no longer in the location from which we import it. We must get it from some other dependency. OK. So, before my simplification of build.gradle, we used a configuration called `compile` and another one called `testCompile`. [Neither of those exist in modern gradle, apparently](https://docs.gradle.org/current/userguide/java_library_plugin.html#sec:java_library_configurations_graph). I found a side-note about the `compile` configuration [here](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_dependency_management_overview) (search for ""compile""):. > **Why no compile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a project’s production code. It is now deprecated, and will issue warnings when used, because it doesn’t distinguish between dependencies that impact the public API of a Java library project and those that don’t. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741
Usability,clear,clearly,"The [TextInputFormat](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html) class clearly comes from hadoop. It's no longer in the location from which we import it. We must get it from some other dependency. OK. So, before my simplification of build.gradle, we used a configuration called `compile` and another one called `testCompile`. [Neither of those exist in modern gradle, apparently](https://docs.gradle.org/current/userguide/java_library_plugin.html#sec:java_library_configurations_graph). I found a side-note about the `compile` configuration [here](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_dependency_management_overview) (search for ""compile""):. > **Why no compile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a project’s production code. It is now deprecated, and will issue warnings when used, because it doesn’t distinguish between dependencies that impact the public API of a Java library project and those that don’t. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741
Usability,feedback,feedback,"What kind of feedback would you like? At a high-level, I think constructing these URLs and embedding links like this into the PR page is a great idea. Love how little code it takes to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13744#issuecomment-1739576643
Deployability,upgrade,upgrades,"my strategy here was just to undo everything added in [this commit](https://github.com/hail-is/hail/commit/12e0f497db0f3e5453f870495e48e44191b315f4), except the version upgrades, so i'm not sure if there are some changes i'm making here that are unnecessary or produce weird results as far as what all ends up in the jar or anything. from running `jar -tf` on the jar produced by the current `main` and the one produced by the commit prior to the one i'm partially reverting, it looked like the updates to the config only added things to the jar, rather than removing any, so hopefully that should be fine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13759#issuecomment-1743654792
Modifiability,config,config,"my strategy here was just to undo everything added in [this commit](https://github.com/hail-is/hail/commit/12e0f497db0f3e5453f870495e48e44191b315f4), except the version upgrades, so i'm not sure if there are some changes i'm making here that are unnecessary or produce weird results as far as what all ends up in the jar or anything. from running `jar -tf` on the jar produced by the current `main` and the one produced by the commit prior to the one i'm partially reverting, it looked like the updates to the config only added things to the jar, rather than removing any, so hopefully that should be fine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13759#issuecomment-1743654792
Usability,undo,undo,"my strategy here was just to undo everything added in [this commit](https://github.com/hail-is/hail/commit/12e0f497db0f3e5453f870495e48e44191b315f4), except the version upgrades, so i'm not sure if there are some changes i'm making here that are unnecessary or produce weird results as far as what all ends up in the jar or anything. from running `jar -tf` on the jar produced by the current `main` and the one produced by the commit prior to the one i'm partially reverting, it looked like the updates to the config only added things to the jar, rather than removing any, so hopefully that should be fine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13759#issuecomment-1743654792
Safety,safe,safe,"Seems to average 60MB/s. No clear culprits. Zstd decoding is the top hit right now. The hottest generated code is inplace decoding of an optional array of optional int32. Really sucks because things like `LA` are somehow getting written as element optional, even though, by construction their elements are not optional. ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:; +EArray[EBaseStruct{; LA:EArray[EInt32]; ,LGT:EInt32; ,LAD:EArray[EInt32]; ,LPGT:EInt32; ,LPL:EArray[EInt32]; ,RGQ:EInt32,; gvcf_info: EBaseStruct{; AC:EArray[EInt32]; ,AF:EArray[EFloat64]; ,AN:EInt32,AS_BaseQRankSum:EArray[EFloat64]; ,AS_FS:EArray[EFloat64]; ,AS_InbreedingCoeff:EArray[EFloat64]; ,AS_MQ:EArray[EFloat64]; ,AS_MQRankSum:EArray[EFloat64]; ,AS_QD:EArray[EFloat64]; ,AS_QUALapprox:EArray[EInt32]; ,AS_RAW_BaseQRankSum:EBinary,AS_RAW_MQ:EArray[EFloat64]; ,AS_RAW_MQRankSum:EArray[EBaseStruct{`0`:EFloat64,`1`:EInt32}]; ,AS_RAW_ReadPosRankSum:EArray[EBaseStruct{`0`:EFloat64,`1`:EInt32}]; ,AS_ReadPosRankSum:EArray[EFloat64]; ,AS_SB_TABLE:EArray[EArray[EInt32]]; ,AS_SOR:EArray[EFloat64]; ,AS_VarDP:EArray[EInt32]; ,BaseQRankSum:EFloat64,ExcessHet:EFloat64,FS:EFloat64,InbreedingCoeff:EFloat64,MQ:EFloat64,MQRankSum:EFloat64,MQ_DP:EInt32,QD:EFloat64,QUALapprox:EInt32,RAW_GT_COUNT:EArray[EInt32]; ,RAW_MQandDP:EArray[EInt32]; ,ReadPosRankSum:EFloat64,SOR:EFloat64,VarDP:EInt32}; ,DP:EInt32; ,GQ:EInt32; ,MIN_DP:EInt32; ,PID:EBinary; ,PS:EInt32; ,SB:EArray[EInt32]; }; ]; }; ```. Async profiler periodic sampling:; <img width=""2032"" alt=""Screenshot 2023-10-10 at 18 06 38"" src=""https://github.com/hail-is/hail/assets/106194/ee5df1c7-9c4a-4a4c-9ff4-caf599f1883b"">; <img width=""517"" alt=""Screenshot 2023-10-10 at 18 07 10"" src=""https://github.com/hail-is/hail/assets/106194/2bb5ba37-dab4-4b29-bb03-6cc2b08dafb9"">; Sync profiler (note safe point bias); <img width=""2032"" alt=""Screenshot 2023-10-10 at 18 32 14"" src=""https://github.com/hail-is/hail/assets/106194/85f1c1b6-3ac1-4b87-9e32-e6abdd02bb49"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13787#issuecomment-1756358633
Usability,clear,clear,"Seems to average 60MB/s. No clear culprits. Zstd decoding is the top hit right now. The hottest generated code is inplace decoding of an optional array of optional int32. Really sucks because things like `LA` are somehow getting written as element optional, even though, by construction their elements are not optional. ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:; +EArray[EBaseStruct{; LA:EArray[EInt32]; ,LGT:EInt32; ,LAD:EArray[EInt32]; ,LPGT:EInt32; ,LPL:EArray[EInt32]; ,RGQ:EInt32,; gvcf_info: EBaseStruct{; AC:EArray[EInt32]; ,AF:EArray[EFloat64]; ,AN:EInt32,AS_BaseQRankSum:EArray[EFloat64]; ,AS_FS:EArray[EFloat64]; ,AS_InbreedingCoeff:EArray[EFloat64]; ,AS_MQ:EArray[EFloat64]; ,AS_MQRankSum:EArray[EFloat64]; ,AS_QD:EArray[EFloat64]; ,AS_QUALapprox:EArray[EInt32]; ,AS_RAW_BaseQRankSum:EBinary,AS_RAW_MQ:EArray[EFloat64]; ,AS_RAW_MQRankSum:EArray[EBaseStruct{`0`:EFloat64,`1`:EInt32}]; ,AS_RAW_ReadPosRankSum:EArray[EBaseStruct{`0`:EFloat64,`1`:EInt32}]; ,AS_ReadPosRankSum:EArray[EFloat64]; ,AS_SB_TABLE:EArray[EArray[EInt32]]; ,AS_SOR:EArray[EFloat64]; ,AS_VarDP:EArray[EInt32]; ,BaseQRankSum:EFloat64,ExcessHet:EFloat64,FS:EFloat64,InbreedingCoeff:EFloat64,MQ:EFloat64,MQRankSum:EFloat64,MQ_DP:EInt32,QD:EFloat64,QUALapprox:EInt32,RAW_GT_COUNT:EArray[EInt32]; ,RAW_MQandDP:EArray[EInt32]; ,ReadPosRankSum:EFloat64,SOR:EFloat64,VarDP:EInt32}; ,DP:EInt32; ,GQ:EInt32; ,MIN_DP:EInt32; ,PID:EBinary; ,PS:EInt32; ,SB:EArray[EInt32]; }; ]; }; ```. Async profiler periodic sampling:; <img width=""2032"" alt=""Screenshot 2023-10-10 at 18 06 38"" src=""https://github.com/hail-is/hail/assets/106194/ee5df1c7-9c4a-4a4c-9ff4-caf599f1883b"">; <img width=""517"" alt=""Screenshot 2023-10-10 at 18 07 10"" src=""https://github.com/hail-is/hail/assets/106194/2bb5ba37-dab4-4b29-bb03-6cc2b08dafb9"">; Sync profiler (note safe point bias); <img width=""2032"" alt=""Screenshot 2023-10-10 at 18 32 14"" src=""https://github.com/hail-is/hail/assets/106194/85f1c1b6-3ac1-4b87-9e32-e6abdd02bb49"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13787#issuecomment-1756358633
Usability,simpl,simple,"So, here's a profile of the no-leb dataset. I applied all the simple tricks I could think of. We memcpy runs of present ints or floats in arrays. Patrick's improvements. A few other little cleanups. Read bandwidth with one core is ~60MB/s (of uncompressed bytes). Zstd is getting ~28% of our core. A comment from 2019 claimed [200 MB/s](https://github.com/luben/zstd-jni/issues/94#issuecomment-471114842) from the Zstd JNI library we're using. That roughly tracks (0.28 * 200 = 56). The native C library, on a 400MB/s link (roughly my SSD's link) claims closer to 1000 MB/s (of uncompressed bytes) (see [my sheet](https://docs.google.com/spreadsheets/d/1uDYfXWDIwt_-XiclFWdLDiXamVbxhswt_D-v8fVrlRU/edit#gid=0)). I suspect we need either different on-disk formats, different in-memory formats, or vectorized storage to substantially improve the speed. Reading all these tiny arrays is unfortunately really branchy. If we had a PType and EType for arrays that was memcpy-able, that might give us a big win? In particular, suppose we could determine the bytesize of a nested structure including arrays. We write that size into the stream, then write all the bytes. Decoding is: read size, read that many bytes, done. <img width=""1545"" alt=""Screenshot 2023-10-11 at 16 36 27"" src=""https://github.com/hail-is/hail/assets/106194/afaeeacc-ab07-4e2c-9638-8d007276333d"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13792#issuecomment-1758518398
Usability,feedback,feedback,"I did not fix the variant chunks or phenotypes classes to not be actual classes. But I would like your feedback on the rewritten `io.py`. It still might be overkill, but getting closer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13804#issuecomment-1781180710
Usability,simpl,simpler,"Hmm. What's special about `hl.concordance`... The other issue seems like it might be simpler, we're just setting a bad type on a field:; ```; E java.lang.ClassFormatError: Invalid signature for field in class __C8802Compiled referenced from constant pool index 1605 in method __C8802Compiled.addAndDecodeLiterals_region0_0(L__C9346addAndDecodeLiteralsSpills;)V; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1778262286
Usability,simpl,simple,"This seems very simple, I'm just not sure how to modify the job creation to do what we want it to. https://github.com/hail-is/hail/blob/86f21400e3f2ac127f084f15ab72594a6add6f15/hail/python/hailtop/batch/backend.py#L799-L807",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13816#issuecomment-1775716804
Deployability,update,updated,"There is occasional use of this in other projects, e.g., gnomad_methods (`SimpleRichProgressBar` in this case). Do you consider these classes to be part of the API? Was it intended to rename these without any compatibility shim, e.g., having the old names as aliases for a while?. It's not the end of the world and gnomad_methods has already updated accordingly. It does however mean that older gnomad_methods is only compatible with hail ≤ 0.2.125 and newer gnomad_methods is only compatible with hail ≥ 0.2.126, which is an otherwise unnecessary lock-step restriction. ETA: gnomad_methods have now updated by removing the (apparently unused) progress bar reference, so now newer gnomad_methods is compatible with hail both ≤ 0.2.125 and ≥ 0.2.126 again. So this is no longer a significant problem for gnomad_methods, but remains FYI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13832#issuecomment-1788106993
Usability,progress bar,progress bar,"There is occasional use of this in other projects, e.g., gnomad_methods (`SimpleRichProgressBar` in this case). Do you consider these classes to be part of the API? Was it intended to rename these without any compatibility shim, e.g., having the old names as aliases for a while?. It's not the end of the world and gnomad_methods has already updated accordingly. It does however mean that older gnomad_methods is only compatible with hail ≤ 0.2.125 and newer gnomad_methods is only compatible with hail ≥ 0.2.126, which is an otherwise unnecessary lock-step restriction. ETA: gnomad_methods have now updated by removing the (apparently unused) progress bar reference, so now newer gnomad_methods is compatible with hail both ≤ 0.2.125 and ≥ 0.2.126 again. So this is no longer a significant problem for gnomad_methods, but remains FYI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13832#issuecomment-1788106993
Availability,echo,echo,"@mhebrard. Just to be clear: *after* installing Hail, `/usr/bin/spark-shell --version` now shows `2.12.13` but `pip3 show pyspark` still shows ""Warning: Package(s) not found: pyspark""?. Is `/usr/bin/spark-shell` a symlink? What does it point to? Is `/usr/lib/spark` a symlink? Does it point to the same place? Actually, let's just check a bunch of things:; ```; ls -al /usr/bin/spark-shell; echo $(which spark-shell); ls -al $(which spark-shell); spark-shell --version. ls -al /usr/bin/spark-submit; echo $(which spark-submit); ls -al $(which spark-submit); spark-submit --version. ls -al /usr/bin/spark-class; echo $(which spark-class); ls -al $(which spark-class). echo SPARK_SCALA_VERSION=$SPARK_SCALA_VERSION. echo "">>>>>>>>>> before load-spark-env.sh <<<<<<<<<""; env. load-spark-env.sh. echo "">>>>>>>>>> after load-spark-env.sh <<<<<<<<<""; env. which scala; ls -al $(which scala); cat $(which scala); ```. And one more thing, can you edit `$(which spark-shell)` to add `set -x` then try `spark-shell` and see if there's anything mysterious?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1771296222
Deployability,install,installing,"@mhebrard. Just to be clear: *after* installing Hail, `/usr/bin/spark-shell --version` now shows `2.12.13` but `pip3 show pyspark` still shows ""Warning: Package(s) not found: pyspark""?. Is `/usr/bin/spark-shell` a symlink? What does it point to? Is `/usr/lib/spark` a symlink? Does it point to the same place? Actually, let's just check a bunch of things:; ```; ls -al /usr/bin/spark-shell; echo $(which spark-shell); ls -al $(which spark-shell); spark-shell --version. ls -al /usr/bin/spark-submit; echo $(which spark-submit); ls -al $(which spark-submit); spark-submit --version. ls -al /usr/bin/spark-class; echo $(which spark-class); ls -al $(which spark-class). echo SPARK_SCALA_VERSION=$SPARK_SCALA_VERSION. echo "">>>>>>>>>> before load-spark-env.sh <<<<<<<<<""; env. load-spark-env.sh. echo "">>>>>>>>>> after load-spark-env.sh <<<<<<<<<""; env. which scala; ls -al $(which scala); cat $(which scala); ```. And one more thing, can you edit `$(which spark-shell)` to add `set -x` then try `spark-shell` and see if there's anything mysterious?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1771296222
Performance,load,load-spark-env,"@mhebrard. Just to be clear: *after* installing Hail, `/usr/bin/spark-shell --version` now shows `2.12.13` but `pip3 show pyspark` still shows ""Warning: Package(s) not found: pyspark""?. Is `/usr/bin/spark-shell` a symlink? What does it point to? Is `/usr/lib/spark` a symlink? Does it point to the same place? Actually, let's just check a bunch of things:; ```; ls -al /usr/bin/spark-shell; echo $(which spark-shell); ls -al $(which spark-shell); spark-shell --version. ls -al /usr/bin/spark-submit; echo $(which spark-submit); ls -al $(which spark-submit); spark-submit --version. ls -al /usr/bin/spark-class; echo $(which spark-class); ls -al $(which spark-class). echo SPARK_SCALA_VERSION=$SPARK_SCALA_VERSION. echo "">>>>>>>>>> before load-spark-env.sh <<<<<<<<<""; env. load-spark-env.sh. echo "">>>>>>>>>> after load-spark-env.sh <<<<<<<<<""; env. which scala; ls -al $(which scala); cat $(which scala); ```. And one more thing, can you edit `$(which spark-shell)` to add `set -x` then try `spark-shell` and see if there's anything mysterious?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1771296222
Usability,clear,clear,"@mhebrard. Just to be clear: *after* installing Hail, `/usr/bin/spark-shell --version` now shows `2.12.13` but `pip3 show pyspark` still shows ""Warning: Package(s) not found: pyspark""?. Is `/usr/bin/spark-shell` a symlink? What does it point to? Is `/usr/lib/spark` a symlink? Does it point to the same place? Actually, let's just check a bunch of things:; ```; ls -al /usr/bin/spark-shell; echo $(which spark-shell); ls -al $(which spark-shell); spark-shell --version. ls -al /usr/bin/spark-submit; echo $(which spark-submit); ls -al $(which spark-submit); spark-submit --version. ls -al /usr/bin/spark-class; echo $(which spark-class); ls -al $(which spark-class). echo SPARK_SCALA_VERSION=$SPARK_SCALA_VERSION. echo "">>>>>>>>>> before load-spark-env.sh <<<<<<<<<""; env. load-spark-env.sh. echo "">>>>>>>>>> after load-spark-env.sh <<<<<<<<<""; env. which scala; ls -al $(which scala); cat $(which scala); ```. And one more thing, can you edit `$(which spark-shell)` to add `set -x` then try `spark-shell` and see if there's anything mysterious?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1771296222
Performance,optimiz,optimization,"One obvious point of optimization that Dan already identified is the way we were keying our data is causing full shuffles, as we were keying the data by a string variant ID in the form `1-32683987-ACTCTT-A` instead of locus-allele. Changing back to keying on locus-allele fixes this issue for our more straightforward searches, but we have a search that looks for pairs of possible compound heterozygous variants in the same gene, and that still is resulting in 2 full shuffles. I'm a little at a loss for how to fix this, because we are grouping by an unsorted field so I'm not sure how to prevent us from working with an unsorted dataset. The offending code right now is as follows (somewhat simplified for readability):. ```; primary_variants = hl.agg.filter(ch_ht[HAS_ALLOWED_ANNOTATION], hl.agg.collect(ch_ht.row)); secondary_variants = hl.agg.filter(ch_ht[HAS_ALLOWED_SECONDARY_ANNOTATION], hl.agg.collect(ch_ht.row)); ch_ht = ch_ht.group_by('gene_ids').aggregate(v1=primary_variants, v2=secondary_variants); ch_ht = ch_ht.explode(ch_ht.v1); ch_ht = ch_ht.explode(ch_ht.v2); ch_ht = ch_ht.annotate(grouped_variants=hl.sorted([ch_ht.v1, ch_ht.v2], key=lambda v: (v.locus, v.alleles))); ch_ht = ch_ht.key_by(; locus=ch_ht.grouped_variants[0].locus, ; alleles=ch_ht.grouped_variants[0].alleles,; locus2=ch_ht.grouped_variants[1].locus, ; alleles2=ch_ht.grouped_variants[1].alleles,; ); ch_ht = ch_ht.distinct(); ...; # more filtering and annotating; ...; return ch_ht._key_by_assert_sorted('locus', 'alleles'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1776044401
Usability,simpl,simplified,"One obvious point of optimization that Dan already identified is the way we were keying our data is causing full shuffles, as we were keying the data by a string variant ID in the form `1-32683987-ACTCTT-A` instead of locus-allele. Changing back to keying on locus-allele fixes this issue for our more straightforward searches, but we have a search that looks for pairs of possible compound heterozygous variants in the same gene, and that still is resulting in 2 full shuffles. I'm a little at a loss for how to fix this, because we are grouping by an unsorted field so I'm not sure how to prevent us from working with an unsorted dataset. The offending code right now is as follows (somewhat simplified for readability):. ```; primary_variants = hl.agg.filter(ch_ht[HAS_ALLOWED_ANNOTATION], hl.agg.collect(ch_ht.row)); secondary_variants = hl.agg.filter(ch_ht[HAS_ALLOWED_SECONDARY_ANNOTATION], hl.agg.collect(ch_ht.row)); ch_ht = ch_ht.group_by('gene_ids').aggregate(v1=primary_variants, v2=secondary_variants); ch_ht = ch_ht.explode(ch_ht.v1); ch_ht = ch_ht.explode(ch_ht.v2); ch_ht = ch_ht.annotate(grouped_variants=hl.sorted([ch_ht.v1, ch_ht.v2], key=lambda v: (v.locus, v.alleles))); ch_ht = ch_ht.key_by(; locus=ch_ht.grouped_variants[0].locus, ; alleles=ch_ht.grouped_variants[0].alleles,; locus2=ch_ht.grouped_variants[1].locus, ; alleles2=ch_ht.grouped_variants[1].alleles,; ); ch_ht = ch_ht.distinct(); ...; # more filtering and annotating; ...; return ch_ht._key_by_assert_sorted('locus', 'alleles'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1776044401
Usability,simpl,simple,"Oh I see. Thanks for clarifying - I wasn't sure what that bit did! That should be a simple fix, though perhaps at this point not worth it as this is not a fruitful optimisation for this query",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1830665552
Usability,feedback,feedback,"Next steps:; 1. upload the profile, the `mt.describe()`, metadata.json.gz from the MT/HT to the team chat and get feedback (Chris, Patrick take a look). Decode appears quite slow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1839225404
Availability,avail,available,"> That should be a simple fix, though perhaps at this point not worth it as this is not a fruitful optimization for this query. Agreed, although depending on the time line for a good optimization for this query I may circle back on this, as there is currently a bug in this deduplication so if the actual optimizaion won;t be available for a while it might be worth fixing in the meantime",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1839244525
Integrability,depend,depending,"> That should be a simple fix, though perhaps at this point not worth it as this is not a fruitful optimization for this query. Agreed, although depending on the time line for a good optimization for this query I may circle back on this, as there is currently a bug in this deduplication so if the actual optimizaion won;t be available for a while it might be worth fixing in the meantime",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1839244525
Performance,optimiz,optimization,"> That should be a simple fix, though perhaps at this point not worth it as this is not a fruitful optimization for this query. Agreed, although depending on the time line for a good optimization for this query I may circle back on this, as there is currently a bug in this deduplication so if the actual optimizaion won;t be available for a while it might be worth fixing in the meantime",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1839244525
Usability,simpl,simple,"> That should be a simple fix, though perhaps at this point not worth it as this is not a fruitful optimization for this query. Agreed, although depending on the time line for a good optimization for this query I may circle back on this, as there is currently a bug in this deduplication so if the actual optimizaion won;t be available for a while it might be worth fixing in the meantime",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1839244525
Usability,resume,resume,"Sorry, I've been on my honeymoon and just got back. I resume work on this in the new year. Apologies for the delay.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1866888177
Deployability,pipeline,pipeline,"I'm not sure whether we should add this proactively or not, and to be clear I don't intend users to generally use this, but this is the best solution I can think of so far for @illusional's question about what to do when we have removed support for the old hail access tokens but users still are forced to run a pipeline on an old hail version. Old hail access tokens are stored in JSON in `~/.hail/tokens.json`, so I believe (though have not yet tested, that the following should allow a user to use an old version of hail against a version of batch that only supports cloud access tokens:. On the *new* version of hail, run. ```; hailctl auth login; hailctl auth print-access-token | jq -R -c '{ default: . }' > ~/.hail/tokens.json; ```. Then switch to an old version and proceed as usual (but don't run `hailctl auth login`!).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1783136528
Security,access,access,"I'm not sure whether we should add this proactively or not, and to be clear I don't intend users to generally use this, but this is the best solution I can think of so far for @illusional's question about what to do when we have removed support for the old hail access tokens but users still are forced to run a pipeline on an old hail version. Old hail access tokens are stored in JSON in `~/.hail/tokens.json`, so I believe (though have not yet tested, that the following should allow a user to use an old version of hail against a version of batch that only supports cloud access tokens:. On the *new* version of hail, run. ```; hailctl auth login; hailctl auth print-access-token | jq -R -c '{ default: . }' > ~/.hail/tokens.json; ```. Then switch to an old version and proceed as usual (but don't run `hailctl auth login`!).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1783136528
Testability,test,tested,"I'm not sure whether we should add this proactively or not, and to be clear I don't intend users to generally use this, but this is the best solution I can think of so far for @illusional's question about what to do when we have removed support for the old hail access tokens but users still are forced to run a pipeline on an old hail version. Old hail access tokens are stored in JSON in `~/.hail/tokens.json`, so I believe (though have not yet tested, that the following should allow a user to use an old version of hail against a version of batch that only supports cloud access tokens:. On the *new* version of hail, run. ```; hailctl auth login; hailctl auth print-access-token | jq -R -c '{ default: . }' > ~/.hail/tokens.json; ```. Then switch to an old version and proceed as usual (but don't run `hailctl auth login`!).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1783136528
Usability,clear,clear,"I'm not sure whether we should add this proactively or not, and to be clear I don't intend users to generally use this, but this is the best solution I can think of so far for @illusional's question about what to do when we have removed support for the old hail access tokens but users still are forced to run a pipeline on an old hail version. Old hail access tokens are stored in JSON in `~/.hail/tokens.json`, so I believe (though have not yet tested, that the following should allow a user to use an old version of hail against a version of batch that only supports cloud access tokens:. On the *new* version of hail, run. ```; hailctl auth login; hailctl auth print-access-token | jq -R -c '{ default: . }' > ~/.hail/tokens.json; ```. Then switch to an old version and proceed as usual (but don't run `hailctl auth login`!).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1783136528
Usability,simpl,simpler,"This is a lot simpler, I'll try to run this tomorrow morning.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1803234894
Availability,failure,failure,"crossposting from a message I sent to the variants team. ---. #### executive summary. Excess JVM memory use is almost certainly not the issue. I've taken a close look at the import_gvs.py loop and the related Hail Python code. No obvious accumulation of RAM use. AFAICT, the oomkiller keeps killing the pipelines. We need to stop this because the oomkiller (a) acts before the JVM GC can free things and (b) prevents us from getting JVM diagnostics on failure. We control the JVM's max heap with hailctl's --master-memory-fraction (default is 0.8 for 80% of the master machine type's advertised RAM). I suggest we set this down to 0.6 and continue using an n1-highmem-16 driver.; If Hail is (incorrectly) accumulating garbage memory per-group, we'll have a better chance diagnosing that with a running JVM instead of one that's been SIGKILL'ed. To understand what's going on, we gotta see what is using RAM in the n1-highmem-16 case. If I could SSH to the cluster, a simple solution is a screen with top -s 300 -n 100 >memory.log (I'd guess no more than 500KiB per hour of logs) and retrieve that file if the cluster fails. If we could get Google Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the drive",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449
Deployability,pipeline,pipelines,"crossposting from a message I sent to the variants team. ---. #### executive summary. Excess JVM memory use is almost certainly not the issue. I've taken a close look at the import_gvs.py loop and the related Hail Python code. No obvious accumulation of RAM use. AFAICT, the oomkiller keeps killing the pipelines. We need to stop this because the oomkiller (a) acts before the JVM GC can free things and (b) prevents us from getting JVM diagnostics on failure. We control the JVM's max heap with hailctl's --master-memory-fraction (default is 0.8 for 80% of the master machine type's advertised RAM). I suggest we set this down to 0.6 and continue using an n1-highmem-16 driver.; If Hail is (incorrectly) accumulating garbage memory per-group, we'll have a better chance diagnosing that with a running JVM instead of one that's been SIGKILL'ed. To understand what's going on, we gotta see what is using RAM in the n1-highmem-16 case. If I could SSH to the cluster, a simple solution is a screen with top -s 300 -n 100 >memory.log (I'd guess no more than 500KiB per hour of logs) and retrieve that file if the cluster fails. If we could get Google Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the drive",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449
Energy Efficiency,allocate,allocated,"ogle Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the driver was killed by the system.; Let's focus on the driver machines. In Run A, we used an n1-highmem-8 which is advertised to have 52GiB (53248 MiB). In Run B, we used an n1-highmem-16 which is advertised to have 104GiB (106,496 MiB). hailctl sets the JVM max heap size to 80% of the advertised RAM, so 42598 MiB (see hailctl's --master-memory-fraction). In Run A (the only run for which we have syslogs), based on the driver's syslog, before Spark starts, the system has already allocated 8500 MiB to Linux/Google/Dataproc daemons. Moreover, the actual RAM of the system (as reported by the earlyoom daemon) is 52223 MiB (51 GiB, 1GiB less than Google advertises for n1-highmem-8). Assuming these daemons never release their memory, all our user code must fit in 43723 MiB. Since the JVM's max heap is 42598 MiB, Python (and indeed, anything else on the system) is limited to allocating 1125 MiB. I assume that an n1-highmem-16 uses the same amount of memory for system daemons, so I'd expect just over ten GiB that is used neither by system daemons nor the JVM. Assuming that's right, I can't explain why the oomkiller killed the JVM in Run B.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449
Integrability,message,message,"crossposting from a message I sent to the variants team. ---. #### executive summary. Excess JVM memory use is almost certainly not the issue. I've taken a close look at the import_gvs.py loop and the related Hail Python code. No obvious accumulation of RAM use. AFAICT, the oomkiller keeps killing the pipelines. We need to stop this because the oomkiller (a) acts before the JVM GC can free things and (b) prevents us from getting JVM diagnostics on failure. We control the JVM's max heap with hailctl's --master-memory-fraction (default is 0.8 for 80% of the master machine type's advertised RAM). I suggest we set this down to 0.6 and continue using an n1-highmem-16 driver.; If Hail is (incorrectly) accumulating garbage memory per-group, we'll have a better chance diagnosing that with a running JVM instead of one that's been SIGKILL'ed. To understand what's going on, we gotta see what is using RAM in the n1-highmem-16 case. If I could SSH to the cluster, a simple solution is a screen with top -s 300 -n 100 >memory.log (I'd guess no more than 500KiB per hour of logs) and retrieve that file if the cluster fails. If we could get Google Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the drive",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449
Testability,log,log,"ve summary. Excess JVM memory use is almost certainly not the issue. I've taken a close look at the import_gvs.py loop and the related Hail Python code. No obvious accumulation of RAM use. AFAICT, the oomkiller keeps killing the pipelines. We need to stop this because the oomkiller (a) acts before the JVM GC can free things and (b) prevents us from getting JVM diagnostics on failure. We control the JVM's max heap with hailctl's --master-memory-fraction (default is 0.8 for 80% of the master machine type's advertised RAM). I suggest we set this down to 0.6 and continue using an n1-highmem-16 driver.; If Hail is (incorrectly) accumulating garbage memory per-group, we'll have a better chance diagnosing that with a running JVM instead of one that's been SIGKILL'ed. To understand what's going on, we gotta see what is using RAM in the n1-highmem-16 case. If I could SSH to the cluster, a simple solution is a screen with top -s 300 -n 100 >memory.log (I'd guess no more than 500KiB per hour of logs) and retrieve that file if the cluster fails. If we could get Google Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the driver was killed by the system.; Let's focus on the driver machines. In Run A,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449
Usability,simpl,simple,"crossposting from a message I sent to the variants team. ---. #### executive summary. Excess JVM memory use is almost certainly not the issue. I've taken a close look at the import_gvs.py loop and the related Hail Python code. No obvious accumulation of RAM use. AFAICT, the oomkiller keeps killing the pipelines. We need to stop this because the oomkiller (a) acts before the JVM GC can free things and (b) prevents us from getting JVM diagnostics on failure. We control the JVM's max heap with hailctl's --master-memory-fraction (default is 0.8 for 80% of the master machine type's advertised RAM). I suggest we set this down to 0.6 and continue using an n1-highmem-16 driver.; If Hail is (incorrectly) accumulating garbage memory per-group, we'll have a better chance diagnosing that with a running JVM instead of one that's been SIGKILL'ed. To understand what's going on, we gotta see what is using RAM in the n1-highmem-16 case. If I could SSH to the cluster, a simple solution is a screen with top -s 300 -n 100 >memory.log (I'd guess no more than 500KiB per hour of logs) and retrieve that file if the cluster fails. If we could get Google Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the drive",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449
Availability,down,down,"I'm fairly certain I know understand this and the AoU VDS creation issue. In Dataproc versions 1.5.74, 2.0.48, and 2.1.0, Dataproc introduced ""memory protection"" which is a euphemism for a newly aggressive OOMKiller. When the OOMKiller kills the JVM driver process, there is no hs_err_pid...log file, no exceptional log statements, and no clean shutdown of any sockets. The process is simply SIGTERM'ed and then SIGKILL'ed. From Hail 0.2.83 through Hail 0.2.109 (released February 2023), Hail was pinned to Dataproc 2.0.44. From Hail 0.2.15 onwards, `hailctl dataproc`, by default, reserves 80% of the advertised memory of the driver node for the use of the Hail Query Driver JVM process. For example, Google advertises that an n1-highmem-8 has 52 GiB of RAM, so Hail sets the `spark:spark.driver.memory` property to `41g` (we always round down). Before aggressive memory protection, this setting was sufficient to protect the driver from starving itself of memory. Unfortunately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790
Deployability,release,released,"I'm fairly certain I know understand this and the AoU VDS creation issue. In Dataproc versions 1.5.74, 2.0.48, and 2.1.0, Dataproc introduced ""memory protection"" which is a euphemism for a newly aggressive OOMKiller. When the OOMKiller kills the JVM driver process, there is no hs_err_pid...log file, no exceptional log statements, and no clean shutdown of any sockets. The process is simply SIGTERM'ed and then SIGKILL'ed. From Hail 0.2.83 through Hail 0.2.109 (released February 2023), Hail was pinned to Dataproc 2.0.44. From Hail 0.2.15 onwards, `hailctl dataproc`, by default, reserves 80% of the advertised memory of the driver node for the use of the Hail Query Driver JVM process. For example, Google advertises that an n1-highmem-8 has 52 GiB of RAM, so Hail sets the `spark:spark.driver.memory` property to `41g` (we always round down). Before aggressive memory protection, this setting was sufficient to protect the driver from starving itself of memory. Unfortunately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790
Energy Efficiency,allocate,allocates,"1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: All done; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. Notice:; 1. The total memory available on the machine is less than 52 GiB (= 53,248 MiB), indeed it is a full 1025 MiB below the advertised amount.; 2. Once all the components of the Dataproc cluster have started (but before any Hail Query jobs are submitted) the total memory available is already depleted to 42760 MiB. Recall that Hail allocates 41 GiB (= 41,984 MiB) to its JVM. This leaves the Python process and all other daemons on the system only 776 MiB of excess RAM. For reference `python3 -c 'import hail'` needs 206 MiB. ---. We must address this situation. It seems safe to assume that the system daemons will use a constant 9.5 GiB of RAM. Moreover the advertised RAM amount is at least 1 GiB larger than reality. I propose:; 1. The driver memory calculation in `hailctl dataproc` should take the advertised RAM amount, subtract 10.5 GiB, and then use 90% of the remaining value. For an n1-highmem-8, that reduces our allocation from 41 GiB to 37 GiB yielding an additional 4GiB to Python and deamon memory fluctuations.; 2. AoU RWB needs to review its memory settings for Spark driver nodes to ensure that the JVM is set to an appropriate maximum heap size. For what it's worth, I think the reason we didn't get an outcry from our local scientific community is that many of them have transitioned to Query-on-Batch where we have exact a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790
Safety,safe,safe,"al: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: All done; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. Notice:; 1. The total memory available on the machine is less than 52 GiB (= 53,248 MiB), indeed it is a full 1025 MiB below the advertised amount.; 2. Once all the components of the Dataproc cluster have started (but before any Hail Query jobs are submitted) the total memory available is already depleted to 42760 MiB. Recall that Hail allocates 41 GiB (= 41,984 MiB) to its JVM. This leaves the Python process and all other daemons on the system only 776 MiB of excess RAM. For reference `python3 -c 'import hail'` needs 206 MiB. ---. We must address this situation. It seems safe to assume that the system daemons will use a constant 9.5 GiB of RAM. Moreover the advertised RAM amount is at least 1 GiB larger than reality. I propose:; 1. The driver memory calculation in `hailctl dataproc` should take the advertised RAM amount, subtract 10.5 GiB, and then use 90% of the remaining value. For an n1-highmem-8, that reduces our allocation from 41 GiB to 37 GiB yielding an additional 4GiB to Python and deamon memory fluctuations.; 2. AoU RWB needs to review its memory settings for Spark driver nodes to ensure that the JVM is set to an appropriate maximum heap size. For what it's worth, I think the reason we didn't get an outcry from our local scientific community is that many of them have transitioned to Query-on-Batch where we have exact and total control over the memory available to the driver and the workers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790
Testability,log,log,"I'm fairly certain I know understand this and the AoU VDS creation issue. In Dataproc versions 1.5.74, 2.0.48, and 2.1.0, Dataproc introduced ""memory protection"" which is a euphemism for a newly aggressive OOMKiller. When the OOMKiller kills the JVM driver process, there is no hs_err_pid...log file, no exceptional log statements, and no clean shutdown of any sockets. The process is simply SIGTERM'ed and then SIGKILL'ed. From Hail 0.2.83 through Hail 0.2.109 (released February 2023), Hail was pinned to Dataproc 2.0.44. From Hail 0.2.15 onwards, `hailctl dataproc`, by default, reserves 80% of the advertised memory of the driver node for the use of the Hail Query Driver JVM process. For example, Google advertises that an n1-highmem-8 has 52 GiB of RAM, so Hail sets the `spark:spark.driver.memory` property to `41g` (we always round down). Before aggressive memory protection, this setting was sufficient to protect the driver from starving itself of memory. Unfortunately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790
Usability,simpl,simply,"I'm fairly certain I know understand this and the AoU VDS creation issue. In Dataproc versions 1.5.74, 2.0.48, and 2.1.0, Dataproc introduced ""memory protection"" which is a euphemism for a newly aggressive OOMKiller. When the OOMKiller kills the JVM driver process, there is no hs_err_pid...log file, no exceptional log statements, and no clean shutdown of any sockets. The process is simply SIGTERM'ed and then SIGKILL'ed. From Hail 0.2.83 through Hail 0.2.109 (released February 2023), Hail was pinned to Dataproc 2.0.44. From Hail 0.2.15 onwards, `hailctl dataproc`, by default, reserves 80% of the advertised memory of the driver node for the use of the Hail Query Driver JVM process. For example, Google advertises that an n1-highmem-8 has 52 GiB of RAM, so Hail sets the `spark:spark.driver.memory` property to `41g` (we always round down). Before aggressive memory protection, this setting was sufficient to protect the driver from starving itself of memory. Unfortunately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790
Availability,failure,failure,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385
Energy Efficiency,allocate,allocated,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385
Integrability,message,message,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385
Performance,cache,cache,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385
Security,hash,hash,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385
Testability,log,log,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385
Usability,learn,learn,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385
Availability,down,downloaded,"OK, so, I took the GRCh38 file that we test against and named it `bar`. I downloaded the gist and named it `foo`. | header | footer | success? |; |---|---|---|; |foo|bar|success|; |bar|bar|success|; |bar|foo|failure|; |foo|foo|failure|. So clearly the issue is the variants. Here's an example of running on just the first handful of variants: https://batch.hail.is/batches/8089052. ```; chr1	1339585	.	G	A	.	.	.; chr1	24907372	.	C	T	.	.	.; chr1	36859143	.	G	T	.	.	.; chr1	37969436	.	T	C	.	.	.; chr1	40416828	.	G	A	.	.	.; chr1	41581842	.	G	A	.	.	.; chr1	43920822	.	T	C	.	.	.; chr1	45327881	.	G	A	.	.	.; chr1	46817055	.	CT	C	.	.	.; chr1	54999203	.	C	T	.	.	.; chr1	65218884	.	C	T	.	.	.; chr1	102962250	.	G	T	.	.	.; chr1	111756087	.	G	C	.	.	.; chr1	113881802	.	G	A	.	.	.; chr1	117920205	.	G	A	.	.	.; chr1	151408784	.	G	C	.	.	.; chr1	151428261	.	C	T	.	.	.; chr1	152305539	.	G	C	.	.	.; chr1	152884596	.	C	A	.	.	.; chr1	153933240	.	C	T	.	.	.; chr1	156624012	.	G	A	.	.	.; chr1	159205821	.	CT	C	.	.	.; chr1	173803162	.	G	T	.	.	.; chr1	179813831	.	G	A	.	.	.; chr1	179917551	.	T	C	.	.	.; chr1	180935962	.	G	C	.	.	.; chr1	180941229	.	G	A	.	.	.; chr1	186893053	.	C	A	.	.	.; chr1	201363319	.	G	A	.	.	.; chr1	223749094	.	A	G	.	.	.; chr1	224294328	.	G	A	.	.	.; chr1	235809337	.	G	A	.	.	.; chr1	241592073	.	G	T	.	.	.; chr2	9376947	.	G	A	.	.	.; chr2	11618532	.	C	T	.	.	.; ```. We can see the characteristic super high memory use.; <img width=""570"" alt=""Screenshot 2023-11-28 at 16 35 26"" src=""https://github.com/hail-is/hail/assets/106194/e5dfa586-5c77-479b-8050-9b0b7d2fe319"">. ---. If we use the same header, but just one variant, it succeeds, but notice that the RAM use grows rapidly. https://batch.hail.is/batches/8089064/jobs/3; ```; chr1	241592073	.	G	T	.	.	.; ```; <img width=""577"" alt=""Screenshot 2023-11-28 at 16 37 39"" src=""https://github.com/hail-is/hail/assets/106194/90c5ab45-9ca4-43e0-9a97-bf6032863f32"">. ---. If we use the same header with this variant from our (successful) test VCF, the RAM use grows",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830846344
Testability,test,test,"OK, so, I took the GRCh38 file that we test against and named it `bar`. I downloaded the gist and named it `foo`. | header | footer | success? |; |---|---|---|; |foo|bar|success|; |bar|bar|success|; |bar|foo|failure|; |foo|foo|failure|. So clearly the issue is the variants. Here's an example of running on just the first handful of variants: https://batch.hail.is/batches/8089052. ```; chr1	1339585	.	G	A	.	.	.; chr1	24907372	.	C	T	.	.	.; chr1	36859143	.	G	T	.	.	.; chr1	37969436	.	T	C	.	.	.; chr1	40416828	.	G	A	.	.	.; chr1	41581842	.	G	A	.	.	.; chr1	43920822	.	T	C	.	.	.; chr1	45327881	.	G	A	.	.	.; chr1	46817055	.	CT	C	.	.	.; chr1	54999203	.	C	T	.	.	.; chr1	65218884	.	C	T	.	.	.; chr1	102962250	.	G	T	.	.	.; chr1	111756087	.	G	C	.	.	.; chr1	113881802	.	G	A	.	.	.; chr1	117920205	.	G	A	.	.	.; chr1	151408784	.	G	C	.	.	.; chr1	151428261	.	C	T	.	.	.; chr1	152305539	.	G	C	.	.	.; chr1	152884596	.	C	A	.	.	.; chr1	153933240	.	C	T	.	.	.; chr1	156624012	.	G	A	.	.	.; chr1	159205821	.	CT	C	.	.	.; chr1	173803162	.	G	T	.	.	.; chr1	179813831	.	G	A	.	.	.; chr1	179917551	.	T	C	.	.	.; chr1	180935962	.	G	C	.	.	.; chr1	180941229	.	G	A	.	.	.; chr1	186893053	.	C	A	.	.	.; chr1	201363319	.	G	A	.	.	.; chr1	223749094	.	A	G	.	.	.; chr1	224294328	.	G	A	.	.	.; chr1	235809337	.	G	A	.	.	.; chr1	241592073	.	G	T	.	.	.; chr2	9376947	.	G	A	.	.	.; chr2	11618532	.	C	T	.	.	.; ```. We can see the characteristic super high memory use.; <img width=""570"" alt=""Screenshot 2023-11-28 at 16 35 26"" src=""https://github.com/hail-is/hail/assets/106194/e5dfa586-5c77-479b-8050-9b0b7d2fe319"">. ---. If we use the same header, but just one variant, it succeeds, but notice that the RAM use grows rapidly. https://batch.hail.is/batches/8089064/jobs/3; ```; chr1	241592073	.	G	T	.	.	.; ```; <img width=""577"" alt=""Screenshot 2023-11-28 at 16 37 39"" src=""https://github.com/hail-is/hail/assets/106194/90c5ab45-9ca4-43e0-9a97-bf6032863f32"">. ---. If we use the same header with this variant from our (successful) test VCF, the RAM use grows",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830846344
Usability,clear,clearly,"OK, so, I took the GRCh38 file that we test against and named it `bar`. I downloaded the gist and named it `foo`. | header | footer | success? |; |---|---|---|; |foo|bar|success|; |bar|bar|success|; |bar|foo|failure|; |foo|foo|failure|. So clearly the issue is the variants. Here's an example of running on just the first handful of variants: https://batch.hail.is/batches/8089052. ```; chr1	1339585	.	G	A	.	.	.; chr1	24907372	.	C	T	.	.	.; chr1	36859143	.	G	T	.	.	.; chr1	37969436	.	T	C	.	.	.; chr1	40416828	.	G	A	.	.	.; chr1	41581842	.	G	A	.	.	.; chr1	43920822	.	T	C	.	.	.; chr1	45327881	.	G	A	.	.	.; chr1	46817055	.	CT	C	.	.	.; chr1	54999203	.	C	T	.	.	.; chr1	65218884	.	C	T	.	.	.; chr1	102962250	.	G	T	.	.	.; chr1	111756087	.	G	C	.	.	.; chr1	113881802	.	G	A	.	.	.; chr1	117920205	.	G	A	.	.	.; chr1	151408784	.	G	C	.	.	.; chr1	151428261	.	C	T	.	.	.; chr1	152305539	.	G	C	.	.	.; chr1	152884596	.	C	A	.	.	.; chr1	153933240	.	C	T	.	.	.; chr1	156624012	.	G	A	.	.	.; chr1	159205821	.	CT	C	.	.	.; chr1	173803162	.	G	T	.	.	.; chr1	179813831	.	G	A	.	.	.; chr1	179917551	.	T	C	.	.	.; chr1	180935962	.	G	C	.	.	.; chr1	180941229	.	G	A	.	.	.; chr1	186893053	.	C	A	.	.	.; chr1	201363319	.	G	A	.	.	.; chr1	223749094	.	A	G	.	.	.; chr1	224294328	.	G	A	.	.	.; chr1	235809337	.	G	A	.	.	.; chr1	241592073	.	G	T	.	.	.; chr2	9376947	.	G	A	.	.	.; chr2	11618532	.	C	T	.	.	.; ```. We can see the characteristic super high memory use.; <img width=""570"" alt=""Screenshot 2023-11-28 at 16 35 26"" src=""https://github.com/hail-is/hail/assets/106194/e5dfa586-5c77-479b-8050-9b0b7d2fe319"">. ---. If we use the same header, but just one variant, it succeeds, but notice that the RAM use grows rapidly. https://batch.hail.is/batches/8089064/jobs/3; ```; chr1	241592073	.	G	T	.	.	.; ```; <img width=""577"" alt=""Screenshot 2023-11-28 at 16 37 39"" src=""https://github.com/hail-is/hail/assets/106194/90c5ab45-9ca4-43e0-9a97-bf6032863f32"">. ---. If we use the same header with this variant from our (successful) test VCF, the RAM use grows",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830846344
Availability,down,download,"@jigold OK, so here's the summary of what I learned:. We don't have tabix files for GRCh38 and we only test on small positions. Many large positions without tabix files seems to cause a problem for VEP (and make it slow, unsurprisingly). Fix seems to be to download the *indexed* homo_sapiens cache https://ftp.ensembl.org/pub/release-95/variation/indexed_vep_cache/ and upload that to our QoB VEP bucket. I presume you copied from the data we use in Dataproc? If yes, we should update that to also have tabix files. Also, in Dataproc, we use highmem machines for VEP. We should change _service_vep to also use highmem machines. <details><summary>Listing the tabix files for GRCh38 and GRCh37</summary>. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch38-us-central1/homo_sapiens/95_GRCh38/\*/\*.tbi; CommandException: One or more URLs matched no objects.; ```. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/\*/\*.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/1/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/10/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/11/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/12/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/13/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/14/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/15/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/16/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/17/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/18/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/19/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/2/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830868145
Deployability,release,release-,"@jigold OK, so here's the summary of what I learned:. We don't have tabix files for GRCh38 and we only test on small positions. Many large positions without tabix files seems to cause a problem for VEP (and make it slow, unsurprisingly). Fix seems to be to download the *indexed* homo_sapiens cache https://ftp.ensembl.org/pub/release-95/variation/indexed_vep_cache/ and upload that to our QoB VEP bucket. I presume you copied from the data we use in Dataproc? If yes, we should update that to also have tabix files. Also, in Dataproc, we use highmem machines for VEP. We should change _service_vep to also use highmem machines. <details><summary>Listing the tabix files for GRCh38 and GRCh37</summary>. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch38-us-central1/homo_sapiens/95_GRCh38/\*/\*.tbi; CommandException: One or more URLs matched no objects.; ```. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/\*/\*.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/1/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/10/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/11/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/12/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/13/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/14/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/15/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/16/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/17/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/18/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/19/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/2/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830868145
Performance,cache,cache,"@jigold OK, so here's the summary of what I learned:. We don't have tabix files for GRCh38 and we only test on small positions. Many large positions without tabix files seems to cause a problem for VEP (and make it slow, unsurprisingly). Fix seems to be to download the *indexed* homo_sapiens cache https://ftp.ensembl.org/pub/release-95/variation/indexed_vep_cache/ and upload that to our QoB VEP bucket. I presume you copied from the data we use in Dataproc? If yes, we should update that to also have tabix files. Also, in Dataproc, we use highmem machines for VEP. We should change _service_vep to also use highmem machines. <details><summary>Listing the tabix files for GRCh38 and GRCh37</summary>. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch38-us-central1/homo_sapiens/95_GRCh38/\*/\*.tbi; CommandException: One or more URLs matched no objects.; ```. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/\*/\*.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/1/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/10/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/11/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/12/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/13/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/14/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/15/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/16/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/17/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/18/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/19/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/2/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830868145
Testability,test,test,"@jigold OK, so here's the summary of what I learned:. We don't have tabix files for GRCh38 and we only test on small positions. Many large positions without tabix files seems to cause a problem for VEP (and make it slow, unsurprisingly). Fix seems to be to download the *indexed* homo_sapiens cache https://ftp.ensembl.org/pub/release-95/variation/indexed_vep_cache/ and upload that to our QoB VEP bucket. I presume you copied from the data we use in Dataproc? If yes, we should update that to also have tabix files. Also, in Dataproc, we use highmem machines for VEP. We should change _service_vep to also use highmem machines. <details><summary>Listing the tabix files for GRCh38 and GRCh37</summary>. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch38-us-central1/homo_sapiens/95_GRCh38/\*/\*.tbi; CommandException: One or more URLs matched no objects.; ```. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/\*/\*.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/1/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/10/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/11/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/12/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/13/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/14/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/15/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/16/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/17/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/18/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/19/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/2/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830868145
Usability,learn,learned,"@jigold OK, so here's the summary of what I learned:. We don't have tabix files for GRCh38 and we only test on small positions. Many large positions without tabix files seems to cause a problem for VEP (and make it slow, unsurprisingly). Fix seems to be to download the *indexed* homo_sapiens cache https://ftp.ensembl.org/pub/release-95/variation/indexed_vep_cache/ and upload that to our QoB VEP bucket. I presume you copied from the data we use in Dataproc? If yes, we should update that to also have tabix files. Also, in Dataproc, we use highmem machines for VEP. We should change _service_vep to also use highmem machines. <details><summary>Listing the tabix files for GRCh38 and GRCh37</summary>. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch38-us-central1/homo_sapiens/95_GRCh38/\*/\*.tbi; CommandException: One or more URLs matched no objects.; ```. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/\*/\*.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/1/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/10/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/11/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/12/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/13/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/14/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/15/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/16/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/17/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/18/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/19/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/2/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830868145
Modifiability,rewrite,rewrite,"> This has significantly improved the simplicity of the parser, so much so that much of the logic therein could be simplified further, though I think that's beyond the scope of this change. Agreed. But as my follow up will be a complete rewrite of the parser, I definitely don't want to do more to simplify the current one. > I like the separation of type-checking and parsing, however I'd prefer in your implementations of `typecheck` that you assert one thing at a time. That way when things fail, it'll be clear which assertion was fired (ie if `(a && b && c)` fails, you don't know if it's `a` or `b` or `c`, whereas; > ; > ```scala; > assert(a); > assert(b); > assert(c); > ```; > ; > would give you that information. Good suggestion. I only moved assertions here, didn't add any new ones, but I don't mind splitting up some that I moved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13990#issuecomment-1809203400
Testability,log,logic,"> This has significantly improved the simplicity of the parser, so much so that much of the logic therein could be simplified further, though I think that's beyond the scope of this change. Agreed. But as my follow up will be a complete rewrite of the parser, I definitely don't want to do more to simplify the current one. > I like the separation of type-checking and parsing, however I'd prefer in your implementations of `typecheck` that you assert one thing at a time. That way when things fail, it'll be clear which assertion was fired (ie if `(a && b && c)` fails, you don't know if it's `a` or `b` or `c`, whereas; > ; > ```scala; > assert(a); > assert(b); > assert(c); > ```; > ; > would give you that information. Good suggestion. I only moved assertions here, didn't add any new ones, but I don't mind splitting up some that I moved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13990#issuecomment-1809203400
Usability,simpl,simplicity,"> This has significantly improved the simplicity of the parser, so much so that much of the logic therein could be simplified further, though I think that's beyond the scope of this change. Agreed. But as my follow up will be a complete rewrite of the parser, I definitely don't want to do more to simplify the current one. > I like the separation of type-checking and parsing, however I'd prefer in your implementations of `typecheck` that you assert one thing at a time. That way when things fail, it'll be clear which assertion was fired (ie if `(a && b && c)` fails, you don't know if it's `a` or `b` or `c`, whereas; > ; > ```scala; > assert(a); > assert(b); > assert(c); > ```; > ; > would give you that information. Good suggestion. I only moved assertions here, didn't add any new ones, but I don't mind splitting up some that I moved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/13990#issuecomment-1809203400
Availability,error,error,"Adding a simple reproducible example. ```python; ht = hl.Table.from_pandas(pd.DataFrame({""variant"":['chr1:123:C:T']})); ht = ht.key_by(**hl.parse_variant(ht.variant)); pd_table = ht.to_pandas(); pd_table.to_pickle(os.path.join(bucket, 'test.pkl')); ```. The two examples below do not cause the same error. ; ```python; ht = hl.Table.from_pandas(pd.DataFrame({""foo"":['bar']})); ht = hl.Table.from_pandas(pd.DataFrame({""foo"":[1, 2, 3]})); ```. Hope this helps.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14004#issuecomment-1808416604
Testability,test,test,"Adding a simple reproducible example. ```python; ht = hl.Table.from_pandas(pd.DataFrame({""variant"":['chr1:123:C:T']})); ht = ht.key_by(**hl.parse_variant(ht.variant)); pd_table = ht.to_pandas(); pd_table.to_pickle(os.path.join(bucket, 'test.pkl')); ```. The two examples below do not cause the same error. ; ```python; ht = hl.Table.from_pandas(pd.DataFrame({""foo"":['bar']})); ht = hl.Table.from_pandas(pd.DataFrame({""foo"":[1, 2, 3]})); ```. Hope this helps.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14004#issuecomment-1808416604
Usability,simpl,simple,"Adding a simple reproducible example. ```python; ht = hl.Table.from_pandas(pd.DataFrame({""variant"":['chr1:123:C:T']})); ht = ht.key_by(**hl.parse_variant(ht.variant)); pd_table = ht.to_pandas(); pd_table.to_pickle(os.path.join(bucket, 'test.pkl')); ```. The two examples below do not cause the same error. ; ```python; ht = hl.Table.from_pandas(pd.DataFrame({""foo"":['bar']})); ht = hl.Table.from_pandas(pd.DataFrame({""foo"":[1, 2, 3]})); ```. Hope this helps.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14004#issuecomment-1808416604
Usability,feedback,feedback,"Sounds good. Thanks for the feedback. If it requires a significant effort to modify the code base to remove this behavior or if the change is not desired, it may be worth including a warning/info section describing this behavior in the documentation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14004#issuecomment-1813182236
Usability,feedback,feedback,> Or maybe this should be called [base_path](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base)?. Also would appreciate feedback on the naming if you have any,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14056#issuecomment-1877749736
Usability,guid,guide-analysis,@daniel-goldstein Let's put it at guide-analysis.hail.is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14067#issuecomment-1839682395
Performance,optimiz,optimizing,"Ya you're right, I was over-optimizing trying to share credentials between jobs of the same user. Just kept it as 1:1 jobs to credentials and it got a lot simpler. I changed the key to the job id because using the name of the identity would cause collisions between jobs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14125#issuecomment-1881179738
Usability,simpl,simpler,"Ya you're right, I was over-optimizing trying to share credentials between jobs of the same user. Just kept it as 1:1 jobs to credentials and it got a lot simpler. I changed the key to the job id because using the name of the identity would cause collisions between jobs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14125#issuecomment-1881179738
Modifiability,variab,variable,"looks like the differences between `--unsafe-fixes` and my manual edits based on the feedback the linter gave were:; * `assert <boolean value> == True` becomes `assert <boolean value> is True`, not `assert <boolean value>`; * `if <value> == foo or <value> == bar` becomes `if <value> in (foo, bar)`, not `if <value> in {foo, bar}`; * `<unused variable> = foo` becomes `foo` instead of being deleted outright. those seem fine, though i think the manual version of the latter two is better in the cases where i had added it, as i only used sets for hashable types and deleted things that didn't have side effects, afaik",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14128#issuecomment-1883396355
Safety,unsafe,unsafe-fixes,"looks like the differences between `--unsafe-fixes` and my manual edits based on the feedback the linter gave were:; * `assert <boolean value> == True` becomes `assert <boolean value> is True`, not `assert <boolean value>`; * `if <value> == foo or <value> == bar` becomes `if <value> in (foo, bar)`, not `if <value> in {foo, bar}`; * `<unused variable> = foo` becomes `foo` instead of being deleted outright. those seem fine, though i think the manual version of the latter two is better in the cases where i had added it, as i only used sets for hashable types and deleted things that didn't have side effects, afaik",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14128#issuecomment-1883396355
Security,hash,hashable,"looks like the differences between `--unsafe-fixes` and my manual edits based on the feedback the linter gave were:; * `assert <boolean value> == True` becomes `assert <boolean value> is True`, not `assert <boolean value>`; * `if <value> == foo or <value> == bar` becomes `if <value> in (foo, bar)`, not `if <value> in {foo, bar}`; * `<unused variable> = foo` becomes `foo` instead of being deleted outright. those seem fine, though i think the manual version of the latter two is better in the cases where i had added it, as i only used sets for hashable types and deleted things that didn't have side effects, afaik",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14128#issuecomment-1883396355
Testability,assert,assert,"looks like the differences between `--unsafe-fixes` and my manual edits based on the feedback the linter gave were:; * `assert <boolean value> == True` becomes `assert <boolean value> is True`, not `assert <boolean value>`; * `if <value> == foo or <value> == bar` becomes `if <value> in (foo, bar)`, not `if <value> in {foo, bar}`; * `<unused variable> = foo` becomes `foo` instead of being deleted outright. those seem fine, though i think the manual version of the latter two is better in the cases where i had added it, as i only used sets for hashable types and deleted things that didn't have side effects, afaik",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14128#issuecomment-1883396355
Usability,feedback,feedback,"looks like the differences between `--unsafe-fixes` and my manual edits based on the feedback the linter gave were:; * `assert <boolean value> == True` becomes `assert <boolean value> is True`, not `assert <boolean value>`; * `if <value> == foo or <value> == bar` becomes `if <value> in (foo, bar)`, not `if <value> in {foo, bar}`; * `<unused variable> = foo` becomes `foo` instead of being deleted outright. those seem fine, though i think the manual version of the latter two is better in the cases where i had added it, as i only used sets for hashable types and deleted things that didn't have side effects, afaik",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14128#issuecomment-1883396355
Availability,error,error,"EDIT2:. OK, so, I'm not sure when this behavior changed but Make 4.0 wants a `\` to indicate that the recipe continues on the next line *but also* passes that backslash and newline to the shell. In Make 3.81, the `\` was also required but the newline and backslash *are not passed* to the shell. In other words: in 3.81, backslash-newline is always replaced with a space and in 4.0, backslash-newline is replaced with a space *except on recipe lines in which case it is necessary to indicate the recipe continues but it is also passed literally to the shell*. The docs page you linked directly addresses our use case and suggests we put the command inside of a make variable (thus triggering normal backslash-newline rules rather than the special recipe line rules). > Sometimes you want to split a long line inside of single quotes, but you don’t want the backslash/newline to appear in the quoted content. This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less leg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324
Deployability,release,release,"ill get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less legible than literal JSON. Putting the whole JSON array on one line is quite long. I guess we can go with double quotes for now. I tested on Make 3.81 and Make 4.4.1. The first EDIT and the original comment follow for context. ---. EDIT: Nope, I still appear to be wrong. Hold on. ---. I have bash 3.2.57; ```; (base) dking@wm28c-761 /tmp % make print-shell; /bin/sh; (base) dking@wm28c-761 /tmp % /bin/sh --version; GNU bash, version 3.2.57(1)-release (arm64-apple-darwin22); Copyright (C) 2007 Free Software Foundation, Inc.; ```. Looks like this was an intentionally backwards incompatible change [in Make 4.0](https://git.savannah.gnu.org/cgit/make.git/tree/NEWS?h=4.0&id=52191d9d613819a77a321ad6c3ab16e1bc73c381#n18) which removed the POSIX-compatible behavior on which our Makefile relies:; ```; * WARNING: Backward-incompatibility!; If .POSIX is specified, then make adheres to the POSIX backslash/newline; handling requirements, which introduces the following changes to the; standard backslash/newline handling in non-recipe lines:; * Any trailing space before the backslash is preserved; * Each backslash/newline (plus subsequent whitespace) is converted to a; single space; ```. They seem to have [broken the behavior in order to fix something else](https://git.savannah.gnu.org/cgit/make.git/commit/?id=391456a) and then added a [`.POSIX`](https://git.savannah.gnu.org/cgit/make.git/commit/?id=88f1bc8) escape hatch for Makefiles that want POSIX compatibility. For the Ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324
Modifiability,variab,variable,"EDIT2:. OK, so, I'm not sure when this behavior changed but Make 4.0 wants a `\` to indicate that the recipe continues on the next line *but also* passes that backslash and newline to the shell. In Make 3.81, the `\` was also required but the newline and backslash *are not passed* to the shell. In other words: in 3.81, backslash-newline is always replaced with a space and in 4.0, backslash-newline is replaced with a space *except on recipe lines in which case it is necessary to indicate the recipe continues but it is also passed literally to the shell*. The docs page you linked directly addresses our use case and suggests we put the command inside of a make variable (thus triggering normal backslash-newline rules rather than the special recipe line rules). > Sometimes you want to split a long line inside of single quotes, but you don’t want the backslash/newline to appear in the quoted content. This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less leg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324
Testability,test,tested,"g, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less legible than literal JSON. Putting the whole JSON array on one line is quite long. I guess we can go with double quotes for now. I tested on Make 3.81 and Make 4.4.1. The first EDIT and the original comment follow for context. ---. EDIT: Nope, I still appear to be wrong. Hold on. ---. I have bash 3.2.57; ```; (base) dking@wm28c-761 /tmp % make print-shell; /bin/sh; (base) dking@wm28c-761 /tmp % /bin/sh --version; GNU bash, version 3.2.57(1)-release (arm64-apple-darwin22); Copyright (C) 2007 Free Software Foundation, Inc.; ```. Looks like this was an intentionally backwards incompatible change [in Make 4.0](https://git.savannah.gnu.org/cgit/make.git/tree/NEWS?h=4.0&id=52191d9d613819a77a321ad6c3ab16e1bc73c381#n18) which removed the POSIX-compatible behavior on which our Makefile relies:; ```; * WARNING: Backward-incompatibility!; If .POSIX is specified, then make adheres to the POSIX backslash/newline; handling requirements, which introduces the following changes to the; standard backslash/newline handling in non-recipe lines:; * Any trailing space before the backslash is preserved; * Each backslash/newline (plus subsequen",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324
Usability,simpl,simple,"sses that backslash and newline to the shell. In Make 3.81, the `\` was also required but the newline and backslash *are not passed* to the shell. In other words: in 3.81, backslash-newline is always replaced with a space and in 4.0, backslash-newline is replaced with a space *except on recipe lines in which case it is necessary to indicate the recipe continues but it is also passed literally to the shell*. The docs page you linked directly addresses our use case and suggests we put the command inside of a make variable (thus triggering normal backslash-newline rules rather than the special recipe line rules). > Sometimes you want to split a long line inside of single quotes, but you don’t want the backslash/newline to appear in the quoted content. This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less legible than literal JSON. Putting the whole JSON array on one line is quite long. I guess we can go with double quotes for now. I tested on Make 3.81 a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324
Safety,unsafe,unsafe-fixes,"i went through and manually fixed everything that `--unsafe-fixes` had initially addressed (after undoing the unsafe fixes, obv), and i broke all my changes up into separate commits based on which linter rule they were addressing",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14150#issuecomment-1894479704
Usability,undo,undoing,"i went through and manually fixed everything that `--unsafe-fixes` had initially addressed (after undoing the unsafe fixes, obv), and i broke all my changes up into separate commits based on which linter rule they were addressing",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14150#issuecomment-1894479704
Deployability,deploy,deploy,Just to make sure I understand -- the variable rename is to make sure it is clear that `HAIL_PRODUCTION_DOMAIN` means something different than `HAIL_DOMAIN` and is only applicable for CI? This is because the other services will have the correct deploy config?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14164#issuecomment-1898843580
Modifiability,variab,variable,Just to make sure I understand -- the variable rename is to make sure it is clear that `HAIL_PRODUCTION_DOMAIN` means something different than `HAIL_DOMAIN` and is only applicable for CI? This is because the other services will have the correct deploy config?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14164#issuecomment-1898843580
Usability,clear,clear,Just to make sure I understand -- the variable rename is to make sure it is clear that `HAIL_PRODUCTION_DOMAIN` means something different than `HAIL_DOMAIN` and is only applicable for CI? This is because the other services will have the correct deploy config?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14164#issuecomment-1898843580
Usability,feedback,feedback,"Hey @shengqh !. Yeah, this is a bug in Kryo, a serialization library used by Spark, which cannot handle the size of data you're producing. This is partly a deficiency in Hail: we assume that PLINK files are relatively small, in particular that the number of variants is small. This issue was supposedly resolved in Spark 2.4.0+ and 3.0.0+ by https://github.com/apache/spark/commit/3e033035a3c0b7d46c2ae18d0d322d4af3808711 . You appear to be running Apache Spark version 3.3.2, so I'm surprised you encountered this. Can you confirm which version of the Kryo JAR you have in your environment?. Can you also share a bit of information about this PLINK file? `import_plink` could obviously be modified to support 30M+ variant PLINK files, but I'd like to understand better why such large PLINK files exist. Do you expect these files to continue to grow in size? Do other consumers of these PLINK files want one PLINK file per chromosome? Would it be possible to generate many PLINK files per chromosome such that all the PLINK files have roughly the same size in bytes?. Thanks for your feedback and help improving Hail!. Related issue: https://github.com/hail-is/hail/issues/5564 .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1904412205
Testability,test,tested,"@danking:. > Hey @shengqh !; > ; > Yeah, this is a bug in Kryo, a serialization library used by Spark, which cannot handle the size of data you're producing.; > ; > This is partly a deficiency in Hail: we assume that PLINK files are relatively small, in particular that the number of variants is small.; > ; > This issue was supposedly resolved in Spark 2.4.0+ and 3.0.0+ by [apache/spark@3e03303](https://github.com/apache/spark/commit/3e033035a3c0b7d46c2ae18d0d322d4af3808711) . You appear to be running Apache Spark version 3.3.2, so I'm surprised you encountered this. Can you confirm which version of the Kryo JAR you have in your environment?. I don't know the Kryo JAR. I tested on both docker images hailgenetics/hail:0.2.126-py3.11 and hailgenetics/hail:0.2.127-py3.11. > ; > Can you also share a bit of information about this PLINK file? `import_plink` could obviously be modified to support 30M+ variant PLINK files, but I'd like to understand better why such large PLINK files exist. Do you expect these files to continue to grow in size? Do other consumers of these PLINK files want one PLINK file per chromosome? Would it be possible to generate many PLINK files per chromosome such that all the PLINK files have roughly the same size in bytes?. We have a 35K cohort. The VCF format of chr1 is 2.4T. So we prefer to deliver plink bed format and hail matrix. And, the cohort will continue to grow in future. I will prefer to keep one file per chromosome. For large cohort, which format do you prefer? Hail matrix or Hail VDS?. > ; > Thanks for your feedback and help improving Hail!; > ; > Related issue: #5564 .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1904727993
Usability,feedback,feedback,"@danking:. > Hey @shengqh !; > ; > Yeah, this is a bug in Kryo, a serialization library used by Spark, which cannot handle the size of data you're producing.; > ; > This is partly a deficiency in Hail: we assume that PLINK files are relatively small, in particular that the number of variants is small.; > ; > This issue was supposedly resolved in Spark 2.4.0+ and 3.0.0+ by [apache/spark@3e03303](https://github.com/apache/spark/commit/3e033035a3c0b7d46c2ae18d0d322d4af3808711) . You appear to be running Apache Spark version 3.3.2, so I'm surprised you encountered this. Can you confirm which version of the Kryo JAR you have in your environment?. I don't know the Kryo JAR. I tested on both docker images hailgenetics/hail:0.2.126-py3.11 and hailgenetics/hail:0.2.127-py3.11. > ; > Can you also share a bit of information about this PLINK file? `import_plink` could obviously be modified to support 30M+ variant PLINK files, but I'd like to understand better why such large PLINK files exist. Do you expect these files to continue to grow in size? Do other consumers of these PLINK files want one PLINK file per chromosome? Would it be possible to generate many PLINK files per chromosome such that all the PLINK files have roughly the same size in bytes?. We have a 35K cohort. The VCF format of chr1 is 2.4T. So we prefer to deliver plink bed format and hail matrix. And, the cohort will continue to grow in future. I will prefer to keep one file per chromosome. For large cohort, which format do you prefer? Hail matrix or Hail VDS?. > ; > Thanks for your feedback and help improving Hail!; > ; > Related issue: #5564 .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1904727993
Testability,test,testing,"I'm going to start testing, but I think the only thing that wasn't clear to me was how to resolve these sorts of comments. Do I try and fix them now or in a separate PR with an issue to make sure it gets noted? https://github.com/hail-is/hail/pull/14170#discussion_r1473442106",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1929629871
Usability,clear,clear,"I'm going to start testing, but I think the only thing that wasn't clear to me was how to resolve these sorts of comments. Do I try and fix them now or in a separate PR with an issue to make sure it gets noted? https://github.com/hail-is/hail/pull/14170#discussion_r1473442106",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1929629871
Energy Efficiency,efficient,efficiently," below needs to be ironed out. Should the number of jobs and state of the job group be recursive or specific to that job group? It's a bit weird for the billing and cancellation to be nested, but the number of jobs etc. are not. More concretely, if a child batch is running, should the parent also be running even if it has no direct child jobs that are running? Thoughts?; > ; > cc: @daniel-goldstein; > ; > ```sql; > UPDATE batches SET; > `state` = 'running',; > time_completed = NULL,; > n_jobs = n_jobs + expected_n_jobs; > WHERE id = in_batch_id;; > ; > ### FIXME FIXME what should the state be of nested job groups?; > UPDATE job_groups; > INNER JOIN (; > SELECT batch_id, job_group_id, CAST(COALESCE(SUM(n_jobs), 0) AS SIGNED) AS staged_n_jobs; > FROM job_groups_inst_coll_staging; > WHERE batch_id = in_batch_id AND update_id = in_update_id; > GROUP BY batch_id, job_group_id; > ) AS t ON job_groups.batch_id = t.batch_id AND job_groups.job_group_id = t.job_group_id; > SET `state` = 'running', time_completed = NULL, n_jobs = n_jobs + t.staged_n_jobs;; > ```. When you say ""billing and cancellation [is] nested"" do you mean that the bill for a group is the sum of the bill for all jobs directly in the group with all jobs in any descendent group?. Since we decided that groups are nested, my inclination is for everything to represent a sum total over the direct jobs and jobs within any descendant groups. From here on out ""sum total"" means exactly that. OK, so:. 1. In the UI (database should do what makes sense and is fast), the number of jobs should be the sum total *but* the pagination should page through the direct jobs.; 2. In the UI (same caveat), the total bill should be the sum total. (Including a sum of direct jobs cost seems fine if it is efficiently computable from the database).; 3. Yes, a group is running if any direct job or job within any descendant group is running (reasoning: if cancelling the group could cancel a running job, the UI needs to indicate that fact).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1940341021
Testability,test,testing,"> I'm going to start testing, but I think the only thing that wasn't clear to me was how to resolve these sorts of comments. Do I try and fix them now or in a separate PR with an issue to make sure it gets noted? [#14170 (comment)](https://github.com/hail-is/hail/pull/14170#discussion_r1473442106). Since this is merely propagating existing bad behavior, let's change it separately so as to keep the conceptual overhead of this change as small as possible. Please make an issue and paste the link into each comment so that we can later track how we resolved each comment. . > Ok. Still working on getting the tests to pass and cleaning things up. However, I ran into a small snag. The code below needs to be ironed out. Should the number of jobs and state of the job group be recursive or specific to that job group? It's a bit weird for the billing and cancellation to be nested, but the number of jobs etc. are not. More concretely, if a child batch is running, should the parent also be running even if it has no direct child jobs that are running? Thoughts?; > ; > cc: @daniel-goldstein; > ; > ```sql; > UPDATE batches SET; > `state` = 'running',; > time_completed = NULL,; > n_jobs = n_jobs + expected_n_jobs; > WHERE id = in_batch_id;; > ; > ### FIXME FIXME what should the state be of nested job groups?; > UPDATE job_groups; > INNER JOIN (; > SELECT batch_id, job_group_id, CAST(COALESCE(SUM(n_jobs), 0) AS SIGNED) AS staged_n_jobs; > FROM job_groups_inst_coll_staging; > WHERE batch_id = in_batch_id AND update_id = in_update_id; > GROUP BY batch_id, job_group_id; > ) AS t ON job_groups.batch_id = t.batch_id AND job_groups.job_group_id = t.job_group_id; > SET `state` = 'running', time_completed = NULL, n_jobs = n_jobs + t.staged_n_jobs;; > ```. When you say ""billing and cancellation [is] nested"" do you mean that the bill for a group is the sum of the bill for all jobs directly in the group with all jobs in any descendent group?. Since we decided that groups are nested, my inclinatio",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1940341021
Usability,clear,clear,"> I'm going to start testing, but I think the only thing that wasn't clear to me was how to resolve these sorts of comments. Do I try and fix them now or in a separate PR with an issue to make sure it gets noted? [#14170 (comment)](https://github.com/hail-is/hail/pull/14170#discussion_r1473442106). Since this is merely propagating existing bad behavior, let's change it separately so as to keep the conceptual overhead of this change as small as possible. Please make an issue and paste the link into each comment so that we can later track how we resolved each comment. . > Ok. Still working on getting the tests to pass and cleaning things up. However, I ran into a small snag. The code below needs to be ironed out. Should the number of jobs and state of the job group be recursive or specific to that job group? It's a bit weird for the billing and cancellation to be nested, but the number of jobs etc. are not. More concretely, if a child batch is running, should the parent also be running even if it has no direct child jobs that are running? Thoughts?; > ; > cc: @daniel-goldstein; > ; > ```sql; > UPDATE batches SET; > `state` = 'running',; > time_completed = NULL,; > n_jobs = n_jobs + expected_n_jobs; > WHERE id = in_batch_id;; > ; > ### FIXME FIXME what should the state be of nested job groups?; > UPDATE job_groups; > INNER JOIN (; > SELECT batch_id, job_group_id, CAST(COALESCE(SUM(n_jobs), 0) AS SIGNED) AS staged_n_jobs; > FROM job_groups_inst_coll_staging; > WHERE batch_id = in_batch_id AND update_id = in_update_id; > GROUP BY batch_id, job_group_id; > ) AS t ON job_groups.batch_id = t.batch_id AND job_groups.job_group_id = t.job_group_id; > SET `state` = 'running', time_completed = NULL, n_jobs = n_jobs + t.staged_n_jobs;; > ```. When you say ""billing and cancellation [is] nested"" do you mean that the bill for a group is the sum of the bill for all jobs directly in the group with all jobs in any descendent group?. Since we decided that groups are nested, my inclinatio",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1940341021
Security,access,access,"r, there is a [GCS connector](https://www.hdfgroup.org/solutions/cloud-amazon-s3-storage-hdf5-connector/). It's not an object store, but there's also support for [Hadoop HDFS](https://www.hdfgroup.org/solutions/hadoop-hdfs-hdf5-connector/). There's also [the Virtual Object Layer](https://docs.hdfgroup.org/hdf5/develop/_h5_v_l__u_g.html) which appears to be a file system abstraction that would permit storing HDF5 ""files"" in multiple objects which plays well with cloud object store scaling. We should prioritize an importer because no one has asked for HDF5 export nor is it clear that the HDF5 client libraries make it easy to write a single HDF5 ""file"" from a cluster of cores separated by a network. An importer would look something like `MatrixVCFReader`. It will need to use an HDF5 Java client library. An HDF5 client API is described [here](https://docs.hdfgroup.org/hdf5/develop/_h_d_f5_l_i_b.html) but they don't link to any JARs or maven repositories. This [support thread from 2022](https://forum.hdfgroup.org/t/how-to-get-started-wih-hdf5-java/10346/14) appears to ultimately conclude that [netcdf-java](https://forum.hdfgroup.org/t/how-to-get-started-wih-hdf5-java/10346/24) supports reading HDF5 files. Including netcdf-java in a gradle or maven project is described [here](https://docs.unidata.ucar.edu/netcdf-java/current/userguide/using_netcdf_java_artifacts.html). It is not entirely clear how to use netcdf-java to access objects in Google Cloud Storage or Azure Blob Storage. There's an [open issue to support S3](https://github.com/Unidata/netcdf-java/issues/111). ---. OK, so, this is roughly what I'd do:. Driver side:; 1. Get the schema, cook up a corresponding Hail type.; 2. Choose a partitioning of the index space. Worker side:; 1. Read the same slice of each field/column based on the partition information.; 2. Construct a Hail SType/PType. See `GVCFPartitionReader` for an example. That class is misnamed, it's just a VCF partition reader, its not specific to GVCFs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14311#issuecomment-1955112694
Usability,clear,clear,"HDF5 ""files"" are usually literally a single file. While fine for traditional file systems, this is not a good fit for object stores like GCS and S3. Object stores tend to scale horizontally providing high aggregate bandwidth across many individual objects. There appear to be some efforts to permit HDF5 to read and write to object stores in an object-store-friendly manner. In particular, there is a [GCS connector](https://www.hdfgroup.org/solutions/cloud-amazon-s3-storage-hdf5-connector/). It's not an object store, but there's also support for [Hadoop HDFS](https://www.hdfgroup.org/solutions/hadoop-hdfs-hdf5-connector/). There's also [the Virtual Object Layer](https://docs.hdfgroup.org/hdf5/develop/_h5_v_l__u_g.html) which appears to be a file system abstraction that would permit storing HDF5 ""files"" in multiple objects which plays well with cloud object store scaling. We should prioritize an importer because no one has asked for HDF5 export nor is it clear that the HDF5 client libraries make it easy to write a single HDF5 ""file"" from a cluster of cores separated by a network. An importer would look something like `MatrixVCFReader`. It will need to use an HDF5 Java client library. An HDF5 client API is described [here](https://docs.hdfgroup.org/hdf5/develop/_h_d_f5_l_i_b.html) but they don't link to any JARs or maven repositories. This [support thread from 2022](https://forum.hdfgroup.org/t/how-to-get-started-wih-hdf5-java/10346/14) appears to ultimately conclude that [netcdf-java](https://forum.hdfgroup.org/t/how-to-get-started-wih-hdf5-java/10346/24) supports reading HDF5 files. Including netcdf-java in a gradle or maven project is described [here](https://docs.unidata.ucar.edu/netcdf-java/current/userguide/using_netcdf_java_artifacts.html). It is not entirely clear how to use netcdf-java to access objects in Google Cloud Storage or Azure Blob Storage. There's an [open issue to support S3](https://github.com/Unidata/netcdf-java/issues/111). ---. OK, so, this is roug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14311#issuecomment-1955112694
Usability,clear,clear,To be clear this is a straight copy-paste job.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14314#issuecomment-1949442248
Deployability,release,releases,@jmarshall thanks again for the feedback 0.2.128 should now be fixed: https://github.com/hail-is/hail/releases/tag/0.2.128 .,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14322#issuecomment-1967719780
Usability,feedback,feedback,@jmarshall thanks again for the feedback 0.2.128 should now be fixed: https://github.com/hail-is/hail/releases/tag/0.2.128 .,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14322#issuecomment-1967719780
Safety,predict,predicted,". Then the paper computes a $p \times n$ matrix $W$ called the **SNP weight matrix**:. $$W \coloneqq X^T U.$$. Suppose that there are $n_r$ individuals in the related set and let $Y$ be the $n_r \times p$ standardized genotype matrix for the related individuals. The paper computes the principal components associated with the related samples with. $$ \frac{1}{p} Y W (\Sigma^2)^{-1}.$$. ### Simplifications. The first simplification that I noticed is that we can do away with the $\frac{1}{p}$ terms. Because $\Psi$ is scaled by $p^{-1}$, the inverse of the eigenvalues, $(\Sigma^2)^{-1}$ is scaled by $p$, which cancels out the $1/p$ term in the calculation of the principal components for the related individuals. From here on, let us redefine $\Sigma^2$ as the diagonal matrix containing the eigenvalues of $XX^T$ (not $\frac{1}{p} XX^T$). Next, by examining the relationship between singular value decomposition (SVD) and eigendecomposition ([Wikipedia link](https://en.m.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition)), I realized that it is not necessary to compute $\Psi$. Instead, we can get $U$ and $\Sigma$ from the SVD of $X$:. $$X = U\Sigma V^T,$$. where $V$ is a $p \times p$ basis of the new PCA coordinate space. Then while investigating the meaning of $W$, I realized that $W = X^T U = V \Sigma^T U^T U = V \Sigma^T$. Taking these simplifications into account, I realized that the paper is, in essence, computing $Y V$ to get the predicted scores associated with the related individuals. (Technically, I think that the paper is computing $Y V \Sigma^{-1}$. I am not sure why they scale the columns by the reciprocal of the eigenvalues here.). ### Simplified Approach. As I understand it, $V$ is a change-of-basis matrix from the original coordinates to the PCA scores. Letting $G$ denote the full standardized genotype matrix of all the individuals, I think we can just return $GV$, where $V$ is defined by the SVD of $X$:. $$X = U\Sigma V^T.$$",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1962279184
Usability,simpl,simplify,"I think I can simplify the computation of the principal components. First let me summarize how the paper presents PC-AiR. ### Original Presentation. Suppose that there are $n_u$ individuals in the unrelated set and $p$ SNPs. Let $X$ be the $n_u \times p$ standardized genotype matrix for the unrelated individuals. The paper starts by computing. $$\Psi \coloneqq \frac{1}{p}XX^T.$$. Next the paper eigendecomposes $\Psi$:. $$\Psi = U \Sigma^2 U^T,$$. where $U$ are the eigenvectors and $\Sigma^2$ is the diagonal matrix with the eigenvalues along the diagonal. Then the paper computes a $p \times n$ matrix $W$ called the **SNP weight matrix**:. $$W \coloneqq X^T U.$$. Suppose that there are $n_r$ individuals in the related set and let $Y$ be the $n_r \times p$ standardized genotype matrix for the related individuals. The paper computes the principal components associated with the related samples with. $$ \frac{1}{p} Y W (\Sigma^2)^{-1}.$$. ### Simplifications. The first simplification that I noticed is that we can do away with the $\frac{1}{p}$ terms. Because $\Psi$ is scaled by $p^{-1}$, the inverse of the eigenvalues, $(\Sigma^2)^{-1}$ is scaled by $p$, which cancels out the $1/p$ term in the calculation of the principal components for the related individuals. From here on, let us redefine $\Sigma^2$ as the diagonal matrix containing the eigenvalues of $XX^T$ (not $\frac{1}{p} XX^T$). Next, by examining the relationship between singular value decomposition (SVD) and eigendecomposition ([Wikipedia link](https://en.m.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition)), I realized that it is not necessary to compute $\Psi$. Instead, we can get $U$ and $\Sigma$ from the SVD of $X$:. $$X = U\Sigma V^T,$$. where $V$ is a $p \times p$ basis of the new PCA coordinate space. Then while investigating the meaning of $W$, I realized that $W = X^T U = V \Sigma^T U^T U = V \Sigma^T$. Taking these simplifications into account, I realized that the paper",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1962279184
Usability,feedback,feedback,"Hi @daniel-goldstein / @danking - thanks for the feedback, and sorry for the delay! I've pushed those changes now :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14328#issuecomment-1978272030
Usability,clear,clearly,"@jmarshall @illusional Likewise, hopefully I captured the request clearly here but please provide more information if necessary!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14356#issuecomment-1964591127
Usability,simpl,simpler,I can where this change makes many things simpler. I like the way you're going with it. Thank you.; There seem to be a few changes related to scopes - perhaps promoting bindings to various scopes eagerly. I think these have made the review slightly harder.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14402#issuecomment-1989477810
Modifiability,refactor,refactoring,"We use feature flags to communicate requester pays information to the service backend.; In this change, I've made the local backend do the same to make a future refactoring simpler.; I intend to follow up this change that'll split config from feature flags.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14407#issuecomment-1998508308
Usability,simpl,simpler,"We use feature flags to communicate requester pays information to the service backend.; In this change, I've made the local backend do the same to make a future refactoring simpler.; I intend to follow up this change that'll split config from feature flags.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14407#issuecomment-1998508308
Integrability,rout,route,"> @ehigham the only reason i represented it this way is because that's how it is in the database, like we have job attributes and then we have the always run flag for a job separately from those, but on the frontend i don't think there would be a problem with representing it as an attribute. i could change the new table column on the Batch page to be Attributes and put ""Always Run"" in there only for jobs that are always run, and i could also move ""Always Run"" to be under the Attributes header on the Job page, instead of the Properties header. happy to make either or both changes, just lmk!. That sounds not too dissimilar to how google adds 'labels' to tabular data. Personally I prefer that style, especially as more information gets added and tables get wider.; That said, I don't know how much extra data we'd want to add to this UI so that route may be slightly crystal-balling! Let's keep it simple for now as you've done already and consider something like that in the future if we have more data to add.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14425#issuecomment-2030060565
Usability,simpl,simple,"> @ehigham the only reason i represented it this way is because that's how it is in the database, like we have job attributes and then we have the always run flag for a job separately from those, but on the frontend i don't think there would be a problem with representing it as an attribute. i could change the new table column on the Batch page to be Attributes and put ""Always Run"" in there only for jobs that are always run, and i could also move ""Always Run"" to be under the Attributes header on the Job page, instead of the Properties header. happy to make either or both changes, just lmk!. That sounds not too dissimilar to how google adds 'labels' to tabular data. Personally I prefer that style, especially as more information gets added and tables get wider.; That said, I don't know how much extra data we'd want to add to this UI so that route may be slightly crystal-balling! Let's keep it simple for now as you've done already and consider something like that in the future if we have more data to add.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14425#issuecomment-2030060565
Availability,error,error,"For your consideration… We often have a `pathlib.Path` or `cloudpathlib.CloudPath` that we've built up by parts, which is then the path to be used as an input resource:. ```python; res = mybatch.read_input(str(mycloudpath)); ```. Periodically we accidentally omit the `str(…)`, which leads to a semi-obscure error message and an extra editing round-trip. There is a point of view that `read_input()` and `read_input_group()` could also accept `os.PathLike` objects directly, and have Hail convert them to `str` itself, e.g. in `_new_input_resource_file()` which underlies both methods, as per this PR. The difficulty is how to do that conversion: `str(…)` does the trick for [`pathlib.Path`](https://docs.python.org/3.12/library/pathlib.html#operators) and [`cloudpathlib.CloudPath`](https://cloudpathlib.drivendata.org/stable/api-reference/cloudpath/), returning the path and URL, respectively, as a string. But it looks like in theory there might be [`os.PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) subclasses that don't define `__str__()` to produce a usable path/URL. The official conversion method appears to be [`os.fspath()`](https://docs.python.org/3/library/os.html#os.fspath), but that does not do the right thing for `cloudpath.CloudPath` — there it downloads the remote file and returns a local path — which is not at all what Hail needs. However probably this is a theoretical concern and `str(…)` will be fine…",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14544#issuecomment-2105616965
Integrability,message,message,"For your consideration… We often have a `pathlib.Path` or `cloudpathlib.CloudPath` that we've built up by parts, which is then the path to be used as an input resource:. ```python; res = mybatch.read_input(str(mycloudpath)); ```. Periodically we accidentally omit the `str(…)`, which leads to a semi-obscure error message and an extra editing round-trip. There is a point of view that `read_input()` and `read_input_group()` could also accept `os.PathLike` objects directly, and have Hail convert them to `str` itself, e.g. in `_new_input_resource_file()` which underlies both methods, as per this PR. The difficulty is how to do that conversion: `str(…)` does the trick for [`pathlib.Path`](https://docs.python.org/3.12/library/pathlib.html#operators) and [`cloudpathlib.CloudPath`](https://cloudpathlib.drivendata.org/stable/api-reference/cloudpath/), returning the path and URL, respectively, as a string. But it looks like in theory there might be [`os.PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) subclasses that don't define `__str__()` to produce a usable path/URL. The official conversion method appears to be [`os.fspath()`](https://docs.python.org/3/library/os.html#os.fspath), but that does not do the right thing for `cloudpath.CloudPath` — there it downloads the remote file and returns a local path — which is not at all what Hail needs. However probably this is a theoretical concern and `str(…)` will be fine…",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14544#issuecomment-2105616965
Usability,usab,usable,"For your consideration… We often have a `pathlib.Path` or `cloudpathlib.CloudPath` that we've built up by parts, which is then the path to be used as an input resource:. ```python; res = mybatch.read_input(str(mycloudpath)); ```. Periodically we accidentally omit the `str(…)`, which leads to a semi-obscure error message and an extra editing round-trip. There is a point of view that `read_input()` and `read_input_group()` could also accept `os.PathLike` objects directly, and have Hail convert them to `str` itself, e.g. in `_new_input_resource_file()` which underlies both methods, as per this PR. The difficulty is how to do that conversion: `str(…)` does the trick for [`pathlib.Path`](https://docs.python.org/3.12/library/pathlib.html#operators) and [`cloudpathlib.CloudPath`](https://cloudpathlib.drivendata.org/stable/api-reference/cloudpath/), returning the path and URL, respectively, as a string. But it looks like in theory there might be [`os.PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) subclasses that don't define `__str__()` to produce a usable path/URL. The official conversion method appears to be [`os.fspath()`](https://docs.python.org/3/library/os.html#os.fspath), but that does not do the right thing for `cloudpath.CloudPath` — there it downloads the remote file and returns a local path — which is not at all what Hail needs. However probably this is a theoretical concern and `str(…)` will be fine…",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14544#issuecomment-2105616965
Usability,feedback,feedback,Putting WIP because I'm also going to get some feedback from Wenhan start of next week,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14562#issuecomment-2129755103
Availability,error,error,Example error:. ```; Caused by: is.hail.relocated.org.json4s.MappingException: No usable value for value_parameter_names; No usable value for str; Did not find value which can be converted into java.lang.String; 	at is.hail.relocated.org.json4s.reflect.package$.fail(package.scala:53); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:638); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:689); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:688); 	at scala.PartialFunction.$anonfun$runWith$1$adapted(PartialFunction.scala:145); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at scala.collection.TraversableLike.collect(TraversableLike.scala:407); 	at scala.collection.TraversableLike.collect$(TraversableLike.scala:405); 	at scala.collection.AbstractTraversable.collect(Traversable.scala:108); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:688); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:767); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$extract$10(Extraction.scala:462); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$customOrElse$1(Extraction.scala:780); 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127); 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126); 	at scala.PartialFunction$$anon$1.applyOrElse(PartialFunction.scala:257); 	at is.hail.relocated.org.json4s.Extraction$.customOrElse(Extraction.scala:780); 	at is.hail.relocated.org.json4s.Extraction$.extract(Extraction.scala:454); 	at is.hail.relocated.org.json4s.Extraction$.org$json4s$Extraction$$extractDete,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890
Energy Efficiency,adapt,adapted,Example error:. ```; Caused by: is.hail.relocated.org.json4s.MappingException: No usable value for value_parameter_names; No usable value for str; Did not find value which can be converted into java.lang.String; 	at is.hail.relocated.org.json4s.reflect.package$.fail(package.scala:53); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:638); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:689); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:688); 	at scala.PartialFunction.$anonfun$runWith$1$adapted(PartialFunction.scala:145); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at scala.collection.TraversableLike.collect(TraversableLike.scala:407); 	at scala.collection.TraversableLike.collect$(TraversableLike.scala:405); 	at scala.collection.AbstractTraversable.collect(Traversable.scala:108); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:688); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:767); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$extract$10(Extraction.scala:462); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$customOrElse$1(Extraction.scala:780); 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127); 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126); 	at scala.PartialFunction$$anon$1.applyOrElse(PartialFunction.scala:257); 	at is.hail.relocated.org.json4s.Extraction$.customOrElse(Extraction.scala:780); 	at is.hail.relocated.org.json4s.Extraction$.extract(Extraction.scala:454); 	at is.hail.relocated.org.json4s.Extraction$.org$json4s$Extraction$$extractDete,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890
Modifiability,adapt,adapted,Example error:. ```; Caused by: is.hail.relocated.org.json4s.MappingException: No usable value for value_parameter_names; No usable value for str; Did not find value which can be converted into java.lang.String; 	at is.hail.relocated.org.json4s.reflect.package$.fail(package.scala:53); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:638); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:689); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:688); 	at scala.PartialFunction.$anonfun$runWith$1$adapted(PartialFunction.scala:145); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at scala.collection.TraversableLike.collect(TraversableLike.scala:407); 	at scala.collection.TraversableLike.collect$(TraversableLike.scala:405); 	at scala.collection.AbstractTraversable.collect(Traversable.scala:108); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:688); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:767); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$extract$10(Extraction.scala:462); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$customOrElse$1(Extraction.scala:780); 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127); 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126); 	at scala.PartialFunction$$anon$1.applyOrElse(PartialFunction.scala:257); 	at is.hail.relocated.org.json4s.Extraction$.customOrElse(Extraction.scala:780); 	at is.hail.relocated.org.json4s.Extraction$.extract(Extraction.scala:454); 	at is.hail.relocated.org.json4s.Extraction$.org$json4s$Extraction$$extractDete,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890
Usability,usab,usable,Example error:. ```; Caused by: is.hail.relocated.org.json4s.MappingException: No usable value for value_parameter_names; No usable value for str; Did not find value which can be converted into java.lang.String; 	at is.hail.relocated.org.json4s.reflect.package$.fail(package.scala:53); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:638); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:689); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:688); 	at scala.PartialFunction.$anonfun$runWith$1$adapted(PartialFunction.scala:145); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at scala.collection.TraversableLike.collect(TraversableLike.scala:407); 	at scala.collection.TraversableLike.collect$(TraversableLike.scala:405); 	at scala.collection.AbstractTraversable.collect(Traversable.scala:108); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:688); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:767); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$extract$10(Extraction.scala:462); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$customOrElse$1(Extraction.scala:780); 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127); 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126); 	at scala.PartialFunction$$anon$1.applyOrElse(PartialFunction.scala:257); 	at is.hail.relocated.org.json4s.Extraction$.customOrElse(Extraction.scala:780); 	at is.hail.relocated.org.json4s.Extraction$.extract(Extraction.scala:454); 	at is.hail.relocated.org.json4s.Extraction$.org$json4s$Extraction$$extractDete,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890
Integrability,bridg,bridge,"So I think the root issue here is the unnecessary duplication between `pyRegisterIR` and `pyRegisterIRForServiceBackend`. The only real difference is that one takes and already parsed IR, and the other takes a string and calls the parser. The callers of `pyRegisterIR` in python all call into the parser first, but I don't see any reason it has to make two calls across the python/scala bridge; I think `pyRegisterIR` should just take the IR as a string and call the parser like `pyRegisterIRForServiceBackend` does. With that change, it should be possible to make one a simple wrapper around the other (or maybe even get rid of `pyRegisterIRForServiceBackend` completely). That way the core logic is shared between backends and is getting tested. Let me know if you want help with this, or if you'd like me to make a separate PR for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2174358685
Testability,log,logic,"So I think the root issue here is the unnecessary duplication between `pyRegisterIR` and `pyRegisterIRForServiceBackend`. The only real difference is that one takes and already parsed IR, and the other takes a string and calls the parser. The callers of `pyRegisterIR` in python all call into the parser first, but I don't see any reason it has to make two calls across the python/scala bridge; I think `pyRegisterIR` should just take the IR as a string and call the parser like `pyRegisterIRForServiceBackend` does. With that change, it should be possible to make one a simple wrapper around the other (or maybe even get rid of `pyRegisterIRForServiceBackend` completely). That way the core logic is shared between backends and is getting tested. Let me know if you want help with this, or if you'd like me to make a separate PR for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2174358685
Usability,simpl,simple,"So I think the root issue here is the unnecessary duplication between `pyRegisterIR` and `pyRegisterIRForServiceBackend`. The only real difference is that one takes and already parsed IR, and the other takes a string and calls the parser. The callers of `pyRegisterIR` in python all call into the parser first, but I don't see any reason it has to make two calls across the python/scala bridge; I think `pyRegisterIR` should just take the IR as a string and call the parser like `pyRegisterIRForServiceBackend` does. With that change, it should be possible to make one a simple wrapper around the other (or maybe even get rid of `pyRegisterIRForServiceBackend` completely). That way the core logic is shared between backends and is getting tested. Let me know if you want help with this, or if you'd like me to make a separate PR for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2174358685
Testability,log,logging,"@daniel-goldstein Here's my thoughts. Sound good?; I think we simply need to add an `is_rate_limit` check to `retry_transient_errors` in hailtop, and pass rather than logging it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14595#issuecomment-2200975779
Usability,simpl,simply,"@daniel-goldstein Here's my thoughts. Sound good?; I think we simply need to add an `is_rate_limit` check to `retry_transient_errors` in hailtop, and pass rather than logging it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14595#issuecomment-2200975779
Deployability,deploy,deploying,"There's more work to be done here. This adds a new route to the batch UI for getting a certain job group within a batch. It then, instead of listing all jobs, only lists the jobs that belong directly to the currently viewed job group and also shows the child job groups of the current job group. When picking up this PR I would make sure to go through the Batch development tutorial to make sure you are familiar with dev deploying. Then, read [this](https://github.com/hail-is/hail/blob/main/dev-docs/development-process.md#alternatives-to-dev-deploy) to learn about all the ways you can avoid dev deploying 😄 . If you are only making tweaks in the HTML templates, you don't need to keep deploying for every little change. Instead, run. ```bash; make devserver SERVICE=batch; ```. in your terminal and you'll get a local server that proxies the Batch that your `hail` installation is pointed to. You can then make changes to HTML and refresh your browser to see the results. Note that this is just rendering the HTML locally, and will have any effect on what's deployed, meaning you can't use it for python changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14600#issuecomment-2239854998
Integrability,rout,route,"There's more work to be done here. This adds a new route to the batch UI for getting a certain job group within a batch. It then, instead of listing all jobs, only lists the jobs that belong directly to the currently viewed job group and also shows the child job groups of the current job group. When picking up this PR I would make sure to go through the Batch development tutorial to make sure you are familiar with dev deploying. Then, read [this](https://github.com/hail-is/hail/blob/main/dev-docs/development-process.md#alternatives-to-dev-deploy) to learn about all the ways you can avoid dev deploying 😄 . If you are only making tweaks in the HTML templates, you don't need to keep deploying for every little change. Instead, run. ```bash; make devserver SERVICE=batch; ```. in your terminal and you'll get a local server that proxies the Batch that your `hail` installation is pointed to. You can then make changes to HTML and refresh your browser to see the results. Note that this is just rendering the HTML locally, and will have any effect on what's deployed, meaning you can't use it for python changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14600#issuecomment-2239854998
Safety,avoid,avoid,"There's more work to be done here. This adds a new route to the batch UI for getting a certain job group within a batch. It then, instead of listing all jobs, only lists the jobs that belong directly to the currently viewed job group and also shows the child job groups of the current job group. When picking up this PR I would make sure to go through the Batch development tutorial to make sure you are familiar with dev deploying. Then, read [this](https://github.com/hail-is/hail/blob/main/dev-docs/development-process.md#alternatives-to-dev-deploy) to learn about all the ways you can avoid dev deploying 😄 . If you are only making tweaks in the HTML templates, you don't need to keep deploying for every little change. Instead, run. ```bash; make devserver SERVICE=batch; ```. in your terminal and you'll get a local server that proxies the Batch that your `hail` installation is pointed to. You can then make changes to HTML and refresh your browser to see the results. Note that this is just rendering the HTML locally, and will have any effect on what's deployed, meaning you can't use it for python changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14600#issuecomment-2239854998
Usability,learn,learn,"There's more work to be done here. This adds a new route to the batch UI for getting a certain job group within a batch. It then, instead of listing all jobs, only lists the jobs that belong directly to the currently viewed job group and also shows the child job groups of the current job group. When picking up this PR I would make sure to go through the Batch development tutorial to make sure you are familiar with dev deploying. Then, read [this](https://github.com/hail-is/hail/blob/main/dev-docs/development-process.md#alternatives-to-dev-deploy) to learn about all the ways you can avoid dev deploying 😄 . If you are only making tweaks in the HTML templates, you don't need to keep deploying for every little change. Instead, run. ```bash; make devserver SERVICE=batch; ```. in your terminal and you'll get a local server that proxies the Batch that your `hail` installation is pointed to. You can then make changes to HTML and refresh your browser to see the results. Note that this is just rendering the HTML locally, and will have any effect on what's deployed, meaning you can't use it for python changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14600#issuecomment-2239854998
Energy Efficiency,schedul,scheduler,"> @daniel-goldstein - do you know if compacting or deleting will impact other backgroud processes? Is this table used for anything else after a job group completes?; > ; > ; > ; > Also, should we also compact failed job groups?. I don't believe so, this should be fine. I would usually grep the codebase for the table name. IIRC you should see it used in:; - job insertion, clearly no longer relevant; - triggers which won't fire on cold batches; - the scheduler/fair share which should only care about rows with >0 values and active job groups. Off the top of my head, I see no reason to distinguish between different `complete` states.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14623#issuecomment-2264264725
Usability,clear,clearly,"> @daniel-goldstein - do you know if compacting or deleting will impact other backgroud processes? Is this table used for anything else after a job group completes?; > ; > ; > ; > Also, should we also compact failed job groups?. I don't believe so, this should be fine. I would usually grep the codebase for the table name. IIRC you should see it used in:; - job insertion, clearly no longer relevant; - triggers which won't fire on cold batches; - the scheduler/fair share which should only care about rows with >0 values and active job groups. Off the top of my head, I see no reason to distinguish between different `complete` states.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14623#issuecomment-2264264725
Integrability,depend,dependence,Thanks for reporting this. I had tried to remove our dependence on `setuptools` but had not done a good enough job clearly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14630#issuecomment-2243322335
Usability,clear,clearly,Thanks for reporting this. I had tried to remove our dependence on `setuptools` but had not done a good enough job clearly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14630#issuecomment-2243322335
Availability,avail,available,"This is caused by domain-by-domain CSRF tokens introduced in [#14180](https://github.com/hail-is/hail/issues/14180). An unfortunate side effect is that the tokens available on non-auth pages are no longer able to validate requests to the auth/logout API. Given the lack of apparent noise about this bug in our issues and zulip I suspect that this is not a common path for users, and that a fix along the lines of ""require add one button click to go to the User page first before logging out is acceptable"". On the other hand, the risk of a user clicking on the broken Logout button and believing themselves to be logged out when seeing a `401: Unauthorized` page (but actually still having logged-in state in their browser) raises this in my mind to a security bug rather than just a UX bug or an unfortunate user experience. Therefore my proposal is:; 1. To fix the bug as soon as possible; 2. Accept an additional redirect in a user flow which is rarely exercised; 3. To make the smallest number of potentially risky changes to the underlying security architecture; 4. Therefore: Remove the broken ""log out"" link in page headers and replace with a Log out button on the auth[...]/users page which is guaranteed to have the correct CSRF token in state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14635#issuecomment-2253086187
Safety,risk,risk,"This is caused by domain-by-domain CSRF tokens introduced in [#14180](https://github.com/hail-is/hail/issues/14180). An unfortunate side effect is that the tokens available on non-auth pages are no longer able to validate requests to the auth/logout API. Given the lack of apparent noise about this bug in our issues and zulip I suspect that this is not a common path for users, and that a fix along the lines of ""require add one button click to go to the User page first before logging out is acceptable"". On the other hand, the risk of a user clicking on the broken Logout button and believing themselves to be logged out when seeing a `401: Unauthorized` page (but actually still having logged-in state in their browser) raises this in my mind to a security bug rather than just a UX bug or an unfortunate user experience. Therefore my proposal is:; 1. To fix the bug as soon as possible; 2. Accept an additional redirect in a user flow which is rarely exercised; 3. To make the smallest number of potentially risky changes to the underlying security architecture; 4. Therefore: Remove the broken ""log out"" link in page headers and replace with a Log out button on the auth[...]/users page which is guaranteed to have the correct CSRF token in state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14635#issuecomment-2253086187
Security,validat,validate,"This is caused by domain-by-domain CSRF tokens introduced in [#14180](https://github.com/hail-is/hail/issues/14180). An unfortunate side effect is that the tokens available on non-auth pages are no longer able to validate requests to the auth/logout API. Given the lack of apparent noise about this bug in our issues and zulip I suspect that this is not a common path for users, and that a fix along the lines of ""require add one button click to go to the User page first before logging out is acceptable"". On the other hand, the risk of a user clicking on the broken Logout button and believing themselves to be logged out when seeing a `401: Unauthorized` page (but actually still having logged-in state in their browser) raises this in my mind to a security bug rather than just a UX bug or an unfortunate user experience. Therefore my proposal is:; 1. To fix the bug as soon as possible; 2. Accept an additional redirect in a user flow which is rarely exercised; 3. To make the smallest number of potentially risky changes to the underlying security architecture; 4. Therefore: Remove the broken ""log out"" link in page headers and replace with a Log out button on the auth[...]/users page which is guaranteed to have the correct CSRF token in state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14635#issuecomment-2253086187
Testability,log,logout,"This is caused by domain-by-domain CSRF tokens introduced in [#14180](https://github.com/hail-is/hail/issues/14180). An unfortunate side effect is that the tokens available on non-auth pages are no longer able to validate requests to the auth/logout API. Given the lack of apparent noise about this bug in our issues and zulip I suspect that this is not a common path for users, and that a fix along the lines of ""require add one button click to go to the User page first before logging out is acceptable"". On the other hand, the risk of a user clicking on the broken Logout button and believing themselves to be logged out when seeing a `401: Unauthorized` page (but actually still having logged-in state in their browser) raises this in my mind to a security bug rather than just a UX bug or an unfortunate user experience. Therefore my proposal is:; 1. To fix the bug as soon as possible; 2. Accept an additional redirect in a user flow which is rarely exercised; 3. To make the smallest number of potentially risky changes to the underlying security architecture; 4. Therefore: Remove the broken ""log out"" link in page headers and replace with a Log out button on the auth[...]/users page which is guaranteed to have the correct CSRF token in state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14635#issuecomment-2253086187
Usability,UX,UX,"This is caused by domain-by-domain CSRF tokens introduced in [#14180](https://github.com/hail-is/hail/issues/14180). An unfortunate side effect is that the tokens available on non-auth pages are no longer able to validate requests to the auth/logout API. Given the lack of apparent noise about this bug in our issues and zulip I suspect that this is not a common path for users, and that a fix along the lines of ""require add one button click to go to the User page first before logging out is acceptable"". On the other hand, the risk of a user clicking on the broken Logout button and believing themselves to be logged out when seeing a `401: Unauthorized` page (but actually still having logged-in state in their browser) raises this in my mind to a security bug rather than just a UX bug or an unfortunate user experience. Therefore my proposal is:; 1. To fix the bug as soon as possible; 2. Accept an additional redirect in a user flow which is rarely exercised; 3. To make the smallest number of potentially risky changes to the underlying security architecture; 4. Therefore: Remove the broken ""log out"" link in page headers and replace with a Log out button on the auth[...]/users page which is guaranteed to have the correct CSRF token in state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/14635#issuecomment-2253086187
Availability,error,error,"> Hi Ivan! Thanks so much for picking this up :) I haven't much experience on the batch system but I'll try my best to give accurate feedback. I have a few questions/observations up front:; > ; > 1. Your changes to stored procedures under `batch/sql` make me a little nervous.; > ; > Most of these are migrations applied in the order defined in the build step mentioned in [NOTE 1] except `estimated-current.sql` [NOTE 2].; > ; > I don't think changing these will have the desired effect and may make it impossible for someone to reproduce the database. The only changes to _existing_ sql you'll need to make are in the sql strings in python code.; > ; > 2. This needs to be written as a migration and maybe could be simplified?; > ; > I think this needs to be done as a database migration. We'll have no need for a stored procedure once complete. You can assume current columns and constraints exist, dispense with the error checking and simplify. Can you convert this to a sql script and add it to the end of the list of migrations in `build.yaml`? You'll probably want `online: false` too. I fear you'll have to take inspiration from `rename-job-groups-tables.sql` by applying one `ALTER TABLE` command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename. Alternatively, create, execute then drop the procedure within `rename-job-groups-cancelled`.; > ; > [NOTE 1] migration applied in `build.yaml`; > ; > The relevant build step in `build.yaml` can be found by searching for the entry starting with the yaml below. This controls which migrations are applied and in what order.; > ; > ```yaml; > kind: createDatabase2; > name: batch_database; > databaseName: batch; > ```; > ; > [NOTE 2] estimated-current.yaml; > ; > I don't agree with why we have this. It would be nice to generate this automatically. Anyway, please keep your changes to this file as it's meant for documentation purposes only. None ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2334778045
Deployability,update,update,"TE 2].; > ; > I don't think changing these will have the desired effect and may make it impossible for someone to reproduce the database. The only changes to _existing_ sql you'll need to make are in the sql strings in python code.; > ; > 2. This needs to be written as a migration and maybe could be simplified?; > ; > I think this needs to be done as a database migration. We'll have no need for a stored procedure once complete. You can assume current columns and constraints exist, dispense with the error checking and simplify. Can you convert this to a sql script and add it to the end of the list of migrations in `build.yaml`? You'll probably want `online: false` too. I fear you'll have to take inspiration from `rename-job-groups-tables.sql` by applying one `ALTER TABLE` command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename. Alternatively, create, execute then drop the procedure within `rename-job-groups-cancelled`.; > ; > [NOTE 1] migration applied in `build.yaml`; > ; > The relevant build step in `build.yaml` can be found by searching for the entry starting with the yaml below. This controls which migrations are applied and in what order.; > ; > ```yaml; > kind: createDatabase2; > name: batch_database; > databaseName: batch; > ```; > ; > [NOTE 2] estimated-current.yaml; > ; > I don't agree with why we have this. It would be nice to generate this automatically. Anyway, please keep your changes to this file as it's meant for documentation purposes only. None of it is applied and who knows how much of it works. Got it! I wasn't sure how Hail usually does schema update. Based on your above description the process becomes clearer ro me. Here's my second try:. - Updated `build.yaml` in the `batch` database migrations section.; - Simplified the sql in `rename-job-groups-cancelled-column.sql`. Do you mean `estimated-current.sql` rather than `estimated-current.yaml` above?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2334778045
Usability,feedback,feedback,"> Hi Ivan! Thanks so much for picking this up :) I haven't much experience on the batch system but I'll try my best to give accurate feedback. I have a few questions/observations up front:; > ; > 1. Your changes to stored procedures under `batch/sql` make me a little nervous.; > ; > Most of these are migrations applied in the order defined in the build step mentioned in [NOTE 1] except `estimated-current.sql` [NOTE 2].; > ; > I don't think changing these will have the desired effect and may make it impossible for someone to reproduce the database. The only changes to _existing_ sql you'll need to make are in the sql strings in python code.; > ; > 2. This needs to be written as a migration and maybe could be simplified?; > ; > I think this needs to be done as a database migration. We'll have no need for a stored procedure once complete. You can assume current columns and constraints exist, dispense with the error checking and simplify. Can you convert this to a sql script and add it to the end of the list of migrations in `build.yaml`? You'll probably want `online: false` too. I fear you'll have to take inspiration from `rename-job-groups-tables.sql` by applying one `ALTER TABLE` command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename. Alternatively, create, execute then drop the procedure within `rename-job-groups-cancelled`.; > ; > [NOTE 1] migration applied in `build.yaml`; > ; > The relevant build step in `build.yaml` can be found by searching for the entry starting with the yaml below. This controls which migrations are applied and in what order.; > ; > ```yaml; > kind: createDatabase2; > name: batch_database; > databaseName: batch; > ```; > ; > [NOTE 2] estimated-current.yaml; > ; > I don't agree with why we have this. It would be nice to generate this automatically. Anyway, please keep your changes to this file as it's meant for documentation purposes only. None ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2334778045
Deployability,update,update,"> > Do you mean `estimated-current.sql` rather than `estimated-current.yaml` above?; > ; > Yes, sorry for the confusion; > ; > > Also another question is how does the schema update enforce certain order of operations.; > > The rename-job-groups-cancelled-column sql should run before other sqls that depend on the modified column name in job_groups_cancelled table, correct?; > ; > Migrations are applied successively. You cannot edit a previous migration or the order in which they're applied as they've already been applied to the production database. That's why I said this:; > ; > > > I fear you'll have to take inspiration from rename-job-groups-tables.sql by applying one ALTER TABLE command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename.; > ; > I think you need to find any trigger or stored procedure that references that column, drop it and recreate it with the field renamed. It's a little scary. @ehigham Thanks for your comments above. I’ve added the triggers and stored procedures referencing the `job_groups_cancelled` table in `rename-job-groups-cancelled-column.sql`. I was initially confused by `estimate-current.sql`; I thought it was a system-generated file to track the latest batch DDLs after a schema update, rather than a file that is manually updated. After reading this [thread](https://hail.zulipchat.com/#narrow/stream/300487-Hail-Batch-Dev/topic/mysqldump), I completely agree with your point. In other organizations I've worked with, we maintained schema changes in a separate folder, identified by release versions (e.g., semver) and the DLLs are ordered by sequence number. This way, we had a clear history of DDLs and the order they were applied, eliminating the need for files like `estimate-current.sql`. I just have one question: Do we need to manually update `estimate-current.sql` with the schema changes from `rename-job-groups-cancelled-column.sql`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2340993612
Integrability,depend,depend,"> > Do you mean `estimated-current.sql` rather than `estimated-current.yaml` above?; > ; > Yes, sorry for the confusion; > ; > > Also another question is how does the schema update enforce certain order of operations.; > > The rename-job-groups-cancelled-column sql should run before other sqls that depend on the modified column name in job_groups_cancelled table, correct?; > ; > Migrations are applied successively. You cannot edit a previous migration or the order in which they're applied as they've already been applied to the production database. That's why I said this:; > ; > > > I fear you'll have to take inspiration from rename-job-groups-tables.sql by applying one ALTER TABLE command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename.; > ; > I think you need to find any trigger or stored procedure that references that column, drop it and recreate it with the field renamed. It's a little scary. @ehigham Thanks for your comments above. I’ve added the triggers and stored procedures referencing the `job_groups_cancelled` table in `rename-job-groups-cancelled-column.sql`. I was initially confused by `estimate-current.sql`; I thought it was a system-generated file to track the latest batch DDLs after a schema update, rather than a file that is manually updated. After reading this [thread](https://hail.zulipchat.com/#narrow/stream/300487-Hail-Batch-Dev/topic/mysqldump), I completely agree with your point. In other organizations I've worked with, we maintained schema changes in a separate folder, identified by release versions (e.g., semver) and the DLLs are ordered by sequence number. This way, we had a clear history of DDLs and the order they were applied, eliminating the need for files like `estimate-current.sql`. I just have one question: Do we need to manually update `estimate-current.sql` with the schema changes from `rename-job-groups-cancelled-column.sql`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2340993612
Usability,clear,clear,"> > Do you mean `estimated-current.sql` rather than `estimated-current.yaml` above?; > ; > Yes, sorry for the confusion; > ; > > Also another question is how does the schema update enforce certain order of operations.; > > The rename-job-groups-cancelled-column sql should run before other sqls that depend on the modified column name in job_groups_cancelled table, correct?; > ; > Migrations are applied successively. You cannot edit a previous migration or the order in which they're applied as they've already been applied to the production database. That's why I said this:; > ; > > > I fear you'll have to take inspiration from rename-job-groups-tables.sql by applying one ALTER TABLE command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename.; > ; > I think you need to find any trigger or stored procedure that references that column, drop it and recreate it with the field renamed. It's a little scary. @ehigham Thanks for your comments above. I’ve added the triggers and stored procedures referencing the `job_groups_cancelled` table in `rename-job-groups-cancelled-column.sql`. I was initially confused by `estimate-current.sql`; I thought it was a system-generated file to track the latest batch DDLs after a schema update, rather than a file that is manually updated. After reading this [thread](https://hail.zulipchat.com/#narrow/stream/300487-Hail-Batch-Dev/topic/mysqldump), I completely agree with your point. In other organizations I've worked with, we maintained schema changes in a separate folder, identified by release versions (e.g., semver) and the DLLs are ordered by sequence number. This way, we had a clear history of DDLs and the order they were applied, eliminating the need for files like `estimate-current.sql`. I just have one question: Do we need to manually update `estimate-current.sql` with the schema changes from `rename-job-groups-cancelled-column.sql`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2340993612
Modifiability,refactor,refactoring,"> The changes around `toJSON` seem to be unrelated to the introduction of `VType`. Can you explain the motivation there?. This was added as a step towards a greater refactoring effort where I applied a number of changes to try and make various backend implementations look the same.; `Type`, `TableType` and `MatrixType` had a `toJSON` method, with the exception of `BlockMatrixType`. Since these are all virtual types, it seemed like a simple change to unify these methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14678#issuecomment-2341475173
Usability,simpl,simple,"> The changes around `toJSON` seem to be unrelated to the introduction of `VType`. Can you explain the motivation there?. This was added as a step towards a greater refactoring effort where I applied a number of changes to try and make various backend implementations look the same.; `Type`, `TableType` and `MatrixType` had a `toJSON` method, with the exception of `BlockMatrixType`. Since these are all virtual types, it seemed like a simple change to unify these methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14678#issuecomment-2341475173
Availability,down,down,"@jmarshall, thanks doing this - would you mind adding a simple unit test to lock down the behaviour?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2397543555
Testability,test,test,"@jmarshall, thanks doing this - would you mind adding a simple unit test to lock down the behaviour?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2397543555
Usability,simpl,simple,"@jmarshall, thanks doing this - would you mind adding a simple unit test to lock down the behaviour?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2397543555
