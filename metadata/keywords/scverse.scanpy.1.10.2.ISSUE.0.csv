id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/11:228,Energy Efficiency,reduce,reduced,228,"Is there any way to estimate the number of branching points? It seems that this number has to be explicitly given before running the algorithm, which might be difficult if it's not clear from simply looking at the dimensionally reduced data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/11
https://github.com/scverse/scanpy/issues/11:181,Usability,clear,clear,181,"Is there any way to estimate the number of branching points? It seems that this number has to be explicitly given before running the algorithm, which might be difficult if it's not clear from simply looking at the dimensionally reduced data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/11
https://github.com/scverse/scanpy/issues/11:192,Usability,simpl,simply,192,"Is there any way to estimate the number of branching points? It seems that this number has to be explicitly given before running the algorithm, which might be difficult if it's not clear from simply looking at the dimensionally reduced data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/11
https://github.com/scverse/scanpy/issues/19:96,Usability,simpl,simply,96,"so idk about you, but i think [CFFI] is pretty painless for interacting with C/C++ code:. * you simply compile a `.so` file, use `ffi.dlopen('….so')`, `ffi.cdef('void myfunc(…)')` and are able to call it from python.; * it works with PyPy; * it can be used with numpy ([example](https://gist.github.com/arjones6/5533938)). we just have to think if we need to pass complex data structures, but i assume a few dense and sparse matrices is all you need. we just have to figure that out beforehand, and how to create a sparse matrix from raw memory (AFAIK everything under the sun supports at least column compressed layout, we might want to switch `csr_matrix` → `csc_matrix`). [CFFI]: https://cffi.readthedocs.io/en/latest",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/19
https://github.com/scverse/scanpy/issues/29:100,Deployability,install,install,100,"Hi Guys,; Thanks for developing such a wonderful tool in python.; Do you mind to point me on how to install scanpy under anaconda environment? currently ""conda search scanpy"" doesn't find it?!; Thanks; Hashem",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/29
https://github.com/scverse/scanpy/issues/29:202,Security,Hash,Hashem,202,"Hi Guys,; Thanks for developing such a wonderful tool in python.; Do you mind to point me on how to install scanpy under anaconda environment? currently ""conda search scanpy"" doesn't find it?!; Thanks; Hashem",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/29
https://github.com/scverse/scanpy/issues/32:80,Availability,error,error,80,"Hello,; When I call 'dpt_scatter' with the groups parameter I get the following error:; NameError: name 'names' is not defined. It looks like this is from line 230 in scanpy/plotting/ann_data.py and the 'names' variable just doesn't exist.; I'm assuming it should just be 'groups'?. Thanks,; Sarah",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32
https://github.com/scverse/scanpy/issues/32:211,Modifiability,variab,variable,211,"Hello,; When I call 'dpt_scatter' with the groups parameter I get the following error:; NameError: name 'names' is not defined. It looks like this is from line 230 in scanpy/plotting/ann_data.py and the 'names' variable just doesn't exist.; I'm assuming it should just be 'groups'?. Thanks,; Sarah",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32
https://github.com/scverse/scanpy/issues/34:15,Availability,error,error,15,"Hi, I hit this error when trying to filter genes.; A minimal working example is included below. Any help appreciated. ```; def paul15_raw():; filename = 'data/paul15/paul15.h5'; backup_url = 'http://falexwolf.de/data/paul15.h5'; adata = sc.read(filename, 'data.debatched', backup_url=backup_url); # each row has to correspond to a sample, therefore transpose ; adata = adata.transpose() # cluster assocations identified by Paul et al.; clusters = sc.read(filename, 'cluster.id', return_dict=True)['X'].flatten(); # names reflecting the cell type identifications from the paper; cell_types = {i: 'Ery' for i in range(1, 7)}; cell_types[7] = 'MEP'; cell_types[8] = 'Mk'; cell_types[9] = 'GMP'; cell_types[10] = 'GMP'; cell_types[11] = 'DC'; cell_types[12] = 'Baso'; cell_types[13] = 'Baso'; cell_types[14] = 'Mo'; cell_types[15] = 'Mo'; cell_types[16] = 'Neu'; cell_types[17] = 'Neu'; cell_types[18] = 'Eos'; cell_types[19] = 'Other'; adata.smp['paul15_clusters'] = [str(i) + cell_types[i] for i in clusters.astype(int)]; infogenes_names = sc.read(filename, 'info.genes_strings', return_dict=True)['X']; # just keep the first of the two equivalent names per gene ; adata.var_names = np.array([gn.split(';')[0] for gn in adata.var_names]); # remove 10 corrupted gene names ; infogenes_names = np.intersect1d(infogenes_names, adata.var_names); # restrict the data to the 3461 informative genes ; adata = adata[:, infogenes_names]; adata.add['iroot'] = np.flatnonzero(adata.smp['paul15_clusters'] == '7MEP')[0]; return adata; ; adata = paul15_raw(); afilter = sc.pp.recipe_zheng17(adata, n_top_genes=1000, zero_center=True, plot=True, copy=True); ```. or ; ```; afilter = sc.pp.filter_genes_dispersion(adata, n_top_genes=1000); ```. both fail with ; ```AttributeError: 'Series' object has no attribute 'is_dtype_equal'```; when computing the dispersion norm (line 207, simple.py); ```; 207 df['dispersion_norm'] = (df['dispersion'].values # use values here as index differs; --> 208 - disp_mean_bin[df['mea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34
https://github.com/scverse/scanpy/issues/34:1864,Usability,simpl,simple,1864,"```; def paul15_raw():; filename = 'data/paul15/paul15.h5'; backup_url = 'http://falexwolf.de/data/paul15.h5'; adata = sc.read(filename, 'data.debatched', backup_url=backup_url); # each row has to correspond to a sample, therefore transpose ; adata = adata.transpose() # cluster assocations identified by Paul et al.; clusters = sc.read(filename, 'cluster.id', return_dict=True)['X'].flatten(); # names reflecting the cell type identifications from the paper; cell_types = {i: 'Ery' for i in range(1, 7)}; cell_types[7] = 'MEP'; cell_types[8] = 'Mk'; cell_types[9] = 'GMP'; cell_types[10] = 'GMP'; cell_types[11] = 'DC'; cell_types[12] = 'Baso'; cell_types[13] = 'Baso'; cell_types[14] = 'Mo'; cell_types[15] = 'Mo'; cell_types[16] = 'Neu'; cell_types[17] = 'Neu'; cell_types[18] = 'Eos'; cell_types[19] = 'Other'; adata.smp['paul15_clusters'] = [str(i) + cell_types[i] for i in clusters.astype(int)]; infogenes_names = sc.read(filename, 'info.genes_strings', return_dict=True)['X']; # just keep the first of the two equivalent names per gene ; adata.var_names = np.array([gn.split(';')[0] for gn in adata.var_names]); # remove 10 corrupted gene names ; infogenes_names = np.intersect1d(infogenes_names, adata.var_names); # restrict the data to the 3461 informative genes ; adata = adata[:, infogenes_names]; adata.add['iroot'] = np.flatnonzero(adata.smp['paul15_clusters'] == '7MEP')[0]; return adata; ; adata = paul15_raw(); afilter = sc.pp.recipe_zheng17(adata, n_top_genes=1000, zero_center=True, plot=True, copy=True); ```. or ; ```; afilter = sc.pp.filter_genes_dispersion(adata, n_top_genes=1000); ```. both fail with ; ```AttributeError: 'Series' object has no attribute 'is_dtype_equal'```; when computing the dispersion norm (line 207, simple.py); ```; 207 df['dispersion_norm'] = (df['dispersion'].values # use values here as index differs; --> 208 - disp_mean_bin[df['mean_bin']].values) \; 209 / disp_std_bin[df['mean_bin']].values; ```. Running Scanpy version 0.2.6 on 2017-08-23 14:35.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34
https://github.com/scverse/scanpy/pull/38:200,Availability,avail,available,200,"this makes it possible to use `pip install` without installing numpy. it also includes automation for cython again, as currently the `python setup.py build_ext` command will never use cython, even if available. once the .pyx is changed and `build_ext` is executed, this now refreshes the `.c` file.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/38
https://github.com/scverse/scanpy/pull/38:35,Deployability,install,install,35,"this makes it possible to use `pip install` without installing numpy. it also includes automation for cython again, as currently the `python setup.py build_ext` command will never use cython, even if available. once the .pyx is changed and `build_ext` is executed, this now refreshes the `.c` file.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/38
https://github.com/scverse/scanpy/pull/38:52,Deployability,install,installing,52,"this makes it possible to use `pip install` without installing numpy. it also includes automation for cython again, as currently the `python setup.py build_ext` command will never use cython, even if available. once the .pyx is changed and `build_ext` is executed, this now refreshes the `.c` file.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/38
https://github.com/scverse/scanpy/issues/39:591,Integrability,message,message,591,"I was trying to reproduce the results in Example 1 on notebook; https://github.com/theislab/scanpy_usage/tree/master/170505_seurat. I'm getting two problems in the filtering steps in cell 9:; 1) although genes seem to be filtered (there are 1838 genes left versus 13714 before), the plot does not show a different colour for 'highly variable' and 'other' genes. Both appear black (see attached figure). I've both tried it in a jupyter notebook and ipython. I'm running python in a conda environment with matplotlib 4.3.2.25.py35_0 and seaborn 0.8_py35. 2) There's also the following warning message, that seems to complain of a divide by zero on the mean:; /anaconda/lib/python3.5/site-packages/scanpy/preprocessing/simple.py:193: RuntimeWarning: invalid value encountered in true_divide; dispersion = var / mean; Is ; ![figure_10](https://user-images.githubusercontent.com/10065683/30990958-f0e3dec6-a457-11e7-9921-f1b6b9f72861.png). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/39
https://github.com/scverse/scanpy/issues/39:333,Modifiability,variab,variable,333,"I was trying to reproduce the results in Example 1 on notebook; https://github.com/theislab/scanpy_usage/tree/master/170505_seurat. I'm getting two problems in the filtering steps in cell 9:; 1) although genes seem to be filtered (there are 1838 genes left versus 13714 before), the plot does not show a different colour for 'highly variable' and 'other' genes. Both appear black (see attached figure). I've both tried it in a jupyter notebook and ipython. I'm running python in a conda environment with matplotlib 4.3.2.25.py35_0 and seaborn 0.8_py35. 2) There's also the following warning message, that seems to complain of a divide by zero on the mean:; /anaconda/lib/python3.5/site-packages/scanpy/preprocessing/simple.py:193: RuntimeWarning: invalid value encountered in true_divide; dispersion = var / mean; Is ; ![figure_10](https://user-images.githubusercontent.com/10065683/30990958-f0e3dec6-a457-11e7-9921-f1b6b9f72861.png). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/39
https://github.com/scverse/scanpy/issues/39:716,Usability,simpl,simple,716,"I was trying to reproduce the results in Example 1 on notebook; https://github.com/theislab/scanpy_usage/tree/master/170505_seurat. I'm getting two problems in the filtering steps in cell 9:; 1) although genes seem to be filtered (there are 1838 genes left versus 13714 before), the plot does not show a different colour for 'highly variable' and 'other' genes. Both appear black (see attached figure). I've both tried it in a jupyter notebook and ipython. I'm running python in a conda environment with matplotlib 4.3.2.25.py35_0 and seaborn 0.8_py35. 2) There's also the following warning message, that seems to complain of a divide by zero on the mean:; /anaconda/lib/python3.5/site-packages/scanpy/preprocessing/simple.py:193: RuntimeWarning: invalid value encountered in true_divide; dispersion = var / mean; Is ; ![figure_10](https://user-images.githubusercontent.com/10065683/30990958-f0e3dec6-a457-11e7-9921-f1b6b9f72861.png). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/39
https://github.com/scverse/scanpy/issues/40:60,Availability,error,error,60,"Hello,. I'm trying out the Graph abstraction and I get this error:; ```; SetKeyError Traceback (most recent call last); <ipython-input-12-928a85d4478e> in <module>(); ----> 1 sc.tl.tsne(adata); 2 sc.tl.draw_graph(adata, random_state=5) # random_state just makes a cosmetic change; 3 sc.write('krumsiek11_blobs', adata). ~/Downloads/scanpy/scanpy/tools/tsne.py in tsne(adata, n_pcs, perplexity, early_exaggeration, learning_rate, random_state, use_fast_tsne, recompute_pca, n_jobs, copy); 108 X_tsne = tsne.fit_transform(X); 109 # update AnnData instance; --> 110 adata.smp['X_tsne'] = X_tsne # annotate samples with tSNE coordinates; 111 logg.info(' finished', t=True, end=' '); 112 logg.info('and added\n'. ~/Downloads/scanpy/scanpy/data_structs/ann_data.py in __setitem__(self, keys, values); 382 # TODO: need to reallocate memory; 383 # or allow storing objects, or use pd.dataframes; --> 384 raise SetKeyError(k, v.dtype, self.dtype[k]); 385 super(BoundStructArray, self).__setitem__(k, v); 386 . SetKeyError: Currently you cannot implicitly reallocate memory:; Setting the array for key X_tsne001of002 with dtype float64 requires too much memory, you should init AnnData with a large enough data type from the beginning.; Probably you try to assign a string of length 8 although the array can only store strings of length 4.; ```. I'm using the latest git version of scanpy.; Any ideas?; Best wishes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40
https://github.com/scverse/scanpy/issues/40:322,Availability,Down,Downloads,322,"Hello,. I'm trying out the Graph abstraction and I get this error:; ```; SetKeyError Traceback (most recent call last); <ipython-input-12-928a85d4478e> in <module>(); ----> 1 sc.tl.tsne(adata); 2 sc.tl.draw_graph(adata, random_state=5) # random_state just makes a cosmetic change; 3 sc.write('krumsiek11_blobs', adata). ~/Downloads/scanpy/scanpy/tools/tsne.py in tsne(adata, n_pcs, perplexity, early_exaggeration, learning_rate, random_state, use_fast_tsne, recompute_pca, n_jobs, copy); 108 X_tsne = tsne.fit_transform(X); 109 # update AnnData instance; --> 110 adata.smp['X_tsne'] = X_tsne # annotate samples with tSNE coordinates; 111 logg.info(' finished', t=True, end=' '); 112 logg.info('and added\n'. ~/Downloads/scanpy/scanpy/data_structs/ann_data.py in __setitem__(self, keys, values); 382 # TODO: need to reallocate memory; 383 # or allow storing objects, or use pd.dataframes; --> 384 raise SetKeyError(k, v.dtype, self.dtype[k]); 385 super(BoundStructArray, self).__setitem__(k, v); 386 . SetKeyError: Currently you cannot implicitly reallocate memory:; Setting the array for key X_tsne001of002 with dtype float64 requires too much memory, you should init AnnData with a large enough data type from the beginning.; Probably you try to assign a string of length 8 although the array can only store strings of length 4.; ```. I'm using the latest git version of scanpy.; Any ideas?; Best wishes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40
https://github.com/scverse/scanpy/issues/40:710,Availability,Down,Downloads,710,"Hello,. I'm trying out the Graph abstraction and I get this error:; ```; SetKeyError Traceback (most recent call last); <ipython-input-12-928a85d4478e> in <module>(); ----> 1 sc.tl.tsne(adata); 2 sc.tl.draw_graph(adata, random_state=5) # random_state just makes a cosmetic change; 3 sc.write('krumsiek11_blobs', adata). ~/Downloads/scanpy/scanpy/tools/tsne.py in tsne(adata, n_pcs, perplexity, early_exaggeration, learning_rate, random_state, use_fast_tsne, recompute_pca, n_jobs, copy); 108 X_tsne = tsne.fit_transform(X); 109 # update AnnData instance; --> 110 adata.smp['X_tsne'] = X_tsne # annotate samples with tSNE coordinates; 111 logg.info(' finished', t=True, end=' '); 112 logg.info('and added\n'. ~/Downloads/scanpy/scanpy/data_structs/ann_data.py in __setitem__(self, keys, values); 382 # TODO: need to reallocate memory; 383 # or allow storing objects, or use pd.dataframes; --> 384 raise SetKeyError(k, v.dtype, self.dtype[k]); 385 super(BoundStructArray, self).__setitem__(k, v); 386 . SetKeyError: Currently you cannot implicitly reallocate memory:; Setting the array for key X_tsne001of002 with dtype float64 requires too much memory, you should init AnnData with a large enough data type from the beginning.; Probably you try to assign a string of length 8 although the array can only store strings of length 4.; ```. I'm using the latest git version of scanpy.; Any ideas?; Best wishes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40
https://github.com/scverse/scanpy/issues/40:530,Deployability,update,update,530,"Hello,. I'm trying out the Graph abstraction and I get this error:; ```; SetKeyError Traceback (most recent call last); <ipython-input-12-928a85d4478e> in <module>(); ----> 1 sc.tl.tsne(adata); 2 sc.tl.draw_graph(adata, random_state=5) # random_state just makes a cosmetic change; 3 sc.write('krumsiek11_blobs', adata). ~/Downloads/scanpy/scanpy/tools/tsne.py in tsne(adata, n_pcs, perplexity, early_exaggeration, learning_rate, random_state, use_fast_tsne, recompute_pca, n_jobs, copy); 108 X_tsne = tsne.fit_transform(X); 109 # update AnnData instance; --> 110 adata.smp['X_tsne'] = X_tsne # annotate samples with tSNE coordinates; 111 logg.info(' finished', t=True, end=' '); 112 logg.info('and added\n'. ~/Downloads/scanpy/scanpy/data_structs/ann_data.py in __setitem__(self, keys, values); 382 # TODO: need to reallocate memory; 383 # or allow storing objects, or use pd.dataframes; --> 384 raise SetKeyError(k, v.dtype, self.dtype[k]); 385 super(BoundStructArray, self).__setitem__(k, v); 386 . SetKeyError: Currently you cannot implicitly reallocate memory:; Setting the array for key X_tsne001of002 with dtype float64 requires too much memory, you should init AnnData with a large enough data type from the beginning.; Probably you try to assign a string of length 8 although the array can only store strings of length 4.; ```. I'm using the latest git version of scanpy.; Any ideas?; Best wishes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40
https://github.com/scverse/scanpy/issues/40:638,Testability,log,logg,638,"Hello,. I'm trying out the Graph abstraction and I get this error:; ```; SetKeyError Traceback (most recent call last); <ipython-input-12-928a85d4478e> in <module>(); ----> 1 sc.tl.tsne(adata); 2 sc.tl.draw_graph(adata, random_state=5) # random_state just makes a cosmetic change; 3 sc.write('krumsiek11_blobs', adata). ~/Downloads/scanpy/scanpy/tools/tsne.py in tsne(adata, n_pcs, perplexity, early_exaggeration, learning_rate, random_state, use_fast_tsne, recompute_pca, n_jobs, copy); 108 X_tsne = tsne.fit_transform(X); 109 # update AnnData instance; --> 110 adata.smp['X_tsne'] = X_tsne # annotate samples with tSNE coordinates; 111 logg.info(' finished', t=True, end=' '); 112 logg.info('and added\n'. ~/Downloads/scanpy/scanpy/data_structs/ann_data.py in __setitem__(self, keys, values); 382 # TODO: need to reallocate memory; 383 # or allow storing objects, or use pd.dataframes; --> 384 raise SetKeyError(k, v.dtype, self.dtype[k]); 385 super(BoundStructArray, self).__setitem__(k, v); 386 . SetKeyError: Currently you cannot implicitly reallocate memory:; Setting the array for key X_tsne001of002 with dtype float64 requires too much memory, you should init AnnData with a large enough data type from the beginning.; Probably you try to assign a string of length 8 although the array can only store strings of length 4.; ```. I'm using the latest git version of scanpy.; Any ideas?; Best wishes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40
https://github.com/scverse/scanpy/issues/40:683,Testability,log,logg,683,"Hello,. I'm trying out the Graph abstraction and I get this error:; ```; SetKeyError Traceback (most recent call last); <ipython-input-12-928a85d4478e> in <module>(); ----> 1 sc.tl.tsne(adata); 2 sc.tl.draw_graph(adata, random_state=5) # random_state just makes a cosmetic change; 3 sc.write('krumsiek11_blobs', adata). ~/Downloads/scanpy/scanpy/tools/tsne.py in tsne(adata, n_pcs, perplexity, early_exaggeration, learning_rate, random_state, use_fast_tsne, recompute_pca, n_jobs, copy); 108 X_tsne = tsne.fit_transform(X); 109 # update AnnData instance; --> 110 adata.smp['X_tsne'] = X_tsne # annotate samples with tSNE coordinates; 111 logg.info(' finished', t=True, end=' '); 112 logg.info('and added\n'. ~/Downloads/scanpy/scanpy/data_structs/ann_data.py in __setitem__(self, keys, values); 382 # TODO: need to reallocate memory; 383 # or allow storing objects, or use pd.dataframes; --> 384 raise SetKeyError(k, v.dtype, self.dtype[k]); 385 super(BoundStructArray, self).__setitem__(k, v); 386 . SetKeyError: Currently you cannot implicitly reallocate memory:; Setting the array for key X_tsne001of002 with dtype float64 requires too much memory, you should init AnnData with a large enough data type from the beginning.; Probably you try to assign a string of length 8 although the array can only store strings of length 4.; ```. I'm using the latest git version of scanpy.; Any ideas?; Best wishes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40
https://github.com/scverse/scanpy/pull/42:85,Testability,Test,Test,85,This pull request contains mainly 3 changes:. 1. Implementation of Wilcoxon-Rank-Sum Test in sc.tl.rank_gene_groups; 2. Test function for pytest (including two binary files containing ground truth for test results); 3. A (preliminary) export function for the interactive SPRING exploration tool ; https://github.com/AllonKleinLab/SPRING,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/42
https://github.com/scverse/scanpy/pull/42:120,Testability,Test,Test,120,This pull request contains mainly 3 changes:. 1. Implementation of Wilcoxon-Rank-Sum Test in sc.tl.rank_gene_groups; 2. Test function for pytest (including two binary files containing ground truth for test results); 3. A (preliminary) export function for the interactive SPRING exploration tool ; https://github.com/AllonKleinLab/SPRING,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/42
https://github.com/scverse/scanpy/pull/42:201,Testability,test,test,201,This pull request contains mainly 3 changes:. 1. Implementation of Wilcoxon-Rank-Sum Test in sc.tl.rank_gene_groups; 2. Test function for pytest (including two binary files containing ground truth for test results); 3. A (preliminary) export function for the interactive SPRING exploration tool ; https://github.com/AllonKleinLab/SPRING,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/42
https://github.com/scverse/scanpy/issues/43:85,Availability,error,error,85,"I'm trying to install scanpy through `pip install scanpy` but I'm getting this weird error; ```; $ pip install scanpy; Collecting scanpy; Downloading scanpy-0.2.9.1.tar.gz (208kB); Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-lbk_t73k/scanpy/setup.py"", line 39, in <module>; readme = readme_f.read(); File ""/opt/conda/lib/python3.6/encodings/ascii.py"", line 26, in decode; return codecs.ascii_decode(input, self.errors)[0]; UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); ```. That's with pip version `9.0.1` and python 3.6. I'm getting similar errors for older versions of scanpy, including 0.2.1. Perhaps this is a bug in pip but I'm not sure, I installed a bunch of other unrelated packages without any issues.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43
https://github.com/scverse/scanpy/issues/43:138,Availability,Down,Downloading,138,"I'm trying to install scanpy through `pip install scanpy` but I'm getting this weird error; ```; $ pip install scanpy; Collecting scanpy; Downloading scanpy-0.2.9.1.tar.gz (208kB); Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-lbk_t73k/scanpy/setup.py"", line 39, in <module>; readme = readme_f.read(); File ""/opt/conda/lib/python3.6/encodings/ascii.py"", line 26, in decode; return codecs.ascii_decode(input, self.errors)[0]; UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); ```. That's with pip version `9.0.1` and python 3.6. I'm getting similar errors for older versions of scanpy, including 0.2.1. Perhaps this is a bug in pip but I'm not sure, I installed a bunch of other unrelated packages without any issues.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43
https://github.com/scverse/scanpy/issues/43:518,Availability,error,errors,518,"I'm trying to install scanpy through `pip install scanpy` but I'm getting this weird error; ```; $ pip install scanpy; Collecting scanpy; Downloading scanpy-0.2.9.1.tar.gz (208kB); Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-lbk_t73k/scanpy/setup.py"", line 39, in <module>; readme = readme_f.read(); File ""/opt/conda/lib/python3.6/encodings/ascii.py"", line 26, in decode; return codecs.ascii_decode(input, self.errors)[0]; UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); ```. That's with pip version `9.0.1` and python 3.6. I'm getting similar errors for older versions of scanpy, including 0.2.1. Perhaps this is a bug in pip but I'm not sure, I installed a bunch of other unrelated packages without any issues.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43
https://github.com/scverse/scanpy/issues/43:704,Availability,error,errors,704,"I'm trying to install scanpy through `pip install scanpy` but I'm getting this weird error; ```; $ pip install scanpy; Collecting scanpy; Downloading scanpy-0.2.9.1.tar.gz (208kB); Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-lbk_t73k/scanpy/setup.py"", line 39, in <module>; readme = readme_f.read(); File ""/opt/conda/lib/python3.6/encodings/ascii.py"", line 26, in decode; return codecs.ascii_decode(input, self.errors)[0]; UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); ```. That's with pip version `9.0.1` and python 3.6. I'm getting similar errors for older versions of scanpy, including 0.2.1. Perhaps this is a bug in pip but I'm not sure, I installed a bunch of other unrelated packages without any issues.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43
https://github.com/scverse/scanpy/issues/43:14,Deployability,install,install,14,"I'm trying to install scanpy through `pip install scanpy` but I'm getting this weird error; ```; $ pip install scanpy; Collecting scanpy; Downloading scanpy-0.2.9.1.tar.gz (208kB); Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-lbk_t73k/scanpy/setup.py"", line 39, in <module>; readme = readme_f.read(); File ""/opt/conda/lib/python3.6/encodings/ascii.py"", line 26, in decode; return codecs.ascii_decode(input, self.errors)[0]; UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); ```. That's with pip version `9.0.1` and python 3.6. I'm getting similar errors for older versions of scanpy, including 0.2.1. Perhaps this is a bug in pip but I'm not sure, I installed a bunch of other unrelated packages without any issues.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43
https://github.com/scverse/scanpy/issues/43:42,Deployability,install,install,42,"I'm trying to install scanpy through `pip install scanpy` but I'm getting this weird error; ```; $ pip install scanpy; Collecting scanpy; Downloading scanpy-0.2.9.1.tar.gz (208kB); Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-lbk_t73k/scanpy/setup.py"", line 39, in <module>; readme = readme_f.read(); File ""/opt/conda/lib/python3.6/encodings/ascii.py"", line 26, in decode; return codecs.ascii_decode(input, self.errors)[0]; UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); ```. That's with pip version `9.0.1` and python 3.6. I'm getting similar errors for older versions of scanpy, including 0.2.1. Perhaps this is a bug in pip but I'm not sure, I installed a bunch of other unrelated packages without any issues.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43
https://github.com/scverse/scanpy/issues/43:103,Deployability,install,install,103,"I'm trying to install scanpy through `pip install scanpy` but I'm getting this weird error; ```; $ pip install scanpy; Collecting scanpy; Downloading scanpy-0.2.9.1.tar.gz (208kB); Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-lbk_t73k/scanpy/setup.py"", line 39, in <module>; readme = readme_f.read(); File ""/opt/conda/lib/python3.6/encodings/ascii.py"", line 26, in decode; return codecs.ascii_decode(input, self.errors)[0]; UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); ```. That's with pip version `9.0.1` and python 3.6. I'm getting similar errors for older versions of scanpy, including 0.2.1. Perhaps this is a bug in pip but I'm not sure, I installed a bunch of other unrelated packages without any issues.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43
https://github.com/scverse/scanpy/issues/43:807,Deployability,install,installed,807,"I'm trying to install scanpy through `pip install scanpy` but I'm getting this weird error; ```; $ pip install scanpy; Collecting scanpy; Downloading scanpy-0.2.9.1.tar.gz (208kB); Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-lbk_t73k/scanpy/setup.py"", line 39, in <module>; readme = readme_f.read(); File ""/opt/conda/lib/python3.6/encodings/ascii.py"", line 26, in decode; return codecs.ascii_decode(input, self.errors)[0]; UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); ```. That's with pip version `9.0.1` and python 3.6. I'm getting similar errors for older versions of scanpy, including 0.2.1. Perhaps this is a bug in pip but I'm not sure, I installed a bunch of other unrelated packages without any issues.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43
https://github.com/scverse/scanpy/issues/45:581,Testability,log,log,581,"hi Alex; here my list :) thanks a lot! and I might expand it.... 'needed'. - [x] concatenate mulitple data sets; - [ ] scale up sc.pp.regress_out, add some function for batch correction?; - [x] additional heatmap annotation for cells/genes (e.g. .smp); - [ ] aga-graph: labels in pie charts are misleading (sometimes switched), maybe better to have just a legend with the colors than labeling every node. 'nice to have'; - [x] In scatterplot showing gene expression: plot points ordered by expression, i.d. cells with higher expression on top of cells with lower expression; - [x] log transform (let user choose the base in sc.pp.log1p (natural log., log2 or log10); - [x] cell cycle scoring (and scoring for any other gene list); - [x] function and plots for basic qc metrics (n_counts, n_genes, CV, %drop out) ; - [ ] sc.pl.scatter also for genes (adata.var, e.g. to plot e.g. mean expression vs dropout rate); - [x] table for high scoring genes, in addtition to sc.pl.rank_genes_groups; - [x] additional differential expression test; - [x] Heatmap for genes per cluster/sample/condition (not just along a path in pseuodtime order) including custom sample and gene annotation; - [ ] Maybe also an option for simplified visualization with just mean expression per cluster.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45
https://github.com/scverse/scanpy/issues/45:645,Testability,log,log,645,"hi Alex; here my list :) thanks a lot! and I might expand it.... 'needed'. - [x] concatenate mulitple data sets; - [ ] scale up sc.pp.regress_out, add some function for batch correction?; - [x] additional heatmap annotation for cells/genes (e.g. .smp); - [ ] aga-graph: labels in pie charts are misleading (sometimes switched), maybe better to have just a legend with the colors than labeling every node. 'nice to have'; - [x] In scatterplot showing gene expression: plot points ordered by expression, i.d. cells with higher expression on top of cells with lower expression; - [x] log transform (let user choose the base in sc.pp.log1p (natural log., log2 or log10); - [x] cell cycle scoring (and scoring for any other gene list); - [x] function and plots for basic qc metrics (n_counts, n_genes, CV, %drop out) ; - [ ] sc.pl.scatter also for genes (adata.var, e.g. to plot e.g. mean expression vs dropout rate); - [x] table for high scoring genes, in addtition to sc.pl.rank_genes_groups; - [x] additional differential expression test; - [x] Heatmap for genes per cluster/sample/condition (not just along a path in pseuodtime order) including custom sample and gene annotation; - [ ] Maybe also an option for simplified visualization with just mean expression per cluster.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45
https://github.com/scverse/scanpy/issues/45:1031,Testability,test,test,1031,"hi Alex; here my list :) thanks a lot! and I might expand it.... 'needed'. - [x] concatenate mulitple data sets; - [ ] scale up sc.pp.regress_out, add some function for batch correction?; - [x] additional heatmap annotation for cells/genes (e.g. .smp); - [ ] aga-graph: labels in pie charts are misleading (sometimes switched), maybe better to have just a legend with the colors than labeling every node. 'nice to have'; - [x] In scatterplot showing gene expression: plot points ordered by expression, i.d. cells with higher expression on top of cells with lower expression; - [x] log transform (let user choose the base in sc.pp.log1p (natural log., log2 or log10); - [x] cell cycle scoring (and scoring for any other gene list); - [x] function and plots for basic qc metrics (n_counts, n_genes, CV, %drop out) ; - [ ] sc.pl.scatter also for genes (adata.var, e.g. to plot e.g. mean expression vs dropout rate); - [x] table for high scoring genes, in addtition to sc.pl.rank_genes_groups; - [x] additional differential expression test; - [x] Heatmap for genes per cluster/sample/condition (not just along a path in pseuodtime order) including custom sample and gene annotation; - [ ] Maybe also an option for simplified visualization with just mean expression per cluster.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45
https://github.com/scverse/scanpy/issues/45:1210,Usability,simpl,simplified,1210,"hi Alex; here my list :) thanks a lot! and I might expand it.... 'needed'. - [x] concatenate mulitple data sets; - [ ] scale up sc.pp.regress_out, add some function for batch correction?; - [x] additional heatmap annotation for cells/genes (e.g. .smp); - [ ] aga-graph: labels in pie charts are misleading (sometimes switched), maybe better to have just a legend with the colors than labeling every node. 'nice to have'; - [x] In scatterplot showing gene expression: plot points ordered by expression, i.d. cells with higher expression on top of cells with lower expression; - [x] log transform (let user choose the base in sc.pp.log1p (natural log., log2 or log10); - [x] cell cycle scoring (and scoring for any other gene list); - [x] function and plots for basic qc metrics (n_counts, n_genes, CV, %drop out) ; - [ ] sc.pl.scatter also for genes (adata.var, e.g. to plot e.g. mean expression vs dropout rate); - [x] table for high scoring genes, in addtition to sc.pl.rank_genes_groups; - [x] additional differential expression test; - [x] Heatmap for genes per cluster/sample/condition (not just along a path in pseuodtime order) including custom sample and gene annotation; - [ ] Maybe also an option for simplified visualization with just mean expression per cluster.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45
https://github.com/scverse/scanpy/issues/47:37,Usability,clear,clearly,37,"Hi,. May be I do not understand this clearly, but does the function `sc.pp.normalize_per_cell` not supposed normalize the counts? However when I run this function, I see that the values do not change at all. Am I doing something wrong here?. raw_data.X.toarray().sum(axis=1); ; Out[18]: array([ 23037., 18883., 20755., ..., 14785., 20996., 7604.], dtype=float32). normed_data = sc.pp.normalize_per_cell(raw_data, copy=True); normed_data.X.toarray().sum(axis=1). Out[19]: array([ 23037., 18883., 20755., ..., 14785., 20996., 7604.], dtype=float32). Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/47
https://github.com/scverse/scanpy/issues/48:383,Safety,avoid,avoided,383,"Hi,. In the example notebook, `seurat.ipynb`, the function `sc.pp.normalize_per_cell()` is run before `sc.pp.regress_out()`. Is it not better to regress out the effect of n_counts before normalization? I do not completely understand this and it would be great if the authors could explain this order of pre-processing. Also, is there certain order(s) of steps which should always be avoided?. Thank you. Best,; Parashar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48
https://github.com/scverse/scanpy/issues/49:160,Availability,Down,Downloading,160,"Hello,. I've tried setting up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:635,Availability,error,errors,635,"Hello,. I've tried setting up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:845,Availability,error,error,845,"Hello,. I've tried setting up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:958,Availability,down,downloaded,958,"Hello,. I've tried setting up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:1270,Availability,avail,available,1270,"ing up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevance to you too. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:122,Deployability,install,install,122,"Hello,. I've tried setting up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:1024,Deployability,install,installation,1024,"Hello,. I've tried setting up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:1121,Deployability,install,installation,1121,"ing up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevance to you too. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:1309,Deployability,install,install,1309,"ing up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevance to you too. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:1404,Deployability,install,installer,1404,"ing up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevance to you too. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:1461,Deployability,install,install,1461,"ing up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevance to you too. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:1510,Deployability,Install,Installing,1510,"ing up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevance to you too. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:1590,Deployability,install,installer,1590,"ing up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevance to you too. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/49:1863,Deployability,Install,Installing,1863,"ing up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevance to you too. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49
https://github.com/scverse/scanpy/issues/51:50,Modifiability,extend,extending,50,This collects new features of version 0.3.1 while extending other issues such as https://github.com/theislab/scanpy/issues/45.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/51
https://github.com/scverse/scanpy/issues/52:973,Availability,error,error,973,"I am trying to toy with the krumsiek11 model, but the ```sc.tl.sim``` call seems to ignore parameters and always uses the parameters from the ```krumsiek11_params.txt``` file. In particular, running:. ```; adam_krumsiek11 = sc.tl.sim('krumsiek11'); adam_krumsiek11_2 = sc.tl.sim('krumsiek11', nrRealizations = 1, seed = 1665487); sc.pl.sim(adam_krumsiek11 ); sc.pl.sim(adam_krumsiek11_2); ```; produces two exactly identical figures with 4 realizations. I also tried to set ```read_params_from_file = False``` (this is not documented at http://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.sim.html but seemed relevant). However, running; ```; adam_krumsiek11 = sc.tl.sim('krumsiek11', nrRealizations = 1, read_params_from_file = False); sc.pl.sim(adam_krumsiek11); ```; results in ```IndexError```; and running ; ```; adam_krumsiek11 = sc.tl.sim('krumsiek11', nrRealizations = 1, tmax = 800, read_params_from_file = False); sc.pl.sim(adam_krumsiek11); ```; avoids the error, but gives the exact same figure as the first code segment. Maybe I am not understanding correctly, how the function should work? (in which case this would be a documentation issue) Or is there really something wrong?. Thanks for any hints.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/52
https://github.com/scverse/scanpy/issues/52:962,Safety,avoid,avoids,962,"I am trying to toy with the krumsiek11 model, but the ```sc.tl.sim``` call seems to ignore parameters and always uses the parameters from the ```krumsiek11_params.txt``` file. In particular, running:. ```; adam_krumsiek11 = sc.tl.sim('krumsiek11'); adam_krumsiek11_2 = sc.tl.sim('krumsiek11', nrRealizations = 1, seed = 1665487); sc.pl.sim(adam_krumsiek11 ); sc.pl.sim(adam_krumsiek11_2); ```; produces two exactly identical figures with 4 realizations. I also tried to set ```read_params_from_file = False``` (this is not documented at http://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.sim.html but seemed relevant). However, running; ```; adam_krumsiek11 = sc.tl.sim('krumsiek11', nrRealizations = 1, read_params_from_file = False); sc.pl.sim(adam_krumsiek11); ```; results in ```IndexError```; and running ; ```; adam_krumsiek11 = sc.tl.sim('krumsiek11', nrRealizations = 1, tmax = 800, read_params_from_file = False); sc.pl.sim(adam_krumsiek11); ```; avoids the error, but gives the exact same figure as the first code segment. Maybe I am not understanding correctly, how the function should work? (in which case this would be a documentation issue) Or is there really something wrong?. Thanks for any hints.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/52
https://github.com/scverse/scanpy/issues/53:137,Safety,predict,predict,137,"Thanks for you work!. There is a feature that I would be nice to have. Once the DPT has been calculated, it would be nice to be able to ""predict"" (like in the R package destiny) new data into the calculate DifussionMap . It would be great to include a label that tells apart the new cells from the already existing ones. Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/53
https://github.com/scverse/scanpy/issues/55:95,Performance,perform,performed,95,"In DropSeq experiments cell names are encoded by 12nt barcodes. It seems that no name check is performed when merging multiple datasets in ScanPy. . ```python; >>> from collections import Counter; >>> import scanpy.api as sc. >>> f = sc.read(""data1.txt"").transpose(); >>> g = sc.read(""data2.txt"").transpose(); >>> c = f.concatenate(g). >>> len(c.obs_names); 7932; >>> len(set(c.obs_names)); 7890. >>> cc = Counter(c.obs_names); >>> cc.most_common(10); [('AAAAAAAAAAAA', 2), ('TCCTGTCTCTTA', 2), ('CGCAAGGGAAAG', 2), ('ACCCGTCTATGT', 2), ('CTCCTGTCTCTT', 2), ('TTCCTGTCTCTT', 2), ('CCCTGTCTCTTA', 2), ('CCGCTGTCTCTT', 2), ('GACAAACCTACC', 2), ('ACACTGTCTCTT', 2)]. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55
https://github.com/scverse/scanpy/issues/56:11,Deployability,install,installed,11,"Recently I installed scanpy 0.4. However, with this new version I could not correctly load result files generated by an old version (v0.2.8). In particular, I could not load the old add_keys as uni_keys. Any suggestions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56
https://github.com/scverse/scanpy/issues/56:86,Performance,load,load,86,"Recently I installed scanpy 0.4. However, with this new version I could not correctly load result files generated by an old version (v0.2.8). In particular, I could not load the old add_keys as uni_keys. Any suggestions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56
https://github.com/scverse/scanpy/issues/56:169,Performance,load,load,169,"Recently I installed scanpy 0.4. However, with this new version I could not correctly load result files generated by an old version (v0.2.8). In particular, I could not load the old add_keys as uni_keys. Any suggestions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56
https://github.com/scverse/scanpy/issues/58:452,Modifiability,inherit,inherits,452,"- [x] It would be better to refer to the anndata docs for the AnnData class (http://anndata.readthedocs.io/en/latest/anndata.AnnData.html) whenever `:class:~scanpy.api.AnnData` appears. `:class:AnnData <http://anndata.readthedocs.io/en/latest/anndata.AnnData.html>` has the correct css style, but does not hyperlink. Probably a solution via http://www.sphinx-doc.org/en/1.5.1/ext/extlinks.html together with the definition of an `:extclass:` role that inherits the `:class:` properties would be the correct way to do it.; - [x] A few references, like ""[Traag1723]"" are not rendered correctly... Who knows what's going on there. I couldn't figure it out with a few tests... Let's see.; - [x] the Neighbors class docstring doesn't render properly; - [ ] changing to the slim docstring style from the numpy docstring style messes up readability when calling the docstring lookup in jupyter or other IDEs, hence I'd advocate for maintaining this information. ## AnnData; - [x] `__init__` method appears in `AnnData`; - [x] `attributes` appear after `methods`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/58
https://github.com/scverse/scanpy/issues/58:664,Testability,test,tests,664,"- [x] It would be better to refer to the anndata docs for the AnnData class (http://anndata.readthedocs.io/en/latest/anndata.AnnData.html) whenever `:class:~scanpy.api.AnnData` appears. `:class:AnnData <http://anndata.readthedocs.io/en/latest/anndata.AnnData.html>` has the correct css style, but does not hyperlink. Probably a solution via http://www.sphinx-doc.org/en/1.5.1/ext/extlinks.html together with the definition of an `:extclass:` role that inherits the `:class:` properties would be the correct way to do it.; - [x] A few references, like ""[Traag1723]"" are not rendered correctly... Who knows what's going on there. I couldn't figure it out with a few tests... Let's see.; - [x] the Neighbors class docstring doesn't render properly; - [ ] changing to the slim docstring style from the numpy docstring style messes up readability when calling the docstring lookup in jupyter or other IDEs, hence I'd advocate for maintaining this information. ## AnnData; - [x] `__init__` method appears in `AnnData`; - [x] `attributes` appear after `methods`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/58
https://github.com/scverse/scanpy/issues/59:413,Availability,error,errors,413,"E.g. matplotlib is only necessary when plotting, and for e.g. Docker images, it would be useful to have a slim scanpy core. An idea would be to do it like Jupyter:. - A `scanpy-core` PyPI package with just the essentials.; - A `scanpy` metapackage, which depends on `scanpy-core` and most (or all) of the optional dependencies. Users doing `pip install scanpy` will get the full package, with no annoying runtime errors, and packagers needing flexibility get `scanpy-core` and can slim everything down as needed. cc @hensing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59
https://github.com/scverse/scanpy/issues/59:497,Availability,down,down,497,"E.g. matplotlib is only necessary when plotting, and for e.g. Docker images, it would be useful to have a slim scanpy core. An idea would be to do it like Jupyter:. - A `scanpy-core` PyPI package with just the essentials.; - A `scanpy` metapackage, which depends on `scanpy-core` and most (or all) of the optional dependencies. Users doing `pip install scanpy` will get the full package, with no annoying runtime errors, and packagers needing flexibility get `scanpy-core` and can slim everything down as needed. cc @hensing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59
https://github.com/scverse/scanpy/issues/59:345,Deployability,install,install,345,"E.g. matplotlib is only necessary when plotting, and for e.g. Docker images, it would be useful to have a slim scanpy core. An idea would be to do it like Jupyter:. - A `scanpy-core` PyPI package with just the essentials.; - A `scanpy` metapackage, which depends on `scanpy-core` and most (or all) of the optional dependencies. Users doing `pip install scanpy` will get the full package, with no annoying runtime errors, and packagers needing flexibility get `scanpy-core` and can slim everything down as needed. cc @hensing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59
https://github.com/scverse/scanpy/issues/59:255,Integrability,depend,depends,255,"E.g. matplotlib is only necessary when plotting, and for e.g. Docker images, it would be useful to have a slim scanpy core. An idea would be to do it like Jupyter:. - A `scanpy-core` PyPI package with just the essentials.; - A `scanpy` metapackage, which depends on `scanpy-core` and most (or all) of the optional dependencies. Users doing `pip install scanpy` will get the full package, with no annoying runtime errors, and packagers needing flexibility get `scanpy-core` and can slim everything down as needed. cc @hensing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59
https://github.com/scverse/scanpy/issues/59:314,Integrability,depend,dependencies,314,"E.g. matplotlib is only necessary when plotting, and for e.g. Docker images, it would be useful to have a slim scanpy core. An idea would be to do it like Jupyter:. - A `scanpy-core` PyPI package with just the essentials.; - A `scanpy` metapackage, which depends on `scanpy-core` and most (or all) of the optional dependencies. Users doing `pip install scanpy` will get the full package, with no annoying runtime errors, and packagers needing flexibility get `scanpy-core` and can slim everything down as needed. cc @hensing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59
https://github.com/scverse/scanpy/pull/60:20,Testability,test,tests,20,I also re-activated tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/60
https://github.com/scverse/scanpy/issues/61:324,Testability,test,tests,324,"`rank_genes_groups` “returns” two recarrays, each with the shape #cells×#groups. one of them stores gene IDs, one the genes’ scores. the problem with this is that recarrays store their column index (names) in the dtype, in a place where only strings are accepted. however users (and indeed both our wilcoxon example and the tests) may choose to use numeric group IDs. genes with score 0 are unimportant anyway, so maybe we should return sparse data, in the form of a long-form recarray with something like this shape (with `<group_by>` being the `rank_genes_groups` parameter of the same name):. obs | var | <group_by> | score; -- | -- | -- | --; 0 | ENSGXXXX | 5 | 9.728; … | … | … | …. This way the three IDs can have user-defined types, and the data is easier to process via e.g. `pd.DataFrame.fromrecords(adata.obs['gene_ranking'])` . The data should probably be sorted by descending z-scores by group, i.e. if it was a DataFrame: `return gene_ranking.groupby(group_by).sort_values('score')`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/61
https://github.com/scverse/scanpy/issues/62:156,Availability,error,error,156,"I want to split AnnData after tl.diffmap according to each cell's library. But it appears that row-slicing AnnData after diffmap, dpt, or louvain gives the error message `AttributeError: 'AnnData' object has no attribute '_n_obs'`. But AnnData.X and AnnData.obs can be sliced. Could you please give me advice?. ```py; >>> adata = sc.read_10x_h5('filtered_gene_bc_matrices_h5.h5', 'mm10'); >>> scanpy.api.tl.diffmap(adata); >>> adata_diffmap[:, 0]; View of AnnData object with n_obs × n_vars = 5000 × 1; >>> adata_diffmap[0, :] ; AttributeError: 'AnnData' object has no attribute '_n_obs'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/62
https://github.com/scverse/scanpy/issues/62:162,Integrability,message,message,162,"I want to split AnnData after tl.diffmap according to each cell's library. But it appears that row-slicing AnnData after diffmap, dpt, or louvain gives the error message `AttributeError: 'AnnData' object has no attribute '_n_obs'`. But AnnData.X and AnnData.obs can be sliced. Could you please give me advice?. ```py; >>> adata = sc.read_10x_h5('filtered_gene_bc_matrices_h5.h5', 'mm10'); >>> scanpy.api.tl.diffmap(adata); >>> adata_diffmap[:, 0]; View of AnnData object with n_obs × n_vars = 5000 × 1; >>> adata_diffmap[0, :] ; AttributeError: 'AnnData' object has no attribute '_n_obs'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/62
https://github.com/scverse/scanpy/issues/63:67,Availability,error,error,67,I found that running the function 'tl.rank_genes_groups' gives the error the following error message:; UnboundLocalError: local variable 'adata_comp' referenced before assignment. ![scanpy api tl rank_genes_groups_error](https://user-images.githubusercontent.com/35155633/34642043-0191dce0-f305-11e7-847f-37b1ff34a77d.png),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63
https://github.com/scverse/scanpy/issues/63:87,Availability,error,error,87,I found that running the function 'tl.rank_genes_groups' gives the error the following error message:; UnboundLocalError: local variable 'adata_comp' referenced before assignment. ![scanpy api tl rank_genes_groups_error](https://user-images.githubusercontent.com/35155633/34642043-0191dce0-f305-11e7-847f-37b1ff34a77d.png),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63
https://github.com/scverse/scanpy/issues/63:93,Integrability,message,message,93,I found that running the function 'tl.rank_genes_groups' gives the error the following error message:; UnboundLocalError: local variable 'adata_comp' referenced before assignment. ![scanpy api tl rank_genes_groups_error](https://user-images.githubusercontent.com/35155633/34642043-0191dce0-f305-11e7-847f-37b1ff34a77d.png),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63
https://github.com/scverse/scanpy/issues/63:128,Modifiability,variab,variable,128,I found that running the function 'tl.rank_genes_groups' gives the error the following error message:; UnboundLocalError: local variable 'adata_comp' referenced before assignment. ![scanpy api tl rank_genes_groups_error](https://user-images.githubusercontent.com/35155633/34642043-0191dce0-f305-11e7-847f-37b1ff34a77d.png),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63
https://github.com/scverse/scanpy/issues/64:19,Availability,error,error,19,"I encountered this error when using data with a relatively small number of cells (~2,600). I have not encountered this error with my previous data with more cells (>10,000). ![sc pp scale_error](https://user-images.githubusercontent.com/35155633/34744836-6f325a80-f586-11e7-963f-34d14c1e1399.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/64
https://github.com/scverse/scanpy/issues/64:119,Availability,error,error,119,"I encountered this error when using data with a relatively small number of cells (~2,600). I have not encountered this error with my previous data with more cells (>10,000). ![sc pp scale_error](https://user-images.githubusercontent.com/35155633/34744836-6f325a80-f586-11e7-963f-34d14c1e1399.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/64
https://github.com/scverse/scanpy/issues/65:169,Usability,simpl,simple,169,"I've been able to successfully use scanpy for MEX-formatted datasets, and the documentation here was great in the Jupyter notebooks. Many of our other datasets are in a simple tab format where the first column is gene symbol and the rest are in GROUP--CELLID format, like this:. ```; Gene_symbol	lymph_1--cell_avg	lymph_1--Cell_1	lymph_1--Cell_10; A1BG.AS1	13.9085855833156	0	54.3778851449283; A2M.AS1	10.2185780428145	0	0; A2MP1	0	0	0; AADACL2	0	0	0; AAGAB	136.889472532613	0	0; AAR2	76.3090843598131	0	0; AATF	360.127068564485	0	0; AATK	2.93980819712579	0	0; AATK.AS1	0	0	0; ```. These contain up to 30,000 rows and (so far) 400,000 columns. . I'd love to see more use cases for how to absorb different formats of data into scanpy. I'd be happy to help write it, but so far have only got MEX data to work.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/65
https://github.com/scverse/scanpy/issues/66:466,Performance,cache,cache,466,"I have a short script which reads a tab file and writes h5 using scanpy. I've found that unless I provide a full path to the write() function or at least a relative one via ""./foo.h5"" it fails. Simplified version:. ```py; adata = sc.read(args.input_file, ext='txt', first_column_names=True).transpose(); adata.write('./test.h5') # this works; adata.write('test2.h5') # this fails; ```. Here's the stack:. ```pytb; WARNING: This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; Traceback (most recent call last):; File ""./convert_gear_group_single_cell_to_hdf5.py"", line 47, in <module>; main(); File ""./convert_gear_group_single_cell_to_hdf5.py"", line 43, in main; adata.write('test2.h5'); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1471, in write; compression=compression, compression_opts=compression_opts); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1513, in _write_h5ad; os.makedirs(os.path.dirname(filename)); File ""/usr/lib/python3.5/os.py"", line 241, in makedirs; mkdir(name, mode); FileNotFoundError: [Errno 2] No such file or directory: ''; _____________________________. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/66
https://github.com/scverse/scanpy/issues/66:520,Performance,cache,cache,520,"I have a short script which reads a tab file and writes h5 using scanpy. I've found that unless I provide a full path to the write() function or at least a relative one via ""./foo.h5"" it fails. Simplified version:. ```py; adata = sc.read(args.input_file, ext='txt', first_column_names=True).transpose(); adata.write('./test.h5') # this works; adata.write('test2.h5') # this fails; ```. Here's the stack:. ```pytb; WARNING: This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; Traceback (most recent call last):; File ""./convert_gear_group_single_cell_to_hdf5.py"", line 47, in <module>; main(); File ""./convert_gear_group_single_cell_to_hdf5.py"", line 43, in main; adata.write('test2.h5'); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1471, in write; compression=compression, compression_opts=compression_opts); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1513, in _write_h5ad; os.makedirs(os.path.dirname(filename)); File ""/usr/lib/python3.5/os.py"", line 241, in makedirs; mkdir(name, mode); FileNotFoundError: [Errno 2] No such file or directory: ''; _____________________________. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/66
https://github.com/scverse/scanpy/issues/66:319,Testability,test,test,319,"I have a short script which reads a tab file and writes h5 using scanpy. I've found that unless I provide a full path to the write() function or at least a relative one via ""./foo.h5"" it fails. Simplified version:. ```py; adata = sc.read(args.input_file, ext='txt', first_column_names=True).transpose(); adata.write('./test.h5') # this works; adata.write('test2.h5') # this fails; ```. Here's the stack:. ```pytb; WARNING: This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; Traceback (most recent call last):; File ""./convert_gear_group_single_cell_to_hdf5.py"", line 47, in <module>; main(); File ""./convert_gear_group_single_cell_to_hdf5.py"", line 43, in main; adata.write('test2.h5'); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1471, in write; compression=compression, compression_opts=compression_opts); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1513, in _write_h5ad; os.makedirs(os.path.dirname(filename)); File ""/usr/lib/python3.5/os.py"", line 241, in makedirs; mkdir(name, mode); FileNotFoundError: [Errno 2] No such file or directory: ''; _____________________________. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/66
https://github.com/scverse/scanpy/issues/66:194,Usability,Simpl,Simplified,194,"I have a short script which reads a tab file and writes h5 using scanpy. I've found that unless I provide a full path to the write() function or at least a relative one via ""./foo.h5"" it fails. Simplified version:. ```py; adata = sc.read(args.input_file, ext='txt', first_column_names=True).transpose(); adata.write('./test.h5') # this works; adata.write('test2.h5') # this fails; ```. Here's the stack:. ```pytb; WARNING: This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; Traceback (most recent call last):; File ""./convert_gear_group_single_cell_to_hdf5.py"", line 47, in <module>; main(); File ""./convert_gear_group_single_cell_to_hdf5.py"", line 43, in main; adata.write('test2.h5'); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1471, in write; compression=compression, compression_opts=compression_opts); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1513, in _write_h5ad; os.makedirs(os.path.dirname(filename)); File ""/usr/lib/python3.5/os.py"", line 241, in makedirs; mkdir(name, mode); FileNotFoundError: [Errno 2] No such file or directory: ''; _____________________________. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/66
https://github.com/scverse/scanpy/pull/67:91,Availability,error,error,91,`sc.datasets.paul15_raw()` fails with `attempted relative import beyond top-level package` error due to the wrong module path in `sc.utils.check_presence_download `. . Simply run `sc.datasets.paul15_raw()` or `sc.datasets.paul15()` to reproduce.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/67
https://github.com/scverse/scanpy/pull/67:168,Usability,Simpl,Simply,168,`sc.datasets.paul15_raw()` fails with `attempted relative import beyond top-level package` error due to the wrong module path in `sc.utils.check_presence_download `. . Simply run `sc.datasets.paul15_raw()` or `sc.datasets.paul15()` to reproduce.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/67
https://github.com/scverse/scanpy/issues/70:71,Performance,load,load,71,"I'd be happy to add this once I could figure it out. I've been able to load my tabular text files, store them as h5ad, then load them back again. I cannot see how to iterate over rows, then columns so that I can access all values of the dataframe with an awareness of which row/column each belongs to. My wishful code example:. ```py; adata = sc.read_h5ad(filename); selected = adata[:, adata.var_names.isin({'AAR2', 'ECT2'})]. ## this line spews information on the columns like:; # Empty DataFrameView; # Columns: []; # Index: [Cancer--Cell_1, Cancer--Cell_10, Cancer--Cell_100, Cancer--Cell_1000, Cancer--Cell_1001; #print(selected.obs). ## this line gives the row information:; # Empty DataFrameView; # Columns: []; #Index: [AAR2, ECT2]; #print(selected.var); ; # Nothing happens here at all; #for i, row in selected.obs.iteritems():; # print(i, row). for gene_name, row in selected.var.iterrows():; # this prints like: Series([], Name: AAR2, dtype: float64); print(row). # Nothing happens here; for cell_name, val in row.iteritems():; print(""{0}\t{1}\t{2}"".format(gene_name, cell_name, val)); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/70
https://github.com/scverse/scanpy/issues/70:124,Performance,load,load,124,"I'd be happy to add this once I could figure it out. I've been able to load my tabular text files, store them as h5ad, then load them back again. I cannot see how to iterate over rows, then columns so that I can access all values of the dataframe with an awareness of which row/column each belongs to. My wishful code example:. ```py; adata = sc.read_h5ad(filename); selected = adata[:, adata.var_names.isin({'AAR2', 'ECT2'})]. ## this line spews information on the columns like:; # Empty DataFrameView; # Columns: []; # Index: [Cancer--Cell_1, Cancer--Cell_10, Cancer--Cell_100, Cancer--Cell_1000, Cancer--Cell_1001; #print(selected.obs). ## this line gives the row information:; # Empty DataFrameView; # Columns: []; #Index: [AAR2, ECT2]; #print(selected.var); ; # Nothing happens here at all; #for i, row in selected.obs.iteritems():; # print(i, row). for gene_name, row in selected.var.iterrows():; # this prints like: Series([], Name: AAR2, dtype: float64); print(row). # Nothing happens here; for cell_name, val in row.iteritems():; print(""{0}\t{1}\t{2}"".format(gene_name, cell_name, val)); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/70
https://github.com/scverse/scanpy/issues/70:212,Security,access,access,212,"I'd be happy to add this once I could figure it out. I've been able to load my tabular text files, store them as h5ad, then load them back again. I cannot see how to iterate over rows, then columns so that I can access all values of the dataframe with an awareness of which row/column each belongs to. My wishful code example:. ```py; adata = sc.read_h5ad(filename); selected = adata[:, adata.var_names.isin({'AAR2', 'ECT2'})]. ## this line spews information on the columns like:; # Empty DataFrameView; # Columns: []; # Index: [Cancer--Cell_1, Cancer--Cell_10, Cancer--Cell_100, Cancer--Cell_1000, Cancer--Cell_1001; #print(selected.obs). ## this line gives the row information:; # Empty DataFrameView; # Columns: []; #Index: [AAR2, ECT2]; #print(selected.var); ; # Nothing happens here at all; #for i, row in selected.obs.iteritems():; # print(i, row). for gene_name, row in selected.var.iterrows():; # this prints like: Series([], Name: AAR2, dtype: float64); print(row). # Nothing happens here; for cell_name, val in row.iteritems():; print(""{0}\t{1}\t{2}"".format(gene_name, cell_name, val)); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/70
https://github.com/scverse/scanpy/issues/71:720,Usability,clear,clear,720,"After remembering the originally specified number of genes:. https://github.com/theislab/scanpy/blob/ce15a2c5d5ef45c8eb0c041b1513f768a02a5299/scanpy/tools/rank_genes_groups.py#L73-L74. We set `n_genes` to `X.shape[1]`, …twice:. https://github.com/theislab/scanpy/blob/ce15a2c5d5ef45c8eb0c041b1513f768a02a5299/scanpy/tools/rank_genes_groups.py#L99-L100. https://github.com/theislab/scanpy/blob/ce15a2c5d5ef45c8eb0c041b1513f768a02a5299/scanpy/tools/rank_genes_groups.py#L105. Afterwards, we still use, `n_genes_user`, even though `np.argpartition` can only handle at most `X.shape[1]` genes. https://github.com/theislab/scanpy/blob/ce15a2c5d5ef45c8eb0c041b1513f768a02a5299/scanpy/tools/rank_genes_groups.py#L145. It’s not clear to me what `n_genes` is supposed to hold, and what the parameter to `np.argpartition` should be, but the current code crashes if `X.shape[1] < 100` (or the user-specified `n_genes`)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/71
https://github.com/scverse/scanpy/issues/72:31,Performance,scalab,scalability,31,"We are very impressed with the scalability of scanpy. We are interested in performing gene co-expression clustering on large single-cell RNAseq datasets. This typically involves calculating pairwise correlations between genes, then using these correlations as distance metrics for hierarchical and k-means clustering. Does scanpy already support these kinds of analyses?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72
https://github.com/scverse/scanpy/issues/72:75,Performance,perform,performing,75,"We are very impressed with the scalability of scanpy. We are interested in performing gene co-expression clustering on large single-cell RNAseq datasets. This typically involves calculating pairwise correlations between genes, then using these correlations as distance metrics for hierarchical and k-means clustering. Does scanpy already support these kinds of analyses?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72
https://github.com/scverse/scanpy/issues/73:376,Availability,error,error,376,"I assume I'm missing something here, but when I try a simple example of plotting a gene dispersion I get two plots 'normalized' and 'not normalized' version in Jupyter, but when I use the save argument to sc.pl.filter_genes_dispersion() I get an image with only one of these. Screenshot attached. Just in case, I tried also passing the multi_panel argument but that caused an error. . Also, is it no possible to specify the path where the files should be stored when using the save arguments to the plotting methods? I want to point to a directory where it should place them, but it seems ""./figures/"" is hard-coded and you can only modify the end of that. Thanks. The attached screenshot shows the dual image within Jupyter, but only the single plot which appears in the PNG file exported. ![screenshot from 2018-01-30 12-34-56](https://user-images.githubusercontent.com/330899/35584410-945d7bb6-05ba-11e8-89fc-14f615a9c6a6.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/73
https://github.com/scverse/scanpy/issues/73:54,Usability,simpl,simple,54,"I assume I'm missing something here, but when I try a simple example of plotting a gene dispersion I get two plots 'normalized' and 'not normalized' version in Jupyter, but when I use the save argument to sc.pl.filter_genes_dispersion() I get an image with only one of these. Screenshot attached. Just in case, I tried also passing the multi_panel argument but that caused an error. . Also, is it no possible to specify the path where the files should be stored when using the save arguments to the plotting methods? I want to point to a directory where it should place them, but it seems ""./figures/"" is hard-coded and you can only modify the end of that. Thanks. The attached screenshot shows the dual image within Jupyter, but only the single plot which appears in the PNG file exported. ![screenshot from 2018-01-30 12-34-56](https://user-images.githubusercontent.com/330899/35584410-945d7bb6-05ba-11e8-89fc-14f615a9c6a6.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/73
https://github.com/scverse/scanpy/issues/74:133,Availability,error,error,133,"Hi Alex, thank you for this amazing package. I would like to use it for my analysis, but I cannot figure it out why I'm getting this error when I try to include more annotation on my samples. ; Basically, I was following your example here: . import pandas as pd; anno = pd.read_csv(filename_sample_annotation); adata.obs['cell_groups'] = anno['cell_groups'] . However, when I tried with my cvs file, I got Nan for each row and I don't understand. ; The pd data frame is fine, but then the data.var['key'] = NaN NaN NaN ... everywhere..; I post here my code: . **import pandas as pd. anno = pd.read_csv(path+'sample_anno.csv',header=0); anno.head(); adata.var['pools']= anno['pools']; adata.var**; ![updated_adata var](https://user-images.githubusercontent.com/20638667/35852492-59e9806c-0b2b-11e8-94e2-103c18792cbb.png); ![anno_dataframe](https://user-images.githubusercontent.com/20638667/35852504-5f5a04d6-0b2b-11e8-9c91-0e0d61da3810.png). Thank you in advance, . Elisabetta",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74
https://github.com/scverse/scanpy/pull/76:141,Integrability,wrap,wrapped,141,I believe everything is in place. We should have now a generic `add_score` which scores cells according to expression of gene lists. That is wrapped twice in `cell_cycle_score`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/76
https://github.com/scverse/scanpy/issues/3:104,Integrability,message,message,104,"everyone agrees it’s objectively bad 😉. therefore the characters “jet” should only appear in the commit message “jettisoned bad colormap defaults”. * https://jakevdp.github.io/blog/2014/10/16/how-bad-is-your-colormap/; * http://cresspahl.blogspot.de/2012/03/expanded-control-of-octaves-colormap.html; * http://stats.stackexchange.com/questions/223315/why-use-colormap-viridis-over-jet; * https://eagereyes.org/basics/rainbow-color-map; * https://courses.washington.edu/engageuw/why-you-should-dump-the-rainbow/; * http://researchweb.watson.ibm.com/people/l/lloydt/color/color.HTM. i’ll do it if you want, and i’ll make sure contrast and distinction is preserved in all figures",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3
https://github.com/scverse/scanpy/issues/4:508,Deployability,update,update,508,"The task is to write the following preprocessing sequence using an AnnData instance adata.; ```py; meanFilter = 0.01; cvFilter = 2; nr_pcs = 50. ddata = adata.to_dict(); X = ddata['X']; # row normalize ; X = row_norm(X, max_fraction=0.05, mult_with_mean=True); # filter out genes with mean expression < 0.1 and coefficient of variance < ; # cvFilter ; X, gene_filter = filter_genes_cv(X, meanFilter, cvFilter); # compute zscore of filtered matrix ; Xz = zscore(X); # PCA ; Xpca = pca(Xz, nr_comps=nr_pcs); # update dictionary ; ddata['X'] = X; ddata['Xpca'] = Xpca; ddata['var_names'] = ddata['var_names'][gene_filter]; sett.m(0, 'Xpca has shape',; ddata['Xpca'].shape[0], 'x', ddata['Xpca'].shape[1]); from ..ann_data import AnnData; adata = AnnData(ddata); print(adata.X); ```; While the previous snippet works just as expected, when I want to do the same without a ddata object, some uncontrolled behavior comes up. Indexing doesn't work as expected anymore. @flying-sheep: could you have a look at why `adata['Xpca'] = Xpca` in the following throws an; ```py; >>> adata['Xpca'] = Xpca; IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices; ```; in the following snippet; ```py; X = adata.X; # row normalize ; X = row_norm(X, max_fraction=0.05, mult_with_mean=True); # filter out genes with mean expression < 0.1 and coefficient of variance < ; # cvFilter ; X, gene_filter = filter_genes_cv(X, meanFilter, cvFilter); # compute zscore of filtered matrix ; Xz = zscore(X); # PCA ; Xpca = pca(Xz, nr_comps=nr_pcs); # update adata ; adata.X = X; adata = adata.var_names[gene_filter] # filter genes ; adata['Xpca'] = Xpca; sett.m(0, 'Xpca has shape',; adata['Xpca'].shape[0], 'x', adata['Xpca'].shape[1]); print(adata.X); ```; I played around quite some bit, but the only solution that I got running then had the numerically incorrect result. It's quite to hard to keep this sequence of steps nicely organized. PS: the snippet",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/4
https://github.com/scverse/scanpy/issues/4:1594,Deployability,update,update,1594," = 0.01; cvFilter = 2; nr_pcs = 50. ddata = adata.to_dict(); X = ddata['X']; # row normalize ; X = row_norm(X, max_fraction=0.05, mult_with_mean=True); # filter out genes with mean expression < 0.1 and coefficient of variance < ; # cvFilter ; X, gene_filter = filter_genes_cv(X, meanFilter, cvFilter); # compute zscore of filtered matrix ; Xz = zscore(X); # PCA ; Xpca = pca(Xz, nr_comps=nr_pcs); # update dictionary ; ddata['X'] = X; ddata['Xpca'] = Xpca; ddata['var_names'] = ddata['var_names'][gene_filter]; sett.m(0, 'Xpca has shape',; ddata['Xpca'].shape[0], 'x', ddata['Xpca'].shape[1]); from ..ann_data import AnnData; adata = AnnData(ddata); print(adata.X); ```; While the previous snippet works just as expected, when I want to do the same without a ddata object, some uncontrolled behavior comes up. Indexing doesn't work as expected anymore. @flying-sheep: could you have a look at why `adata['Xpca'] = Xpca` in the following throws an; ```py; >>> adata['Xpca'] = Xpca; IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices; ```; in the following snippet; ```py; X = adata.X; # row normalize ; X = row_norm(X, max_fraction=0.05, mult_with_mean=True); # filter out genes with mean expression < 0.1 and coefficient of variance < ; # cvFilter ; X, gene_filter = filter_genes_cv(X, meanFilter, cvFilter); # compute zscore of filtered matrix ; Xz = zscore(X); # PCA ; Xpca = pca(Xz, nr_comps=nr_pcs); # update adata ; adata.X = X; adata = adata.var_names[gene_filter] # filter genes ; adata['Xpca'] = Xpca; sett.m(0, 'Xpca has shape',; adata['Xpca'].shape[0], 'x', adata['Xpca'].shape[1]); print(adata.X); ```; I played around quite some bit, but the only solution that I got running then had the numerically incorrect result. It's quite to hard to keep this sequence of steps nicely organized. PS: the snippet appears in `scanpy/preprocess/advanced.py` and an example would be `./scanpy.py nestorowa16 diffmap -r pp`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/4
https://github.com/scverse/scanpy/pull/5:270,Deployability,install,install,270,"this makes it easy to have a basic code style in place without configuring individual editors: http://editorconfig.org. my PyCharm and your [Emacs](https://github.com/editorconfig/editorconfig-emacs#readme) both support it (Emacs with that plugin). if you don’t want to install the plugin, it’s at least useful for me when i switch machines (or future contributors)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/5
https://github.com/scverse/scanpy/pull/5:63,Modifiability,config,configuring,63,"this makes it easy to have a basic code style in place without configuring individual editors: http://editorconfig.org. my PyCharm and your [Emacs](https://github.com/editorconfig/editorconfig-emacs#readme) both support it (Emacs with that plugin). if you don’t want to install the plugin, it’s at least useful for me when i switch machines (or future contributors)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/5
https://github.com/scverse/scanpy/pull/5:240,Modifiability,plugin,plugin,240,"this makes it easy to have a basic code style in place without configuring individual editors: http://editorconfig.org. my PyCharm and your [Emacs](https://github.com/editorconfig/editorconfig-emacs#readme) both support it (Emacs with that plugin). if you don’t want to install the plugin, it’s at least useful for me when i switch machines (or future contributors)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/5
https://github.com/scverse/scanpy/pull/5:282,Modifiability,plugin,plugin,282,"this makes it easy to have a basic code style in place without configuring individual editors: http://editorconfig.org. my PyCharm and your [Emacs](https://github.com/editorconfig/editorconfig-emacs#readme) both support it (Emacs with that plugin). if you don’t want to install the plugin, it’s at least useful for me when i switch machines (or future contributors)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/5
https://github.com/scverse/scanpy/issues/6:52,Availability,error,error,52,dont think to csv is actually coded; to xlsx raises error: pandas.core.common.PandasError: DataFrame constructor not properly called!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/6
https://github.com/scverse/scanpy/issues/7:234,Availability,down,download,234,"Hi, I was just trying to use the package but it seems that somebody is working on the master branch right now. Would it be possible to set up a development branch and maybe add a few tags for the working versions so that people could download a particular release instead of an in-progress master branch? I also noticed that the notebooks disappeared right after I cloned the repository. It seems like there are some big changes going on, so sorry if the timing for this issue is not right and you are just cleaning up the repository.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7
https://github.com/scverse/scanpy/issues/7:256,Deployability,release,release,256,"Hi, I was just trying to use the package but it seems that somebody is working on the master branch right now. Would it be possible to set up a development branch and maybe add a few tags for the working versions so that people could download a particular release instead of an in-progress master branch? I also noticed that the notebooks disappeared right after I cloned the repository. It seems like there are some big changes going on, so sorry if the timing for this issue is not right and you are just cleaning up the repository.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7
https://github.com/scverse/scanpy/issues/10:169,Availability,error,error,169,"I have some issues runnign tSNE with `sc.tsne(adata)`. It seems to work on the `moignard15` data set but running the same code with my data set results in the following error. ```; compute tSNE; preprocess using PCA with 50 PCs; --> avoid this by setting n_pcs = 0; 0:00:02.013 - compute PCA with n_comps = 50; 0:00:00.162 - finished; ---------------------------------------------------------------------------; UnboundLocalError Traceback (most recent call last); <ipython-input-5-ea03cbb426c5> in <module>(); ----> 1 sc.tsne(adata). /opt/conda/lib/python3.6/site-packages/scanpy/tools/tsne.py in tsne(adata, random_state, n_pcs, perplexity); 59 sett.m(0, 'preprocess using PCA with', n_pcs, 'PCs'); 60 sett.m(0, '--> avoid this by setting n_pcs = 0'); ---> 61 X = pca(adata.X, random_state=random_state, n_comps=n_pcs); 62 adata['X_pca'] = X; 63 else:. /opt/conda/lib/python3.6/site-packages/scanpy/tools/pca.py in pca(adata_or_X, n_comps, zero_center, svd_solver, random_state); 60 zero_center, svd_solver,; 61 random_state=random_state); ---> 62 adata['X_pca'] = X_pca; 63 if isadata:; 64 return adata. UnboundLocalError: local variable 'adata' referenced before assignment; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/10
https://github.com/scverse/scanpy/issues/10:1132,Modifiability,variab,variable,1132,"I have some issues runnign tSNE with `sc.tsne(adata)`. It seems to work on the `moignard15` data set but running the same code with my data set results in the following error. ```; compute tSNE; preprocess using PCA with 50 PCs; --> avoid this by setting n_pcs = 0; 0:00:02.013 - compute PCA with n_comps = 50; 0:00:00.162 - finished; ---------------------------------------------------------------------------; UnboundLocalError Traceback (most recent call last); <ipython-input-5-ea03cbb426c5> in <module>(); ----> 1 sc.tsne(adata). /opt/conda/lib/python3.6/site-packages/scanpy/tools/tsne.py in tsne(adata, random_state, n_pcs, perplexity); 59 sett.m(0, 'preprocess using PCA with', n_pcs, 'PCs'); 60 sett.m(0, '--> avoid this by setting n_pcs = 0'); ---> 61 X = pca(adata.X, random_state=random_state, n_comps=n_pcs); 62 adata['X_pca'] = X; 63 else:. /opt/conda/lib/python3.6/site-packages/scanpy/tools/pca.py in pca(adata_or_X, n_comps, zero_center, svd_solver, random_state); 60 zero_center, svd_solver,; 61 random_state=random_state); ---> 62 adata['X_pca'] = X_pca; 63 if isadata:; 64 return adata. UnboundLocalError: local variable 'adata' referenced before assignment; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/10
https://github.com/scverse/scanpy/issues/10:233,Safety,avoid,avoid,233,"I have some issues runnign tSNE with `sc.tsne(adata)`. It seems to work on the `moignard15` data set but running the same code with my data set results in the following error. ```; compute tSNE; preprocess using PCA with 50 PCs; --> avoid this by setting n_pcs = 0; 0:00:02.013 - compute PCA with n_comps = 50; 0:00:00.162 - finished; ---------------------------------------------------------------------------; UnboundLocalError Traceback (most recent call last); <ipython-input-5-ea03cbb426c5> in <module>(); ----> 1 sc.tsne(adata). /opt/conda/lib/python3.6/site-packages/scanpy/tools/tsne.py in tsne(adata, random_state, n_pcs, perplexity); 59 sett.m(0, 'preprocess using PCA with', n_pcs, 'PCs'); 60 sett.m(0, '--> avoid this by setting n_pcs = 0'); ---> 61 X = pca(adata.X, random_state=random_state, n_comps=n_pcs); 62 adata['X_pca'] = X; 63 else:. /opt/conda/lib/python3.6/site-packages/scanpy/tools/pca.py in pca(adata_or_X, n_comps, zero_center, svd_solver, random_state); 60 zero_center, svd_solver,; 61 random_state=random_state); ---> 62 adata['X_pca'] = X_pca; 63 if isadata:; 64 return adata. UnboundLocalError: local variable 'adata' referenced before assignment; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/10
https://github.com/scverse/scanpy/issues/10:719,Safety,avoid,avoid,719,"I have some issues runnign tSNE with `sc.tsne(adata)`. It seems to work on the `moignard15` data set but running the same code with my data set results in the following error. ```; compute tSNE; preprocess using PCA with 50 PCs; --> avoid this by setting n_pcs = 0; 0:00:02.013 - compute PCA with n_comps = 50; 0:00:00.162 - finished; ---------------------------------------------------------------------------; UnboundLocalError Traceback (most recent call last); <ipython-input-5-ea03cbb426c5> in <module>(); ----> 1 sc.tsne(adata). /opt/conda/lib/python3.6/site-packages/scanpy/tools/tsne.py in tsne(adata, random_state, n_pcs, perplexity); 59 sett.m(0, 'preprocess using PCA with', n_pcs, 'PCs'); 60 sett.m(0, '--> avoid this by setting n_pcs = 0'); ---> 61 X = pca(adata.X, random_state=random_state, n_comps=n_pcs); 62 adata['X_pca'] = X; 63 else:. /opt/conda/lib/python3.6/site-packages/scanpy/tools/pca.py in pca(adata_or_X, n_comps, zero_center, svd_solver, random_state); 60 zero_center, svd_solver,; 61 random_state=random_state); ---> 62 adata['X_pca'] = X_pca; 63 if isadata:; 64 return adata. UnboundLocalError: local variable 'adata' referenced before assignment; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/10
https://github.com/scverse/scanpy/issues/14:504,Performance,cache,caches,504,"* `struct_dict['k']` should return a simple 1D ndarray, not a `StructDict`; * `struct_dict[['a', 'b']]` should return a proper `StructDict` including all additional fields like `_keys`. both problems are visible in this test: https://travis-ci.org/theislab/scanpy/jobs/227216146#L224. but a test should be added for the second point once the first point is fixed. ---. i would have fixed it, but i don’t have the slightest idea what all the “multicolumn” fields are for. also i don’t believe in too many caches and private fields, they get out of sync too easily. recomputing tiny things is fast, e.g. `self._keys` could simply be replaced with `self.dtype.names`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/14
https://github.com/scverse/scanpy/issues/14:220,Testability,test,test,220,"* `struct_dict['k']` should return a simple 1D ndarray, not a `StructDict`; * `struct_dict[['a', 'b']]` should return a proper `StructDict` including all additional fields like `_keys`. both problems are visible in this test: https://travis-ci.org/theislab/scanpy/jobs/227216146#L224. but a test should be added for the second point once the first point is fixed. ---. i would have fixed it, but i don’t have the slightest idea what all the “multicolumn” fields are for. also i don’t believe in too many caches and private fields, they get out of sync too easily. recomputing tiny things is fast, e.g. `self._keys` could simply be replaced with `self.dtype.names`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/14
https://github.com/scverse/scanpy/issues/14:291,Testability,test,test,291,"* `struct_dict['k']` should return a simple 1D ndarray, not a `StructDict`; * `struct_dict[['a', 'b']]` should return a proper `StructDict` including all additional fields like `_keys`. both problems are visible in this test: https://travis-ci.org/theislab/scanpy/jobs/227216146#L224. but a test should be added for the second point once the first point is fixed. ---. i would have fixed it, but i don’t have the slightest idea what all the “multicolumn” fields are for. also i don’t believe in too many caches and private fields, they get out of sync too easily. recomputing tiny things is fast, e.g. `self._keys` could simply be replaced with `self.dtype.names`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/14
https://github.com/scverse/scanpy/issues/14:37,Usability,simpl,simple,37,"* `struct_dict['k']` should return a simple 1D ndarray, not a `StructDict`; * `struct_dict[['a', 'b']]` should return a proper `StructDict` including all additional fields like `_keys`. both problems are visible in this test: https://travis-ci.org/theislab/scanpy/jobs/227216146#L224. but a test should be added for the second point once the first point is fixed. ---. i would have fixed it, but i don’t have the slightest idea what all the “multicolumn” fields are for. also i don’t believe in too many caches and private fields, they get out of sync too easily. recomputing tiny things is fast, e.g. `self._keys` could simply be replaced with `self.dtype.names`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/14
https://github.com/scverse/scanpy/issues/14:621,Usability,simpl,simply,621,"* `struct_dict['k']` should return a simple 1D ndarray, not a `StructDict`; * `struct_dict[['a', 'b']]` should return a proper `StructDict` including all additional fields like `_keys`. both problems are visible in this test: https://travis-ci.org/theislab/scanpy/jobs/227216146#L224. but a test should be added for the second point once the first point is fixed. ---. i would have fixed it, but i don’t have the slightest idea what all the “multicolumn” fields are for. also i don’t believe in too many caches and private fields, they get out of sync too easily. recomputing tiny things is fast, e.g. `self._keys` could simply be replaced with `self.dtype.names`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/14
https://github.com/scverse/scanpy/issues/15:165,Availability,error,error,165,"* installing the master branch via `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git` fails:. ```; UPDATING build/lib.linux-x86_64-3.5/None; error: [Errno 2] No such file or directory: 'build/lib.linux-x86_64-3.5/None'; ```; ; maybe `_version.py` returns `None` instead of a version string for the master branch. * installing the tag works: `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git@0.0` but the installed version will be `0.1`, not `0.0` as the tag says (i’m not sure 0.0 is a legal version anyway)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15
https://github.com/scverse/scanpy/issues/15:2,Deployability,install,installing,2,"* installing the master branch via `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git` fails:. ```; UPDATING build/lib.linux-x86_64-3.5/None; error: [Errno 2] No such file or directory: 'build/lib.linux-x86_64-3.5/None'; ```; ; maybe `_version.py` returns `None` instead of a version string for the master branch. * installing the tag works: `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git@0.0` but the installed version will be `0.1`, not `0.0` as the tag says (i’m not sure 0.0 is a legal version anyway)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15
https://github.com/scverse/scanpy/issues/15:41,Deployability,install,install,41,"* installing the master branch via `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git` fails:. ```; UPDATING build/lib.linux-x86_64-3.5/None; error: [Errno 2] No such file or directory: 'build/lib.linux-x86_64-3.5/None'; ```; ; maybe `_version.py` returns `None` instead of a version string for the master branch. * installing the tag works: `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git@0.0` but the installed version will be `0.1`, not `0.0` as the tag says (i’m not sure 0.0 is a legal version anyway)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15
https://github.com/scverse/scanpy/issues/15:58,Deployability,upgrade,upgrade,58,"* installing the master branch via `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git` fails:. ```; UPDATING build/lib.linux-x86_64-3.5/None; error: [Errno 2] No such file or directory: 'build/lib.linux-x86_64-3.5/None'; ```; ; maybe `_version.py` returns `None` instead of a version string for the master branch. * installing the tag works: `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git@0.0` but the installed version will be `0.1`, not `0.0` as the tag says (i’m not sure 0.0 is a legal version anyway)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15
https://github.com/scverse/scanpy/issues/15:339,Deployability,install,installing,339,"* installing the master branch via `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git` fails:. ```; UPDATING build/lib.linux-x86_64-3.5/None; error: [Errno 2] No such file or directory: 'build/lib.linux-x86_64-3.5/None'; ```; ; maybe `_version.py` returns `None` instead of a version string for the master branch. * installing the tag works: `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git@0.0` but the installed version will be `0.1`, not `0.0` as the tag says (i’m not sure 0.0 is a legal version anyway)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15
https://github.com/scverse/scanpy/issues/15:371,Deployability,install,install,371,"* installing the master branch via `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git` fails:. ```; UPDATING build/lib.linux-x86_64-3.5/None; error: [Errno 2] No such file or directory: 'build/lib.linux-x86_64-3.5/None'; ```; ; maybe `_version.py` returns `None` instead of a version string for the master branch. * installing the tag works: `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git@0.0` but the installed version will be `0.1`, not `0.0` as the tag says (i’m not sure 0.0 is a legal version anyway)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15
https://github.com/scverse/scanpy/issues/15:388,Deployability,upgrade,upgrade,388,"* installing the master branch via `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git` fails:. ```; UPDATING build/lib.linux-x86_64-3.5/None; error: [Errno 2] No such file or directory: 'build/lib.linux-x86_64-3.5/None'; ```; ; maybe `_version.py` returns `None` instead of a version string for the master branch. * installing the tag works: `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git@0.0` but the installed version will be `0.1`, not `0.0` as the tag says (i’m not sure 0.0 is a legal version anyway)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15
https://github.com/scverse/scanpy/issues/15:452,Deployability,install,installed,452,"* installing the master branch via `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git` fails:. ```; UPDATING build/lib.linux-x86_64-3.5/None; error: [Errno 2] No such file or directory: 'build/lib.linux-x86_64-3.5/None'; ```; ; maybe `_version.py` returns `None` instead of a version string for the master branch. * installing the tag works: `pip3 install --user --upgrade git+https://github.com/theislab/scanpy.git@0.0` but the installed version will be `0.1`, not `0.0` as the tag says (i’m not sure 0.0 is a legal version anyway)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15
https://github.com/scverse/scanpy/issues/16:721,Modifiability,variab,variable,721,"```py; %matplotlib inline; import scanpy; ```. when importing scanpy from a jupyter notebook I get this warning, because apparently scanpy calls `matplotlib.use()`. if at all, it should only do that after checking that no backend is already selected. ```; /home/icb/philipp.angerer/.local/lib/python3.5/site-packages/matplotlib/__init__.py:1401: UserWarning: This call to matplotlib.use() has no effect; because the backend has already been chosen;; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,; or matplotlib.backends is imported for the first time. warnings.warn(_use_error_msg); ```. and this, which isn’t actually a `Warning`, is printed to stdout (why?). ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```. in an interactive notebook or other shell, `sett.savefigs` shouldn’t be automatically set to `True`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16
https://github.com/scverse/scanpy/issues/17:154,Security,access,access,154,"e.g `pp.filter_genes_dispersion` returns a dict with known keys. if you know the keys, this means you should return an object where you can use attribute access instead. usually the pythonic thing to do in this case is to return a namedtuple. in this case though, we have. ```py; >>> {k: v.shape for k, v in filter_result.items()}; {'dispersions': (173351,),; 'dispersions_norm': (173351,),; 'gene_filter': (173351,),; 'means': (173351,)}; ```. which is a perfect case for a recarray.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/17
https://github.com/scverse/scanpy/pull/21:6,Testability,test,test,6,"In my test, even we check `if isinstance(index, int) or isinstance(index, np.int64) or isinstance(index, np.int32):`, it still doesn't work on my 32-bit numpy and an _TypeErorr_ will be raised. Anyway, _isinstance(index, np.integer)_ works well and is a better choice.; Reference: [http://stackoverflow.com/questions/37726830/how-to-determine-if-a-number-is-any-type-of-int-core-or-numpy-signed-or-not](url)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/21
https://github.com/scverse/scanpy/issues/22:9,Availability,error,error,9,"I got an error doing `pip3 install -e .`:. > clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/local/include -I/usr/local/opt/openssl/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c scanpy/cython/utils_cy.c -o build/temp.macosx-10.11-x86_64-3.6/scanpy/cython/utils_cy.o; > scanpy/cython/utils_cy.c:435:10: fatal error: 'numpy/arrayobject.h' file not found; > #include ""numpy/arrayobject.h""; > ^; > 1 error generated.; > error: command 'clang' failed with exit status 1; > ; > ----------------------------------------; > Command ""/usr/local/opt/python3/bin/python3.6 -c ""import setuptools, tokenize;__file__='/Users/jyhung/Documents/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" develop --no-deps"" failed with error code 1. It worked after I installed cython: `pip3 install cython`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/22
https://github.com/scverse/scanpy/issues/22:488,Availability,error,error,488,"I got an error doing `pip3 install -e .`:. > clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/local/include -I/usr/local/opt/openssl/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c scanpy/cython/utils_cy.c -o build/temp.macosx-10.11-x86_64-3.6/scanpy/cython/utils_cy.o; > scanpy/cython/utils_cy.c:435:10: fatal error: 'numpy/arrayobject.h' file not found; > #include ""numpy/arrayobject.h""; > ^; > 1 error generated.; > error: command 'clang' failed with exit status 1; > ; > ----------------------------------------; > Command ""/usr/local/opt/python3/bin/python3.6 -c ""import setuptools, tokenize;__file__='/Users/jyhung/Documents/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" develop --no-deps"" failed with error code 1. It worked after I installed cython: `pip3 install cython`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/22
https://github.com/scverse/scanpy/issues/22:576,Availability,error,error,576,"I got an error doing `pip3 install -e .`:. > clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/local/include -I/usr/local/opt/openssl/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c scanpy/cython/utils_cy.c -o build/temp.macosx-10.11-x86_64-3.6/scanpy/cython/utils_cy.o; > scanpy/cython/utils_cy.c:435:10: fatal error: 'numpy/arrayobject.h' file not found; > #include ""numpy/arrayobject.h""; > ^; > 1 error generated.; > error: command 'clang' failed with exit status 1; > ; > ----------------------------------------; > Command ""/usr/local/opt/python3/bin/python3.6 -c ""import setuptools, tokenize;__file__='/Users/jyhung/Documents/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" develop --no-deps"" failed with error code 1. It worked after I installed cython: `pip3 install cython`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/22
https://github.com/scverse/scanpy/issues/22:596,Availability,error,error,596,"I got an error doing `pip3 install -e .`:. > clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/local/include -I/usr/local/opt/openssl/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c scanpy/cython/utils_cy.c -o build/temp.macosx-10.11-x86_64-3.6/scanpy/cython/utils_cy.o; > scanpy/cython/utils_cy.c:435:10: fatal error: 'numpy/arrayobject.h' file not found; > #include ""numpy/arrayobject.h""; > ^; > 1 error generated.; > error: command 'clang' failed with exit status 1; > ; > ----------------------------------------; > Command ""/usr/local/opt/python3/bin/python3.6 -c ""import setuptools, tokenize;__file__='/Users/jyhung/Documents/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" develop --no-deps"" failed with error code 1. It worked after I installed cython: `pip3 install cython`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/22
https://github.com/scverse/scanpy/issues/22:985,Availability,error,error,985,"I got an error doing `pip3 install -e .`:. > clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/local/include -I/usr/local/opt/openssl/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c scanpy/cython/utils_cy.c -o build/temp.macosx-10.11-x86_64-3.6/scanpy/cython/utils_cy.o; > scanpy/cython/utils_cy.c:435:10: fatal error: 'numpy/arrayobject.h' file not found; > #include ""numpy/arrayobject.h""; > ^; > 1 error generated.; > error: command 'clang' failed with exit status 1; > ; > ----------------------------------------; > Command ""/usr/local/opt/python3/bin/python3.6 -c ""import setuptools, tokenize;__file__='/Users/jyhung/Documents/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" develop --no-deps"" failed with error code 1. It worked after I installed cython: `pip3 install cython`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/22
https://github.com/scverse/scanpy/issues/22:27,Deployability,install,install,27,"I got an error doing `pip3 install -e .`:. > clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/local/include -I/usr/local/opt/openssl/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c scanpy/cython/utils_cy.c -o build/temp.macosx-10.11-x86_64-3.6/scanpy/cython/utils_cy.o; > scanpy/cython/utils_cy.c:435:10: fatal error: 'numpy/arrayobject.h' file not found; > #include ""numpy/arrayobject.h""; > ^; > 1 error generated.; > error: command 'clang' failed with exit status 1; > ; > ----------------------------------------; > Command ""/usr/local/opt/python3/bin/python3.6 -c ""import setuptools, tokenize;__file__='/Users/jyhung/Documents/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" develop --no-deps"" failed with error code 1. It worked after I installed cython: `pip3 install cython`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/22
https://github.com/scverse/scanpy/issues/22:1017,Deployability,install,installed,1017,"I got an error doing `pip3 install -e .`:. > clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/local/include -I/usr/local/opt/openssl/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c scanpy/cython/utils_cy.c -o build/temp.macosx-10.11-x86_64-3.6/scanpy/cython/utils_cy.o; > scanpy/cython/utils_cy.c:435:10: fatal error: 'numpy/arrayobject.h' file not found; > #include ""numpy/arrayobject.h""; > ^; > 1 error generated.; > error: command 'clang' failed with exit status 1; > ; > ----------------------------------------; > Command ""/usr/local/opt/python3/bin/python3.6 -c ""import setuptools, tokenize;__file__='/Users/jyhung/Documents/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" develop --no-deps"" failed with error code 1. It worked after I installed cython: `pip3 install cython`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/22
https://github.com/scverse/scanpy/issues/22:1041,Deployability,install,install,1041,"I got an error doing `pip3 install -e .`:. > clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/local/include -I/usr/local/opt/openssl/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c scanpy/cython/utils_cy.c -o build/temp.macosx-10.11-x86_64-3.6/scanpy/cython/utils_cy.o; > scanpy/cython/utils_cy.c:435:10: fatal error: 'numpy/arrayobject.h' file not found; > #include ""numpy/arrayobject.h""; > ^; > 1 error generated.; > error: command 'clang' failed with exit status 1; > ; > ----------------------------------------; > Command ""/usr/local/opt/python3/bin/python3.6 -c ""import setuptools, tokenize;__file__='/Users/jyhung/Documents/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" develop --no-deps"" failed with error code 1. It worked after I installed cython: `pip3 install cython`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/22
https://github.com/scverse/scanpy/issues/24:117,Availability,error,error,117,"I tried running https://github.com/theislab/scanpy_usage/blob/master/170501_moignard15/moignard15.ipynb and got this error:; ```; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/networkx/classes/graph.py in neighbors(self, n); 1058 try:; -> 1059 return list(self.adj[n]); 1060 except KeyError:. KeyError: None. During handling of the above exception, another exception occurred:. NetworkXError Traceback (most recent call last); <ipython-input-11-f3d9663e2b3b> in <module>(); 1 adata.add['dpt_groups_names'] = ['undecided/endothelial', 'endothelial', 'erythrocytes', 'trunk'] # optional; ----> 2 sc.pl.dpt(adata, color=['dpt_pseudotime', 'dpt_groups', 'exp_groups'], legendloc='upper left'). ~/Documents/scanpy/scanpy/plotting/__init__.py in dpt(adata, basis, color, names, comps, cont, layout, legendloc, cmap, pal, right_margin, size, titles, show); 385 if not isinstance(color, list): colors = color.split(','); 386 else: colors = color; --> 387 if 'dpt_groups' in colors: dpt_tree(adata, show=False); 388 dpt_timeseries(adata, cmap=cmap, show=show); 389 . ~/Documents/scanpy/scanpy/plotting/__init__.py in dpt_tree(adata, root, colors, names, show, fontsize); 463 if name in sett._ignore_categories: colors[iname] = 'grey'; 464 G = nx.Graph(adata.add['dpt_groups_adjacency']); --> 465 pos = utils.hierarchy_pos(G, root); 466 fig = pl.figure(figsize=(5, 5)); 467 ax = pl.axes([0, 0, 1, 1], frameon=False). ~/Documents/scanpy/scanpy/plotting/utils.py in hierarchy_pos(G, root, levels, width, height); 455 ; 456 if levels is None:; --> 457 levels = make_levels({}); 458 else:; 459 levels = {l: {TOTAL: levels[l], CURRENT: 0} for l in levels}. ~/Documents/scanpy/scanpy/plotting/utils.py in make_levels(levels, node, currentLevel, parent); 434 levels[currentLevel] = {TOTAL: 0, CURRENT: 0}; 435 levels[currentLevel][TOTAL] += 1; --> 436 neighbors = G.neighbors(node); 437 if parent is not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/24
https://github.com/scverse/scanpy/issues/26:639,Deployability,release,release,639,"Hi, first thanks for sharing this analysis tool. I prefer Python much more to R, though most Bioinformatics tools are written in R. Here I want to ask a question about data processing before we feed it as _adata_ into _dpt_ for _pseudotime_ ordering. . As the DPT algorithm can accept multiple types of data, such as the most commonly single-cell qPCR (Ct values) and RNA-Seq (FPKM/TPM) data, is the data processing procedure identical with each other? Since I have also checked the Monocle 2 algorithm, it seems much more complicated in Monocle 2. For instance, in the 4th page of its document [link](http://www.bioconductor.org/packages/release/bioc/vignettes/monocle/inst/doc/monocle-vignette.pdf), it asks you to specify different _expressionFamily_, i.e., the proper distribution of the data, for different kinds of data. Then, how about the _dpt_ function in scanpy? Does it take all kinds of data the same way?. According to my understanding, ; - For qPCR data, we should provide delta_Ct=LOD-Ct values to _dpt_ (LOD: limit of detection);; - For RNA-Seq data, we should offer log2(FPKM+1) to _dpt_.; Is it right?. Any help is appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26
https://github.com/scverse/scanpy/issues/26:1034,Safety,detect,detection,1034,"Hi, first thanks for sharing this analysis tool. I prefer Python much more to R, though most Bioinformatics tools are written in R. Here I want to ask a question about data processing before we feed it as _adata_ into _dpt_ for _pseudotime_ ordering. . As the DPT algorithm can accept multiple types of data, such as the most commonly single-cell qPCR (Ct values) and RNA-Seq (FPKM/TPM) data, is the data processing procedure identical with each other? Since I have also checked the Monocle 2 algorithm, it seems much more complicated in Monocle 2. For instance, in the 4th page of its document [link](http://www.bioconductor.org/packages/release/bioc/vignettes/monocle/inst/doc/monocle-vignette.pdf), it asks you to specify different _expressionFamily_, i.e., the proper distribution of the data, for different kinds of data. Then, how about the _dpt_ function in scanpy? Does it take all kinds of data the same way?. According to my understanding, ; - For qPCR data, we should provide delta_Ct=LOD-Ct values to _dpt_ (LOD: limit of detection);; - For RNA-Seq data, we should offer log2(FPKM+1) to _dpt_.; Is it right?. Any help is appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26
https://github.com/scverse/scanpy/issues/30:82,Availability,error,error,82,"hi, ; I installed the scanpy-master , but when I type `scanpy --help` in bash the error occurred. I noticed that diffrank was replaced by rank_genes_groups, but I don't know how to fix it. ; ```; Traceback (most recent call last):; File ""/public/bioapps/ana/anaconda3/envs/python35/bin/scanpy"", line 11, in <module>; load_entry_point('scanpy==0+unknown', 'console_scripts', 'scanpy')(); File ""/public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/scanpy-0+unknown-py3.5-linux-x86_64.egg/scanpy/__main__.py"", line 278, in main; init_main_parser().print_help(); File ""/public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/scanpy-0+unknown-py3.5-linux-x86_64.egg/scanpy/__main__.py"", line 117, in init_main_parser; descr = 78*'-' + '\n' + getattr(tools, key).__doc__; AttributeError: module 'scanpy.api.tools' has no attribute 'diffrank'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/30
https://github.com/scverse/scanpy/issues/30:8,Deployability,install,installed,8,"hi, ; I installed the scanpy-master , but when I type `scanpy --help` in bash the error occurred. I noticed that diffrank was replaced by rank_genes_groups, but I don't know how to fix it. ; ```; Traceback (most recent call last):; File ""/public/bioapps/ana/anaconda3/envs/python35/bin/scanpy"", line 11, in <module>; load_entry_point('scanpy==0+unknown', 'console_scripts', 'scanpy')(); File ""/public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/scanpy-0+unknown-py3.5-linux-x86_64.egg/scanpy/__main__.py"", line 278, in main; init_main_parser().print_help(); File ""/public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/scanpy-0+unknown-py3.5-linux-x86_64.egg/scanpy/__main__.py"", line 117, in init_main_parser; descr = 78*'-' + '\n' + getattr(tools, key).__doc__; AttributeError: module 'scanpy.api.tools' has no attribute 'diffrank'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/30
https://github.com/scverse/scanpy/issues/33:212,Availability,error,error,212,"Hello,; it's me again, really thanks for your kindly reply before.; when I analyze my own data using `sc.tl.dpt` with default `n_branches`, it worked well, but when I set `n_branches` more than 0, it occurred an error:; ```no root cell found, no computation of pseudotime; --> To enable computation of pseudotime, pass the index or expression vector; of a root cell. Either add; adata.add['iroot'] = root_cell_index; or (robust to subsampling); adata.var['xroot'] = adata.X[root_cell_index, :]; where ""root_cell_index"" is the integer index of the root cell, or; adata.var['xroot'] = adata[root_cell_name, :].X; where ""root_cell_name"" is the name (a string) of the root cell.; perform Diffusion Pseudotime analysis; using ""X_pca"" for building graph; using stored data graph with n_neighbors = 30 and spectrum; [ 1. 0.9944264293 0.9934666753 0.9925051928 0.9899699688; 0.9893597364 0.9855745435 0.9840251803 0.981688261 0.9806631804]; detect 1 branching; do not consider groups with less than 2742 points for splitting; branching 1: split group 0; WARNING: detected group with only [] cells. ValueError Traceback (most recent call last); <ipython-input-3-b1749d943ac4> in <module>(); ----> 1 get_ipython().run_cell_magic('time', '', 'sc.tl.dpt(adata_corrected,n_jobs=48,n_pcs=30,allow_kendall_tau_shift=False,n_branchings=1)\nsc.logging.print_memory_usage()'). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell); 2113 magic_arg_s = self.var_expand(line, stack_depth); 2114 with self.builtin_trap:; -> 2115 result = fn(magic_arg_s, cell); 2116 return result; 2117 . <decorator-gen-59> in time(self, line, cell, local_ns). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k); 186 # but it's overkill for just that one bit of state.; 187 def magic_deco(arg):; --> 188 call = lambda f, *a, **k: f(*a, **k); 189 ; 190 if callable(arg):. /pu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33
https://github.com/scverse/scanpy/issues/33:421,Availability,robust,robust,421,"Hello,; it's me again, really thanks for your kindly reply before.; when I analyze my own data using `sc.tl.dpt` with default `n_branches`, it worked well, but when I set `n_branches` more than 0, it occurred an error:; ```no root cell found, no computation of pseudotime; --> To enable computation of pseudotime, pass the index or expression vector; of a root cell. Either add; adata.add['iroot'] = root_cell_index; or (robust to subsampling); adata.var['xroot'] = adata.X[root_cell_index, :]; where ""root_cell_index"" is the integer index of the root cell, or; adata.var['xroot'] = adata[root_cell_name, :].X; where ""root_cell_name"" is the name (a string) of the root cell.; perform Diffusion Pseudotime analysis; using ""X_pca"" for building graph; using stored data graph with n_neighbors = 30 and spectrum; [ 1. 0.9944264293 0.9934666753 0.9925051928 0.9899699688; 0.9893597364 0.9855745435 0.9840251803 0.981688261 0.9806631804]; detect 1 branching; do not consider groups with less than 2742 points for splitting; branching 1: split group 0; WARNING: detected group with only [] cells. ValueError Traceback (most recent call last); <ipython-input-3-b1749d943ac4> in <module>(); ----> 1 get_ipython().run_cell_magic('time', '', 'sc.tl.dpt(adata_corrected,n_jobs=48,n_pcs=30,allow_kendall_tau_shift=False,n_branchings=1)\nsc.logging.print_memory_usage()'). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell); 2113 magic_arg_s = self.var_expand(line, stack_depth); 2114 with self.builtin_trap:; -> 2115 result = fn(magic_arg_s, cell); 2116 return result; 2117 . <decorator-gen-59> in time(self, line, cell, local_ns). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k); 186 # but it's overkill for just that one bit of state.; 187 def magic_deco(arg):; --> 188 call = lambda f, *a, **k: f(*a, **k); 189 ; 190 if callable(arg):. /pu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33
https://github.com/scverse/scanpy/issues/33:676,Performance,perform,perform,676,"Hello,; it's me again, really thanks for your kindly reply before.; when I analyze my own data using `sc.tl.dpt` with default `n_branches`, it worked well, but when I set `n_branches` more than 0, it occurred an error:; ```no root cell found, no computation of pseudotime; --> To enable computation of pseudotime, pass the index or expression vector; of a root cell. Either add; adata.add['iroot'] = root_cell_index; or (robust to subsampling); adata.var['xroot'] = adata.X[root_cell_index, :]; where ""root_cell_index"" is the integer index of the root cell, or; adata.var['xroot'] = adata[root_cell_name, :].X; where ""root_cell_name"" is the name (a string) of the root cell.; perform Diffusion Pseudotime analysis; using ""X_pca"" for building graph; using stored data graph with n_neighbors = 30 and spectrum; [ 1. 0.9944264293 0.9934666753 0.9925051928 0.9899699688; 0.9893597364 0.9855745435 0.9840251803 0.981688261 0.9806631804]; detect 1 branching; do not consider groups with less than 2742 points for splitting; branching 1: split group 0; WARNING: detected group with only [] cells. ValueError Traceback (most recent call last); <ipython-input-3-b1749d943ac4> in <module>(); ----> 1 get_ipython().run_cell_magic('time', '', 'sc.tl.dpt(adata_corrected,n_jobs=48,n_pcs=30,allow_kendall_tau_shift=False,n_branchings=1)\nsc.logging.print_memory_usage()'). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell); 2113 magic_arg_s = self.var_expand(line, stack_depth); 2114 with self.builtin_trap:; -> 2115 result = fn(magic_arg_s, cell); 2116 return result; 2117 . <decorator-gen-59> in time(self, line, cell, local_ns). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k); 186 # but it's overkill for just that one bit of state.; 187 def magic_deco(arg):; --> 188 call = lambda f, *a, **k: f(*a, **k); 189 ; 190 if callable(arg):. /pu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33
https://github.com/scverse/scanpy/issues/33:933,Safety,detect,detect,933,"o,; it's me again, really thanks for your kindly reply before.; when I analyze my own data using `sc.tl.dpt` with default `n_branches`, it worked well, but when I set `n_branches` more than 0, it occurred an error:; ```no root cell found, no computation of pseudotime; --> To enable computation of pseudotime, pass the index or expression vector; of a root cell. Either add; adata.add['iroot'] = root_cell_index; or (robust to subsampling); adata.var['xroot'] = adata.X[root_cell_index, :]; where ""root_cell_index"" is the integer index of the root cell, or; adata.var['xroot'] = adata[root_cell_name, :].X; where ""root_cell_name"" is the name (a string) of the root cell.; perform Diffusion Pseudotime analysis; using ""X_pca"" for building graph; using stored data graph with n_neighbors = 30 and spectrum; [ 1. 0.9944264293 0.9934666753 0.9925051928 0.9899699688; 0.9893597364 0.9855745435 0.9840251803 0.981688261 0.9806631804]; detect 1 branching; do not consider groups with less than 2742 points for splitting; branching 1: split group 0; WARNING: detected group with only [] cells. ValueError Traceback (most recent call last); <ipython-input-3-b1749d943ac4> in <module>(); ----> 1 get_ipython().run_cell_magic('time', '', 'sc.tl.dpt(adata_corrected,n_jobs=48,n_pcs=30,allow_kendall_tau_shift=False,n_branchings=1)\nsc.logging.print_memory_usage()'). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell); 2113 magic_arg_s = self.var_expand(line, stack_depth); 2114 with self.builtin_trap:; -> 2115 result = fn(magic_arg_s, cell); 2116 return result; 2117 . <decorator-gen-59> in time(self, line, cell, local_ns). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k); 186 # but it's overkill for just that one bit of state.; 187 def magic_deco(arg):; --> 188 call = lambda f, *a, **k: f(*a, **k); 189 ; 190 if callable(arg):. /public",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33
https://github.com/scverse/scanpy/issues/33:1055,Safety,detect,detected,1055,"o,; it's me again, really thanks for your kindly reply before.; when I analyze my own data using `sc.tl.dpt` with default `n_branches`, it worked well, but when I set `n_branches` more than 0, it occurred an error:; ```no root cell found, no computation of pseudotime; --> To enable computation of pseudotime, pass the index or expression vector; of a root cell. Either add; adata.add['iroot'] = root_cell_index; or (robust to subsampling); adata.var['xroot'] = adata.X[root_cell_index, :]; where ""root_cell_index"" is the integer index of the root cell, or; adata.var['xroot'] = adata[root_cell_name, :].X; where ""root_cell_name"" is the name (a string) of the root cell.; perform Diffusion Pseudotime analysis; using ""X_pca"" for building graph; using stored data graph with n_neighbors = 30 and spectrum; [ 1. 0.9944264293 0.9934666753 0.9925051928 0.9899699688; 0.9893597364 0.9855745435 0.9840251803 0.981688261 0.9806631804]; detect 1 branching; do not consider groups with less than 2742 points for splitting; branching 1: split group 0; WARNING: detected group with only [] cells. ValueError Traceback (most recent call last); <ipython-input-3-b1749d943ac4> in <module>(); ----> 1 get_ipython().run_cell_magic('time', '', 'sc.tl.dpt(adata_corrected,n_jobs=48,n_pcs=30,allow_kendall_tau_shift=False,n_branchings=1)\nsc.logging.print_memory_usage()'). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell); 2113 magic_arg_s = self.var_expand(line, stack_depth); 2114 with self.builtin_trap:; -> 2115 result = fn(magic_arg_s, cell); 2116 return result; 2117 . <decorator-gen-59> in time(self, line, cell, local_ns). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k); 186 # but it's overkill for just that one bit of state.; 187 def magic_deco(arg):; --> 188 call = lambda f, *a, **k: f(*a, **k); 189 ; 190 if callable(arg):. /public",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33
https://github.com/scverse/scanpy/issues/33:2545,Safety,detect,detect,2545,"14 with self.builtin_trap:; -> 2115 result = fn(magic_arg_s, cell); 2116 return result; 2117 . <decorator-gen-59> in time(self, line, cell, local_ns). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k); 186 # but it's overkill for just that one bit of state.; 187 def magic_deco(arg):; --> 188 call = lambda f, *a, **k: f(*a, **k); 189 ; 190 if callable(arg):. /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magics/execution.py in time(self, line, cell, local_ns); 1178 else:; 1179 st = clock2(); -> 1180 exec(code, glob, local_ns); 1181 end = clock2(); 1182 out = None. <timed exec> in <module>(). /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in dpt(adata, n_branchings, n_neighbors, knn, n_pcs, n_dcs, min_group_size, n_jobs, recompute_graph, recompute_pca, allow_kendall_tau_shift, flavor, copy); 127 adata.smp['dpt_pseudotime'] = dpt.pseudotime; 128 # detect branchings and partition the data into segments; --> 129 dpt.branchings_segments(); 130 # vector of length n_groups; 131 adata.add['dpt_groups_order'] = [str(n) for n in dpt.segs_names_unique]. /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in branchings_segments(self); 188 for each segment.; 189 """"""; --> 190 self.detect_branchings(); 191 self.postprocess_segments(); 192 self.set_segs_names(). /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in detect_branchings(self); 258 segs_connects,; 259 segs_undecided,; --> 260 segs_adjacency, iseg, tips3); 261 # store as class members; 262 self.segs = segs. /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3); 464 # branching on the segment, return the list ssegs of segments that; 465 # are defined by splitting this segment; --> 466 result = self._detect_branching(Dseg, tips3, seg); 467 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connec",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33
https://github.com/scverse/scanpy/issues/33:3795,Safety,detect,detected,3795,"artition the data into segments; --> 129 dpt.branchings_segments(); 130 # vector of length n_groups; 131 adata.add['dpt_groups_order'] = [str(n) for n in dpt.segs_names_unique]. /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in branchings_segments(self); 188 for each segment.; 189 """"""; --> 190 self.detect_branchings(); 191 self.postprocess_segments(); 192 self.set_segs_names(). /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in detect_branchings(self); 258 segs_connects,; 259 segs_undecided,; --> 260 segs_adjacency, iseg, tips3); 261 # store as class members; 262 self.segs = segs. /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3); 464 # branching on the segment, return the list ssegs of segments that; 465 # are defined by splitting this segment; --> 466 result = self._detect_branching(Dseg, tips3, seg); 467 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result; 468 # map back to global indices. /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in _detect_branching(self, Dseg, tips, seg_reference); 632 if len(np.flatnonzero(newseg)) <= 1:; 633 logg.warn('detected group with only {} cells'.format(np.flatnonzero(newseg))); --> 634 secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]; 635 ssegs_tips.append([tips[inewseg], secondtip]); 636 undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]. /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/numpy/core/fromnumeric.py in argmax(a, axis, out); 971 except AttributeError:; 972 return _wrapit(a, 'argmax', axis, out); --> 973 return argmax(axis, out); 974 ; 975 . ValueError: attempt to get argmax of an empty sequence```. Does it mean that this data didn't contain any branches? ; Howerer, I do see some small tips when I plot using `diffmap` result or `sc.tl.dpt` with default branch. Could you help me to figure it out?. Thanks,; jiping",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33
https://github.com/scverse/scanpy/issues/33:1327,Testability,log,logging,1327," expression vector; of a root cell. Either add; adata.add['iroot'] = root_cell_index; or (robust to subsampling); adata.var['xroot'] = adata.X[root_cell_index, :]; where ""root_cell_index"" is the integer index of the root cell, or; adata.var['xroot'] = adata[root_cell_name, :].X; where ""root_cell_name"" is the name (a string) of the root cell.; perform Diffusion Pseudotime analysis; using ""X_pca"" for building graph; using stored data graph with n_neighbors = 30 and spectrum; [ 1. 0.9944264293 0.9934666753 0.9925051928 0.9899699688; 0.9893597364 0.9855745435 0.9840251803 0.981688261 0.9806631804]; detect 1 branching; do not consider groups with less than 2742 points for splitting; branching 1: split group 0; WARNING: detected group with only [] cells. ValueError Traceback (most recent call last); <ipython-input-3-b1749d943ac4> in <module>(); ----> 1 get_ipython().run_cell_magic('time', '', 'sc.tl.dpt(adata_corrected,n_jobs=48,n_pcs=30,allow_kendall_tau_shift=False,n_branchings=1)\nsc.logging.print_memory_usage()'). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell); 2113 magic_arg_s = self.var_expand(line, stack_depth); 2114 with self.builtin_trap:; -> 2115 result = fn(magic_arg_s, cell); 2116 return result; 2117 . <decorator-gen-59> in time(self, line, cell, local_ns). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k); 186 # but it's overkill for just that one bit of state.; 187 def magic_deco(arg):; --> 188 call = lambda f, *a, **k: f(*a, **k); 189 ; 190 if callable(arg):. /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magics/execution.py in time(self, line, cell, local_ns); 1178 else:; 1179 st = clock2(); -> 1180 exec(code, glob, local_ns); 1181 end = clock2(); 1182 out = None. <timed exec> in <module>(). /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33
https://github.com/scverse/scanpy/issues/33:3784,Testability,log,logg,3784,"artition the data into segments; --> 129 dpt.branchings_segments(); 130 # vector of length n_groups; 131 adata.add['dpt_groups_order'] = [str(n) for n in dpt.segs_names_unique]. /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in branchings_segments(self); 188 for each segment.; 189 """"""; --> 190 self.detect_branchings(); 191 self.postprocess_segments(); 192 self.set_segs_names(). /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in detect_branchings(self); 258 segs_connects,; 259 segs_undecided,; --> 260 segs_adjacency, iseg, tips3); 261 # store as class members; 262 self.segs = segs. /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3); 464 # branching on the segment, return the list ssegs of segments that; 465 # are defined by splitting this segment; --> 466 result = self._detect_branching(Dseg, tips3, seg); 467 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result; 468 # map back to global indices. /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in _detect_branching(self, Dseg, tips, seg_reference); 632 if len(np.flatnonzero(newseg)) <= 1:; 633 logg.warn('detected group with only {} cells'.format(np.flatnonzero(newseg))); --> 634 secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]; 635 ssegs_tips.append([tips[inewseg], secondtip]); 636 undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]. /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/numpy/core/fromnumeric.py in argmax(a, axis, out); 971 except AttributeError:; 972 return _wrapit(a, 'argmax', axis, out); --> 973 return argmax(axis, out); 974 ; 975 . ValueError: attempt to get argmax of an empty sequence```. Does it mean that this data didn't contain any branches? ; Howerer, I do see some small tips when I plot using `diffmap` result or `sc.tl.dpt` with default branch. Could you help me to figure it out?. Thanks,; jiping",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33
https://github.com/scverse/scanpy/issues/35:118,Availability,error,error,118,"I just have scanpy 0.2.7 and am trying to produce bpmc3 results. BUT right at the beginning (sc.read()) the following error! I will appreciate your help.; thanks. `--------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-ef7315cdb8ff> in <module>(); 2 filename_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filenam",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35
https://github.com/scverse/scanpy/issues/35:647,Performance,cache,cache,647,"I just have scanpy 0.2.7 and am trying to produce bpmc3 results. BUT right at the beginning (sc.read()) the following error! I will appreciate your help.; thanks. `--------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-ef7315cdb8ff> in <module>(); 2 filename_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filenam",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35
https://github.com/scverse/scanpy/issues/35:980,Performance,cache,cache,980,"pmc3 results. BUT right at the beginning (sc.read()) the following error! I will appreciate your help.; thanks. `--------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-ef7315cdb8ff> in <module>(); 2 filename_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filename); 935 ; 936 def wait_until_file_unused(filename)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35
https://github.com/scverse/scanpy/issues/35:1134,Performance,cache,cache,1134,"pmc3 results. BUT right at the beginning (sc.read()) the following error! I will appreciate your help.; thanks. `--------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-ef7315cdb8ff> in <module>(); 2 filename_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filename); 935 ; 936 def wait_until_file_unused(filename)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35
https://github.com/scverse/scanpy/issues/35:1379,Performance,cache,cache,1379,"_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filename); 935 ; 936 def wait_until_file_unused(filename):; --> 937 while (filename in get_used_files()):; 938 time.sleep(1); 939 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in get_used_files(); 919 def get_used_files():; 920 """"""Get files used by processes with name scanpy.""""""; --> 921 loop_over_scanpy_processes = ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35
https://github.com/scverse/scanpy/issues/35:1814,Safety,safe,safe,1814,"proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filename); 935 ; 936 def wait_until_file_unused(filename):; --> 937 while (filename in get_used_files()):; 938 time.sleep(1); 939 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in get_used_files(); 919 def get_used_files():; 920 """"""Get files used by processes with name scanpy.""""""; --> 921 loop_over_scanpy_processes = (proc for proc in psutil.process_iter(); 922 if proc.name() == 'scanpy'); 923 filenames = []. AttributeError: module 'psutil' has no attribute 'process_iter'. `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35
https://github.com/scverse/scanpy/issues/35:816,Security,hash,hashem,816,"I just have scanpy 0.2.7 and am trying to produce bpmc3 results. BUT right at the beginning (sc.read()) the following error! I will appreciate your help.; thanks. `--------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-ef7315cdb8ff> in <module>(); 2 filename_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filenam",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35
https://github.com/scverse/scanpy/issues/35:1234,Security,hash,hashem,1234,"ributeError Traceback (most recent call last); <ipython-input-3-ef7315cdb8ff> in <module>(); 2 filename_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filename); 935 ; 936 def wait_until_file_unused(filename):; --> 937 while (filename in get_used_files()):; 938 time.sleep(1); 939 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in get_used_files(); 919 def get_used_fi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35
https://github.com/scverse/scanpy/issues/35:1605,Security,hash,hashem,1605,"proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filename); 935 ; 936 def wait_until_file_unused(filename):; --> 937 while (filename in get_used_files()):; 938 time.sleep(1); 939 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in get_used_files(); 919 def get_used_files():; 920 """"""Get files used by processes with name scanpy.""""""; --> 921 loop_over_scanpy_processes = (proc for proc in psutil.process_iter(); 922 if proc.name() == 'scanpy'); 923 filenames = []. AttributeError: module 'psutil' has no attribute 'process_iter'. `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35
https://github.com/scverse/scanpy/issues/35:1901,Security,hash,hashem,1901,"proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filename); 935 ; 936 def wait_until_file_unused(filename):; --> 937 while (filename in get_used_files()):; 938 time.sleep(1); 939 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in get_used_files(); 919 def get_used_files():; 920 """"""Get files used by processes with name scanpy.""""""; --> 921 loop_over_scanpy_processes = (proc for proc in psutil.process_iter(); 922 if proc.name() == 'scanpy'); 923 filenames = []. AttributeError: module 'psutil' has no attribute 'process_iter'. `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35
https://github.com/scverse/scanpy/issues/35:2137,Security,hash,hashem,2137,"proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filename); 935 ; 936 def wait_until_file_unused(filename):; --> 937 while (filename in get_used_files()):; 938 time.sleep(1); 939 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in get_used_files(); 919 def get_used_files():; 920 """"""Get files used by processes with name scanpy.""""""; --> 921 loop_over_scanpy_processes = (proc for proc in psutil.process_iter(); 922 if proc.name() == 'scanpy'); 923 filenames = []. AttributeError: module 'psutil' has no attribute 'process_iter'. `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35
https://github.com/scverse/scanpy/issues/78:219,Deployability,patch,patch,219,"I've just find out there's a little bug in the scoring function used to calculate cell cycle score. The random genes should be chosen from the same bins of gene_list, but now they are chosen from the whole dataset. The patch to make it work properly:. ```; --- a/scanpy/tools/score_gene_lists.py; +++ b/scanpy/tools/score_gene_lists.py; @@ -79,7 +79,7 @@ def score_gene_list(; control_genes = set(); ; # now pick 100 genes from every cut; - for cut in np.unique(obs_cut):; + for cut in np.unique(obs_cut.loc[gene_list]):; r_genes = np.array(obs_cut[obs_cut == cut].index); np.random.shuffle(r_genes); control_genes.update(set(r_genes[:ctrl_size])) # if ctrl_size > len(r_genes) is not a problem for numpy...; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/78
https://github.com/scverse/scanpy/issues/78:615,Deployability,update,update,615,"I've just find out there's a little bug in the scoring function used to calculate cell cycle score. The random genes should be chosen from the same bins of gene_list, but now they are chosen from the whole dataset. The patch to make it work properly:. ```; --- a/scanpy/tools/score_gene_lists.py; +++ b/scanpy/tools/score_gene_lists.py; @@ -79,7 +79,7 @@ def score_gene_list(; control_genes = set(); ; # now pick 100 genes from every cut; - for cut in np.unique(obs_cut):; + for cut in np.unique(obs_cut.loc[gene_list]):; r_genes = np.array(obs_cut[obs_cut == cut].index); np.random.shuffle(r_genes); control_genes.update(set(r_genes[:ctrl_size])) # if ctrl_size > len(r_genes) is not a problem for numpy...; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/78
https://github.com/scverse/scanpy/issues/84:166,Availability,avail,available,166,"First of all congratulations for the awesome package, intuitive and works great. Just a suggestion if you have time to implement ridgeplots or joyplots like the ones available now in Seurat. They are useful to present several distributions in a compact (and attractive) way. Seems possible through the seaborn kdeplot functions (https://seaborn.pydata.org/examples/kde_joyplot.html). Just a suggestion, so feel free to close to issue at any point!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/84
https://github.com/scverse/scanpy/issues/84:54,Usability,intuit,intuitive,54,"First of all congratulations for the awesome package, intuitive and works great. Just a suggestion if you have time to implement ridgeplots or joyplots like the ones available now in Seurat. They are useful to present several distributions in a compact (and attractive) way. Seems possible through the seaborn kdeplot functions (https://seaborn.pydata.org/examples/kde_joyplot.html). Just a suggestion, so feel free to close to issue at any point!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/84
https://github.com/scverse/scanpy/issues/86:257,Security,access,access,257,"Hello everyone,. In order to get eigenvalues of the PCs after running PCA, I modified the simple.py code by adding a line to return pca_.explained_variance_ and store it in adata.uns in the pca function definition. This might be useful for users wanting to access eigenvalues of the PCs, which is not possible as far as I know with only the explained_variance_ratio. Would it be a good idea to open a pull request for that?. else:; logg.m('compute PCA with n_comps =', n_comps, r=True, v=4); result = pca(adata.X, n_comps=n_comps, zero_center=zero_center,; svd_solver=svd_solver, random_state=random_state,; recompute=recompute, mute=mute, return_info=True); X_pca, components, pca_variance_ratio, pca_eigenval = result; adata.obsm['X_pca'] = X_pca; adata.varm['PCs'] = components.T; adata.uns['pca_eigenvalues']=pca_eigenval; adata.uns['pca_variance_ratio'] = pca_variance_ratio; logg.m(' finished', t=True, end=' ', v=4). ....... if False if return_info is None else return_info:. return X_pca, pca_.components_, pca_.explained_variance_ratio_, pca_.explained_variance_; else:; return X_pca. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/86
https://github.com/scverse/scanpy/issues/86:432,Testability,log,logg,432,"Hello everyone,. In order to get eigenvalues of the PCs after running PCA, I modified the simple.py code by adding a line to return pca_.explained_variance_ and store it in adata.uns in the pca function definition. This might be useful for users wanting to access eigenvalues of the PCs, which is not possible as far as I know with only the explained_variance_ratio. Would it be a good idea to open a pull request for that?. else:; logg.m('compute PCA with n_comps =', n_comps, r=True, v=4); result = pca(adata.X, n_comps=n_comps, zero_center=zero_center,; svd_solver=svd_solver, random_state=random_state,; recompute=recompute, mute=mute, return_info=True); X_pca, components, pca_variance_ratio, pca_eigenval = result; adata.obsm['X_pca'] = X_pca; adata.varm['PCs'] = components.T; adata.uns['pca_eigenvalues']=pca_eigenval; adata.uns['pca_variance_ratio'] = pca_variance_ratio; logg.m(' finished', t=True, end=' ', v=4). ....... if False if return_info is None else return_info:. return X_pca, pca_.components_, pca_.explained_variance_ratio_, pca_.explained_variance_; else:; return X_pca. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/86
https://github.com/scverse/scanpy/issues/86:881,Testability,log,logg,881,"Hello everyone,. In order to get eigenvalues of the PCs after running PCA, I modified the simple.py code by adding a line to return pca_.explained_variance_ and store it in adata.uns in the pca function definition. This might be useful for users wanting to access eigenvalues of the PCs, which is not possible as far as I know with only the explained_variance_ratio. Would it be a good idea to open a pull request for that?. else:; logg.m('compute PCA with n_comps =', n_comps, r=True, v=4); result = pca(adata.X, n_comps=n_comps, zero_center=zero_center,; svd_solver=svd_solver, random_state=random_state,; recompute=recompute, mute=mute, return_info=True); X_pca, components, pca_variance_ratio, pca_eigenval = result; adata.obsm['X_pca'] = X_pca; adata.varm['PCs'] = components.T; adata.uns['pca_eigenvalues']=pca_eigenval; adata.uns['pca_variance_ratio'] = pca_variance_ratio; logg.m(' finished', t=True, end=' ', v=4). ....... if False if return_info is None else return_info:. return X_pca, pca_.components_, pca_.explained_variance_ratio_, pca_.explained_variance_; else:; return X_pca. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/86
https://github.com/scverse/scanpy/issues/86:90,Usability,simpl,simple,90,"Hello everyone,. In order to get eigenvalues of the PCs after running PCA, I modified the simple.py code by adding a line to return pca_.explained_variance_ and store it in adata.uns in the pca function definition. This might be useful for users wanting to access eigenvalues of the PCs, which is not possible as far as I know with only the explained_variance_ratio. Would it be a good idea to open a pull request for that?. else:; logg.m('compute PCA with n_comps =', n_comps, r=True, v=4); result = pca(adata.X, n_comps=n_comps, zero_center=zero_center,; svd_solver=svd_solver, random_state=random_state,; recompute=recompute, mute=mute, return_info=True); X_pca, components, pca_variance_ratio, pca_eigenval = result; adata.obsm['X_pca'] = X_pca; adata.varm['PCs'] = components.T; adata.uns['pca_eigenvalues']=pca_eigenval; adata.uns['pca_variance_ratio'] = pca_variance_ratio; logg.m(' finished', t=True, end=' ', v=4). ....... if False if return_info is None else return_info:. return X_pca, pca_.components_, pca_.explained_variance_ratio_, pca_.explained_variance_; else:; return X_pca. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/86
https://github.com/scverse/scanpy/issues/88:419,Availability,error,error,419,"I can't seem to plot the labels in the margin or on the data when plotting a t-SNE with louvain_groups labels. I am loosely following the scanpy seurat tutorial code with my own data at:; https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb. When running:. `sc.pl.tsne(adata_bc, size=28, color='louvain_groups', legend_loc='on_data', legend_fontsize=12, legend_fontweight='bold')`. I get the error:. > anaconda3/lib/python3.6/site-packages/matplotlib/legend.py:326: UserWarning: Unrecognized location ""on_data"". Falling back on ""best""; valid locations are; > 	best; > 	upper right; > 	upper left; > 	lower left; > 	lower right; > 	right; > 	center left; > 	center right; > 	lower center; > 	upper center; > 	center; > ; > six.iterkeys(self.codes)))). putting the legend on the data or in the margin does not seem possible. I am using matplotlib version 2.0.0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/88
https://github.com/scverse/scanpy/issues/89:102,Usability,simpl,simple,102,https://github.com/theislab/scanpy/blob/0caaa9d2e684e2aa76acdf6672d71d7ac38b33cf/scanpy/preprocessing/simple.py#L822; it is not possible to calculate mean for 'SparseDataset' because it is not implemented.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/89
https://github.com/scverse/scanpy/issues/90:2812,Availability,error,error,2812,"2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already up-to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command /cluster/software/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py';; running install; running build; running build_py; creating build; creating build/lib.linux-x86_64-3.6; creating build/lib.linux-x86_64-3.6/scanpy; copying scanpy/settings.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/_version.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__init__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__main__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/readwrite.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/utils.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/exporting.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/logging.py -> build/lib.linux-x86_64-3.6/scanpy; creating build/lib.linux-x86_64-3.6/scanpy/plotting copying scanpy/plotting/top_genes_visual.py -> build/lib.linux-x86_64-3.6/scanpy/plotting copying scanpy/plotting/rcmod.py -> bui",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:8469,Availability,error,error,8469," run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: must be str, not list; ```. </details>. I have never had this error message before and Google can't find anything. . Debugging this a bit, it happens because in this line in build_py.py:. ```py; globs = (self.package_data.get('', []); + self.package_data.get(package, [])); ```. package is ""scanpy"" and the first part before the + is `""*.txt""` and the second part after the + is `[]`. This is Python 3.6.0a1 and pip 9.0.1 on Centos 6.8",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:53,Deployability,install,install,53,"Any ideas what's going on here? I can't do a ""pip3.6 install scanpy"" on our linux cluster:. <details>. ```; Collecting scanpy; Using cached scanpy-0.4.3.tar.gz; Requirement already up-to-date: anndata>=0.5 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: matplotlib==2.0.0 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: pandas>=0.21 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scipy in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: seaborn in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: psutil in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: h5py in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: xlrd in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scikit-learn>=0.19.1 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: statsmodels in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: networkx in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: natsort in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: joblib in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: profilehooks in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirem",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:2645,Deployability,Install,Installing,2645,"already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already up-to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command /cluster/software/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py';; running install; running build; running build_py; creating build; creating build/lib.linux-x86_64-3.6; creating build/lib.linux-x86_64-3.6/scanpy; copying scanpy/settings.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/_version.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__init__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__main__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/readwrite.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/utils.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/exporting.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/logging.py -> build/lib.linux-x86_64-3.6/scanpy; creating build/lib.li",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:2724,Deployability,install,install,2724,"(from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already up-to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command /cluster/software/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py';; running install; running build; running build_py; creating build; creating build/lib.linux-x86_64-3.6; creating build/lib.linux-x86_64-3.6/scanpy; copying scanpy/settings.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/_version.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__init__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__main__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/readwrite.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/utils.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/exporting.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/logging.py -> build/lib.linux-x86_64-3.6/scanpy; creating build/lib.linux-x86_64-3.6/scanpy/plotting copying scanpy/plotting/top_genes_visual.py -> bui",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:2770,Deployability,install,install,2770,"2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already up-to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command /cluster/software/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py';; running install; running build; running build_py; creating build; creating build/lib.linux-x86_64-3.6; creating build/lib.linux-x86_64-3.6/scanpy; copying scanpy/settings.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/_version.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__init__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__main__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/readwrite.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/utils.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/exporting.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/logging.py -> build/lib.linux-x86_64-3.6/scanpy; creating build/lib.linux-x86_64-3.6/scanpy/plotting copying scanpy/plotting/top_genes_visual.py -> build/lib.linux-x86_64-3.6/scanpy/plotting copying scanpy/plotting/rcmod.py -> bui",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:2985,Deployability,install,install,2985,"to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command /cluster/software/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py';; running install; running build; running build_py; creating build; creating build/lib.linux-x86_64-3.6; creating build/lib.linux-x86_64-3.6/scanpy; copying scanpy/settings.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/_version.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__init__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__main__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/readwrite.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/utils.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/exporting.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/logging.py -> build/lib.linux-x86_64-3.6/scanpy; creating build/lib.linux-x86_64-3.6/scanpy/plotting copying scanpy/plotting/top_genes_visual.py -> build/lib.linux-x86_64-3.6/scanpy/plotting copying scanpy/plotting/rcmod.py -> build/lib.linux-x86_64-3.6/scanpy/plotting; copying scanpy/plotting/ann_data.py -> build/lib.linux-x86_64-3.6/scanpy/plotting; copying scanpy/plotting/preprocessing.py -> build/lib.linux-x86_64-3.6",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:6598,Deployability,install,install,6598,"py/tools/louvain.py -> build/lib.linux-x86_64-3.6/scanpy/tools; writing scanpy.egg-info/PKG-INFO; writing entry points to scanpy.egg-info/entry_points.txt; writing top-level names to scanpy.egg-info/top_level.txt; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; ```; ```pytb; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py"", line 83, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/cluster/software/lib/python3.6/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/install.py"", line 61, in run; return orig.install.run(self); File ""/cluster/software/lib/python3.6/distutils/command/install.py"", line 539, in run; self.run_command('build'); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/cluster/software/l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:6640,Deployability,install,install,6640,"ls; writing scanpy.egg-info/PKG-INFO; writing entry points to scanpy.egg-info/entry_points.txt; writing top-level names to scanpy.egg-info/top_level.txt; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; ```; ```pytb; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py"", line 83, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/cluster/software/lib/python3.6/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/install.py"", line 61, in run; return orig.install.run(self); File ""/cluster/software/lib/python3.6/distutils/command/install.py"", line 539, in run; self.run_command('build'); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"",",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:6715,Deployability,install,install,6715,"npy.egg-info/entry_points.txt; writing top-level names to scanpy.egg-info/top_level.txt; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; ```; ```pytb; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py"", line 83, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/cluster/software/lib/python3.6/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/install.py"", line 61, in run; return orig.install.run(self); File ""/cluster/software/lib/python3.6/distutils/command/install.py"", line 539, in run; self.run_command('build'); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:8475,Integrability,message,message,8475," run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: must be str, not list; ```. </details>. I have never had this error message before and Google can't find anything. . Debugging this a bit, it happens because in this line in build_py.py:. ```py; globs = (self.package_data.get('', []); + self.package_data.get(package, [])); ```. package is ""scanpy"" and the first part before the + is `""*.txt""` and the second part after the + is `[]`. This is Python 3.6.0a1 and pip 9.0.1 on Centos 6.8",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:133,Performance,cache,cached,133,"Any ideas what's going on here? I can't do a ""pip3.6 install scanpy"" on our linux cluster:. <details>. ```; Collecting scanpy; Using cached scanpy-0.4.3.tar.gz; Requirement already up-to-date: anndata>=0.5 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: matplotlib==2.0.0 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: pandas>=0.21 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scipy in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: seaborn in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: psutil in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: h5py in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: xlrd in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scikit-learn>=0.19.1 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: statsmodels in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: networkx in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: natsort in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: joblib in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: profilehooks in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirem",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:1845,Performance,cache,cached,1845,"er/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: xlrd in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scikit-learn>=0.19.1 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: statsmodels in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: networkx in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: natsort in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: joblib in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: profilehooks in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already up-to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:1952,Performance,cache,cached,1952,"/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scikit-learn>=0.19.1 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: statsmodels in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: networkx in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: natsort in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: joblib in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: profilehooks in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already up-to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command /cluster/software/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/scratch/tmp/pip-build-g14",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:3593,Testability,log,logging,3593,"oftware/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command /cluster/software/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py';; running install; running build; running build_py; creating build; creating build/lib.linux-x86_64-3.6; creating build/lib.linux-x86_64-3.6/scanpy; copying scanpy/settings.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/_version.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__init__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__main__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/readwrite.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/utils.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/exporting.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/logging.py -> build/lib.linux-x86_64-3.6/scanpy; creating build/lib.linux-x86_64-3.6/scanpy/plotting copying scanpy/plotting/top_genes_visual.py -> build/lib.linux-x86_64-3.6/scanpy/plotting copying scanpy/plotting/rcmod.py -> build/lib.linux-x86_64-3.6/scanpy/plotting; copying scanpy/plotting/ann_data.py -> build/lib.linux-x86_64-3.6/scanpy/plotting; copying scanpy/plotting/preprocessing.py -> build/lib.linux-x86_64-3.6/scanpy/plotting; copying scanpy/plotting/__init__.py -> build/lib.linux-x86_64-3.6/scanpy/plotting; copying scanpy/plotting/palettes.py -> build/lib.linux-x86_64-3.6/scanpy/plotting; copying scanpy/plotting/tools.py -> build/lib.linux-x86_64-3.6/scanpy/plotting; copying scanpy/plotting/utils.py -> build/lib.linux-x86_64-3.6/scanpy/plotting; creating build/lib.linux-x86_64-3.6/scanpy/data_structs; copying scanpy/data_structs/data_graph.py -> build/lib.linux-x86_64-3.6/scanpy/data_structs; copying scanpy/data_structs/__init__.py -> build/lib.linux-x86_64-3.6/scan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/issues/90:1043,Usability,learn,learn,1043,"at's going on here? I can't do a ""pip3.6 install scanpy"" on our linux cluster:. <details>. ```; Collecting scanpy; Using cached scanpy-0.4.3.tar.gz; Requirement already up-to-date: anndata>=0.5 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: matplotlib==2.0.0 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: pandas>=0.21 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scipy in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: seaborn in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: psutil in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: h5py in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: xlrd in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scikit-learn>=0.19.1 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: statsmodels in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: networkx in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: natsort in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: joblib in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: profilehooks in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90
https://github.com/scverse/scanpy/pull/92:350,Modifiability,variab,variable,350,"I added UMAP support for visualization \o/ Here is how MNIST ""single cells"" look like:. ![image](https://user-images.githubusercontent.com/1140359/36549038-bee9d1c4-17bf-11e8-9383-19a70c9ee018.png). ![image](https://user-images.githubusercontent.com/1140359/36549046-c74cbcb4-17bf-11e8-9d8f-595dc7be3e8c.png). I'm not so familiar with code sytle and variable naming etc. yet, and I haven't fully tested things like additional umap kwargs ~and 3d visuazliation~ etc. but let's keep PR here and resolve things along the way.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/92
https://github.com/scverse/scanpy/pull/92:396,Testability,test,tested,396,"I added UMAP support for visualization \o/ Here is how MNIST ""single cells"" look like:. ![image](https://user-images.githubusercontent.com/1140359/36549038-bee9d1c4-17bf-11e8-9383-19a70c9ee018.png). ![image](https://user-images.githubusercontent.com/1140359/36549046-c74cbcb4-17bf-11e8-9d8f-595dc7be3e8c.png). I'm not so familiar with code sytle and variable naming etc. yet, and I haven't fully tested things like additional umap kwargs ~and 3d visuazliation~ etc. but let's keep PR here and resolve things along the way.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/92
https://github.com/scverse/scanpy/issues/94:97,Availability,Error,Error,97,"code:. ```py; sc.tl.rank_genes_groups(adata, 'louvain_groups', groups=['13'], reference= '18' ). Error:; ValueError: reference = 18 needs to be one of group_by = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]; ```. I think the problem is the code comfuse str(18) and int(18). could you solve it? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94
https://github.com/scverse/scanpy/issues/95:237,Testability,Log,Logistic,237,"Hi all, ; Following [this preprint](https://www.biorxiv.org/content/early/2018/02/14/258566) and the polemic [comment](https://liorpachter.wordpress.com/2018/02/15/watermans-egg/amp/?__twitter_impression=true) by its author, I wonder if Logistic Regression should be in scanpy environment.; I tried the `sklearn.linear_model.LogisticRegressionCV` classifier from scikit-learn. It is pretty fast and seems to do the job. Of course there is no urgent need to include in scanpy, as it can be used with two lines like. ```; clf = sklearn.linear_model.LogisticRegressionCV(); clf.fit(adata.X, adata.obs[group]); ```. among the returned elements, `clf.coef_` can be used to rank genes by their importance on each group, `clf.predict_proba` may be used to score the strength of cell/group association given the scored genes. ; Any thought?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95
https://github.com/scverse/scanpy/issues/95:325,Testability,Log,LogisticRegressionCV,325,"Hi all, ; Following [this preprint](https://www.biorxiv.org/content/early/2018/02/14/258566) and the polemic [comment](https://liorpachter.wordpress.com/2018/02/15/watermans-egg/amp/?__twitter_impression=true) by its author, I wonder if Logistic Regression should be in scanpy environment.; I tried the `sklearn.linear_model.LogisticRegressionCV` classifier from scikit-learn. It is pretty fast and seems to do the job. Of course there is no urgent need to include in scanpy, as it can be used with two lines like. ```; clf = sklearn.linear_model.LogisticRegressionCV(); clf.fit(adata.X, adata.obs[group]); ```. among the returned elements, `clf.coef_` can be used to rank genes by their importance on each group, `clf.predict_proba` may be used to score the strength of cell/group association given the scored genes. ; Any thought?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95
https://github.com/scverse/scanpy/issues/95:547,Testability,Log,LogisticRegressionCV,547,"Hi all, ; Following [this preprint](https://www.biorxiv.org/content/early/2018/02/14/258566) and the polemic [comment](https://liorpachter.wordpress.com/2018/02/15/watermans-egg/amp/?__twitter_impression=true) by its author, I wonder if Logistic Regression should be in scanpy environment.; I tried the `sklearn.linear_model.LogisticRegressionCV` classifier from scikit-learn. It is pretty fast and seems to do the job. Of course there is no urgent need to include in scanpy, as it can be used with two lines like. ```; clf = sklearn.linear_model.LogisticRegressionCV(); clf.fit(adata.X, adata.obs[group]); ```. among the returned elements, `clf.coef_` can be used to rank genes by their importance on each group, `clf.predict_proba` may be used to score the strength of cell/group association given the scored genes. ; Any thought?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95
https://github.com/scverse/scanpy/issues/95:370,Usability,learn,learn,370,"Hi all, ; Following [this preprint](https://www.biorxiv.org/content/early/2018/02/14/258566) and the polemic [comment](https://liorpachter.wordpress.com/2018/02/15/watermans-egg/amp/?__twitter_impression=true) by its author, I wonder if Logistic Regression should be in scanpy environment.; I tried the `sklearn.linear_model.LogisticRegressionCV` classifier from scikit-learn. It is pretty fast and seems to do the job. Of course there is no urgent need to include in scanpy, as it can be used with two lines like. ```; clf = sklearn.linear_model.LogisticRegressionCV(); clf.fit(adata.X, adata.obs[group]); ```. among the returned elements, `clf.coef_` can be used to rank genes by their importance on each group, `clf.predict_proba` may be used to score the strength of cell/group association given the scored genes. ; Any thought?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95
https://github.com/scverse/scanpy/issues/96:787,Availability,robust,robust,787,"Hi,. I'm using AGA to build global trajectories on neuronal differentiation datasets. It works well on small subsets of data (only progenitors or only neurons), but produces spurious trajectories between clusters that cannot be explained (progenitors --> inhibitory neurons --> excitatory neurons, rather than progenitors --> excitatory neurons). I'm thinking that part of this may be due to noise/outliers in the dataset. . From the paper (Supplementary Note 3.2), it looks like the connectivity between two partitions are calculated as the minimum distance between all pairs of points, which is prone to outliers. . > Taking the minimum is independent of the specific shape of a partition but is prone to outliers: it is only a viable option as the distance measure d itself is highly robust being computed as an average over all random walks on the graph. . Are there alternative ways to calculate connectivities that are more robust to outliers? (e.g. other connectivity metrics or something like Endpoint Supervision in Slingshot (https://doi.org/10.1101/128843) to avoid connecting endpoints from different lineages.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:930,Availability,robust,robust,930,"Hi,. I'm using AGA to build global trajectories on neuronal differentiation datasets. It works well on small subsets of data (only progenitors or only neurons), but produces spurious trajectories between clusters that cannot be explained (progenitors --> inhibitory neurons --> excitatory neurons, rather than progenitors --> excitatory neurons). I'm thinking that part of this may be due to noise/outliers in the dataset. . From the paper (Supplementary Note 3.2), it looks like the connectivity between two partitions are calculated as the minimum distance between all pairs of points, which is prone to outliers. . > Taking the minimum is independent of the specific shape of a partition but is prone to outliers: it is only a viable option as the distance measure d itself is highly robust being computed as an average over all random walks on the graph. . Are there alternative ways to calculate connectivities that are more robust to outliers? (e.g. other connectivity metrics or something like Endpoint Supervision in Slingshot (https://doi.org/10.1101/128843) to avoid connecting endpoints from different lineages.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/96:1071,Safety,avoid,avoid,1071,"Hi,. I'm using AGA to build global trajectories on neuronal differentiation datasets. It works well on small subsets of data (only progenitors or only neurons), but produces spurious trajectories between clusters that cannot be explained (progenitors --> inhibitory neurons --> excitatory neurons, rather than progenitors --> excitatory neurons). I'm thinking that part of this may be due to noise/outliers in the dataset. . From the paper (Supplementary Note 3.2), it looks like the connectivity between two partitions are calculated as the minimum distance between all pairs of points, which is prone to outliers. . > Taking the minimum is independent of the specific shape of a partition but is prone to outliers: it is only a viable option as the distance measure d itself is highly robust being computed as an average over all random walks on the graph. . Are there alternative ways to calculate connectivities that are more robust to outliers? (e.g. other connectivity metrics or something like Endpoint Supervision in Slingshot (https://doi.org/10.1101/128843) to avoid connecting endpoints from different lineages.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96
https://github.com/scverse/scanpy/issues/97:194,Availability,Down,Downside,194,"AFAIK networkx and python-igraph do the same thing, only that python-igraph is faster. We also need python-igraph anyway for louvain and so on, so maybe it would be good to get rid of networkx. Downside: python-igraph and louvain-igraph is currently deliberately an optional dependency since it’s hard to install on windows. People need to build it themselves (A task that even I didn’t manage by now, and I got *many* things to compile!) or use Christoph Grohlke’s unofficial builds ([here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph) and [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#louvain-igraph))",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:305,Deployability,install,install,305,"AFAIK networkx and python-igraph do the same thing, only that python-igraph is faster. We also need python-igraph anyway for louvain and so on, so maybe it would be good to get rid of networkx. Downside: python-igraph and louvain-igraph is currently deliberately an optional dependency since it’s hard to install on windows. People need to build it themselves (A task that even I didn’t manage by now, and I got *many* things to compile!) or use Christoph Grohlke’s unofficial builds ([here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph) and [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#louvain-igraph))",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/issues/97:275,Integrability,depend,dependency,275,"AFAIK networkx and python-igraph do the same thing, only that python-igraph is faster. We also need python-igraph anyway for louvain and so on, so maybe it would be good to get rid of networkx. Downside: python-igraph and louvain-igraph is currently deliberately an optional dependency since it’s hard to install on windows. People need to build it themselves (A task that even I didn’t manage by now, and I got *many* things to compile!) or use Christoph Grohlke’s unofficial builds ([here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph) and [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#louvain-igraph))",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97
https://github.com/scverse/scanpy/pull/99:16,Availability,down,downsample,16,"Hey! Here's the downsample function I wrote... you may want to change things like defaults or how it does the inplace operation. Best,. Malte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/99
https://github.com/scverse/scanpy/pull/100:17,Availability,down,downsample,17,"Hey!. Here's the downsample function I wrote to downsample count matrices. Now the function is also loaded via the api. Best,. Malte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:48,Availability,down,downsample,48,"Hey!. Here's the downsample function I wrote to downsample count matrices. Now the function is also loaded via the api. Best,. Malte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/pull/100:100,Performance,load,loaded,100,"Hey!. Here's the downsample function I wrote to downsample count matrices. Now the function is also loaded via the api. Best,. Malte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100
https://github.com/scverse/scanpy/issues/101:319,Deployability,update,update,319,"I'm assuming that the `n_jobs` parameter is supposed to control the number of simultaneously threads used in calculating clusters. However, no matter what I set this parameter to the process will spawn as many threads as their are cores on the machine running it. The offending bit of code seems to be a call to `graph.update.diffmap()` in `add_or_update_graph_in_adata` as far as I can tell. I need to be able to control the number of threads used to use scanpy on a farm. How can I do this?. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/101
https://github.com/scverse/scanpy/issues/102:447,Performance,load,load,447,"```pytb; >>> adata.write(""./result.h5ad""); >>> bdata = sc.read(""./result.h5ad). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-15-d90a365327a0> in <module>(); 1 adata.write(“./results.h5ad”); ----> 2 bdata = sc.read(“./results.h5ad""). /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed); 347 # load everything into memory; 348 d = _read_h5ad(filename=filename); --> 349 return AnnData(d); 350 ; 351 . /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, raw, dtype, single_col, filename, filemode, asview, oidx, vidx); 632 obsm=obsm, varm=varm, raw=raw,; 633 dtype=dtype, single_col=single_col,; --> 634 filename=filename, filemode=filemode); 635 ; 636 def _init_as_view(self, adata_ref, oidx, vidx):. /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, dtype, single_col, filename, filemode); 741 raise ValueError(; 742 'If `X` is a dict no further arguments must be provided.'); --> 743 X, obs, var, uns, obsm, varm, raw = self._from_dict(X); 744 ; 745 # init from AnnData. /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _from_dict(ddata); 1591 d_true_keys['obs'][k_stripped] = pd.Categorical.from_codes(; 1592 codes=d_true_keys['obs'][k_stripped].values,; -> 1593 categories=v); 1594 if k_stripped in d_true_keys['var']:; 1595 d_true_keys['var'][k_stripped] = pd.Categorical.from_codes(. /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in from_codes(cls, codes, categories, ordered); 616 ""codes need to be convertible to an arrays of integers""); 617 ; --> 618 categories = CategoricalDtype._validate_categories(categories); 619 ; 620 if len(codes) and (codes.max() >= len(categories) or codes.min() < -1):. /fastdata/chris/bin/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102
https://github.com/scverse/scanpy/issues/105:75,Availability,error,error,75,"Hi,. I have found that using `sc.api.tl.score_genes()` gives the following error if I input a single gene as gene list:; ```; computing score 'score'; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-27-526dfa387800> in <module>(); ----> 1 sc.tl.score_genes(adata=adata,gene_list= genes). ~/Documents/Python/scanpy/scanpy/tools/score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy); 96 gene_list = list(gene_list); 97 ; ---> 98 score = np.mean(adata[:, gene_list].X, axis=1) - np.mean(adata[:, control_genes].X, axis=1); 99 adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names); 100 . ~/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py in mean(a, axis, dtype, out, keepdims); 2904 pass; 2905 else:; -> 2906 return mean(axis=axis, dtype=dtype, out=out, **kwargs); 2907 ; 2908 return _methods._mean(a, axis=axis, dtype=dtype,. ~/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py in _mean(a, axis, dtype, out, keepdims); 55 ; 56 is_float16_result = False; ---> 57 rcount = _count_reduce_items(arr, axis); 58 # Make this warning show up first; 59 if rcount == 0:. ~/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py in _count_reduce_items(arr, axis); 48 items = 1; 49 for ax in axis:; ---> 50 items *= arr.shape[ax]; 51 return items; 52 . IndexError: tuple index out of range; ```; I suggest that you include a check for the length of the input `gene_list`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/105
https://github.com/scverse/scanpy/issues/106:356,Availability,error,error,356,"I'm curious as to what would be the best practice for this situation... I have an np.array containing 18 expression values for gene y. There are 3 replicates, so 6 different conditions. ; `adata.X[:,0] = [ 72. 92. 51. 93. 1. 46. 0. 33. 46. 75. 56. 28. 90. 100. 7. 25. 40. 81.]`. I need to calculate several values: replicate average, pvalue, FDR, standard error and standard deviation. Currently, I can calculate average for the replicates. The result for the above example is:; `adata.Xmean[:,0] = [71.6667 71.6667 71.6667 46.6667 46.6667 46.6667 26.3333 26.3333 26.3333 53. 53. 53. 65.6667 65.6667 65.6667 48.6667 48.6667 48.6667]`. This seems redundant as the average is listed for each replicate. It led me to think about separating the replicates into their own .X, like .X1 for replicate 1, .X2, etc. This would mean the .Xmean[0] would link to .X1[0], .X2[0], .X3[0]. . The averages may be the only one to benefit from this setup, as each replicate will have its' own p-value, FDR, standard error, and standard deviation. This makes me think the redundant .Xmean is the better approach. What are your thoughts? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:646,Availability,redundant,redundant,646,"I'm curious as to what would be the best practice for this situation... I have an np.array containing 18 expression values for gene y. There are 3 replicates, so 6 different conditions. ; `adata.X[:,0] = [ 72. 92. 51. 93. 1. 46. 0. 33. 46. 75. 56. 28. 90. 100. 7. 25. 40. 81.]`. I need to calculate several values: replicate average, pvalue, FDR, standard error and standard deviation. Currently, I can calculate average for the replicates. The result for the above example is:; `adata.Xmean[:,0] = [71.6667 71.6667 71.6667 46.6667 46.6667 46.6667 26.3333 26.3333 26.3333 53. 53. 53. 65.6667 65.6667 65.6667 48.6667 48.6667 48.6667]`. This seems redundant as the average is listed for each replicate. It led me to think about separating the replicates into their own .X, like .X1 for replicate 1, .X2, etc. This would mean the .Xmean[0] would link to .X1[0], .X2[0], .X3[0]. . The averages may be the only one to benefit from this setup, as each replicate will have its' own p-value, FDR, standard error, and standard deviation. This makes me think the redundant .Xmean is the better approach. What are your thoughts? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:998,Availability,error,error,998,"I'm curious as to what would be the best practice for this situation... I have an np.array containing 18 expression values for gene y. There are 3 replicates, so 6 different conditions. ; `adata.X[:,0] = [ 72. 92. 51. 93. 1. 46. 0. 33. 46. 75. 56. 28. 90. 100. 7. 25. 40. 81.]`. I need to calculate several values: replicate average, pvalue, FDR, standard error and standard deviation. Currently, I can calculate average for the replicates. The result for the above example is:; `adata.Xmean[:,0] = [71.6667 71.6667 71.6667 46.6667 46.6667 46.6667 26.3333 26.3333 26.3333 53. 53. 53. 65.6667 65.6667 65.6667 48.6667 48.6667 48.6667]`. This seems redundant as the average is listed for each replicate. It led me to think about separating the replicates into their own .X, like .X1 for replicate 1, .X2, etc. This would mean the .Xmean[0] would link to .X1[0], .X2[0], .X3[0]. . The averages may be the only one to benefit from this setup, as each replicate will have its' own p-value, FDR, standard error, and standard deviation. This makes me think the redundant .Xmean is the better approach. What are your thoughts? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:1053,Availability,redundant,redundant,1053,"I'm curious as to what would be the best practice for this situation... I have an np.array containing 18 expression values for gene y. There are 3 replicates, so 6 different conditions. ; `adata.X[:,0] = [ 72. 92. 51. 93. 1. 46. 0. 33. 46. 75. 56. 28. 90. 100. 7. 25. 40. 81.]`. I need to calculate several values: replicate average, pvalue, FDR, standard error and standard deviation. Currently, I can calculate average for the replicates. The result for the above example is:; `adata.Xmean[:,0] = [71.6667 71.6667 71.6667 46.6667 46.6667 46.6667 26.3333 26.3333 26.3333 53. 53. 53. 65.6667 65.6667 65.6667 48.6667 48.6667 48.6667]`. This seems redundant as the average is listed for each replicate. It led me to think about separating the replicates into their own .X, like .X1 for replicate 1, .X2, etc. This would mean the .Xmean[0] would link to .X1[0], .X2[0], .X3[0]. . The averages may be the only one to benefit from this setup, as each replicate will have its' own p-value, FDR, standard error, and standard deviation. This makes me think the redundant .Xmean is the better approach. What are your thoughts? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:646,Safety,redund,redundant,646,"I'm curious as to what would be the best practice for this situation... I have an np.array containing 18 expression values for gene y. There are 3 replicates, so 6 different conditions. ; `adata.X[:,0] = [ 72. 92. 51. 93. 1. 46. 0. 33. 46. 75. 56. 28. 90. 100. 7. 25. 40. 81.]`. I need to calculate several values: replicate average, pvalue, FDR, standard error and standard deviation. Currently, I can calculate average for the replicates. The result for the above example is:; `adata.Xmean[:,0] = [71.6667 71.6667 71.6667 46.6667 46.6667 46.6667 26.3333 26.3333 26.3333 53. 53. 53. 65.6667 65.6667 65.6667 48.6667 48.6667 48.6667]`. This seems redundant as the average is listed for each replicate. It led me to think about separating the replicates into their own .X, like .X1 for replicate 1, .X2, etc. This would mean the .Xmean[0] would link to .X1[0], .X2[0], .X3[0]. . The averages may be the only one to benefit from this setup, as each replicate will have its' own p-value, FDR, standard error, and standard deviation. This makes me think the redundant .Xmean is the better approach. What are your thoughts? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/106:1053,Safety,redund,redundant,1053,"I'm curious as to what would be the best practice for this situation... I have an np.array containing 18 expression values for gene y. There are 3 replicates, so 6 different conditions. ; `adata.X[:,0] = [ 72. 92. 51. 93. 1. 46. 0. 33. 46. 75. 56. 28. 90. 100. 7. 25. 40. 81.]`. I need to calculate several values: replicate average, pvalue, FDR, standard error and standard deviation. Currently, I can calculate average for the replicates. The result for the above example is:; `adata.Xmean[:,0] = [71.6667 71.6667 71.6667 46.6667 46.6667 46.6667 26.3333 26.3333 26.3333 53. 53. 53. 65.6667 65.6667 65.6667 48.6667 48.6667 48.6667]`. This seems redundant as the average is listed for each replicate. It led me to think about separating the replicates into their own .X, like .X1 for replicate 1, .X2, etc. This would mean the .Xmean[0] would link to .X1[0], .X2[0], .X3[0]. . The averages may be the only one to benefit from this setup, as each replicate will have its' own p-value, FDR, standard error, and standard deviation. This makes me think the redundant .Xmean is the better approach. What are your thoughts? Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106
https://github.com/scverse/scanpy/issues/108:5,Performance,load,loading,5,"When loading scanpy through `import scanpy.api as sc` the plotting style is changed. This prevents my from using it in my general workflow because I have particular plotting styles I use. This used to be a issue with seaborn, but was changed in version 0.8, but scanpy still seems to restyle the entire plotting environment?. If styles are applied locally in particular plotting functions, that is fine, but it shouldn't affect the users other plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108
https://github.com/scverse/scanpy/issues/109:552,Availability,toler,tolerance,552,"#**Here is an example:** . adata.var.ix['Wfdc18']; result: ; gene_ids ENSMUSG00000000983; n_cells 2411; Name: Wfdc18, dtype: object. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); filter_result = sc.pp.filter_genes_dispersion(; adata.X, min_mean=0.0125, max_mean=3, min_disp=0.5); sc.pl.filter_genes_dispersion(filter_result). adata1 = adata[:, filter_result.gene_subset]; adata1.var.ix['Wfdc18']. KeyError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2524 try:; -> 2525 return self._engine.get_loc(key); 2526 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'Wfdc18'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-53-fb4aad8315fd> in <module>(); 1 print (adata.var.ix['Wfdc18']); ----> 2 print (adata1.var.ix['Wfdc18']). /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in __getitem__(self, key); 125 ; 126 key = com._apply_if_callable(key, self.obj); --> 127 return self._getitem_axis(key, axis=axis); 128 ; 129 def _get_label(self, label, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis); 1106 return self._get_loc(key, axis=axis); 1107 ; -> 1108 return self._get_label(key, axis=axis); 1109 ; 1110 def _getitem_iterable(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _get_label(self, label, axis); 143 raise IndexingError('no slices here, handle elsewhere'); 144 ; --> 145 return self.obj._xs(label, axis=axis); 146 ; 147 def _get_loc(self, key, axis=None):. /usr/local/lib/pyth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:2309,Availability,toler,tolerance,2309," 2 print (adata1.var.ix['Wfdc18']). /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in __getitem__(self, key); 125 ; 126 key = com._apply_if_callable(key, self.obj); --> 127 return self._getitem_axis(key, axis=axis); 128 ; 129 def _get_label(self, label, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis); 1106 return self._get_loc(key, axis=axis); 1107 ; -> 1108 return self._get_label(key, axis=axis); 1109 ; 1110 def _getitem_iterable(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _get_label(self, label, axis); 143 raise IndexingError('no slices here, handle elsewhere'); 144 ; --> 145 return self.obj._xs(label, axis=axis); 146 ; 147 def _get_loc(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in xs(self, key, axis, level, drop_level); 2342 drop_level=drop_level); 2343 else:; -> 2344 loc = self.index.get_loc(key); 2345 ; 2346 if isinstance(loc, np.ndarray):. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'Wfdc18'. **Moreover, it still plots it?:** . ax1 = sc.pl.pca_scatter(adata1, components='1,2', color=['Wfdc18'], right_margin=0.2). ![pca_adata1 001](https://user-images.githubusercontent.com/6422882/37835563-d56b07e6-2e86-11e8-8326-c730011de4d4.jpeg). **Any input I would be most grateful. Many thanks, ; Olivia**",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:2512,Availability,toler,tolerance,2512," 2 print (adata1.var.ix['Wfdc18']). /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in __getitem__(self, key); 125 ; 126 key = com._apply_if_callable(key, self.obj); --> 127 return self._getitem_axis(key, axis=axis); 128 ; 129 def _get_label(self, label, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis); 1106 return self._get_loc(key, axis=axis); 1107 ; -> 1108 return self._get_label(key, axis=axis); 1109 ; 1110 def _getitem_iterable(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _get_label(self, label, axis); 143 raise IndexingError('no slices here, handle elsewhere'); 144 ; --> 145 return self.obj._xs(label, axis=axis); 146 ; 147 def _get_loc(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in xs(self, key, axis, level, drop_level); 2342 drop_level=drop_level); 2343 else:; -> 2344 loc = self.index.get_loc(key); 2345 ; 2346 if isinstance(loc, np.ndarray):. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'Wfdc18'. **Moreover, it still plots it?:** . ax1 = sc.pl.pca_scatter(adata1, components='1,2', color=['Wfdc18'], right_margin=0.2). ![pca_adata1 001](https://user-images.githubusercontent.com/6422882/37835563-d56b07e6-2e86-11e8-8326-c730011de4d4.jpeg). **Any input I would be most grateful. Many thanks, ; Olivia**",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:2522,Availability,toler,tolerance,2522," 2 print (adata1.var.ix['Wfdc18']). /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in __getitem__(self, key); 125 ; 126 key = com._apply_if_callable(key, self.obj); --> 127 return self._getitem_axis(key, axis=axis); 128 ; 129 def _get_label(self, label, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis); 1106 return self._get_loc(key, axis=axis); 1107 ; -> 1108 return self._get_label(key, axis=axis); 1109 ; 1110 def _getitem_iterable(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _get_label(self, label, axis); 143 raise IndexingError('no slices here, handle elsewhere'); 144 ; --> 145 return self.obj._xs(label, axis=axis); 146 ; 147 def _get_loc(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in xs(self, key, axis, level, drop_level); 2342 drop_level=drop_level); 2343 else:; -> 2344 loc = self.index.get_loc(key); 2345 ; 2346 if isinstance(loc, np.ndarray):. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'Wfdc18'. **Moreover, it still plots it?:** . ax1 = sc.pl.pca_scatter(adata1, components='1,2', color=['Wfdc18'], right_margin=0.2). ![pca_adata1 001](https://user-images.githubusercontent.com/6422882/37835563-d56b07e6-2e86-11e8-8326-c730011de4d4.jpeg). **Any input I would be most grateful. Many thanks, ; Olivia**",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:832,Security,hash,hashtable,832,"#**Here is an example:** . adata.var.ix['Wfdc18']; result: ; gene_ids ENSMUSG00000000983; n_cells 2411; Name: Wfdc18, dtype: object. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); filter_result = sc.pp.filter_genes_dispersion(; adata.X, min_mean=0.0125, max_mean=3, min_disp=0.5); sc.pl.filter_genes_dispersion(filter_result). adata1 = adata[:, filter_result.gene_subset]; adata1.var.ix['Wfdc18']. KeyError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2524 try:; -> 2525 return self._engine.get_loc(key); 2526 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'Wfdc18'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-53-fb4aad8315fd> in <module>(); 1 print (adata.var.ix['Wfdc18']); ----> 2 print (adata1.var.ix['Wfdc18']). /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in __getitem__(self, key); 125 ; 126 key = com._apply_if_callable(key, self.obj); --> 127 return self._getitem_axis(key, axis=axis); 128 ; 129 def _get_label(self, label, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis); 1106 return self._get_loc(key, axis=axis); 1107 ; -> 1108 return self._get_label(key, axis=axis); 1109 ; 1110 def _getitem_iterable(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _get_label(self, label, axis); 143 raise IndexingError('no slices here, handle elsewhere'); 144 ; --> 145 return self.obj._xs(label, axis=axis); 146 ; 147 def _get_loc(self, key, axis=None):. /usr/local/lib/pyth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:928,Security,hash,hashtable,928,"#**Here is an example:** . adata.var.ix['Wfdc18']; result: ; gene_ids ENSMUSG00000000983; n_cells 2411; Name: Wfdc18, dtype: object. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); filter_result = sc.pp.filter_genes_dispersion(; adata.X, min_mean=0.0125, max_mean=3, min_disp=0.5); sc.pl.filter_genes_dispersion(filter_result). adata1 = adata[:, filter_result.gene_subset]; adata1.var.ix['Wfdc18']. KeyError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2524 try:; -> 2525 return self._engine.get_loc(key); 2526 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'Wfdc18'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-53-fb4aad8315fd> in <module>(); 1 print (adata.var.ix['Wfdc18']); ----> 2 print (adata1.var.ix['Wfdc18']). /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in __getitem__(self, key); 125 ; 126 key = com._apply_if_callable(key, self.obj); --> 127 return self._getitem_axis(key, axis=axis); 128 ; 129 def _get_label(self, label, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis); 1106 return self._get_loc(key, axis=axis); 1107 ; -> 1108 return self._get_label(key, axis=axis); 1109 ; 1110 def _getitem_iterable(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _get_label(self, label, axis); 143 raise IndexingError('no slices here, handle elsewhere'); 144 ; --> 145 return self.obj._xs(label, axis=axis); 146 ; 147 def _get_loc(self, key, axis=None):. /usr/local/lib/pyth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:2726,Security,hash,hashtable,2726," 2 print (adata1.var.ix['Wfdc18']). /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in __getitem__(self, key); 125 ; 126 key = com._apply_if_callable(key, self.obj); --> 127 return self._getitem_axis(key, axis=axis); 128 ; 129 def _get_label(self, label, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis); 1106 return self._get_loc(key, axis=axis); 1107 ; -> 1108 return self._get_label(key, axis=axis); 1109 ; 1110 def _getitem_iterable(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _get_label(self, label, axis); 143 raise IndexingError('no slices here, handle elsewhere'); 144 ; --> 145 return self.obj._xs(label, axis=axis); 146 ; 147 def _get_loc(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in xs(self, key, axis, level, drop_level); 2342 drop_level=drop_level); 2343 else:; -> 2344 loc = self.index.get_loc(key); 2345 ; 2346 if isinstance(loc, np.ndarray):. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'Wfdc18'. **Moreover, it still plots it?:** . ax1 = sc.pl.pca_scatter(adata1, components='1,2', color=['Wfdc18'], right_margin=0.2). ![pca_adata1 001](https://user-images.githubusercontent.com/6422882/37835563-d56b07e6-2e86-11e8-8326-c730011de4d4.jpeg). **Any input I would be most grateful. Many thanks, ; Olivia**",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/109:2822,Security,hash,hashtable,2822," 2 print (adata1.var.ix['Wfdc18']). /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in __getitem__(self, key); 125 ; 126 key = com._apply_if_callable(key, self.obj); --> 127 return self._getitem_axis(key, axis=axis); 128 ; 129 def _get_label(self, label, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis); 1106 return self._get_loc(key, axis=axis); 1107 ; -> 1108 return self._get_label(key, axis=axis); 1109 ; 1110 def _getitem_iterable(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/indexing.py in _get_label(self, label, axis); 143 raise IndexingError('no slices here, handle elsewhere'); 144 ; --> 145 return self.obj._xs(label, axis=axis); 146 ; 147 def _get_loc(self, key, axis=None):. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in xs(self, key, axis, level, drop_level); 2342 drop_level=drop_level); 2343 else:; -> 2344 loc = self.index.get_loc(key); 2345 ; 2346 if isinstance(loc, np.ndarray):. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'Wfdc18'. **Moreover, it still plots it?:** . ax1 = sc.pl.pca_scatter(adata1, components='1,2', color=['Wfdc18'], right_margin=0.2). ![pca_adata1 001](https://user-images.githubusercontent.com/6422882/37835563-d56b07e6-2e86-11e8-8326-c730011de4d4.jpeg). **Any input I would be most grateful. Many thanks, ; Olivia**",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/109
https://github.com/scverse/scanpy/issues/110:66,Availability,error,errors,66,"Hi, ; I am following the example _robustness.ipynb_ and I get the errors ; _KeyError: 'aga_groups'_ and _KeyError: 'aga_groups_order_original'_; when using the function _aga_compare_paths_ in scanpy 0.4.4. I read that _aga_groups_ disappeared in version 0.3, but looks like _aga_compare_paths_ is still using it?. Many thanks,; Maria",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/110
https://github.com/scverse/scanpy/issues/115:29,Modifiability,variab,variable,29,"I was trying to get top 1000 variable genes in 1.3M dataset, but every time I ended up with zero genes after the `sc.pp.filter_genes_dispersion(adata, n_top_genes=1000)` call. The reason is that `sc.pp.filter_genes_dispersion(adata, n_top_genes=x)` actually returns `x - num_zero_expression_genes` genes instead of x, where num_zero_expression_genes represents number of genes without any expression. . Here is a small reproducible example:. ![image](https://user-images.githubusercontent.com/1140359/38215015-9f3de66e-36c6-11e8-8c96-9c9a6458741d.png). It's easy to fix with a prior `sc.pp.filter_genes(adata, min_counts=1)` call, but I think filter_genes_dispersion should retrieve n_top_genes, regardless of presence of zero expression genes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/115
https://github.com/scverse/scanpy/issues/117:93,Safety,abort,aborts,93,"`scanpy.api.pl.paga(adata, export_to_gexf=True)`; If 'write' dir does not exist, then export aborts.; (Solution: manually create 'write' dir)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/117
https://github.com/scverse/scanpy/pull/118:153,Testability,test,tested,153,Currently output figures are often cut off at the edges. Adding bbox_inches = 'tight' to savefig no longer cuts off figure text at edges. As far as I've tested it this shouldn't cause any other problems and simply saves figures with proper padding.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/118
https://github.com/scverse/scanpy/pull/118:207,Usability,simpl,simply,207,Currently output figures are often cut off at the edges. Adding bbox_inches = 'tight' to savefig no longer cuts off figure text at edges. As far as I've tested it this shouldn't cause any other problems and simply saves figures with proper padding.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/118
https://github.com/scverse/scanpy/pull/119:29,Usability,simpl,simplified,29,"As example for the change I [simplified scanpy.preprocessing.simple.filter_cells](https://github.com/theislab/scanpy/pull/119/commits/ae85520fcd16abbb1fb9748cf0b03fb35ce858b6#diff-1aa47c128676c77be1123acc601efd9eL19). Also I fixed a few docs problems. The new format is now custom, so if you dislike the grey headers, that’s easy. Before | After; ----------|----------; ![before](https://user-images.githubusercontent.com/291575/38499002-c6f9dda0-3bf5-11e8-8a76-07a423b366d3.png) | ![after](https://user-images.githubusercontent.com/291575/38512702-79d3ca24-3c1b-11e8-84af-87f033d5008e.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/pull/119:61,Usability,simpl,simple,61,"As example for the change I [simplified scanpy.preprocessing.simple.filter_cells](https://github.com/theislab/scanpy/pull/119/commits/ae85520fcd16abbb1fb9748cf0b03fb35ce858b6#diff-1aa47c128676c77be1123acc601efd9eL19). Also I fixed a few docs problems. The new format is now custom, so if you dislike the grey headers, that’s easy. Before | After; ----------|----------; ![before](https://user-images.githubusercontent.com/291575/38499002-c6f9dda0-3bf5-11e8-8a76-07a423b366d3.png) | ![after](https://user-images.githubusercontent.com/291575/38512702-79d3ca24-3c1b-11e8-84af-87f033d5008e.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119
https://github.com/scverse/scanpy/issues/122:718,Usability,simpl,simpleSingleCell,718,"Hi,. I noticed that Scanpy doesn't have a ready function for filtering cells with a high percentage of reads mapping to genes in the mitochondrial genome. Is there still some easy way to do this? Apparently this type of cells can be bad. _In the absence of spike-in transcripts, the proportion of reads mapped to genes in the mitochondrial genome can also be used. High proportions are indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), possibly because of loss of cytoplasmic RNA from perforated cells. The reasoning is that mitochondria are larger than individual transcript molecules and less likely to escape through tears in the cell membrane._. https://www.bioconductor.org/help/workflows/simpleSingleCell/#examining-gene-level-metrics",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/122
https://github.com/scverse/scanpy/issues/123:1405,Energy Efficiency,allocate,allocates,1405,"Background: ~40k cells, umap, tsne, ... all had no problem. When running diffmap:; `sc.tl.diffmap(adata,n_comps=13)`. >/fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/scanpy/neighbors/__init__.py:793: RuntimeWarning: divide by zero encountered in true_divide; Q = scipy.sparse.spdiags(1.0/q, 0, W.shape[0], W.shape[0]); /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/scanpy/neighbors/__init__.py:803: RuntimeWarning: divide by zero encountered in true_divide; self.Z = scipy.sparse.spdiags(1.0/z, 0, K.shape[0], K.shape[0]); ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-6472f1ef45f7> in <module>(); ----> 1 sc.tl.diffmap(adata,n_comps=13). /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/scanpy/tools/diffmap.py in diffmap(adata, n_comps, copy); 49 dmap.compute_transitions(); 50 dmap.compute_eigen(n_comps=n_comps); ---> 51 adata.obsm['X_diffmap'] = dmap.eigen_basis; 52 adata.uns['diffmap_evals'] = dmap.eigen_values; 53 logg.info(' finished', time=True, end=' ' if settings.verbosity > 2 else '\n'). /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, key, arr); 104 raise ValueError('Can only assign an array of same length ({}), '; 105 'not of length {}.'; --> 106 .format(self.shape[0], arr.shape[0])); 107 # the following always allocates a new array; 108 # even if the key already exists and dimensions match. ValueError: Can only assign an array of same length (43570), not of length 2056. What might be the problem here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123
https://github.com/scverse/scanpy/issues/123:1049,Testability,log,logg,1049,"Background: ~40k cells, umap, tsne, ... all had no problem. When running diffmap:; `sc.tl.diffmap(adata,n_comps=13)`. >/fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/scanpy/neighbors/__init__.py:793: RuntimeWarning: divide by zero encountered in true_divide; Q = scipy.sparse.spdiags(1.0/q, 0, W.shape[0], W.shape[0]); /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/scanpy/neighbors/__init__.py:803: RuntimeWarning: divide by zero encountered in true_divide; self.Z = scipy.sparse.spdiags(1.0/z, 0, K.shape[0], K.shape[0]); ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-6472f1ef45f7> in <module>(); ----> 1 sc.tl.diffmap(adata,n_comps=13). /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/scanpy/tools/diffmap.py in diffmap(adata, n_comps, copy); 49 dmap.compute_transitions(); 50 dmap.compute_eigen(n_comps=n_comps); ---> 51 adata.obsm['X_diffmap'] = dmap.eigen_basis; 52 adata.uns['diffmap_evals'] = dmap.eigen_values; 53 logg.info(' finished', time=True, end=' ' if settings.verbosity > 2 else '\n'). /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, key, arr); 104 raise ValueError('Can only assign an array of same length ({}), '; 105 'not of length {}.'; --> 106 .format(self.shape[0], arr.shape[0])); 107 # the following always allocates a new array; 108 # even if the key already exists and dimensions match. ValueError: Can only assign an array of same length (43570), not of length 2056. What might be the problem here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123
https://github.com/scverse/scanpy/pull/124:41,Modifiability,variab,variable,41,"This fixes the `UnboundLocalError: local variable 'ig_layout' referenced before assignment` exception that happens in following scenario:. ```; sc.tl.louvain(adata); sc.tl.paga(adata); sc.pl.paga(adata); sc.tl.draw_graph(adata); ```. Since there is no else statement for use_paga check, ig_layout is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/124
https://github.com/scverse/scanpy/pull/125:6,Integrability,wrap,wrapped,6,"Hi, I wrapped a R function from scran (mnnCorrect) in the preprocessing module. It's a new algorithm for batch effect removal ([Batch effects in single-cell RNA-sequencing data are corrected by matching mutual nearest neighbors](https://www.nature.com/articles/nbt.4091)) and has not been implemented in Python, so I thought it would be useful. (Extremely useful for myself 😁 )",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125
https://github.com/scverse/scanpy/pull/126:0,Deployability,Update,Updated,0,Updated /api and /tools to include sandbag and cyclone of the pypairs method. For more details see https://github.com/rfechtner/pypairs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/126
https://github.com/scverse/scanpy/issues/128:113,Availability,error,error,113,"Setting one root works well:; `sc.pl.paga(adata, layout='eq_tree', root=[9])`; Setting multiple roots returns an error: ; `TypeError: unhashable type: 'list'`; Please give an example for using multiple roots, and also explain what does `rootlevel=` set?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/128
https://github.com/scverse/scanpy/issues/129:547,Availability,error,error,547,"tl.dpt with no branching events works:; ```; sc.tl.dpt(adata, n_branchings=0, n_dcs=10, min_group_size=0.01, allow_kendall_tau_shift=True); yields; performing Diffusion Pseudotime analysis; initialized `.distances` `.connectivities` `.eigen_values` `.eigen_basis` `.distances_dpt`; eigenvalues of transition matrix; [1. 0.87799305 0.74851424 0.7235198 0.5982796 0.5652917; 0.45321003 0.35327435 0.33786523 0.29598442]; finished (0:01:09.57) --> added; 'dpt_pseudotime', the pseudotime (adata.obs); ```. But calling pl.dpt on this object yields an error, looking at the above outout of tl.dpt it seems that dpt_groups was not created in adata.obs if n_branchings=0?. ```; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2524 try:; -> 2525 return self._engine.get_loc(key); 2526 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-102-eb7d1d859c99> in <module>(); ----> 1 sc.pl.dpt(adata, groups=None). ~/gitDevelopment/scanpy/scanpy/plotting/tools/__init__.py in dpt(adata, basis, color, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:889,Availability,toler,tolerance,889,"tl.dpt with no branching events works:; ```; sc.tl.dpt(adata, n_branchings=0, n_dcs=10, min_group_size=0.01, allow_kendall_tau_shift=True); yields; performing Diffusion Pseudotime analysis; initialized `.distances` `.connectivities` `.eigen_values` `.eigen_basis` `.distances_dpt`; eigenvalues of transition matrix; [1. 0.87799305 0.74851424 0.7235198 0.5982796 0.5652917; 0.45321003 0.35327435 0.33786523 0.29598442]; finished (0:01:09.57) --> added; 'dpt_pseudotime', the pseudotime (adata.obs); ```. But calling pl.dpt on this object yields an error, looking at the above outout of tl.dpt it seems that dpt_groups was not created in adata.obs if n_branchings=0?. ```; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2524 try:; -> 2525 return self._engine.get_loc(key); 2526 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-102-eb7d1d859c99> in <module>(); ----> 1 sc.pl.dpt(adata, groups=None). ~/gitDevelopment/scanpy/scanpy/plotting/tools/__init__.py in dpt(adata, basis, color, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:3024,Availability,toler,tolerance,3024," 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'; ```. ---; scanpy==1.0.4+11.gc241098 anndata==0.5.10 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:3227,Availability,toler,tolerance,3227," 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'; ```. ---; scanpy==1.0.4+11.gc241098 anndata==0.5.10 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:3237,Availability,toler,tolerance,3237," 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'; ```. ---; scanpy==1.0.4+11.gc241098 anndata==0.5.10 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:2410,Energy Efficiency,reduce,reduce,2410,"ccurred:. KeyError Traceback (most recent call last); <ipython-input-102-eb7d1d859c99> in <module>(); ----> 1 sc.pl.dpt(adata, groups=None). ~/gitDevelopment/scanpy/scanpy/plotting/tools/__init__.py in dpt(adata, basis, color, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pand",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:148,Performance,perform,performing,148,"tl.dpt with no branching events works:; ```; sc.tl.dpt(adata, n_branchings=0, n_dcs=10, min_group_size=0.01, allow_kendall_tau_shift=True); yields; performing Diffusion Pseudotime analysis; initialized `.distances` `.connectivities` `.eigen_values` `.eigen_basis` `.distances_dpt`; eigenvalues of transition matrix; [1. 0.87799305 0.74851424 0.7235198 0.5982796 0.5652917; 0.45321003 0.35327435 0.33786523 0.29598442]; finished (0:01:09.57) --> added; 'dpt_pseudotime', the pseudotime (adata.obs); ```. But calling pl.dpt on this object yields an error, looking at the above outout of tl.dpt it seems that dpt_groups was not created in adata.obs if n_branchings=0?. ```; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2524 try:; -> 2525 return self._engine.get_loc(key); 2526 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-102-eb7d1d859c99> in <module>(); ----> 1 sc.pl.dpt(adata, groups=None). ~/gitDevelopment/scanpy/scanpy/plotting/tools/__init__.py in dpt(adata, basis, color, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:2538,Performance,cache,cache,2538,"ups=None). ~/gitDevelopment/scanpy/scanpy/plotting/tools/__init__.py in dpt(adata, basis, color, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:2669,Performance,cache,cache,2669,"d_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'; ```. ---; scanpy==1.0.4+11.gc241098 anndata==0.5.10 numpy==",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:1169,Security,hash,hashtable,1169,"otime analysis; initialized `.distances` `.connectivities` `.eigen_values` `.eigen_basis` `.distances_dpt`; eigenvalues of transition matrix; [1. 0.87799305 0.74851424 0.7235198 0.5982796 0.5652917; 0.45321003 0.35327435 0.33786523 0.29598442]; finished (0:01:09.57) --> added; 'dpt_pseudotime', the pseudotime (adata.obs); ```. But calling pl.dpt on this object yields an error, looking at the above outout of tl.dpt it seems that dpt_groups was not created in adata.obs if n_branchings=0?. ```; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2524 try:; -> 2525 return self._engine.get_loc(key); 2526 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-102-eb7d1d859c99> in <module>(); ----> 1 sc.pl.dpt(adata, groups=None). ~/gitDevelopment/scanpy/scanpy/plotting/tools/__init__.py in dpt(adata, basis, color, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:1265,Security,hash,hashtable,1265,"ances_dpt`; eigenvalues of transition matrix; [1. 0.87799305 0.74851424 0.7235198 0.5982796 0.5652917; 0.45321003 0.35327435 0.33786523 0.29598442]; finished (0:01:09.57) --> added; 'dpt_pseudotime', the pseudotime (adata.obs); ```. But calling pl.dpt on this object yields an error, looking at the above outout of tl.dpt it seems that dpt_groups was not created in adata.obs if n_branchings=0?. ```; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2524 try:; -> 2525 return self._engine.get_loc(key); 2526 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-102-eb7d1d859c99> in <module>(); ----> 1 sc.pl.dpt(adata, groups=None). ~/gitDevelopment/scanpy/scanpy/plotting/tools/__init__.py in dpt(adata, basis, color, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:3441,Security,hash,hashtable,3441," 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'; ```. ---; scanpy==1.0.4+11.gc241098 anndata==0.5.10 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:3537,Security,hash,hashtable,3537," 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'; ```. ---; scanpy==1.0.4+11.gc241098 anndata==0.5.10 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/issues/129:3702,Usability,learn,learn,3702," 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'; ```. ---; scanpy==1.0.4+11.gc241098 anndata==0.5.10 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129
https://github.com/scverse/scanpy/pull/131:6,Integrability,wrap,wrapper,6,Added wrapper for mnnpy in pp,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/131
https://github.com/scverse/scanpy/issues/132:51,Availability,error,errors,51,"Hi, ; I can't manage to use the scanpy read_10x_h5 errors as it raises an exception for the genome I want to use : ; `Exception: Genome GRCm38 does not exist in this file.`; But I'm sure it's this genome string in my file. . Reading the same file with ; `mol_info = sc.read_hdf(""./molecule_info.h5"", key=""genome_ids"" )`; I obtain this error : `ValueError: could not convert string to float: 'GRCm38'`. Do you have any hint oin how to have it working ???; Thanks a lot !!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/132
https://github.com/scverse/scanpy/issues/132:335,Availability,error,error,335,"Hi, ; I can't manage to use the scanpy read_10x_h5 errors as it raises an exception for the genome I want to use : ; `Exception: Genome GRCm38 does not exist in this file.`; But I'm sure it's this genome string in my file. . Reading the same file with ; `mol_info = sc.read_hdf(""./molecule_info.h5"", key=""genome_ids"" )`; I obtain this error : `ValueError: could not convert string to float: 'GRCm38'`. Do you have any hint oin how to have it working ???; Thanks a lot !!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/132
https://github.com/scverse/scanpy/pull/136:371,Energy Efficiency,power,powerful,371,"Hi! I've added two possible APIs for PHATE: I imagine you'll only want to include one of them. . The first, `sc.tl.PHATE`, is an object-oriented interface that matches `phate.PHATE`, which looks like an `sklearn` estimator class. The second is a functional interface which is equivalent to running `phate.PHATE().fit_transform(data)`. The second is more simple, but less powerful as any changes in parameters will require the entire operator to be recomputed, where the object-oriented interface allows partial recomputation. Please let me know which you prefer (or if you would like to include both!) If we only include the functional version I would probably rename it to `phate` rather than `run_phate`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:145,Integrability,interface,interface,145,"Hi! I've added two possible APIs for PHATE: I imagine you'll only want to include one of them. . The first, `sc.tl.PHATE`, is an object-oriented interface that matches `phate.PHATE`, which looks like an `sklearn` estimator class. The second is a functional interface which is equivalent to running `phate.PHATE().fit_transform(data)`. The second is more simple, but less powerful as any changes in parameters will require the entire operator to be recomputed, where the object-oriented interface allows partial recomputation. Please let me know which you prefer (or if you would like to include both!) If we only include the functional version I would probably rename it to `phate` rather than `run_phate`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:257,Integrability,interface,interface,257,"Hi! I've added two possible APIs for PHATE: I imagine you'll only want to include one of them. . The first, `sc.tl.PHATE`, is an object-oriented interface that matches `phate.PHATE`, which looks like an `sklearn` estimator class. The second is a functional interface which is equivalent to running `phate.PHATE().fit_transform(data)`. The second is more simple, but less powerful as any changes in parameters will require the entire operator to be recomputed, where the object-oriented interface allows partial recomputation. Please let me know which you prefer (or if you would like to include both!) If we only include the functional version I would probably rename it to `phate` rather than `run_phate`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:486,Integrability,interface,interface,486,"Hi! I've added two possible APIs for PHATE: I imagine you'll only want to include one of them. . The first, `sc.tl.PHATE`, is an object-oriented interface that matches `phate.PHATE`, which looks like an `sklearn` estimator class. The second is a functional interface which is equivalent to running `phate.PHATE().fit_transform(data)`. The second is more simple, but less powerful as any changes in parameters will require the entire operator to be recomputed, where the object-oriented interface allows partial recomputation. Please let me know which you prefer (or if you would like to include both!) If we only include the functional version I would probably rename it to `phate` rather than `run_phate`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/pull/136:354,Usability,simpl,simple,354,"Hi! I've added two possible APIs for PHATE: I imagine you'll only want to include one of them. . The first, `sc.tl.PHATE`, is an object-oriented interface that matches `phate.PHATE`, which looks like an `sklearn` estimator class. The second is a functional interface which is equivalent to running `phate.PHATE().fit_transform(data)`. The second is more simple, but less powerful as any changes in parameters will require the entire operator to be recomputed, where the object-oriented interface allows partial recomputation. Please let me know which you prefer (or if you would like to include both!) If we only include the functional version I would probably rename it to `phate` rather than `run_phate`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136
https://github.com/scverse/scanpy/issues/137:1003,Deployability,integrat,integrate,1003,"Hi,; Would it be possible to create a panel of plots using both rows and columns when plotting tsne?; I did something similar to this:; ![tsne markers](https://user-images.githubusercontent.com/697622/39486721-19f44a02-4d4b-11e8-986e-d78079a06e0c.png). Basically the code calls the matplotlib subplots method based on the number of plots:. ```py; def _build_subplots(n):; '''; Build subplots grid; n: number of subplots; '''; nrow = int(np.sqrt(n)); ncol = int(np.ceil(n / nrow)); fig, axs = plt.subplots(nrow, ncol, dpi=100, figsize=(ncol*5, nrow*5)). return fig, axs, nrow, ncol; ```. Then the plots are drawn:. ```py; genes = [...list of gene symbols...]; fig, axs, nrow, ncol = _build_subplots(len(genes)). if type(axs) != np.ndarray:; axs = [axs]; else:; axs = axs.ravel(). for i in range(nrow*ncol):; if i < len(genes):; gene = genes[i]; # df is the numpy array containing tSNE; axs[i].scatter(df[:, 0], df[:, 1], ...); ```. Is it something that is already done, planned or that you don't want to integrate?. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/137:1003,Integrability,integrat,integrate,1003,"Hi,; Would it be possible to create a panel of plots using both rows and columns when plotting tsne?; I did something similar to this:; ![tsne markers](https://user-images.githubusercontent.com/697622/39486721-19f44a02-4d4b-11e8-986e-d78079a06e0c.png). Basically the code calls the matplotlib subplots method based on the number of plots:. ```py; def _build_subplots(n):; '''; Build subplots grid; n: number of subplots; '''; nrow = int(np.sqrt(n)); ncol = int(np.ceil(n / nrow)); fig, axs = plt.subplots(nrow, ncol, dpi=100, figsize=(ncol*5, nrow*5)). return fig, axs, nrow, ncol; ```. Then the plots are drawn:. ```py; genes = [...list of gene symbols...]; fig, axs, nrow, ncol = _build_subplots(len(genes)). if type(axs) != np.ndarray:; axs = [axs]; else:; axs = axs.ravel(). for i in range(nrow*ncol):; if i < len(genes):; gene = genes[i]; # df is the numpy array containing tSNE; axs[i].scatter(df[:, 0], df[:, 1], ...); ```. Is it something that is already done, planned or that you don't want to integrate?. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137
https://github.com/scverse/scanpy/issues/138:134,Deployability,install,installed,134,"Hi,; I have some problems running Louvain clustering.; The first time I tried to run, it complains about missing library `igraph`.; I installed `igraph` but now this same library throws a `DeprecationWarning` when calling Louvain clustering. ```; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-17-329d7c2ac26c> in <module>(); ----> 1 sc.tl.louvain(adata). ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, n_jobs, copy); 79 directed = False; 80 if not directed: logg.m(' using the undirected graph', v=4); ---> 81 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 82 if flavor == 'vtraag':; 83 import louvain. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 92 def get_igraph_from_adjacency(adjacency, directed=None):; 93 """"""Get igraph graph from adjacency matrix.""""""; ---> 94 import igraph as ig; 95 sources, targets = adjacency.nonzero(); 96 weights = adjacency[sources, targets]. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```. I can work on a PR and change line 94 of `scanpy/utils.py` to:. ```; import jgraph as ig; ```. This should solve the issue afaik (and `jgraph` should be listed in the dependencies I think). Let me know if you accept PRs. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:1466,Deployability,upgrade,upgrade,1466,"Hi,; I have some problems running Louvain clustering.; The first time I tried to run, it complains about missing library `igraph`.; I installed `igraph` but now this same library throws a `DeprecationWarning` when calling Louvain clustering. ```; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-17-329d7c2ac26c> in <module>(); ----> 1 sc.tl.louvain(adata). ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, n_jobs, copy); 79 directed = False; 80 if not directed: logg.m(' using the undirected graph', v=4); ---> 81 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 82 if flavor == 'vtraag':; 83 import louvain. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 92 def get_igraph_from_adjacency(adjacency, directed=None):; 93 """"""Get igraph graph from adjacency matrix.""""""; ---> 94 import igraph as ig; 95 sources, targets = adjacency.nonzero(); 96 weights = adjacency[sources, targets]. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```. I can work on a PR and change line 94 of `scanpy/utils.py` to:. ```; import jgraph as ig; ```. This should solve the issue afaik (and `jgraph` should be listed in the dependencies I think). Let me know if you accept PRs. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:1627,Deployability,upgrade,upgrade,1627,"Hi,; I have some problems running Louvain clustering.; The first time I tried to run, it complains about missing library `igraph`.; I installed `igraph` but now this same library throws a `DeprecationWarning` when calling Louvain clustering. ```; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-17-329d7c2ac26c> in <module>(); ----> 1 sc.tl.louvain(adata). ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, n_jobs, copy); 79 directed = False; 80 if not directed: logg.m(' using the undirected graph', v=4); ---> 81 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 82 if flavor == 'vtraag':; 83 import louvain. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 92 def get_igraph_from_adjacency(adjacency, directed=None):; 93 """"""Get igraph graph from adjacency matrix.""""""; ---> 94 import igraph as ig; 95 sources, targets = adjacency.nonzero(); 96 weights = adjacency[sources, targets]. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```. I can work on a PR and change line 94 of `scanpy/utils.py` to:. ```; import jgraph as ig; ```. This should solve the issue afaik (and `jgraph` should be listed in the dependencies I think). Let me know if you accept PRs. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:1825,Integrability,depend,dependencies,1825,"Hi,; I have some problems running Louvain clustering.; The first time I tried to run, it complains about missing library `igraph`.; I installed `igraph` but now this same library throws a `DeprecationWarning` when calling Louvain clustering. ```; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-17-329d7c2ac26c> in <module>(); ----> 1 sc.tl.louvain(adata). ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, n_jobs, copy); 79 directed = False; 80 if not directed: logg.m(' using the undirected graph', v=4); ---> 81 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 82 if flavor == 'vtraag':; 83 import louvain. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 92 def get_igraph_from_adjacency(adjacency, directed=None):; 93 """"""Get igraph graph from adjacency matrix.""""""; ---> 94 import igraph as ig; 95 sources, targets = adjacency.nonzero(); 96 weights = adjacency[sources, targets]. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```. I can work on a PR and change line 94 of `scanpy/utils.py` to:. ```; import jgraph as ig; ```. This should solve the issue afaik (and `jgraph` should be listed in the dependencies I think). Let me know if you accept PRs. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:1343,Safety,avoid,avoid,1343,"Hi,; I have some problems running Louvain clustering.; The first time I tried to run, it complains about missing library `igraph`.; I installed `igraph` but now this same library throws a `DeprecationWarning` when calling Louvain clustering. ```; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-17-329d7c2ac26c> in <module>(); ----> 1 sc.tl.louvain(adata). ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, n_jobs, copy); 79 directed = False; 80 if not directed: logg.m(' using the undirected graph', v=4); ---> 81 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 82 if flavor == 'vtraag':; 83 import louvain. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 92 def get_igraph_from_adjacency(adjacency, directed=None):; 93 """"""Get igraph graph from adjacency matrix.""""""; ---> 94 import igraph as ig; 95 sources, targets = adjacency.nonzero(); 96 weights = adjacency[sources, targets]. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```. I can work on a PR and change line 94 of `scanpy/utils.py` to:. ```; import jgraph as ig; ```. This should solve the issue afaik (and `jgraph` should be listed in the dependencies I think). Let me know if you accept PRs. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:1517,Safety,avoid,avoid,1517,"Hi,; I have some problems running Louvain clustering.; The first time I tried to run, it complains about missing library `igraph`.; I installed `igraph` but now this same library throws a `DeprecationWarning` when calling Louvain clustering. ```; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-17-329d7c2ac26c> in <module>(); ----> 1 sc.tl.louvain(adata). ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, n_jobs, copy); 79 directed = False; 80 if not directed: logg.m(' using the undirected graph', v=4); ---> 81 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 82 if flavor == 'vtraag':; 83 import louvain. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 92 def get_igraph_from_adjacency(adjacency, directed=None):; 93 """"""Get igraph graph from adjacency matrix.""""""; ---> 94 import igraph as ig; 95 sources, targets = adjacency.nonzero(); 96 weights = adjacency[sources, targets]. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```. I can work on a PR and change line 94 of `scanpy/utils.py` to:. ```; import jgraph as ig; ```. This should solve the issue afaik (and `jgraph` should be listed in the dependencies I think). Let me know if you accept PRs. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/issues/138:684,Testability,log,logg,684,"Hi,; I have some problems running Louvain clustering.; The first time I tried to run, it complains about missing library `igraph`.; I installed `igraph` but now this same library throws a `DeprecationWarning` when calling Louvain clustering. ```; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-17-329d7c2ac26c> in <module>(); ----> 1 sc.tl.louvain(adata). ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, n_jobs, copy); 79 directed = False; 80 if not directed: logg.m(' using the undirected graph', v=4); ---> 81 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 82 if flavor == 'vtraag':; 83 import louvain. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 92 def get_igraph_from_adjacency(adjacency, directed=None):; 93 """"""Get igraph graph from adjacency matrix.""""""; ---> 94 import igraph as ig; 95 sources, targets = adjacency.nonzero(); 96 weights = adjacency[sources, targets]. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```. I can work on a PR and change line 94 of `scanpy/utils.py` to:. ```; import jgraph as ig; ```. This should solve the issue afaik (and `jgraph` should be listed in the dependencies I think). Let me know if you accept PRs. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138
https://github.com/scverse/scanpy/pull/141:147,Availability,error,error-prone,147,"I added a method for programmatic retrieval of mitochondrial gene symbols through BioMart (instead of using a regular expression, this may be less error-prone and constantly up-to-date).; Let me know if you are interested in merging it, and if the code style is acceptable for this library.; I was unsure on how to test it, in case do you have any suggestions?. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/pull/141:315,Testability,test,test,315,"I added a method for programmatic retrieval of mitochondrial gene symbols through BioMart (instead of using a regular expression, this may be less error-prone and constantly up-to-date).; Let me know if you are interested in merging it, and if the code style is acceptable for this library.; I was unsure on how to test it, in case do you have any suggestions?. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141
https://github.com/scverse/scanpy/issues/142:199,Availability,down,downstream,199,"I'd like to integrate DCA with scanpy and I need to ask some questions regarding that. . There are two use cases for DCA so far. One is denoising as a preprocessing step and using denoised matrix in downstream analyses, especially in the ones that require original feature space such as differential expression. This seems to be a better fit for the `preprocessing` package i.e. `sc.pp.dca(adata)` which overwrites `adata.X` with the denoised matrix. Users can use `copy=True` to keep adata.X of original matrix unchanged. The other use case is the low dimensional representations of cells produced by the encoder. This can also be used for the downstream analyses that do not require original feature space e.g. clustering, and visualization. This is more like `sc.tl.dca(adata)` which stores the new representation into `adata.obsm['X_dca']`. Optionally it can store dropout probabilities (pi matrix) and the dispersion, as well. However, having both sc.tl.dca and sc.pp.dca might be confusing for the users. Any suggestions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/142
https://github.com/scverse/scanpy/issues/142:645,Availability,down,downstream,645,"I'd like to integrate DCA with scanpy and I need to ask some questions regarding that. . There are two use cases for DCA so far. One is denoising as a preprocessing step and using denoised matrix in downstream analyses, especially in the ones that require original feature space such as differential expression. This seems to be a better fit for the `preprocessing` package i.e. `sc.pp.dca(adata)` which overwrites `adata.X` with the denoised matrix. Users can use `copy=True` to keep adata.X of original matrix unchanged. The other use case is the low dimensional representations of cells produced by the encoder. This can also be used for the downstream analyses that do not require original feature space e.g. clustering, and visualization. This is more like `sc.tl.dca(adata)` which stores the new representation into `adata.obsm['X_dca']`. Optionally it can store dropout probabilities (pi matrix) and the dispersion, as well. However, having both sc.tl.dca and sc.pp.dca might be confusing for the users. Any suggestions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/142
https://github.com/scverse/scanpy/issues/142:12,Deployability,integrat,integrate,12,"I'd like to integrate DCA with scanpy and I need to ask some questions regarding that. . There are two use cases for DCA so far. One is denoising as a preprocessing step and using denoised matrix in downstream analyses, especially in the ones that require original feature space such as differential expression. This seems to be a better fit for the `preprocessing` package i.e. `sc.pp.dca(adata)` which overwrites `adata.X` with the denoised matrix. Users can use `copy=True` to keep adata.X of original matrix unchanged. The other use case is the low dimensional representations of cells produced by the encoder. This can also be used for the downstream analyses that do not require original feature space e.g. clustering, and visualization. This is more like `sc.tl.dca(adata)` which stores the new representation into `adata.obsm['X_dca']`. Optionally it can store dropout probabilities (pi matrix) and the dispersion, as well. However, having both sc.tl.dca and sc.pp.dca might be confusing for the users. Any suggestions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/142
https://github.com/scverse/scanpy/issues/142:12,Integrability,integrat,integrate,12,"I'd like to integrate DCA with scanpy and I need to ask some questions regarding that. . There are two use cases for DCA so far. One is denoising as a preprocessing step and using denoised matrix in downstream analyses, especially in the ones that require original feature space such as differential expression. This seems to be a better fit for the `preprocessing` package i.e. `sc.pp.dca(adata)` which overwrites `adata.X` with the denoised matrix. Users can use `copy=True` to keep adata.X of original matrix unchanged. The other use case is the low dimensional representations of cells produced by the encoder. This can also be used for the downstream analyses that do not require original feature space e.g. clustering, and visualization. This is more like `sc.tl.dca(adata)` which stores the new representation into `adata.obsm['X_dca']`. Optionally it can store dropout probabilities (pi matrix) and the dispersion, as well. However, having both sc.tl.dca and sc.pp.dca might be confusing for the users. Any suggestions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/142
https://github.com/scverse/scanpy/issues/143:85,Availability,error,error,85,I tried to install louvain through conda; `conda install -c vtraag louvain`; but got error message:; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - louvain; - python-igraph[version='>=0.7.1.0']. However I could install it by; `conda install -c conda-forge louvain`. Can you please update it on the webpage. Thanks!; [https://scanpy.readthedocs.io/en/latest/installation.html](url),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/143:184,Availability,avail,available,184,I tried to install louvain through conda; `conda install -c vtraag louvain`; but got error message:; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - louvain; - python-igraph[version='>=0.7.1.0']. However I could install it by; `conda install -c conda-forge louvain`. Can you please update it on the webpage. Thanks!; [https://scanpy.readthedocs.io/en/latest/installation.html](url),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/143:11,Deployability,install,install,11,I tried to install louvain through conda; `conda install -c vtraag louvain`; but got error message:; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - louvain; - python-igraph[version='>=0.7.1.0']. However I could install it by; `conda install -c conda-forge louvain`. Can you please update it on the webpage. Thanks!; [https://scanpy.readthedocs.io/en/latest/installation.html](url),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/143:49,Deployability,install,install,49,I tried to install louvain through conda; `conda install -c vtraag louvain`; but got error message:; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - louvain; - python-igraph[version='>=0.7.1.0']. However I could install it by; `conda install -c conda-forge louvain`. Can you please update it on the webpage. Thanks!; [https://scanpy.readthedocs.io/en/latest/installation.html](url),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/143:283,Deployability,install,install,283,I tried to install louvain through conda; `conda install -c vtraag louvain`; but got error message:; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - louvain; - python-igraph[version='>=0.7.1.0']. However I could install it by; `conda install -c conda-forge louvain`. Can you please update it on the webpage. Thanks!; [https://scanpy.readthedocs.io/en/latest/installation.html](url),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/143:305,Deployability,install,install,305,I tried to install louvain through conda; `conda install -c vtraag louvain`; but got error message:; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - louvain; - python-igraph[version='>=0.7.1.0']. However I could install it by; `conda install -c conda-forge louvain`. Can you please update it on the webpage. Thanks!; [https://scanpy.readthedocs.io/en/latest/installation.html](url),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/143:353,Deployability,update,update,353,I tried to install louvain through conda; `conda install -c vtraag louvain`; but got error message:; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - louvain; - python-igraph[version='>=0.7.1.0']. However I could install it by; `conda install -c conda-forge louvain`. Can you please update it on the webpage. Thanks!; [https://scanpy.readthedocs.io/en/latest/installation.html](url),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/143:429,Deployability,install,installation,429,I tried to install louvain through conda; `conda install -c vtraag louvain`; but got error message:; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - louvain; - python-igraph[version='>=0.7.1.0']. However I could install it by; `conda install -c conda-forge louvain`. Can you please update it on the webpage. Thanks!; [https://scanpy.readthedocs.io/en/latest/installation.html](url),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/143:91,Integrability,message,message,91,I tried to install louvain through conda; `conda install -c vtraag louvain`; but got error message:; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - louvain; - python-igraph[version='>=0.7.1.0']. However I could install it by; `conda install -c conda-forge louvain`. Can you please update it on the webpage. Thanks!; [https://scanpy.readthedocs.io/en/latest/installation.html](url),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143
https://github.com/scverse/scanpy/issues/144:872,Deployability,update,updated,872,"I am trying to get clustering labels from Louvain. I followed the Seurat-like notebook (https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). I am able to run the clustering and visualize it on tSNE, with:. ```python; >>> sc.tl.louvain(adata); running Louvain clustering; using the ""louvain"" package of Traag (2017); finished (0:00:00.11) --> found 8 clusters and added; 'louvain', the cluster labels (adata.obs, categorical); >>> sc.pl.tsne(adata, color='louvain'); ```. However, I cannot find the clustering labels under adata:. ```; AnnData object with n_obs × n_vars = 1320 × 5014 ; uns: 'pca'; obsm: 'X_pca', 'X_tsne'; varm: 'PCs'; ```. I tried looking directly into obs, obsm etc., but I didn't find any label. But the plots work out correctly. How is it possible?. I have the default louvain parameters, so it should be updated in-place.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/144
https://github.com/scverse/scanpy/issues/146:1070,Deployability,install,installed,1070,"I was just trying to run the [1.3 million cell clustering example](https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells), but have come across some strange behavior. When loading in the `hdf5` file from 10x to an AnnData object, the whole process uses about 30gb. If I write that AnnData object to disk with `adata.write`, then try to load that file (with `sc.read`) I end up using all the memory on the machine (~60g) before segfault-ing. I'd think that any dataset I wrote from memory, I should be able to read back into memory. I've put the full scripts to reproduce on my system in [this gist](https://gist.github.com/ivirshup/42e70a745704b8c71d78e57dd43e3b0b), but essentially I've run:. ```python; # gen_h5ad.py; import scanpy.api as sc; adata = sc.read_10x_h5(""{DATAPTH}/1M_neurons_filtered_gene_bc_matrices_h5.h5"") # Uses about 30gb; adata.write(""./write/1M_neurons.h5ad""); ```; ```python; # load_anndata.py; import scanpy.api as sc; adata = sc.read(""./write/1M_neurons.h5ad"") # Ends with segfault; ```. I'm running `scanpy` installed with conda with the following versions:. ```; scanpy==1.0.4 anndata==0.6 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0; ```. Thanks for the great package! Let me know if you'd like any more details on my setup.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146
https://github.com/scverse/scanpy/issues/146:208,Performance,load,loading,208,"I was just trying to run the [1.3 million cell clustering example](https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells), but have come across some strange behavior. When loading in the `hdf5` file from 10x to an AnnData object, the whole process uses about 30gb. If I write that AnnData object to disk with `adata.write`, then try to load that file (with `sc.read`) I end up using all the memory on the machine (~60g) before segfault-ing. I'd think that any dataset I wrote from memory, I should be able to read back into memory. I've put the full scripts to reproduce on my system in [this gist](https://gist.github.com/ivirshup/42e70a745704b8c71d78e57dd43e3b0b), but essentially I've run:. ```python; # gen_h5ad.py; import scanpy.api as sc; adata = sc.read_10x_h5(""{DATAPTH}/1M_neurons_filtered_gene_bc_matrices_h5.h5"") # Uses about 30gb; adata.write(""./write/1M_neurons.h5ad""); ```; ```python; # load_anndata.py; import scanpy.api as sc; adata = sc.read(""./write/1M_neurons.h5ad"") # Ends with segfault; ```. I'm running `scanpy` installed with conda with the following versions:. ```; scanpy==1.0.4 anndata==0.6 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0; ```. Thanks for the great package! Let me know if you'd like any more details on my setup.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146
https://github.com/scverse/scanpy/issues/146:372,Performance,load,load,372,"I was just trying to run the [1.3 million cell clustering example](https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells), but have come across some strange behavior. When loading in the `hdf5` file from 10x to an AnnData object, the whole process uses about 30gb. If I write that AnnData object to disk with `adata.write`, then try to load that file (with `sc.read`) I end up using all the memory on the machine (~60g) before segfault-ing. I'd think that any dataset I wrote from memory, I should be able to read back into memory. I've put the full scripts to reproduce on my system in [this gist](https://gist.github.com/ivirshup/42e70a745704b8c71d78e57dd43e3b0b), but essentially I've run:. ```python; # gen_h5ad.py; import scanpy.api as sc; adata = sc.read_10x_h5(""{DATAPTH}/1M_neurons_filtered_gene_bc_matrices_h5.h5"") # Uses about 30gb; adata.write(""./write/1M_neurons.h5ad""); ```; ```python; # load_anndata.py; import scanpy.api as sc; adata = sc.read(""./write/1M_neurons.h5ad"") # Ends with segfault; ```. I'm running `scanpy` installed with conda with the following versions:. ```; scanpy==1.0.4 anndata==0.6 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0; ```. Thanks for the great package! Let me know if you'd like any more details on my setup.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146
https://github.com/scverse/scanpy/issues/146:1202,Usability,learn,learn,1202,"I was just trying to run the [1.3 million cell clustering example](https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells), but have come across some strange behavior. When loading in the `hdf5` file from 10x to an AnnData object, the whole process uses about 30gb. If I write that AnnData object to disk with `adata.write`, then try to load that file (with `sc.read`) I end up using all the memory on the machine (~60g) before segfault-ing. I'd think that any dataset I wrote from memory, I should be able to read back into memory. I've put the full scripts to reproduce on my system in [this gist](https://gist.github.com/ivirshup/42e70a745704b8c71d78e57dd43e3b0b), but essentially I've run:. ```python; # gen_h5ad.py; import scanpy.api as sc; adata = sc.read_10x_h5(""{DATAPTH}/1M_neurons_filtered_gene_bc_matrices_h5.h5"") # Uses about 30gb; adata.write(""./write/1M_neurons.h5ad""); ```; ```python; # load_anndata.py; import scanpy.api as sc; adata = sc.read(""./write/1M_neurons.h5ad"") # Ends with segfault; ```. I'm running `scanpy` installed with conda with the following versions:. ```; scanpy==1.0.4 anndata==0.6 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0; ```. Thanks for the great package! Let me know if you'd like any more details on my setup.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146
https://github.com/scverse/scanpy/pull/147:94,Testability,test,tested,94,"With this PR, I tried to fix issue https://github.com/theislab/scanpy/issues/140. Needs to be tested still. Additionally, I found it useful to provide to scanpy directly the gene names when plotting violins. Use case: you have a different algorithm that is not implemented in scanpy for differential gene expression, and you want to plot its results with scanpy. Feel free to reject if there's a better way to do it currently in scanpy. I didn't find it though.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/147
https://github.com/scverse/scanpy/issues/148:61,Availability,error,error,61,"Hi everyone,; I am using pip3 to install scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copyi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:538,Availability,error,error,538,"Hi everyone,; I am using pip3 to install scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copyi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:8181,Availability,error,error,8181,"/Python/3.6.0/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 47, in run; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 103, in build_package_data; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 59, in __getattr__; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 86, in _get_data_files; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Command ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix="" failed with error code 1 in /tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:33,Deployability,install,install,33,"Hi everyone,; I am using pip3 to install scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copyi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:448,Deployability,Install,Installing,448,"Hi everyone,; I am using pip3 to install scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copyi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:515,Deployability,install,install,515,"Hi everyone,; I am using pip3 to install scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copyi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:707,Deployability,install,install-,707,"Hi everyone,; I am using pip3 to install scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copyi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:870,Deployability,install,install,870,"Hi everyone,; I am using pip3 to install scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copyi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:922,Deployability,install,install-record,922,"Hi everyone,; I am using pip3 to install scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copyi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:1014,Deployability,install,install,1014,"l scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copying scanpy/tools/draw_graph.py -> build/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:5034,Deployability,install,install-,5034,"map; copying scanpy/neighbors/umap/distances.py -> build/lib/scanpy/neighbors/umap; copying scanpy/neighbors/umap/umap_.py -> build/lib/scanpy/neighbors/umap; copying scanpy/neighbors/umap/__init__.py -> build/lib/scanpy/neighbors/umap; copying scanpy/neighbors/umap/sparse.py -> build/lib/scanpy/neighbors/umap; creating build/lib/scanpy/plotting/tools; copying scanpy/plotting/tools/paga.py -> build/lib/scanpy/plotting/tools; copying scanpy/plotting/tools/__init__.py -> build/lib/scanpy/plotting/tools; running egg_info; writing scanpy.egg-info/PKG-INFO; writing dependency_links to scanpy.egg-info/dependency_links.txt; writing requirements to scanpy.egg-info/requires.txt; writing top-level names to scanpy.egg-info/top_level.txt; warning: manifest_maker: standard file '-c' not found. reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py"", line 50, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/install.py"", line 61, in run; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/command/install.py"", line 545, in run; self.run_command('build'); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 31",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:5709,Deployability,install,install,5709," scanpy.egg-info/top_level.txt; warning: manifest_maker: standard file '-c' not found. reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py"", line 50, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/install.py"", line 61, in run; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/command/install.py"", line 545, in run; self.run_command('build'); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/ver",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:5835,Deployability,install,install,5835,"URCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py"", line 50, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/install.py"", line 61, in run; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/command/install.py"", line 545, in run; self.run_command('build'); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:6675,Deployability,install,install-,6675,"3.2-py3.6.egg/setuptools/command/install.py"", line 61, in run; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/command/install.py"", line 545, in run; self.run_command('build'); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 47, in run; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 103, in build_package_data; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 59, in __getattr__; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 86, in _get_data_files; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:7871,Deployability,install,install-,7871,"/Python/3.6.0/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 47, in run; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 103, in build_package_data; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 59, in __getattr__; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 86, in _get_data_files; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Command ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix="" failed with error code 1 in /tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:8034,Deployability,install,install,8034,"/Python/3.6.0/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 47, in run; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 103, in build_package_data; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 59, in __getattr__; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 86, in _get_data_files; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Command ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix="" failed with error code 1 in /tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:8086,Deployability,install,install-record,8086,"/Python/3.6.0/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 47, in run; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 103, in build_package_data; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 59, in __getattr__; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 86, in _get_data_files; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Command ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix="" failed with error code 1 in /tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:8216,Deployability,install,install-,8216,"/Python/3.6.0/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 47, in run; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 103, in build_package_data; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 59, in __getattr__; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 86, in _get_data_files; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Command ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix="" failed with error code 1 in /tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:67,Integrability,message,message,67,"Hi everyone,; I am using pip3 to install scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copyi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:1134,Testability,log,logging,1134,"l scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copying scanpy/tools/draw_graph.py -> build/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/issues/148:2909,Usability,simpl,simple,2909, copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copying scanpy/tools/draw_graph.py -> build/lib/scanpy/tools; copying scanpy/tools/score_genes.py -> build/lib/scanpy/tools; copying scanpy/tools/_tsne_fix.py -> build/lib/scanpy/tools; copying scanpy/tools/rank_genes_groups.py -> build/lib/scanpy/tools; copying scanpy/tools/diffmap.py -> build/lib/scanpy/tools; copying scanpy/tools/top_genes.py -> build/lib/scanpy/tools; creating build/lib/scanpy/sim_models; copying scanpy/sim_models/__init__.py -> build/lib/scanpy/sim_models; creating build/lib/scanpy/datasets; copying scanpy/datasets/__init__.py -> build/lib/scanpy/datasets; copying scanpy/datasets/api_without_datasets.py -> build/lib/scanpy/datasets; creating build/lib/scanpy/neighbors; copying scanpy/neighbors/__init__.py -> build/lib/scanpy/neighbors; creating build/lib/scanpy/preprocessing; copying scanpy/preprocessing/recipes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/simple.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/__init__.py -> build/lib/scanpy/preprocessing; creating build/lib/scanpy/plotting; copying scanpy/plotting/rcmod.py -> build/lib/scanpy/plotting; copying scanpy/plotting/palettes.py -> build/lib/scanpy/plotting; copying scanpy/plotting/utils.py -> build/lib/scanpy/plotting; copying scanpy/plotting/top_genes_visual.py -> build/lib/scanpy/plotting; copying scanpy/plotting/__init__.py -> build/lib/scanpy/plotting; copying scanpy/plotting/anndata.py -> build/lib/scanpy/plotting; copying scanpy/plotting/preprocessing.py -> build/lib/scanpy/plotting; creating build/lib/scanpy/api; copying scanpy/api/datasets.py -> build/lib/scanpy/api; copying scanpy/api/pl.py -> build/lib/scanpy/api; copying scanpy/api/pp.py -> build/lib/scanpy/api; copying scanpy/api/export_to.py -> build/lib/scanpy/api; copying scanpy/api/__init__.py -> build/lib/scanpy/api; copying scanpy/api/tl.py -> build/lib/scanpy/,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148
https://github.com/scverse/scanpy/pull/149:0,Usability,Simpl,Simple,0,"Simple fix for sc.pp.neighbors(..., metric_kwds={...}).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/149
https://github.com/scverse/scanpy/issues/153:106,Availability,error,error,106,"Some of the preprocessing recipes have a `plot` argument, but as far as I can tell, they'll only throw an error. `recipe_zheng17` and `recipe_seurat` have the lines:. ```python; if plot:; from .. import plotting as pl # should not import at the top of the file; pl.filter_genes_dispersion(filter_result, log=True); ```. But `plotting` doesn't have the function `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:421,Availability,error,error,421,"Some of the preprocessing recipes have a `plot` argument, but as far as I can tell, they'll only throw an error. `recipe_zheng17` and `recipe_seurat` have the lines:. ```python; if plot:; from .. import plotting as pl # should not import at the top of the file; pl.filter_genes_dispersion(filter_result, log=True); ```. But `plotting` doesn't have the function `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:659,Availability,error,errors,659,"Some of the preprocessing recipes have a `plot` argument, but as far as I can tell, they'll only throw an error. `recipe_zheng17` and `recipe_seurat` have the lines:. ```python; if plot:; from .. import plotting as pl # should not import at the top of the file; pl.filter_genes_dispersion(filter_result, log=True); ```. But `plotting` doesn't have the function `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:495,Deployability,release,release,495,"Some of the preprocessing recipes have a `plot` argument, but as far as I can tell, they'll only throw an error. `recipe_zheng17` and `recipe_seurat` have the lines:. ```python; if plot:; from .. import plotting as pl # should not import at the top of the file; pl.filter_genes_dispersion(filter_result, log=True); ```. But `plotting` doesn't have the function `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:1428,Performance,cache,cache,1428,"n `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter_genes_dispersion(filter_result, log=True); 109 # actually filter the genes, the following is the inplace version of; 110 # adata = adata[:, filter_result.gene_subset]. AttributeError: module 'scanpy.plotting' has no attribute 'filter_genes_dispersion'; ```. It looks like there's a pretty easy fix here, so I'd be up for making a pull request if you'd like.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:1482,Performance,cache,cache,1482,"n `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter_genes_dispersion(filter_result, log=True); 109 # actually filter the genes, the following is the inplace version of; 110 # adata = adata[:, filter_result.gene_subset]. AttributeError: module 'scanpy.plotting' has no attribute 'filter_genes_dispersion'; ```. It looks like there's a pretty easy fix here, so I'd be up for making a pull request if you'd like.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:387,Security,expose,exposed,387,"Some of the preprocessing recipes have a `plot` argument, but as far as I can tell, they'll only throw an error. `recipe_zheng17` and `recipe_seurat` have the lines:. ```python; if plot:; from .. import plotting as pl # should not import at the top of the file; pl.filter_genes_dispersion(filter_result, log=True); ```. But `plotting` doesn't have the function `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:304,Testability,log,log,304,"Some of the preprocessing recipes have a `plot` argument, but as far as I can tell, they'll only throw an error. `recipe_zheng17` and `recipe_seurat` have the lines:. ```python; if plot:; from .. import plotting as pl # should not import at the top of the file; pl.filter_genes_dispersion(filter_result, log=True); ```. But `plotting` doesn't have the function `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:812,Testability,log,logging,812,"Some of the preprocessing recipes have a `plot` argument, but as far as I can tell, they'll only throw an error. `recipe_zheng17` and `recipe_seurat` have the lines:. ```python; if plot:; from .. import plotting as pl # should not import at the top of the file; pl.filter_genes_dispersion(filter_result, log=True); ```. But `plotting` doesn't have the function `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:1874,Testability,log,log,1874,"n `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter_genes_dispersion(filter_result, log=True); 109 # actually filter the genes, the following is the inplace version of; 110 # adata = adata[:, filter_result.gene_subset]. AttributeError: module 'scanpy.plotting' has no attribute 'filter_genes_dispersion'; ```. It looks like there's a pretty easy fix here, so I'd be up for making a pull request if you'd like.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:2034,Testability,log,log,2034,"n `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter_genes_dispersion(filter_result, log=True); 109 # actually filter the genes, the following is the inplace version of; 110 # adata = adata[:, filter_result.gene_subset]. AttributeError: module 'scanpy.plotting' has no attribute 'filter_genes_dispersion'; ```. It looks like there's a pretty easy fix here, so I'd be up for making a pull request if you'd like.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/issues/153:1260,Usability,learn,learn,1260,"; pl.filter_genes_dispersion(filter_result, log=True); ```. But `plotting` doesn't have the function `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter_genes_dispersion(filter_result, log=True); 109 # actually filter the genes, the following is the inplace version of; 110 # adata = adata[:, filter_result.gene_subset]. AttributeError: module 'scanpy.plotting' has no attribute 'filter_genes_dispersion'; ```. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153
https://github.com/scverse/scanpy/pull/155:155,Availability,error,error,155,"Fixes #153. Fixed usage of `plot=True` for `recipe_zheng17` and `recipe_seurat`. A test was added under `tests/preprocessing.py` which just checks that no error is thrown. Style wise, I just went with changing the fewest lines of code. The test isn't exactly stateless since I've got to deactivate interactive plotting, but I wasn't sure how you'd like to handle that. Lemme know if you'd like any changes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/155
https://github.com/scverse/scanpy/pull/155:83,Testability,test,test,83,"Fixes #153. Fixed usage of `plot=True` for `recipe_zheng17` and `recipe_seurat`. A test was added under `tests/preprocessing.py` which just checks that no error is thrown. Style wise, I just went with changing the fewest lines of code. The test isn't exactly stateless since I've got to deactivate interactive plotting, but I wasn't sure how you'd like to handle that. Lemme know if you'd like any changes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/155
https://github.com/scverse/scanpy/pull/155:105,Testability,test,tests,105,"Fixes #153. Fixed usage of `plot=True` for `recipe_zheng17` and `recipe_seurat`. A test was added under `tests/preprocessing.py` which just checks that no error is thrown. Style wise, I just went with changing the fewest lines of code. The test isn't exactly stateless since I've got to deactivate interactive plotting, but I wasn't sure how you'd like to handle that. Lemme know if you'd like any changes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/155
https://github.com/scverse/scanpy/pull/155:240,Testability,test,test,240,"Fixes #153. Fixed usage of `plot=True` for `recipe_zheng17` and `recipe_seurat`. A test was added under `tests/preprocessing.py` which just checks that no error is thrown. Style wise, I just went with changing the fewest lines of code. The test isn't exactly stateless since I've got to deactivate interactive plotting, but I wasn't sure how you'd like to handle that. Lemme know if you'd like any changes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/155
https://github.com/scverse/scanpy/issues/156:138,Deployability,continuous,continuous,138,"Hello! I'm trying to recolor some categorical variables in the scanpy.api.pl.tsne function but am having some trouble. Specifically, with continuous data, I'm fine using the `color_map` key word to change between scales like ""viridis"" and ""Purples"" but when trying to pass the `palette` key word for categorical data (sample labels, louvain lables), it doesn't seem to update the colors in the plot. Perhaps I'm specifying the color palette incorrectly? Here are a few versions I've tried:. ```; sc.pl.tsne(adata, ; color=['louvain'], ; #palette=['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9'], ; #palette=['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'],; #palette=""Set3"",; palette=sns.color_palette(""hls"", 15),; legend_fontsize=""20""); ```; ![image](https://user-images.githubusercontent.com/33738960/40148279-0b344502-5922-11e8-998c-4d5ece963253.png). But all of these attempts result in the same default cluster color scheme. Any suggestions?. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:369,Deployability,update,update,369,"Hello! I'm trying to recolor some categorical variables in the scanpy.api.pl.tsne function but am having some trouble. Specifically, with continuous data, I'm fine using the `color_map` key word to change between scales like ""viridis"" and ""Purples"" but when trying to pass the `palette` key word for categorical data (sample labels, louvain lables), it doesn't seem to update the colors in the plot. Perhaps I'm specifying the color palette incorrectly? Here are a few versions I've tried:. ```; sc.pl.tsne(adata, ; color=['louvain'], ; #palette=['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9'], ; #palette=['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'],; #palette=""Set3"",; palette=sns.color_palette(""hls"", 15),; legend_fontsize=""20""); ```; ![image](https://user-images.githubusercontent.com/33738960/40148279-0b344502-5922-11e8-998c-4d5ece963253.png). But all of these attempts result in the same default cluster color scheme. Any suggestions?. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:651,Energy Efficiency,green,green,651,"Hello! I'm trying to recolor some categorical variables in the scanpy.api.pl.tsne function but am having some trouble. Specifically, with continuous data, I'm fine using the `color_map` key word to change between scales like ""viridis"" and ""Purples"" but when trying to pass the `palette` key word for categorical data (sample labels, louvain lables), it doesn't seem to update the colors in the plot. Perhaps I'm specifying the color palette incorrectly? Here are a few versions I've tried:. ```; sc.pl.tsne(adata, ; color=['louvain'], ; #palette=['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9'], ; #palette=['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'],; #palette=""Set3"",; palette=sns.color_palette(""hls"", 15),; legend_fontsize=""20""); ```; ![image](https://user-images.githubusercontent.com/33738960/40148279-0b344502-5922-11e8-998c-4d5ece963253.png). But all of these attempts result in the same default cluster color scheme. Any suggestions?. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/issues/156:46,Modifiability,variab,variables,46,"Hello! I'm trying to recolor some categorical variables in the scanpy.api.pl.tsne function but am having some trouble. Specifically, with continuous data, I'm fine using the `color_map` key word to change between scales like ""viridis"" and ""Purples"" but when trying to pass the `palette` key word for categorical data (sample labels, louvain lables), it doesn't seem to update the colors in the plot. Perhaps I'm specifying the color palette incorrectly? Here are a few versions I've tried:. ```; sc.pl.tsne(adata, ; color=['louvain'], ; #palette=['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9'], ; #palette=['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'],; #palette=""Set3"",; palette=sns.color_palette(""hls"", 15),; legend_fontsize=""20""); ```; ![image](https://user-images.githubusercontent.com/33738960/40148279-0b344502-5922-11e8-998c-4d5ece963253.png). But all of these attempts result in the same default cluster color scheme. Any suggestions?. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156
https://github.com/scverse/scanpy/pull/157:2,Deployability,update,updated,2,I updated the export function `sc.export_to.spring_project` to work with our new version of SPRING (currently at https://github.com/allonkleinlab/spring_dev).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/157
https://github.com/scverse/scanpy/issues/158:85,Availability,error,error,85,"Hi, I'm trying scanpy for the first time, but following the first tutorial I got the error below.; Any idea of what is happening?. ```pytb; >>> filter_result = sc.pp.filter_genes_dispersion(; ... adata.X, min_mean=0.0125, max_mean=3, min_disp=0.5); Traceback (most recent call last):; File ""<stdin>"", line 2, in <module>; File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/scanpy/preprocessing/simple.py"", line 328, in filter_genes_dispersion; disp_std_bin[one_gene_per_bin] = disp_mean_bin[one_gene_per_bin]; File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/series.py"", line 938, in __setitem__; setitem(key, value); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/series.py"", line 929, in setitem; self._where(~key, value, inplace=True); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/generic.py"", line 7539, in _where; level=level, fill_value=np.nan); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/series.py"", line 3248, in align; broadcast_axis=broadcast_axis); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/generic.py"", line 7366, in align; fill_axis=fill_axis); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/generic.py"", line 7435, in _align_series; return_indexers=True); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3772, in join; return_indexers=return_indexers); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 4012, in _join_monotonic; ridx = self._left_indexer_unique(sv, ov); File ""pandas/_libs/join_helper.pxi"", line 774, in pandas._libs.join.left_join_indexer_unique_object; ValueError: Buffer dtype mismatch, expected 'Python object' but got 'signed char'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/158:403,Usability,simpl,simple,403,"Hi, I'm trying scanpy for the first time, but following the first tutorial I got the error below.; Any idea of what is happening?. ```pytb; >>> filter_result = sc.pp.filter_genes_dispersion(; ... adata.X, min_mean=0.0125, max_mean=3, min_disp=0.5); Traceback (most recent call last):; File ""<stdin>"", line 2, in <module>; File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/scanpy/preprocessing/simple.py"", line 328, in filter_genes_dispersion; disp_std_bin[one_gene_per_bin] = disp_mean_bin[one_gene_per_bin]; File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/series.py"", line 938, in __setitem__; setitem(key, value); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/series.py"", line 929, in setitem; self._where(~key, value, inplace=True); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/generic.py"", line 7539, in _where; level=level, fill_value=np.nan); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/series.py"", line 3248, in align; broadcast_axis=broadcast_axis); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/generic.py"", line 7366, in align; fill_axis=fill_axis); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/generic.py"", line 7435, in _align_series; return_indexers=True); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3772, in join; return_indexers=return_indexers); File ""/opt/anaconda3/envs/tiget/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 4012, in _join_monotonic; ridx = self._left_indexer_unique(sv, ov); File ""pandas/_libs/join_helper.pxi"", line 774, in pandas._libs.join.left_join_indexer_unique_object; ValueError: Buffer dtype mismatch, expected 'Python object' but got 'signed char'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158
https://github.com/scverse/scanpy/issues/159:18,Testability,log,log,18,"Does scanpy store log fold change, p-value and marker type flag (negative vs positive) somewhere? It looked at scObj.uns[""rank_genes_groups""] but found only 3 fields: params, scores and names.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159
https://github.com/scverse/scanpy/issues/160:225,Availability,error,error,225,"I have followed these instructions to install scanpy into my miniconda environment:; [](https://scanpy.readthedocs.io/en/latest/installation.html). Whether I try to import scanpy straight from terminal, it keeps giving me an error:; ```; Python 3.5.5 |Anaconda, Inc.| (default, Mar 12 2018, 23:12:44); [GCC 7.2.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy.api as sc; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/scanpy/__init__.py"", line 24, in <module>; import anndata; File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/__init__.py"", line 1, in <module>; from .base import AnnData, _MAIN_NARRATIVE; File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/base.py"", line 287; return f'Backing file manager of file {self._filename}.'; ^; SyntaxError: invalid syntax; >>>; ```. I also get the error when I try to use it with jupyter notebook:. ```; import scanpy.api as sc. Traceback (most recent call last):. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns). File ""<ipython-input-1-4a13c503728c>"", line 1, in <module>; import scanpy.api as sc. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/scanpy/__init__.py"", line 24, in <module>; import anndata. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/__init__.py"", line 1, in <module>; from .base import AnnData, _MAIN_NARRATIVE. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/base.py"", line 287; return f'Backing file manager of file {self._filename}.'; ^; SyntaxError: invalid syntax; ```. What is the issue? How can I get over this? . Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:1027,Availability,error,error,1027,"I have followed these instructions to install scanpy into my miniconda environment:; [](https://scanpy.readthedocs.io/en/latest/installation.html). Whether I try to import scanpy straight from terminal, it keeps giving me an error:; ```; Python 3.5.5 |Anaconda, Inc.| (default, Mar 12 2018, 23:12:44); [GCC 7.2.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy.api as sc; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/scanpy/__init__.py"", line 24, in <module>; import anndata; File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/__init__.py"", line 1, in <module>; from .base import AnnData, _MAIN_NARRATIVE; File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/base.py"", line 287; return f'Backing file manager of file {self._filename}.'; ^; SyntaxError: invalid syntax; >>>; ```. I also get the error when I try to use it with jupyter notebook:. ```; import scanpy.api as sc. Traceback (most recent call last):. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns). File ""<ipython-input-1-4a13c503728c>"", line 1, in <module>; import scanpy.api as sc. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/scanpy/__init__.py"", line 24, in <module>; import anndata. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/__init__.py"", line 1, in <module>; from .base import AnnData, _MAIN_NARRATIVE. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/base.py"", line 287; return f'Backing file manager of file {self._filename}.'; ^; SyntaxError: invalid syntax; ```. What is the issue? How can I get over this? . Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:38,Deployability,install,install,38,"I have followed these instructions to install scanpy into my miniconda environment:; [](https://scanpy.readthedocs.io/en/latest/installation.html). Whether I try to import scanpy straight from terminal, it keeps giving me an error:; ```; Python 3.5.5 |Anaconda, Inc.| (default, Mar 12 2018, 23:12:44); [GCC 7.2.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy.api as sc; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/scanpy/__init__.py"", line 24, in <module>; import anndata; File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/__init__.py"", line 1, in <module>; from .base import AnnData, _MAIN_NARRATIVE; File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/base.py"", line 287; return f'Backing file manager of file {self._filename}.'; ^; SyntaxError: invalid syntax; >>>; ```. I also get the error when I try to use it with jupyter notebook:. ```; import scanpy.api as sc. Traceback (most recent call last):. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns). File ""<ipython-input-1-4a13c503728c>"", line 1, in <module>; import scanpy.api as sc. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/scanpy/__init__.py"", line 24, in <module>; import anndata. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/__init__.py"", line 1, in <module>; from .base import AnnData, _MAIN_NARRATIVE. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/base.py"", line 287; return f'Backing file manager of file {self._filename}.'; ^; SyntaxError: invalid syntax; ```. What is the issue? How can I get over this? . Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/issues/160:128,Deployability,install,installation,128,"I have followed these instructions to install scanpy into my miniconda environment:; [](https://scanpy.readthedocs.io/en/latest/installation.html). Whether I try to import scanpy straight from terminal, it keeps giving me an error:; ```; Python 3.5.5 |Anaconda, Inc.| (default, Mar 12 2018, 23:12:44); [GCC 7.2.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy.api as sc; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/scanpy/__init__.py"", line 24, in <module>; import anndata; File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/__init__.py"", line 1, in <module>; from .base import AnnData, _MAIN_NARRATIVE; File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/base.py"", line 287; return f'Backing file manager of file {self._filename}.'; ^; SyntaxError: invalid syntax; >>>; ```. I also get the error when I try to use it with jupyter notebook:. ```; import scanpy.api as sc. Traceback (most recent call last):. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns). File ""<ipython-input-1-4a13c503728c>"", line 1, in <module>; import scanpy.api as sc. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/scanpy/__init__.py"", line 24, in <module>; import anndata. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/__init__.py"", line 1, in <module>; from .base import AnnData, _MAIN_NARRATIVE. File ""/home/unix/tamarao/miniconda3/envs/tamara_env/lib/python3.5/site-packages/anndata/base.py"", line 287; return f'Backing file manager of file {self._filename}.'; ^; SyntaxError: invalid syntax; ```. What is the issue? How can I get over this? . Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160
https://github.com/scverse/scanpy/pull/161:19,Testability,test,test,19,I expect unrelated test fails; see #162,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/161
https://github.com/scverse/scanpy/issues/162:201,Deployability,install,installing,201,"The tests started failing with 4499d9460b8fe6e08fffb3c4cae0948849d40586, which obviously changed nothing that could make tests fail. So I’m sure some dependency updating is responsible (Travis started installing the newwer versions and the tests fail). We need to investigate why that is and how to fix it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/162:150,Integrability,depend,dependency,150,"The tests started failing with 4499d9460b8fe6e08fffb3c4cae0948849d40586, which obviously changed nothing that could make tests fail. So I’m sure some dependency updating is responsible (Travis started installing the newwer versions and the tests fail). We need to investigate why that is and how to fix it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/162:4,Testability,test,tests,4,"The tests started failing with 4499d9460b8fe6e08fffb3c4cae0948849d40586, which obviously changed nothing that could make tests fail. So I’m sure some dependency updating is responsible (Travis started installing the newwer versions and the tests fail). We need to investigate why that is and how to fix it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/162:121,Testability,test,tests,121,"The tests started failing with 4499d9460b8fe6e08fffb3c4cae0948849d40586, which obviously changed nothing that could make tests fail. So I’m sure some dependency updating is responsible (Travis started installing the newwer versions and the tests fail). We need to investigate why that is and how to fix it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/162:240,Testability,test,tests,240,"The tests started failing with 4499d9460b8fe6e08fffb3c4cae0948849d40586, which obviously changed nothing that could make tests fail. So I’m sure some dependency updating is responsible (Travis started installing the newwer versions and the tests fail). We need to investigate why that is and how to fix it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162
https://github.com/scverse/scanpy/issues/163:425,Usability,simpl,simple,425,"I did a time comparison on the scanpy function to compute mean and variance and the sklearn function that does the same. Here some results with a very large matrix (84.7 million values,):. ```python; adata.X; ```; > <8847360x58051 sparse matrix of type '<class 'numpy.float32'>'; > 	with 84702299 stored elements in Compressed Sparse Row format>; > . **Using current implementation**:; ```python; import scanpy.preprocessing.simple as simple; %timeit simple._get_mean_var(adata.X); ```; 2.3 s ± 114 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). **Using sklearn sparse functions**:; ```python; import sklearn.utils.sparsefuncs as sparsefuncs; def unbiased_estimator(X):; mean, var =sparsefuncs.mean_variance_axis(X, 0); # enforce R convention (unbiased estimator) for variance; var *= (X.shape[0]/(X.shape[0]-1)); return mean, var; %timeit unbiased_estimator(adata.X); ```; > 141 ms ± 6.95 ms per loop (mean ± std. dev. of 7 runs, 10 loops each); >. The results returned by both methods are different only in the precision:. ```python; # the variance of both methods; simple._get_mean_var(adata.X)[1], unbiased_estimator(adata.X)[1]; ```; > (array([0.0073651 , 0.00061313, 0.005346 , ..., 0.05514137, 0.01749513,; > 0.05664281], dtype=float32),; > array([0.0073651 , 0.00061313, 0.00534599, ..., 0.05514137, 0.01749513,; > 0.05664282], dtype=float32)); > . ```python; np.allclose(simple._get_mean_var(adata.X)[1], unbiased_estimator(adata.X)[1], rtol=1e-4,); ```; > True. This comparison is for the variance, but for the mean, the same results are obtained. I suggest to replace the `_get_mean_var` function for the sklearn function. **Note** the sklearn function only works with a sparse matrix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:435,Usability,simpl,simple,435,"I did a time comparison on the scanpy function to compute mean and variance and the sklearn function that does the same. Here some results with a very large matrix (84.7 million values,):. ```python; adata.X; ```; > <8847360x58051 sparse matrix of type '<class 'numpy.float32'>'; > 	with 84702299 stored elements in Compressed Sparse Row format>; > . **Using current implementation**:; ```python; import scanpy.preprocessing.simple as simple; %timeit simple._get_mean_var(adata.X); ```; 2.3 s ± 114 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). **Using sklearn sparse functions**:; ```python; import sklearn.utils.sparsefuncs as sparsefuncs; def unbiased_estimator(X):; mean, var =sparsefuncs.mean_variance_axis(X, 0); # enforce R convention (unbiased estimator) for variance; var *= (X.shape[0]/(X.shape[0]-1)); return mean, var; %timeit unbiased_estimator(adata.X); ```; > 141 ms ± 6.95 ms per loop (mean ± std. dev. of 7 runs, 10 loops each); >. The results returned by both methods are different only in the precision:. ```python; # the variance of both methods; simple._get_mean_var(adata.X)[1], unbiased_estimator(adata.X)[1]; ```; > (array([0.0073651 , 0.00061313, 0.005346 , ..., 0.05514137, 0.01749513,; > 0.05664281], dtype=float32),; > array([0.0073651 , 0.00061313, 0.00534599, ..., 0.05514137, 0.01749513,; > 0.05664282], dtype=float32)); > . ```python; np.allclose(simple._get_mean_var(adata.X)[1], unbiased_estimator(adata.X)[1], rtol=1e-4,); ```; > True. This comparison is for the variance, but for the mean, the same results are obtained. I suggest to replace the `_get_mean_var` function for the sklearn function. **Note** the sklearn function only works with a sparse matrix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:451,Usability,simpl,simple,451,"I did a time comparison on the scanpy function to compute mean and variance and the sklearn function that does the same. Here some results with a very large matrix (84.7 million values,):. ```python; adata.X; ```; > <8847360x58051 sparse matrix of type '<class 'numpy.float32'>'; > 	with 84702299 stored elements in Compressed Sparse Row format>; > . **Using current implementation**:; ```python; import scanpy.preprocessing.simple as simple; %timeit simple._get_mean_var(adata.X); ```; 2.3 s ± 114 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). **Using sklearn sparse functions**:; ```python; import sklearn.utils.sparsefuncs as sparsefuncs; def unbiased_estimator(X):; mean, var =sparsefuncs.mean_variance_axis(X, 0); # enforce R convention (unbiased estimator) for variance; var *= (X.shape[0]/(X.shape[0]-1)); return mean, var; %timeit unbiased_estimator(adata.X); ```; > 141 ms ± 6.95 ms per loop (mean ± std. dev. of 7 runs, 10 loops each); >. The results returned by both methods are different only in the precision:. ```python; # the variance of both methods; simple._get_mean_var(adata.X)[1], unbiased_estimator(adata.X)[1]; ```; > (array([0.0073651 , 0.00061313, 0.005346 , ..., 0.05514137, 0.01749513,; > 0.05664281], dtype=float32),; > array([0.0073651 , 0.00061313, 0.00534599, ..., 0.05514137, 0.01749513,; > 0.05664282], dtype=float32)); > . ```python; np.allclose(simple._get_mean_var(adata.X)[1], unbiased_estimator(adata.X)[1], rtol=1e-4,); ```; > True. This comparison is for the variance, but for the mean, the same results are obtained. I suggest to replace the `_get_mean_var` function for the sklearn function. **Note** the sklearn function only works with a sparse matrix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:1076,Usability,simpl,simple,1076,"I did a time comparison on the scanpy function to compute mean and variance and the sklearn function that does the same. Here some results with a very large matrix (84.7 million values,):. ```python; adata.X; ```; > <8847360x58051 sparse matrix of type '<class 'numpy.float32'>'; > 	with 84702299 stored elements in Compressed Sparse Row format>; > . **Using current implementation**:; ```python; import scanpy.preprocessing.simple as simple; %timeit simple._get_mean_var(adata.X); ```; 2.3 s ± 114 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). **Using sklearn sparse functions**:; ```python; import sklearn.utils.sparsefuncs as sparsefuncs; def unbiased_estimator(X):; mean, var =sparsefuncs.mean_variance_axis(X, 0); # enforce R convention (unbiased estimator) for variance; var *= (X.shape[0]/(X.shape[0]-1)); return mean, var; %timeit unbiased_estimator(adata.X); ```; > 141 ms ± 6.95 ms per loop (mean ± std. dev. of 7 runs, 10 loops each); >. The results returned by both methods are different only in the precision:. ```python; # the variance of both methods; simple._get_mean_var(adata.X)[1], unbiased_estimator(adata.X)[1]; ```; > (array([0.0073651 , 0.00061313, 0.005346 , ..., 0.05514137, 0.01749513,; > 0.05664281], dtype=float32),; > array([0.0073651 , 0.00061313, 0.00534599, ..., 0.05514137, 0.01749513,; > 0.05664282], dtype=float32)); > . ```python; np.allclose(simple._get_mean_var(adata.X)[1], unbiased_estimator(adata.X)[1], rtol=1e-4,); ```; > True. This comparison is for the variance, but for the mean, the same results are obtained. I suggest to replace the `_get_mean_var` function for the sklearn function. **Note** the sklearn function only works with a sparse matrix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/issues/163:1388,Usability,simpl,simple,1388,"I did a time comparison on the scanpy function to compute mean and variance and the sklearn function that does the same. Here some results with a very large matrix (84.7 million values,):. ```python; adata.X; ```; > <8847360x58051 sparse matrix of type '<class 'numpy.float32'>'; > 	with 84702299 stored elements in Compressed Sparse Row format>; > . **Using current implementation**:; ```python; import scanpy.preprocessing.simple as simple; %timeit simple._get_mean_var(adata.X); ```; 2.3 s ± 114 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). **Using sklearn sparse functions**:; ```python; import sklearn.utils.sparsefuncs as sparsefuncs; def unbiased_estimator(X):; mean, var =sparsefuncs.mean_variance_axis(X, 0); # enforce R convention (unbiased estimator) for variance; var *= (X.shape[0]/(X.shape[0]-1)); return mean, var; %timeit unbiased_estimator(adata.X); ```; > 141 ms ± 6.95 ms per loop (mean ± std. dev. of 7 runs, 10 loops each); >. The results returned by both methods are different only in the precision:. ```python; # the variance of both methods; simple._get_mean_var(adata.X)[1], unbiased_estimator(adata.X)[1]; ```; > (array([0.0073651 , 0.00061313, 0.005346 , ..., 0.05514137, 0.01749513,; > 0.05664281], dtype=float32),; > array([0.0073651 , 0.00061313, 0.00534599, ..., 0.05514137, 0.01749513,; > 0.05664282], dtype=float32)); > . ```python; np.allclose(simple._get_mean_var(adata.X)[1], unbiased_estimator(adata.X)[1], rtol=1e-4,); ```; > True. This comparison is for the variance, but for the mean, the same results are obtained. I suggest to replace the `_get_mean_var` function for the sklearn function. **Note** the sklearn function only works with a sparse matrix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163
https://github.com/scverse/scanpy/pull/164:374,Modifiability,variab,variables,374,"This PR adds multiprocessing to the pp.regress_out function. . Here some benchmarks:. ```python; import numpy as np; import pandas as pd; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. # create a matrix with 20.000 cells with 3000 genes; adata = AnnData(random(20000, 3000, density=0.6, format='csr')); ```; **Benchmark using ordinal variables**; ```python; # create a categorical column and run regress out using ; # the categorical column; adata.obs['batch'] = pd.Categorical(np.random.randint(1, 4, size=adata.X.shape[0])); %timeit res = sc.pp.regress_out(adata, keys='batch', n_jobs=20, copy=True); ```; > 8.44 s ± 292 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; # import previous version of the function (which I saved in the file simple_old.py); from scanpy.preprocessing.simple_old import regress_out_old; %timeit res_old = regress_out_old(adata, keys='batch', n_jobs=1, copy=True); ```; > 1min 5s ± 4.45 s per loop (mean ± std. dev. of 7 runs, 1 loop each). **Compare that the previous and the new output are the same**; ```python; np.array_equal(res.X, res_old.X); ```; > True. **Benchmark using ordinal variables**; ```python; adata.obs['percent_mito'] = np.random.rand(adata.X.shape[0]); adata.obs['n_counts'] = adata.X.sum(axis=1). %timeit res2 = sc.pp.regress_out(adata, keys=['n_counts', 'percent_mito'], n_jobs=32, copy=True); ```; > 4.99 s ± 501 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; %timeit res2_old = regress_out_old(adata, keys=['n_counts', 'percent_mito'], n_jobs=1, copy=True); ```; > 41.2 s ± 7.79 s per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; np.array_equal(res2.X, res2_old.X); ```; > True",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/pull/164:1175,Modifiability,variab,variables,1175,"This PR adds multiprocessing to the pp.regress_out function. . Here some benchmarks:. ```python; import numpy as np; import pandas as pd; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. # create a matrix with 20.000 cells with 3000 genes; adata = AnnData(random(20000, 3000, density=0.6, format='csr')); ```; **Benchmark using ordinal variables**; ```python; # create a categorical column and run regress out using ; # the categorical column; adata.obs['batch'] = pd.Categorical(np.random.randint(1, 4, size=adata.X.shape[0])); %timeit res = sc.pp.regress_out(adata, keys='batch', n_jobs=20, copy=True); ```; > 8.44 s ± 292 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; # import previous version of the function (which I saved in the file simple_old.py); from scanpy.preprocessing.simple_old import regress_out_old; %timeit res_old = regress_out_old(adata, keys='batch', n_jobs=1, copy=True); ```; > 1min 5s ± 4.45 s per loop (mean ± std. dev. of 7 runs, 1 loop each). **Compare that the previous and the new output are the same**; ```python; np.array_equal(res.X, res_old.X); ```; > True. **Benchmark using ordinal variables**; ```python; adata.obs['percent_mito'] = np.random.rand(adata.X.shape[0]); adata.obs['n_counts'] = adata.X.sum(axis=1). %timeit res2 = sc.pp.regress_out(adata, keys=['n_counts', 'percent_mito'], n_jobs=32, copy=True); ```; > 4.99 s ± 501 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; %timeit res2_old = regress_out_old(adata, keys=['n_counts', 'percent_mito'], n_jobs=1, copy=True); ```; > 41.2 s ± 7.79 s per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; np.array_equal(res2.X, res2_old.X); ```; > True",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/pull/164:73,Testability,benchmark,benchmarks,73,"This PR adds multiprocessing to the pp.regress_out function. . Here some benchmarks:. ```python; import numpy as np; import pandas as pd; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. # create a matrix with 20.000 cells with 3000 genes; adata = AnnData(random(20000, 3000, density=0.6, format='csr')); ```; **Benchmark using ordinal variables**; ```python; # create a categorical column and run regress out using ; # the categorical column; adata.obs['batch'] = pd.Categorical(np.random.randint(1, 4, size=adata.X.shape[0])); %timeit res = sc.pp.regress_out(adata, keys='batch', n_jobs=20, copy=True); ```; > 8.44 s ± 292 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; # import previous version of the function (which I saved in the file simple_old.py); from scanpy.preprocessing.simple_old import regress_out_old; %timeit res_old = regress_out_old(adata, keys='batch', n_jobs=1, copy=True); ```; > 1min 5s ± 4.45 s per loop (mean ± std. dev. of 7 runs, 1 loop each). **Compare that the previous and the new output are the same**; ```python; np.array_equal(res.X, res_old.X); ```; > True. **Benchmark using ordinal variables**; ```python; adata.obs['percent_mito'] = np.random.rand(adata.X.shape[0]); adata.obs['n_counts'] = adata.X.sum(axis=1). %timeit res2 = sc.pp.regress_out(adata, keys=['n_counts', 'percent_mito'], n_jobs=32, copy=True); ```; > 4.99 s ± 501 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; %timeit res2_old = regress_out_old(adata, keys=['n_counts', 'percent_mito'], n_jobs=1, copy=True); ```; > 41.2 s ± 7.79 s per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; np.array_equal(res2.X, res2_old.X); ```; > True",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/pull/164:350,Testability,Benchmark,Benchmark,350,"This PR adds multiprocessing to the pp.regress_out function. . Here some benchmarks:. ```python; import numpy as np; import pandas as pd; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. # create a matrix with 20.000 cells with 3000 genes; adata = AnnData(random(20000, 3000, density=0.6, format='csr')); ```; **Benchmark using ordinal variables**; ```python; # create a categorical column and run regress out using ; # the categorical column; adata.obs['batch'] = pd.Categorical(np.random.randint(1, 4, size=adata.X.shape[0])); %timeit res = sc.pp.regress_out(adata, keys='batch', n_jobs=20, copy=True); ```; > 8.44 s ± 292 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; # import previous version of the function (which I saved in the file simple_old.py); from scanpy.preprocessing.simple_old import regress_out_old; %timeit res_old = regress_out_old(adata, keys='batch', n_jobs=1, copy=True); ```; > 1min 5s ± 4.45 s per loop (mean ± std. dev. of 7 runs, 1 loop each). **Compare that the previous and the new output are the same**; ```python; np.array_equal(res.X, res_old.X); ```; > True. **Benchmark using ordinal variables**; ```python; adata.obs['percent_mito'] = np.random.rand(adata.X.shape[0]); adata.obs['n_counts'] = adata.X.sum(axis=1). %timeit res2 = sc.pp.regress_out(adata, keys=['n_counts', 'percent_mito'], n_jobs=32, copy=True); ```; > 4.99 s ± 501 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; %timeit res2_old = regress_out_old(adata, keys=['n_counts', 'percent_mito'], n_jobs=1, copy=True); ```; > 41.2 s ± 7.79 s per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; np.array_equal(res2.X, res2_old.X); ```; > True",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/pull/164:1151,Testability,Benchmark,Benchmark,1151,"This PR adds multiprocessing to the pp.regress_out function. . Here some benchmarks:. ```python; import numpy as np; import pandas as pd; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. # create a matrix with 20.000 cells with 3000 genes; adata = AnnData(random(20000, 3000, density=0.6, format='csr')); ```; **Benchmark using ordinal variables**; ```python; # create a categorical column and run regress out using ; # the categorical column; adata.obs['batch'] = pd.Categorical(np.random.randint(1, 4, size=adata.X.shape[0])); %timeit res = sc.pp.regress_out(adata, keys='batch', n_jobs=20, copy=True); ```; > 8.44 s ± 292 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; # import previous version of the function (which I saved in the file simple_old.py); from scanpy.preprocessing.simple_old import regress_out_old; %timeit res_old = regress_out_old(adata, keys='batch', n_jobs=1, copy=True); ```; > 1min 5s ± 4.45 s per loop (mean ± std. dev. of 7 runs, 1 loop each). **Compare that the previous and the new output are the same**; ```python; np.array_equal(res.X, res_old.X); ```; > True. **Benchmark using ordinal variables**; ```python; adata.obs['percent_mito'] = np.random.rand(adata.X.shape[0]); adata.obs['n_counts'] = adata.X.sum(axis=1). %timeit res2 = sc.pp.regress_out(adata, keys=['n_counts', 'percent_mito'], n_jobs=32, copy=True); ```; > 4.99 s ± 501 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; %timeit res2_old = regress_out_old(adata, keys=['n_counts', 'percent_mito'], n_jobs=1, copy=True); ```; > 41.2 s ± 7.79 s per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; np.array_equal(res2.X, res2_old.X); ```; > True",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164
https://github.com/scverse/scanpy/issues/165:494,Deployability,update,updated,494,"Hi, Alex, . Fantastic package! As a python guy, I have become such a huge fan of Scanpy. . I'm encountering an issue when trying to add further annotation to a subset, what I'm doing is basically:. ```; adata = sc.read(filename); adata.var_names = pd.read_csv('genes.tsv'); adata.obs_names = pd.read_csv('barcodes.tsv'). adata_subset = adata[ list_of_barcodes,:]; anno = pd.read_csv(filename_sample_annotation); adata_subset.obs = anno; ```. But unfortunately _**adata_subset.obs**_ didn't get updated (I checked the index of _**anno**_ and it's consistent with _**adata_subset.obs_names**_). . It works only if I add the annotation column by column (In my case there are too many columns so it won't be ideal), e.g. `adata_subset.obs['cell_groups'] = anno['cell_groups']`. Could you please help me figure it out? Many thanks!. Huidong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/165
https://github.com/scverse/scanpy/issues/166:29,Availability,error,error,29,"I get quite a strange scanpy error, which appears a bit stochastic... This is has happened for the first time in version 1.1. I am trying to get a scatter plot of a subsetted anndata object like this:; `p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac')`. When I do this the first time round, I get this error message about categorical variables from sanitize_anndata (none of which are actually used in the call). ```---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-66-fc1479c238f7> in <module>(); 9 plt.show(); 10 ; ---> 11 p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac'); 12 p5 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac'); 13 . ~/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 162 show=show,; 163 save=save,; --> 164 ax=ax); 165 ; 166 elif x in adata.var_keys() and y in adata.var_keys() and color not in adata.obs_keys():. ~/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 281 ax=None):; 282 """"""See docstring of scatter.""""""; --> 283 sanitize_anndata(adata); 284 if legend_loc not in VALID_LEGENDLOCS:; 285 raise ValueError(. ~/scanpy/scanpy/utils.py in sanitize_anndata(adata); 481 # backwards compat... remove this in the future; 482 def sanitize_anndata(adata):; --> 483 adata._sanitize(); 484 ; 485 . ~/anndata/anndata/base.py in _sanitize(self); 1284 if len(c.categories) < len(c):; 1285 df[key] = c; -> 1286 df[key].cat.categories = df[key].cat.categories.ast",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:351,Availability,error,error,351,"I get quite a strange scanpy error, which appears a bit stochastic... This is has happened for the first time in version 1.1. I am trying to get a scatter plot of a subsetted anndata object like this:; `p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac')`. When I do this the first time round, I get this error message about categorical variables from sanitize_anndata (none of which are actually used in the call). ```---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-66-fc1479c238f7> in <module>(); 9 plt.show(); 10 ; ---> 11 p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac'); 12 p5 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac'); 13 . ~/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 162 show=show,; 163 save=save,; --> 164 ax=ax); 165 ; 166 elif x in adata.var_keys() and y in adata.var_keys() and color not in adata.obs_keys():. ~/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 281 ax=None):; 282 """"""See docstring of scatter.""""""; --> 283 sanitize_anndata(adata); 284 if legend_loc not in VALID_LEGENDLOCS:; 285 raise ValueError(. ~/scanpy/scanpy/utils.py in sanitize_anndata(adata); 481 # backwards compat... remove this in the future; 482 def sanitize_anndata(adata):; --> 483 adata._sanitize(); 484 ; 485 . ~/anndata/anndata/base.py in _sanitize(self); 1284 if len(c.categories) < len(c):; 1285 df[key] = c; -> 1286 df[key].cat.categories = df[key].cat.categories.ast",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:357,Integrability,message,message,357,"I get quite a strange scanpy error, which appears a bit stochastic... This is has happened for the first time in version 1.1. I am trying to get a scatter plot of a subsetted anndata object like this:; `p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac')`. When I do this the first time round, I get this error message about categorical variables from sanitize_anndata (none of which are actually used in the call). ```---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-66-fc1479c238f7> in <module>(); 9 plt.show(); 10 ; ---> 11 p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac'); 12 p5 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac'); 13 . ~/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 162 show=show,; 163 save=save,; --> 164 ax=ax); 165 ; 166 elif x in adata.var_keys() and y in adata.var_keys() and color not in adata.obs_keys():. ~/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 281 ax=None):; 282 """"""See docstring of scatter.""""""; --> 283 sanitize_anndata(adata); 284 if legend_loc not in VALID_LEGENDLOCS:; 285 raise ValueError(. ~/scanpy/scanpy/utils.py in sanitize_anndata(adata); 481 # backwards compat... remove this in the future; 482 def sanitize_anndata(adata):; --> 483 adata._sanitize(); 484 ; 485 . ~/anndata/anndata/base.py in _sanitize(self); 1284 if len(c.categories) < len(c):; 1285 df[key] = c; -> 1286 df[key].cat.categories = df[key].cat.categories.ast",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:383,Modifiability,variab,variables,383,"I get quite a strange scanpy error, which appears a bit stochastic... This is has happened for the first time in version 1.1. I am trying to get a scatter plot of a subsetted anndata object like this:; `p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac')`. When I do this the first time round, I get this error message about categorical variables from sanitize_anndata (none of which are actually used in the call). ```---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-66-fc1479c238f7> in <module>(); 9 plt.show(); 10 ; ---> 11 p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac'); 12 p5 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac'); 13 . ~/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 162 show=show,; 163 save=save,; --> 164 ax=ax); 165 ; 166 elif x in adata.var_keys() and y in adata.var_keys() and color not in adata.obs_keys():. ~/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 281 ax=None):; 282 """"""See docstring of scatter.""""""; --> 283 sanitize_anndata(adata); 284 if legend_loc not in VALID_LEGENDLOCS:; 285 raise ValueError(. ~/scanpy/scanpy/utils.py in sanitize_anndata(adata); 481 # backwards compat... remove this in the future; 482 def sanitize_anndata(adata):; --> 483 adata._sanitize(); 484 ; 485 . ~/anndata/anndata/base.py in _sanitize(self); 1284 if len(c.categories) < len(c):; 1285 df[key] = c; -> 1286 df[key].cat.categories = df[key].cat.categories.ast",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:3337,Modifiability,variab,variable,3337,"se:; 3612 if name in self._info_axis:. ~/anaconda3/lib/python3.6/site-packages/pandas/core/accessor.py in __get__(self, instance, owner); 52 # this ensures that Series.str.<method> is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. T",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:3398,Modifiability,variab,variable,3398,"python3.6/site-packages/pandas/core/accessor.py in __get__(self, instance, owner); 52 # this ensures that Series.str.<method> is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole annd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:3490,Modifiability,variab,variables,3490," is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole anndata object every time as there is no longer a reason to pass a view of the object. You could then test if a view is being pas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:3522,Modifiability,variab,variables,3522," is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole anndata object every time as there is no longer a reason to pass a view of the object. You could then test if a view is being pas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:3661,Modifiability,variab,variables,3661,"e, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole anndata object every time as there is no longer a reason to pass a view of the object. You could then test if a view is being passed to sanitize anndata, and then say ""please don't pass subsetted anndata objects to plotting functions"" or something like that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:3471,Safety,detect,detects,3471," is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole anndata object every time as there is no longer a reason to pass a view of the object. You could then test if a view is being pas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:2415,Security,access,accessor,2415,"ontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 281 ax=None):; 282 """"""See docstring of scatter.""""""; --> 283 sanitize_anndata(adata); 284 if legend_loc not in VALID_LEGENDLOCS:; 285 raise ValueError(. ~/scanpy/scanpy/utils.py in sanitize_anndata(adata); 481 # backwards compat... remove this in the future; 482 def sanitize_anndata(adata):; --> 483 adata._sanitize(); 484 ; 485 . ~/anndata/anndata/base.py in _sanitize(self); 1284 if len(c.categories) < len(c):; 1285 df[key] = c; -> 1286 df[key].cat.categories = df[key].cat.categories.astype('U'); 1287 logg.info(; 1288 '... storing \'{}\' as categorical'. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __getattr__(self, name); 3608 if (name in self._internal_names_set or name in self._metadata or; 3609 name in self._accessors):; -> 3610 return object.__getattribute__(self, name); 3611 else:; 3612 if name in self._info_axis:. ~/anaconda3/lib/python3.6/site-packages/pandas/core/accessor.py in __get__(self, instance, owner); 52 # this ensures that Series.str.<method> is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:2875,Security,access,accessor,2875," len(c):; 1285 df[key] = c; -> 1286 df[key].cat.categories = df[key].cat.categories.astype('U'); 1287 logg.info(; 1288 '... storing \'{}\' as categorical'. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __getattr__(self, name); 3608 if (name in self._internal_names_set or name in self._metadata or; 3609 name in self._accessors):; -> 3610 return object.__getattribute__(self, name); 3611 else:; 3612 if name in self._info_axis:. ~/anaconda3/lib/python3.6/site-packages/pandas/core/accessor.py in __get__(self, instance, owner); 52 # this ensures that Series.str.<method> is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:3012,Security,access,accessor,3012,"288 '... storing \'{}\' as categorical'. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __getattr__(self, name); 3608 if (name in self._internal_names_set or name in self._metadata or; 3609 name in self._accessors):; -> 3610 return object.__getattribute__(self, name); 3611 else:; 3612 if name in self._info_axis:. ~/anaconda3/lib/python3.6/site-packages/pandas/core/accessor.py in __get__(self, instance, owner); 52 # this ensures that Series.str.<method> is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. I",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:4512,Security,sanitiz,sanitize,4512,"e, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole anndata object every time as there is no longer a reason to pass a view of the object. You could then test if a view is being passed to sanitize anndata, and then say ""please don't pass subsetted anndata objects to plotting functions"" or something like that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:2016,Testability,log,logg,2016,"_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 162 show=show,; 163 save=save,; --> 164 ax=ax); 165 ; 166 elif x in adata.var_keys() and y in adata.var_keys() and color not in adata.obs_keys():. ~/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 281 ax=None):; 282 """"""See docstring of scatter.""""""; --> 283 sanitize_anndata(adata); 284 if legend_loc not in VALID_LEGENDLOCS:; 285 raise ValueError(. ~/scanpy/scanpy/utils.py in sanitize_anndata(adata); 481 # backwards compat... remove this in the future; 482 def sanitize_anndata(adata):; --> 483 adata._sanitize(); 484 ; 485 . ~/anndata/anndata/base.py in _sanitize(self); 1284 if len(c.categories) < len(c):; 1285 df[key] = c; -> 1286 df[key].cat.categories = df[key].cat.categories.astype('U'); 1287 logg.info(; 1288 '... storing \'{}\' as categorical'. ~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __getattr__(self, name); 3608 if (name in self._internal_names_set or name in self._metadata or; 3609 name in self._accessors):; -> 3610 return object.__getattribute__(self, name); 3611 else:; 3612 if name in self._info_axis:. ~/anaconda3/lib/python3.6/site-packages/pandas/core/accessor.py in __get__(self, instance, owner); 52 # this ensures that Series.str.<method> is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .ca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/166:4478,Testability,test,test,4478,"e, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole anndata object every time as there is no longer a reason to pass a view of the object. You could then test if a view is being passed to sanitize anndata, and then say ""please don't pass subsetted anndata objects to plotting functions"" or something like that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166
https://github.com/scverse/scanpy/issues/167:164,Availability,error,error,164,"`anndata.readwrite.read.read_excel` is using pandas when fetching bundled datasets like `moignard15()`. However since xlrd is not bundles with pandas, it throws an error. See https://stackoverflow.com/questions/17063458/reading-an-excel-file-in-python-using-pandas#comment83338990_17063653. xlrd should be listed in the installation instructions.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/167
https://github.com/scverse/scanpy/issues/167:320,Deployability,install,installation,320,"`anndata.readwrite.read.read_excel` is using pandas when fetching bundled datasets like `moignard15()`. However since xlrd is not bundles with pandas, it throws an error. See https://stackoverflow.com/questions/17063458/reading-an-excel-file-in-python-using-pandas#comment83338990_17063653. xlrd should be listed in the installation instructions.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/167
https://github.com/scverse/scanpy/issues/168:8,Modifiability,extend,extend,8,"Can you extend scanpy functions so that I can show gene expression level on plot generated by sc.pl.diffmap? just like that monocle2 does. And, in which step should I execute MNN batch effect correction ? Is it still necessary to regress out some variables ( n_counts, percent_mito, cell cycle et al.,) when I execute MNN ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/168:247,Modifiability,variab,variables,247,"Can you extend scanpy functions so that I can show gene expression level on plot generated by sc.pl.diffmap? just like that monocle2 does. And, in which step should I execute MNN batch effect correction ? Is it still necessary to regress out some variables ( n_counts, percent_mito, cell cycle et al.,) when I execute MNN ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168
https://github.com/scverse/scanpy/issues/171:56,Modifiability,variab,variables,56,"When an anndata object is saved to h5ad the categorical variables in the .raw.var are casted to integers so that e.g. gene names are lost when loading the anndata object again. This functionality is especially unfortunate as anndata allows adata and adata.raw to have different sizes in the .var dimension. Thus, if anndata represents for example a highly variable gene set, where anndata.raw is the whole data set, then we can no longer visualize the expression of genes that were filtered out unless we call them by var_name and not by 'gene_name'. This bug is likely due to these lines:; https://github.com/theislab/anndata/blob/d9727cab88ba2100787e3e2ae0c6d72abd4d92b7/anndata/base.py#L1925-L1951. It would be good to add an uns_raw/ or raw_categories/ folder to the h5ad format which stores the categorical variables for raw.var.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/171
https://github.com/scverse/scanpy/issues/171:356,Modifiability,variab,variable,356,"When an anndata object is saved to h5ad the categorical variables in the .raw.var are casted to integers so that e.g. gene names are lost when loading the anndata object again. This functionality is especially unfortunate as anndata allows adata and adata.raw to have different sizes in the .var dimension. Thus, if anndata represents for example a highly variable gene set, where anndata.raw is the whole data set, then we can no longer visualize the expression of genes that were filtered out unless we call them by var_name and not by 'gene_name'. This bug is likely due to these lines:; https://github.com/theislab/anndata/blob/d9727cab88ba2100787e3e2ae0c6d72abd4d92b7/anndata/base.py#L1925-L1951. It would be good to add an uns_raw/ or raw_categories/ folder to the h5ad format which stores the categorical variables for raw.var.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/171
https://github.com/scverse/scanpy/issues/171:812,Modifiability,variab,variables,812,"When an anndata object is saved to h5ad the categorical variables in the .raw.var are casted to integers so that e.g. gene names are lost when loading the anndata object again. This functionality is especially unfortunate as anndata allows adata and adata.raw to have different sizes in the .var dimension. Thus, if anndata represents for example a highly variable gene set, where anndata.raw is the whole data set, then we can no longer visualize the expression of genes that were filtered out unless we call them by var_name and not by 'gene_name'. This bug is likely due to these lines:; https://github.com/theislab/anndata/blob/d9727cab88ba2100787e3e2ae0c6d72abd4d92b7/anndata/base.py#L1925-L1951. It would be good to add an uns_raw/ or raw_categories/ folder to the h5ad format which stores the categorical variables for raw.var.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/171
https://github.com/scverse/scanpy/issues/171:143,Performance,load,loading,143,"When an anndata object is saved to h5ad the categorical variables in the .raw.var are casted to integers so that e.g. gene names are lost when loading the anndata object again. This functionality is especially unfortunate as anndata allows adata and adata.raw to have different sizes in the .var dimension. Thus, if anndata represents for example a highly variable gene set, where anndata.raw is the whole data set, then we can no longer visualize the expression of genes that were filtered out unless we call them by var_name and not by 'gene_name'. This bug is likely due to these lines:; https://github.com/theislab/anndata/blob/d9727cab88ba2100787e3e2ae0c6d72abd4d92b7/anndata/base.py#L1925-L1951. It would be good to add an uns_raw/ or raw_categories/ folder to the h5ad format which stores the categorical variables for raw.var.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/171
https://github.com/scverse/scanpy/issues/172:30,Modifiability,variab,variable,30,"I am trying to get the highly variable genes for a data set. The data set was normalized by fitting a negative binomial model and using the residuals as expression levels. This gives mean gene expression values that can be negative and are very close to 0. When I use the command:; `disp_filter = sc.pp.filter_genes_dispersion(adata.X, min_mean=0, min_disp=0.5)`; I get very few differentially expressed genes. Looking at the dispersions via `disp_filter['dispersions']` shows that many dispersions appear to be NaN. And superficial inspection shows that the genes with negative means have NaN dispersions. This feels like it shouldn't be the case. It is possible to calculate the variance for the genes that have NaN dispersions. Are all negative dispersion values cast to NaN?. Changing the 'mean_mean' parameter to a negative value changes nothing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172
https://github.com/scverse/scanpy/issues/173:356,Safety,predict,predict,356,"Hi,. I tried the [`DoubletDetection`](https://github.com/JonathanShor/DoubletDetection) Python library on my data and got some interesting result. As it can be run directly on a numpy array of count matrix (`adata.X`), I thought it would be an interesting feature for `scanpy`. . ```; clf = doubletdetection.BoostClassifier() ; doublets = clf.fit(adata.X).predict(); adata.obs['doublet'] = pd.Categorical(doublets.astype(bool)); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173
https://github.com/scverse/scanpy/issues/174:66,Deployability,continuous,continuous,66,"Hi,. UMAP gives me nice embeddings when the cell types are mostly continuous. But I couldn't get it to work well on a dataset with more discrete cell types. When I use smaller min_dist to separate the clusters better, the clusters tend to become very tiny and distant. When I set larger min_dist to make the clusters occupy more space, the spatial separation between coarse clusters is gone. Adjusting other parameters such as n_neighbors, spread, gamma doesn't give me good results either. . This issue is somewhat similar to the first two UMAP plots in this notebook before doing special initialization: https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. `min_dist=0.2, n_neighbor=15`: Clear separation between coarse clusters, but clusters become tiny with too much white space. The clusters on the left seem to be repelling those on the right. ; <img src=https://user-images.githubusercontent.com/5046690/41402335-46cf92c4-6f7f-11e8-9b72-48d23d553356.png width=50%>. `min_dist=0.4`: The coarse-level clusters on the right are intermingled (the continuity between cluster 4 and 6 is now broken); <img src=https://user-images.githubusercontent.com/5046690/41402348-57054968-6f7f-11e8-8e24-696ad4afb243.png width=50%>. `min_dist=0.6`: Cells are distributed more smoothly, but the separation between coarse clusters are much less obvious. ; <img src=https://user-images.githubusercontent.com/5046690/41402394-7252b00c-6f7f-11e8-9ffb-2d09a03fc131.png width=50%>. t-SNE in this case gives me more evenly distributed clusters; <img src=https://user-images.githubusercontent.com/5046690/41403178-8257e722-6f81-11e8-9b64-be274e82eadc.png width=50%>. Is there a way to make the UMAP look more intuitive in this kind of situations? Thanks!. (I changed the original question as I feel that this one is more important.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:707,Usability,Clear,Clear,707,"Hi,. UMAP gives me nice embeddings when the cell types are mostly continuous. But I couldn't get it to work well on a dataset with more discrete cell types. When I use smaller min_dist to separate the clusters better, the clusters tend to become very tiny and distant. When I set larger min_dist to make the clusters occupy more space, the spatial separation between coarse clusters is gone. Adjusting other parameters such as n_neighbors, spread, gamma doesn't give me good results either. . This issue is somewhat similar to the first two UMAP plots in this notebook before doing special initialization: https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. `min_dist=0.2, n_neighbor=15`: Clear separation between coarse clusters, but clusters become tiny with too much white space. The clusters on the left seem to be repelling those on the right. ; <img src=https://user-images.githubusercontent.com/5046690/41402335-46cf92c4-6f7f-11e8-9b72-48d23d553356.png width=50%>. `min_dist=0.4`: The coarse-level clusters on the right are intermingled (the continuity between cluster 4 and 6 is now broken); <img src=https://user-images.githubusercontent.com/5046690/41402348-57054968-6f7f-11e8-8e24-696ad4afb243.png width=50%>. `min_dist=0.6`: Cells are distributed more smoothly, but the separation between coarse clusters are much less obvious. ; <img src=https://user-images.githubusercontent.com/5046690/41402394-7252b00c-6f7f-11e8-9ffb-2d09a03fc131.png width=50%>. t-SNE in this case gives me more evenly distributed clusters; <img src=https://user-images.githubusercontent.com/5046690/41403178-8257e722-6f81-11e8-9b64-be274e82eadc.png width=50%>. Is there a way to make the UMAP look more intuitive in this kind of situations? Thanks!. (I changed the original question as I feel that this one is more important.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/issues/174:1706,Usability,intuit,intuitive,1706,"Hi,. UMAP gives me nice embeddings when the cell types are mostly continuous. But I couldn't get it to work well on a dataset with more discrete cell types. When I use smaller min_dist to separate the clusters better, the clusters tend to become very tiny and distant. When I set larger min_dist to make the clusters occupy more space, the spatial separation between coarse clusters is gone. Adjusting other parameters such as n_neighbors, spread, gamma doesn't give me good results either. . This issue is somewhat similar to the first two UMAP plots in this notebook before doing special initialization: https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. `min_dist=0.2, n_neighbor=15`: Clear separation between coarse clusters, but clusters become tiny with too much white space. The clusters on the left seem to be repelling those on the right. ; <img src=https://user-images.githubusercontent.com/5046690/41402335-46cf92c4-6f7f-11e8-9b72-48d23d553356.png width=50%>. `min_dist=0.4`: The coarse-level clusters on the right are intermingled (the continuity between cluster 4 and 6 is now broken); <img src=https://user-images.githubusercontent.com/5046690/41402348-57054968-6f7f-11e8-8e24-696ad4afb243.png width=50%>. `min_dist=0.6`: Cells are distributed more smoothly, but the separation between coarse clusters are much less obvious. ; <img src=https://user-images.githubusercontent.com/5046690/41402394-7252b00c-6f7f-11e8-9ffb-2d09a03fc131.png width=50%>. t-SNE in this case gives me more evenly distributed clusters; <img src=https://user-images.githubusercontent.com/5046690/41403178-8257e722-6f81-11e8-9b64-be274e82eadc.png width=50%>. Is there a way to make the UMAP look more intuitive in this kind of situations? Thanks!. (I changed the original question as I feel that this one is more important.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174
https://github.com/scverse/scanpy/pull/175:406,Modifiability,variab,variables,406,"This PR adds extra functionality to `pl.violin` adding the option produce stacked violin plots for each `key` passed. The new optional boolean argument `stripplot` was added to add/remove the stripplot on top of the violin plots. An example image is:. ![image](https://user-images.githubusercontent.com/4964309/41411458-3e105336-6fdd-11e8-8e18-07e49fe7c8d1.png). Similarly, I added `pl.heatmap` that plots variables ordered by an observation as follows:. ![image](https://user-images.githubusercontent.com/4964309/41411511-6996f334-6fdd-11e8-93ff-176b89743d83.png). An example notebook using these visualizations is [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/175
https://github.com/scverse/scanpy/issues/177:1290,Availability,down,down,1290,"Hello!. I found an odd bug where computing `sc.pp.neighbors()` with `metric='jaccard'` results in random cluster assignments coming out of `sc.tl.louvain()`. Running with the `euclidean` distance metric yields appropriate cluster assignments from `sc.tl.louvain()`. . Reproduce (adata is a bone marrow data set, with ""ground truth"" cell type annotations in `adata.obs['cell_type']`: ; ```; sc.pl.tsne(adata, color='cell_type', title='Ground truth') # Color tsne plot by ground truth cell annotations; plt.show(); plt.clf(). sc.pp.neighbors(adata, metric='jaccard', random_state=2018) # compute neighbor graph with jaccard metric; sc.tl.louvain(adata,random_state=2018) # Then use the Louvain algorithm to identify clusters; sc.pl.tsne(adata, color='louvain', title='Louvain + jaccard metric'); plt.show(); plt.clf(). sc.pp.neighbors(adata,metric='euclidean', random_state=2018) # compute neighbor graph with euclidean distance metric; sc.tl.louvain(adata, random_state=2018) # Rerun cluster identification; sc.pl.tsne(adata, color='louvain', title='Louvain + default metric'); plt.show(); plt.clf(); ```. ![image](https://user-images.githubusercontent.com/12618847/41443622-ed8840ac-6ff2-11e8-9cd6-d853e017a8ab.png). Thanks! Let me know if you need another set of eyes in tracking this one down :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177
https://github.com/scverse/scanpy/issues/178:106,Modifiability,flexible,flexible,106,"In scanpy, clustermap uses all the clusters and genes by default to plot the heatmap, however, it is more flexible if users can use a certain clusters and marker genes they are interested in. Can scanpy perform this function, or anyone who can add some extensions to scanpy to achieve this goal?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/178:203,Performance,perform,perform,203,"In scanpy, clustermap uses all the clusters and genes by default to plot the heatmap, however, it is more flexible if users can use a certain clusters and marker genes they are interested in. Can scanpy perform this function, or anyone who can add some extensions to scanpy to achieve this goal?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178
https://github.com/scverse/scanpy/issues/182:573,Availability,fault,fault,573,"Hi,. I can't get the ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:2316,Deployability,release,releases,2316,"g telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata = AnnData(random(5000, 2000, density=0.6, format='csr')); adata.obs['percent_mito'] = np.random.rand(adata.X.shape[0]); adata.obs['n_counts'] = adata.X.sum(axis=1); multi = sc.pp.regress_out(adata, keys=['n_counts', 'percent_mito'], n_jobs=8, copy=True); ```. I've gotten the same results on master and the current releases.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:167,Energy Efficiency,monitor,monitor,167,"Hi,. I can't get the ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:254,Integrability,rout,routinely,254,"Hi,. I can't get the ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:372,Performance,queue,queue,372,"Hi,. I can't get the ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:924,Performance,multi-thread,multi-threaded,924,"Hi,. I can't get the ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:1027,Performance,queue,queue,1027,"he ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata = AnnData(random(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:40,Testability,test,test,40,"Hi,. I can't get the ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/182:323,Testability,log,log,323,"Hi,. I can't get the ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182
https://github.com/scverse/scanpy/issues/185:649,Performance,cache,cache,649,"Hi,; I really like using scanpy for our large scale single cell aging project. I am uploading several libraries from different age mice (8 time points) and concatinate them together (see example cmd I used below). I would like to control the color display so that I have a gradient color that changes with age and visualise in umap plot for example sc.pl.umap(adata, color= ['Age','louvain']). How could I do that? Thank you very for your help and for this wonderful package. path = '/home/10XG6_ILC2_thirdrun/cellranger_count_results/13_mouse_IL33_8w_X/outs/filtered_gene_bc_matrices/mm10/'; adata_13_mouse_IL33_8w_X = sc.read(path + 'matrix.mtx', cache=True).transpose(); adata_13_mouse_IL33_8w_X.var_names = np.genfromtxt(path + 'genes.tsv', dtype=str)[:, 1]; adata_13_mouse_IL33_8w_X.obs_names = np.genfromtxt(path + 'barcodes.tsv', dtype=str); adata_13_mouse_IL33_8w_X.obs['Tissue'] = 'X'; adata_13_mouse_IL33_8w_X.obs['Age'] = '8Weeks'; adata_13_mouse_IL33_8w_X.obs['State'] = 'IL33_activated'; sc.pp.filter_cells(adata_13_mouse_IL33_8w_X, min_genes=250). path = '/home/10XG6_ILC2_thirdrun/cellranger_count_results/1_mouse_IL33_16w_X/outs/filtered_gene_bc_matrices/mm10/'; adata_1_mouse_IL33_16w_X = sc.read(path + 'matrix.mtx', cache=True).transpose(); adata_1_mouse_IL33_16w_X.var_names = np.genfromtxt(path + 'genes.tsv', dtype=str)[:, 1]; adata_1_mouse_IL33_16w_X.obs_names = np.genfromtxt(path + 'barcodes.tsv', dtype=str); adata_1_mouse_IL33_16w_X.obs['Tissue'] = 'X'; adata_1_mouse_IL33_16w_X.obs['Age'] = '16Weeks'; adata_1_mouse_IL33_16w_X.obs['State'] = 'IL33_activated'; sc.pp.filter_cells(adata_1_mouse_IL33_16w_X, min_genes=250). etc............ adata = adata_13_mouse_IL33_8w_X.concatenate([adata_1_mouse_IL33_16w_X, etc ....)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/185
https://github.com/scverse/scanpy/issues/185:1235,Performance,cache,cache,1235,"Hi,; I really like using scanpy for our large scale single cell aging project. I am uploading several libraries from different age mice (8 time points) and concatinate them together (see example cmd I used below). I would like to control the color display so that I have a gradient color that changes with age and visualise in umap plot for example sc.pl.umap(adata, color= ['Age','louvain']). How could I do that? Thank you very for your help and for this wonderful package. path = '/home/10XG6_ILC2_thirdrun/cellranger_count_results/13_mouse_IL33_8w_X/outs/filtered_gene_bc_matrices/mm10/'; adata_13_mouse_IL33_8w_X = sc.read(path + 'matrix.mtx', cache=True).transpose(); adata_13_mouse_IL33_8w_X.var_names = np.genfromtxt(path + 'genes.tsv', dtype=str)[:, 1]; adata_13_mouse_IL33_8w_X.obs_names = np.genfromtxt(path + 'barcodes.tsv', dtype=str); adata_13_mouse_IL33_8w_X.obs['Tissue'] = 'X'; adata_13_mouse_IL33_8w_X.obs['Age'] = '8Weeks'; adata_13_mouse_IL33_8w_X.obs['State'] = 'IL33_activated'; sc.pp.filter_cells(adata_13_mouse_IL33_8w_X, min_genes=250). path = '/home/10XG6_ILC2_thirdrun/cellranger_count_results/1_mouse_IL33_16w_X/outs/filtered_gene_bc_matrices/mm10/'; adata_1_mouse_IL33_16w_X = sc.read(path + 'matrix.mtx', cache=True).transpose(); adata_1_mouse_IL33_16w_X.var_names = np.genfromtxt(path + 'genes.tsv', dtype=str)[:, 1]; adata_1_mouse_IL33_16w_X.obs_names = np.genfromtxt(path + 'barcodes.tsv', dtype=str); adata_1_mouse_IL33_16w_X.obs['Tissue'] = 'X'; adata_1_mouse_IL33_16w_X.obs['Age'] = '16Weeks'; adata_1_mouse_IL33_16w_X.obs['State'] = 'IL33_activated'; sc.pp.filter_cells(adata_1_mouse_IL33_16w_X, min_genes=250). etc............ adata = adata_13_mouse_IL33_8w_X.concatenate([adata_1_mouse_IL33_16w_X, etc ....)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/185
https://github.com/scverse/scanpy/pull/186:10,Deployability,integrat,integration,10,"Added DCA integration, as discussed in #142 . For denoising, uses can call:. `sc.pp.dca(adata)`. which replaces adata.X inplace. For latent representations:. `sc.pp.dca(adata, mode='latent')`. can be called, which adds 'X_dca' to adata.obsm. Fixes #142 . This integration uses new DCA API (>= 0.2.1). All DCA API arguments are exposed and usable in scanpy integration.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186
https://github.com/scverse/scanpy/pull/186:260,Deployability,integrat,integration,260,"Added DCA integration, as discussed in #142 . For denoising, uses can call:. `sc.pp.dca(adata)`. which replaces adata.X inplace. For latent representations:. `sc.pp.dca(adata, mode='latent')`. can be called, which adds 'X_dca' to adata.obsm. Fixes #142 . This integration uses new DCA API (>= 0.2.1). All DCA API arguments are exposed and usable in scanpy integration.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186
https://github.com/scverse/scanpy/pull/186:356,Deployability,integrat,integration,356,"Added DCA integration, as discussed in #142 . For denoising, uses can call:. `sc.pp.dca(adata)`. which replaces adata.X inplace. For latent representations:. `sc.pp.dca(adata, mode='latent')`. can be called, which adds 'X_dca' to adata.obsm. Fixes #142 . This integration uses new DCA API (>= 0.2.1). All DCA API arguments are exposed and usable in scanpy integration.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186
https://github.com/scverse/scanpy/pull/186:10,Integrability,integrat,integration,10,"Added DCA integration, as discussed in #142 . For denoising, uses can call:. `sc.pp.dca(adata)`. which replaces adata.X inplace. For latent representations:. `sc.pp.dca(adata, mode='latent')`. can be called, which adds 'X_dca' to adata.obsm. Fixes #142 . This integration uses new DCA API (>= 0.2.1). All DCA API arguments are exposed and usable in scanpy integration.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186
https://github.com/scverse/scanpy/pull/186:260,Integrability,integrat,integration,260,"Added DCA integration, as discussed in #142 . For denoising, uses can call:. `sc.pp.dca(adata)`. which replaces adata.X inplace. For latent representations:. `sc.pp.dca(adata, mode='latent')`. can be called, which adds 'X_dca' to adata.obsm. Fixes #142 . This integration uses new DCA API (>= 0.2.1). All DCA API arguments are exposed and usable in scanpy integration.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186
https://github.com/scverse/scanpy/pull/186:356,Integrability,integrat,integration,356,"Added DCA integration, as discussed in #142 . For denoising, uses can call:. `sc.pp.dca(adata)`. which replaces adata.X inplace. For latent representations:. `sc.pp.dca(adata, mode='latent')`. can be called, which adds 'X_dca' to adata.obsm. Fixes #142 . This integration uses new DCA API (>= 0.2.1). All DCA API arguments are exposed and usable in scanpy integration.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186
https://github.com/scverse/scanpy/pull/186:327,Security,expose,exposed,327,"Added DCA integration, as discussed in #142 . For denoising, uses can call:. `sc.pp.dca(adata)`. which replaces adata.X inplace. For latent representations:. `sc.pp.dca(adata, mode='latent')`. can be called, which adds 'X_dca' to adata.obsm. Fixes #142 . This integration uses new DCA API (>= 0.2.1). All DCA API arguments are exposed and usable in scanpy integration.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186
https://github.com/scverse/scanpy/pull/186:339,Usability,usab,usable,339,"Added DCA integration, as discussed in #142 . For denoising, uses can call:. `sc.pp.dca(adata)`. which replaces adata.X inplace. For latent representations:. `sc.pp.dca(adata, mode='latent')`. can be called, which adds 'X_dca' to adata.obsm. Fixes #142 . This integration uses new DCA API (>= 0.2.1). All DCA API arguments are exposed and usable in scanpy integration.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186
https://github.com/scverse/scanpy/issues/188:24,Modifiability,variab,variable,24,"Hi,; Scanpy detect high variable genes with normalized (but not logarithmized) data (refer to Clustering 3k PBMCs following a Seurat Tutorial), while Seurat do this by first normalize the raw data, then logarithmize the data and finally detect high variable genes, which one is better ? or both of them works well ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:249,Modifiability,variab,variable,249,"Hi,; Scanpy detect high variable genes with normalized (but not logarithmized) data (refer to Clustering 3k PBMCs following a Seurat Tutorial), while Seurat do this by first normalize the raw data, then logarithmize the data and finally detect high variable genes, which one is better ? or both of them works well ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:12,Safety,detect,detect,12,"Hi,; Scanpy detect high variable genes with normalized (but not logarithmized) data (refer to Clustering 3k PBMCs following a Seurat Tutorial), while Seurat do this by first normalize the raw data, then logarithmize the data and finally detect high variable genes, which one is better ? or both of them works well ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:237,Safety,detect,detect,237,"Hi,; Scanpy detect high variable genes with normalized (but not logarithmized) data (refer to Clustering 3k PBMCs following a Seurat Tutorial), while Seurat do this by first normalize the raw data, then logarithmize the data and finally detect high variable genes, which one is better ? or both of them works well ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:64,Testability,log,logarithmized,64,"Hi,; Scanpy detect high variable genes with normalized (but not logarithmized) data (refer to Clustering 3k PBMCs following a Seurat Tutorial), while Seurat do this by first normalize the raw data, then logarithmize the data and finally detect high variable genes, which one is better ? or both of them works well ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/188:203,Testability,log,logarithmize,203,"Hi,; Scanpy detect high variable genes with normalized (but not logarithmized) data (refer to Clustering 3k PBMCs following a Seurat Tutorial), while Seurat do this by first normalize the raw data, then logarithmize the data and finally detect high variable genes, which one is better ? or both of them works well ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188
https://github.com/scverse/scanpy/issues/189:211,Availability,avail,available,211,"@falexwolf, @flying-sheep . From the discussion on #45, I think some more discussion should be had as to what imputation methods are to be included in scanpy. Validation of and comparisons between the currently available imputation methods are both severely lacking---I only know of [1][2][3][4][5], none of which include comprehensive benchmarks, and the updated MAGIC (#187) article at Cell doesn't include relevant comparisons between current methods. . I'd be very interested in hearing/having an open discussion about the motivation, benefits, and limitations of the various imputation methods available. [1]: Zhang and Zhang, 2017. https://www.biorxiv.org/content/early/2017/12/31/241190; [2]: Lopez et al. 2018, https://www.biorxiv.org/content/early/2018/03/30/292037; [3]: Li and Li, 2018. https://www.nature.com/articles/s41467-018-03405-7; [4]: Eraslan et al. 2018. https://www.biorxiv.org/content/early/2018/04/13/300681; [5]: Huang et al. 2018. https://www.biorxiv.org/content/early/2018/03/08/138677",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:599,Availability,avail,available,599,"@falexwolf, @flying-sheep . From the discussion on #45, I think some more discussion should be had as to what imputation methods are to be included in scanpy. Validation of and comparisons between the currently available imputation methods are both severely lacking---I only know of [1][2][3][4][5], none of which include comprehensive benchmarks, and the updated MAGIC (#187) article at Cell doesn't include relevant comparisons between current methods. . I'd be very interested in hearing/having an open discussion about the motivation, benefits, and limitations of the various imputation methods available. [1]: Zhang and Zhang, 2017. https://www.biorxiv.org/content/early/2017/12/31/241190; [2]: Lopez et al. 2018, https://www.biorxiv.org/content/early/2018/03/30/292037; [3]: Li and Li, 2018. https://www.nature.com/articles/s41467-018-03405-7; [4]: Eraslan et al. 2018. https://www.biorxiv.org/content/early/2018/04/13/300681; [5]: Huang et al. 2018. https://www.biorxiv.org/content/early/2018/03/08/138677",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:356,Deployability,update,updated,356,"@falexwolf, @flying-sheep . From the discussion on #45, I think some more discussion should be had as to what imputation methods are to be included in scanpy. Validation of and comparisons between the currently available imputation methods are both severely lacking---I only know of [1][2][3][4][5], none of which include comprehensive benchmarks, and the updated MAGIC (#187) article at Cell doesn't include relevant comparisons between current methods. . I'd be very interested in hearing/having an open discussion about the motivation, benefits, and limitations of the various imputation methods available. [1]: Zhang and Zhang, 2017. https://www.biorxiv.org/content/early/2017/12/31/241190; [2]: Lopez et al. 2018, https://www.biorxiv.org/content/early/2018/03/30/292037; [3]: Li and Li, 2018. https://www.nature.com/articles/s41467-018-03405-7; [4]: Eraslan et al. 2018. https://www.biorxiv.org/content/early/2018/04/13/300681; [5]: Huang et al. 2018. https://www.biorxiv.org/content/early/2018/03/08/138677",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:159,Security,Validat,Validation,159,"@falexwolf, @flying-sheep . From the discussion on #45, I think some more discussion should be had as to what imputation methods are to be included in scanpy. Validation of and comparisons between the currently available imputation methods are both severely lacking---I only know of [1][2][3][4][5], none of which include comprehensive benchmarks, and the updated MAGIC (#187) article at Cell doesn't include relevant comparisons between current methods. . I'd be very interested in hearing/having an open discussion about the motivation, benefits, and limitations of the various imputation methods available. [1]: Zhang and Zhang, 2017. https://www.biorxiv.org/content/early/2017/12/31/241190; [2]: Lopez et al. 2018, https://www.biorxiv.org/content/early/2018/03/30/292037; [3]: Li and Li, 2018. https://www.nature.com/articles/s41467-018-03405-7; [4]: Eraslan et al. 2018. https://www.biorxiv.org/content/early/2018/04/13/300681; [5]: Huang et al. 2018. https://www.biorxiv.org/content/early/2018/03/08/138677",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/issues/189:336,Testability,benchmark,benchmarks,336,"@falexwolf, @flying-sheep . From the discussion on #45, I think some more discussion should be had as to what imputation methods are to be included in scanpy. Validation of and comparisons between the currently available imputation methods are both severely lacking---I only know of [1][2][3][4][5], none of which include comprehensive benchmarks, and the updated MAGIC (#187) article at Cell doesn't include relevant comparisons between current methods. . I'd be very interested in hearing/having an open discussion about the motivation, benefits, and limitations of the various imputation methods available. [1]: Zhang and Zhang, 2017. https://www.biorxiv.org/content/early/2017/12/31/241190; [2]: Lopez et al. 2018, https://www.biorxiv.org/content/early/2018/03/30/292037; [3]: Li and Li, 2018. https://www.nature.com/articles/s41467-018-03405-7; [4]: Eraslan et al. 2018. https://www.biorxiv.org/content/early/2018/04/13/300681; [5]: Huang et al. 2018. https://www.biorxiv.org/content/early/2018/03/08/138677",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189
https://github.com/scverse/scanpy/pull/191:124,Energy Efficiency,efficient,efficient,124,"We noticed some performance issues with `scanpy.api.pp.log1p` and I think these changes will make it faster and more memory efficient. The `out` argument of `np.log1p` can be used to modify data in-place, conserving memory (this does require special-casing for sparse matrices, unfortunately). Because it's a ufunc, it's quite fast and efficient. With those changes I removed the `chunked` and `chunk_size` arguments for this function, because I don't think they would actually help performance. In fact I found that using `chunked=True` was extremely slow, possibly because of having so many function calls. The only advantage I can imagine there is if you used parallelization on the chunks, but until that's implemented I think the option should be removed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:336,Energy Efficiency,efficient,efficient,336,"We noticed some performance issues with `scanpy.api.pp.log1p` and I think these changes will make it faster and more memory efficient. The `out` argument of `np.log1p` can be used to modify data in-place, conserving memory (this does require special-casing for sparse matrices, unfortunately). Because it's a ufunc, it's quite fast and efficient. With those changes I removed the `chunked` and `chunk_size` arguments for this function, because I don't think they would actually help performance. In fact I found that using `chunked=True` was extremely slow, possibly because of having so many function calls. The only advantage I can imagine there is if you used parallelization on the chunks, but until that's implemented I think the option should be removed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:16,Performance,perform,performance,16,"We noticed some performance issues with `scanpy.api.pp.log1p` and I think these changes will make it faster and more memory efficient. The `out` argument of `np.log1p` can be used to modify data in-place, conserving memory (this does require special-casing for sparse matrices, unfortunately). Because it's a ufunc, it's quite fast and efficient. With those changes I removed the `chunked` and `chunk_size` arguments for this function, because I don't think they would actually help performance. In fact I found that using `chunked=True` was extremely slow, possibly because of having so many function calls. The only advantage I can imagine there is if you used parallelization on the chunks, but until that's implemented I think the option should be removed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/191:483,Performance,perform,performance,483,"We noticed some performance issues with `scanpy.api.pp.log1p` and I think these changes will make it faster and more memory efficient. The `out` argument of `np.log1p` can be used to modify data in-place, conserving memory (this does require special-casing for sparse matrices, unfortunately). Because it's a ufunc, it's quite fast and efficient. With those changes I removed the `chunked` and `chunk_size` arguments for this function, because I don't think they would actually help performance. In fact I found that using `chunked=True` was extremely slow, possibly because of having so many function calls. The only advantage I can imagine there is if you used parallelization on the chunks, but until that's implemented I think the option should be removed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191
https://github.com/scverse/scanpy/pull/192:401,Availability,avail,available,401,"Fixes #58. ## readthedocs. For RTD I switched from numpydoc to napoleon, with the same customizations as in anndata:. - Numpydoc-style HTML rendering of param docs with custom CSS; - A custom class template that supersedes numpy’s autodoc hack and makes attributes appear above methods in class docs. ## docstrings. The docstring part is implemented via `obj.getdoc()`, a method invoked by IPython if available, which means that it leaves `__doc__` alone. As an example, it converts `scanpy.api.Neighbors.compute_neighbors`’ docs like this, leaving alone explicit type/default info and adding info from the signature. A huge advantage here is that by *removing* explicit info, we gain always-up-to-date defaults and types. Whenever we change something, we can’t forget to change everything else anymore. ```rst; Compute distances and connectivities of neighbors. Parameters; ----------; n_neighbors; Use this number of nearest neighbors.; knn; Restrict result to `n_neighbors` nearest neighbors.; {n_pcs}; {use_rep}. Returns; -------; Writes sparse graph attributes `.distances` and `.connectivities`.; Also writes `.knn_indices` and `.knn_distances` if; `write_knn_indices==True`.; ```. <p align=center>↓↓↓</p>. ```rst; Compute distances and connectivities of neighbors.; Parameters; ----------; n_neighbors : int, optional (default: 30); Use this number of nearest neighbors.; knn : bool, optional (default: True); Restrict result to `n_neighbors` nearest neighbors.; n_pcs : `int` or `None`, optional (default: `None`); Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`.; use_rep : {`None`, 'X'} or any key for `.obsm`, optional (default: `None`); Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters.; Returns; -------; Writes sparse graph attributes `.distances` and `.connectivities`.; Also writes `.knn_indices` and `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192
https://github.com/scverse/scanpy/pull/193:194,Deployability,update,updated,194,"MAGIC API modelled off the DCA API. A couple of comments:. - I changed `threads` to `n_jobs` to match `sc.tl` API; - I use `sc.settings.verbosity` rather than `False` for `verbose` default; - I updated the PHATE API while I was here; - I'm open to suggestions on how to handle `name_list`. Re: `name_list`: MAGIC accepts both gene names and column indices. I've set it up currently to return to adata.obsm['X_magic'] if genes are specified as the number of columns is reduced from the original data, and to adata.X if all genes are returned. However, adata.obsm['X_magic'] now has no information about the order of the genes, unless I return it from MAGIC as a `pandas.DataFrame`, or as an AnnData object in its own right. What do you think?. P.S. MAGIC handling `genes` on AnnData input is currently on the dev branch, but will be merged to master once we settle on an API.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/193
https://github.com/scverse/scanpy/pull/193:468,Energy Efficiency,reduce,reduced,468,"MAGIC API modelled off the DCA API. A couple of comments:. - I changed `threads` to `n_jobs` to match `sc.tl` API; - I use `sc.settings.verbosity` rather than `False` for `verbose` default; - I updated the PHATE API while I was here; - I'm open to suggestions on how to handle `name_list`. Re: `name_list`: MAGIC accepts both gene names and column indices. I've set it up currently to return to adata.obsm['X_magic'] if genes are specified as the number of columns is reduced from the original data, and to adata.X if all genes are returned. However, adata.obsm['X_magic'] now has no information about the order of the genes, unless I return it from MAGIC as a `pandas.DataFrame`, or as an AnnData object in its own right. What do you think?. P.S. MAGIC handling `genes` on AnnData input is currently on the dev branch, but will be merged to master once we settle on an API.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/193
https://github.com/scverse/scanpy/issues/194:151,Availability,error,error,151,"Anybody experience something similar? . I'm attempting to regress out cell cycle gene information from a single cell dataset. ```; # ; # # Part of the error message that probably matters most; # . Crashed Thread: 0. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 00000001039d0000-00000001039d1000 [ 4K] r-x/rwx SM=COW /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. # ; # ; # ; ```. Any ideas on what the problem could be?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194
https://github.com/scverse/scanpy/issues/194:387,Availability,fault,fault,387,"Anybody experience something similar? . I'm attempting to regress out cell cycle gene information from a single cell dataset. ```; # ; # # Part of the error message that probably matters most; # . Crashed Thread: 0. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 00000001039d0000-00000001039d1000 [ 4K] r-x/rwx SM=COW /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. # ; # ; # ; ```. Any ideas on what the problem could be?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194
https://github.com/scverse/scanpy/issues/194:157,Integrability,message,message,157,"Anybody experience something similar? . I'm attempting to regress out cell cycle gene information from a single cell dataset. ```; # ; # # Part of the error message that probably matters most; # . Crashed Thread: 0. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 00000001039d0000-00000001039d1000 [ 4K] r-x/rwx SM=COW /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. # ; # ; # ; ```. Any ideas on what the problem could be?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194
https://github.com/scverse/scanpy/issues/194:709,Performance,multi-thread,multi-threaded,709,"Anybody experience something similar? . I'm attempting to regress out cell cycle gene information from a single cell dataset. ```; # ; # # Part of the error message that probably matters most; # . Crashed Thread: 0. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 00000001039d0000-00000001039d1000 [ 4K] r-x/rwx SM=COW /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. # ; # ; # ; ```. Any ideas on what the problem could be?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194
https://github.com/scverse/scanpy/issues/196:395,Availability,error,error,395,"Hi, I left an issue to seurat repository as well, but it might be of interest for scanpy:. https://github.com/satijalab/seurat/issues/604#issue-339640125. In my systems, as long as it has `Seurat` and `scanpy` (or `anndata` to be more specific) installed, the above one-liner command to convert a merged seurat object to anndata fails within the anndata python code (with the index out of range error), in the `convert_dictionary_to_structured_array` module. I am not sure whether it is an issue with `Seurat` or `anndata`, but leaving here a link as well (actually curious whether the issue reproduces to anyone using the `scanpy`)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/196
https://github.com/scverse/scanpy/issues/196:245,Deployability,install,installed,245,"Hi, I left an issue to seurat repository as well, but it might be of interest for scanpy:. https://github.com/satijalab/seurat/issues/604#issue-339640125. In my systems, as long as it has `Seurat` and `scanpy` (or `anndata` to be more specific) installed, the above one-liner command to convert a merged seurat object to anndata fails within the anndata python code (with the index out of range error), in the `convert_dictionary_to_structured_array` module. I am not sure whether it is an issue with `Seurat` or `anndata`, but leaving here a link as well (actually curious whether the issue reproduces to anyone using the `scanpy`)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/196
https://github.com/scverse/scanpy/pull/199:714,Deployability,update,updated,714,"This PR adds a visualization of genes, where, for each category (eg. cluster), each gene is represented as a circle whose diameter is proportional to the fraction of cells expressing the gene, and the color represent the mean expression of the gene. I saw this type of visualization on a talk by Dr. Hemant Suryawanshi from Rockefeller University but I couldn't find a citation. ![image](https://user-images.githubusercontent.com/4964309/42698839-59560246-86bf-11e8-8548-2bf0e114d159.png). Furthermore, some improvements are introduced to the `pl.heatmap` and `pl.violin`. In particular, now is possible to swap the axis in multi_panel plot of `pl.violin` and to change the figure size in both types of plots. . I updated an example here: https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199
https://github.com/scverse/scanpy/pull/201:320,Availability,error,error,320,"PYTHON_VERSION variable is empty, so we actually pass `python=` in `conda create` so Travis always tests scanpy with latest Python in Conda distribution. Therefore Python 3.5 is actually never tested. Furthermore, conda switched to python 3.7, so now all test are run on Python 3.7. This is also the reason of weird HDF error message we get in tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/201
https://github.com/scverse/scanpy/pull/201:326,Integrability,message,message,326,"PYTHON_VERSION variable is empty, so we actually pass `python=` in `conda create` so Travis always tests scanpy with latest Python in Conda distribution. Therefore Python 3.5 is actually never tested. Furthermore, conda switched to python 3.7, so now all test are run on Python 3.7. This is also the reason of weird HDF error message we get in tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/201
https://github.com/scverse/scanpy/pull/201:15,Modifiability,variab,variable,15,"PYTHON_VERSION variable is empty, so we actually pass `python=` in `conda create` so Travis always tests scanpy with latest Python in Conda distribution. Therefore Python 3.5 is actually never tested. Furthermore, conda switched to python 3.7, so now all test are run on Python 3.7. This is also the reason of weird HDF error message we get in tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/201
https://github.com/scverse/scanpy/pull/201:99,Testability,test,tests,99,"PYTHON_VERSION variable is empty, so we actually pass `python=` in `conda create` so Travis always tests scanpy with latest Python in Conda distribution. Therefore Python 3.5 is actually never tested. Furthermore, conda switched to python 3.7, so now all test are run on Python 3.7. This is also the reason of weird HDF error message we get in tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/201
https://github.com/scverse/scanpy/pull/201:193,Testability,test,tested,193,"PYTHON_VERSION variable is empty, so we actually pass `python=` in `conda create` so Travis always tests scanpy with latest Python in Conda distribution. Therefore Python 3.5 is actually never tested. Furthermore, conda switched to python 3.7, so now all test are run on Python 3.7. This is also the reason of weird HDF error message we get in tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/201
https://github.com/scverse/scanpy/pull/201:255,Testability,test,test,255,"PYTHON_VERSION variable is empty, so we actually pass `python=` in `conda create` so Travis always tests scanpy with latest Python in Conda distribution. Therefore Python 3.5 is actually never tested. Furthermore, conda switched to python 3.7, so now all test are run on Python 3.7. This is also the reason of weird HDF error message we get in tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/201
https://github.com/scverse/scanpy/pull/201:344,Testability,test,tests,344,"PYTHON_VERSION variable is empty, so we actually pass `python=` in `conda create` so Travis always tests scanpy with latest Python in Conda distribution. Therefore Python 3.5 is actually never tested. Furthermore, conda switched to python 3.7, so now all test are run on Python 3.7. This is also the reason of weird HDF error message we get in tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/201
https://github.com/scverse/scanpy/issues/202:143,Integrability,message,message,143,"Let's say I start a new session and generate a plot, then save it. All is fine. When I plot anything after the first save, a ""Do not localize"" message pops up and so does the previous ""save as"" window. The do not localize message window cannot be exited out of, and so I drag them to the upper right corner of the screen so they're out of the way. Then I exit out of the previous save as window, which by the way if you try to use it to save the current figure, it won't work. So I exit out of that window. But you can click the save button on the figure itself, then you can save. As I continue to plot and save figures, the do not localize windows pile up, and the chain of previous save as windows continues to grow. If you can follow my explanation, you can probably get the sense of how bothersome this can be. I have to go through the process of dragging the accumulating do not localize windows to the corner, and exiting out of the accumulating save as windows as I continue to generate and save figures. Has there been any similar experiences and if so how do I get rid of this situation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/202
https://github.com/scverse/scanpy/issues/202:222,Integrability,message,message,222,"Let's say I start a new session and generate a plot, then save it. All is fine. When I plot anything after the first save, a ""Do not localize"" message pops up and so does the previous ""save as"" window. The do not localize message window cannot be exited out of, and so I drag them to the upper right corner of the screen so they're out of the way. Then I exit out of the previous save as window, which by the way if you try to use it to save the current figure, it won't work. So I exit out of that window. But you can click the save button on the figure itself, then you can save. As I continue to plot and save figures, the do not localize windows pile up, and the chain of previous save as windows continues to grow. If you can follow my explanation, you can probably get the sense of how bothersome this can be. I have to go through the process of dragging the accumulating do not localize windows to the corner, and exiting out of the accumulating save as windows as I continue to generate and save figures. Has there been any similar experiences and if so how do I get rid of this situation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/202
https://github.com/scverse/scanpy/issues/203:219,Deployability,install,installations,219,"Hi,. In scanpy 1.1, sc.pl.pca() and sc.tl.tsne() output changes (but still very similar) if they are executed more than once over an object. I have already set random_state value. I've tested that behavior in different installations in different operating systems. Is there a reason for that? I find that behavior baffling and I don't know which output should be trusted: the first time the functions are run or the following?. Thanks for your time.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/203:185,Testability,test,tested,185,"Hi,. In scanpy 1.1, sc.pl.pca() and sc.tl.tsne() output changes (but still very similar) if they are executed more than once over an object. I have already set random_state value. I've tested that behavior in different installations in different operating systems. Is there a reason for that? I find that behavior baffling and I don't know which output should be trusted: the first time the functions are run or the following?. Thanks for your time.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203
https://github.com/scverse/scanpy/issues/204:504,Testability,test,tests,504,"Hi @fidelram !. I just reintroduced the simple multi-panel violin feature that is used right in the beginning of the [introductory tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb): https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. For now, I simply hope this didn't break anything in your https://github.com/theislab/scanpy/pull/199. For future work on the plotting part of Scanpy: I need to think about adding tests for this... In case that you have an idea for doing this in the best way, I'm happy to ready about it... As a simple solution, if you don't mind, I'd add the calls from your [gist](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c) to the introductory clustering notebook, so that I make sure that I don't break anything. Best,; Alex",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204
https://github.com/scverse/scanpy/issues/204:40,Usability,simpl,simple,40,"Hi @fidelram !. I just reintroduced the simple multi-panel violin feature that is used right in the beginning of the [introductory tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb): https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. For now, I simply hope this didn't break anything in your https://github.com/theislab/scanpy/pull/199. For future work on the plotting part of Scanpy: I need to think about adding tests for this... In case that you have an idea for doing this in the best way, I'm happy to ready about it... As a simple solution, if you don't mind, I'd add the calls from your [gist](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c) to the introductory clustering notebook, so that I make sure that I don't break anything. Best,; Alex",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204
https://github.com/scverse/scanpy/issues/204:335,Usability,simpl,simply,335,"Hi @fidelram !. I just reintroduced the simple multi-panel violin feature that is used right in the beginning of the [introductory tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb): https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. For now, I simply hope this didn't break anything in your https://github.com/theislab/scanpy/pull/199. For future work on the plotting part of Scanpy: I need to think about adding tests for this... In case that you have an idea for doing this in the best way, I'm happy to ready about it... As a simple solution, if you don't mind, I'd add the calls from your [gist](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c) to the introductory clustering notebook, so that I make sure that I don't break anything. Best,; Alex",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204
https://github.com/scverse/scanpy/issues/204:620,Usability,simpl,simple,620,"Hi @fidelram !. I just reintroduced the simple multi-panel violin feature that is used right in the beginning of the [introductory tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb): https://github.com/theislab/scanpy/commit/2a869d6060ba0de12feaf08a809bdf745d39ef10. For now, I simply hope this didn't break anything in your https://github.com/theislab/scanpy/pull/199. For future work on the plotting part of Scanpy: I need to think about adding tests for this... In case that you have an idea for doing this in the best way, I'm happy to ready about it... As a simple solution, if you don't mind, I'd add the calls from your [gist](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c) to the introductory clustering notebook, so that I make sure that I don't break anything. Best,; Alex",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/204
https://github.com/scverse/scanpy/issues/206:15,Availability,error,error,15,"Hi, I hit this error:. AttributeError Traceback (most recent call last); <ipython-input-3-282f1d56354f> in <module>(); ----> 1 sc.pp.magic(adata, verbose=2). c:\scanpy\scanpy\scanpy\preprocessing\magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 148 # replace data with smoothed data; 149 adata.raw = adata; --> 150 adata.X = X_magic.X; 151 ; 152 if copy:. AttributeError: 'numpy.ndarray' object has no attribute 'X'. I fixed it by changing line 150 from adata.X = X_magic.X; to adata.X = X_magic",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206
https://github.com/scverse/scanpy/pull/207:13,Availability,error,error,13,and fixed an error with plotting functions that I think I caused when I merged with master. I will set this PR as work in progress as I will be adding more tests.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/pull/207:156,Testability,test,tests,156,and fixed an error with plotting functions that I think I caused when I merged with master. I will set this PR as work in progress as I will be adding more tests.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207
https://github.com/scverse/scanpy/issues/208:131,Availability,error,error,131,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:172,Availability,error,error,172,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:355,Availability,error,errors,355,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:558,Deployability,update,update,558,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:582,Integrability,depend,dependencies,582,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:702,Integrability,Depend,Dependencies,702,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:498,Testability,log,logging,498,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:599,Testability,log,logging,599,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:919,Testability,log,logarithmized,919,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:958,Testability,log,logarithmized,958,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:1778,Testability,log,logg,1778,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/208:780,Usability,learn,learn,780,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208
https://github.com/scverse/scanpy/issues/209:75,Testability,test,test,75,"When analysing a dataset, I can make gene ranking plots using either the t-test or the logistic regression method and everything looks sane (see image below). <img width=""1101"" alt=""screen shot 2018-07-19 at 21 59 09"" src=""https://user-images.githubusercontent.com/1142918/42969897-99896fc6-8b9f-11e8-8ae6-761b89d1e6f6.png"">. However, if I then assign names to my clusters and do this again, the t-test version looks as expected but the plot using logistic regression data seems to mismatch the cluster names with their top-ranked genes. The plot titles do not match the genes that are plotted. <img width=""1102"" alt=""screen shot 2018-07-19 at 22 00 09"" src=""https://user-images.githubusercontent.com/1142918/42969968-dd19dfc8-8b9f-11e8-8b7f-f671eb42ed82.png"">. Is this a bug or am I doing something wrong? . Hope this makes sense, please let me know if you need any more information.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/209
https://github.com/scverse/scanpy/issues/209:87,Testability,log,logistic,87,"When analysing a dataset, I can make gene ranking plots using either the t-test or the logistic regression method and everything looks sane (see image below). <img width=""1101"" alt=""screen shot 2018-07-19 at 21 59 09"" src=""https://user-images.githubusercontent.com/1142918/42969897-99896fc6-8b9f-11e8-8ae6-761b89d1e6f6.png"">. However, if I then assign names to my clusters and do this again, the t-test version looks as expected but the plot using logistic regression data seems to mismatch the cluster names with their top-ranked genes. The plot titles do not match the genes that are plotted. <img width=""1102"" alt=""screen shot 2018-07-19 at 22 00 09"" src=""https://user-images.githubusercontent.com/1142918/42969968-dd19dfc8-8b9f-11e8-8b7f-f671eb42ed82.png"">. Is this a bug or am I doing something wrong? . Hope this makes sense, please let me know if you need any more information.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/209
https://github.com/scverse/scanpy/issues/209:398,Testability,test,test,398,"When analysing a dataset, I can make gene ranking plots using either the t-test or the logistic regression method and everything looks sane (see image below). <img width=""1101"" alt=""screen shot 2018-07-19 at 21 59 09"" src=""https://user-images.githubusercontent.com/1142918/42969897-99896fc6-8b9f-11e8-8ae6-761b89d1e6f6.png"">. However, if I then assign names to my clusters and do this again, the t-test version looks as expected but the plot using logistic regression data seems to mismatch the cluster names with their top-ranked genes. The plot titles do not match the genes that are plotted. <img width=""1102"" alt=""screen shot 2018-07-19 at 22 00 09"" src=""https://user-images.githubusercontent.com/1142918/42969968-dd19dfc8-8b9f-11e8-8b7f-f671eb42ed82.png"">. Is this a bug or am I doing something wrong? . Hope this makes sense, please let me know if you need any more information.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/209
https://github.com/scverse/scanpy/issues/209:448,Testability,log,logistic,448,"When analysing a dataset, I can make gene ranking plots using either the t-test or the logistic regression method and everything looks sane (see image below). <img width=""1101"" alt=""screen shot 2018-07-19 at 21 59 09"" src=""https://user-images.githubusercontent.com/1142918/42969897-99896fc6-8b9f-11e8-8ae6-761b89d1e6f6.png"">. However, if I then assign names to my clusters and do this again, the t-test version looks as expected but the plot using logistic regression data seems to mismatch the cluster names with their top-ranked genes. The plot titles do not match the genes that are plotted. <img width=""1102"" alt=""screen shot 2018-07-19 at 22 00 09"" src=""https://user-images.githubusercontent.com/1142918/42969968-dd19dfc8-8b9f-11e8-8b7f-f671eb42ed82.png"">. Is this a bug or am I doing something wrong? . Hope this makes sense, please let me know if you need any more information.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/209
https://github.com/scverse/scanpy/issues/211:735,Usability,simpl,simple,735,"Essentially my problem is this. I have another dataframe (df) that I won't to merge with anndata.obs, via matching indices. Except df is a smaller length than anndata.obs. But I want the indices in which there isn't new information for (those missing values in df), to just appear as NaN. . I've tried many functions: merge, join, pd.concat, concatenate, etc; and have tried many different combinations of parameters but none are able to do this, even though merge and join have documentation stating they should be able to. However, if I left join, all the added info becomes NaN. If I right join, the all of the left dataframe (anndata.obs) becomes NaN. Inner makes an empty matrix, outer makes everything NaN. . Does anybody have a simple trick for this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/211
https://github.com/scverse/scanpy/issues/212:18,Deployability,pipeline,pipeline,18,"follow the normal pipeline and only meet problems when i use the function ""sc.pp.regress"". it works well on R; system is windows 8.1; and use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:1512,Deployability,update,update,1512,"ol.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> 783 res = list(map(_regress_out_chunk, tasks)); 784 ; 785 # res is a list of vectors (each corresponding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:3208,Integrability,wrap,wrapper,3208,"ding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools.tools import add_constant, categorical; ----> 5 from . import regression; 6 from .regression.linear_model import OLS, GLS, WLS, GLSAR; 7 from .regression.recursive_ls import RecursiveLS. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\__init__.py in <module>(); ----> 1 from .linear_model import yule_walker; 2 ; 3 from statsmodels import PytestTester; 4 test = PytestTester(); 5 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\linear_model.py in <module>(); 46 cache_readonly,; 47 cache_writable); ---> 48 import statsmodels.base.model as base; 49 import statsmodels.base.wrapper as wrap; 50 from statsmodels.emplike.elregress import _ELRegOpts. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\base\model.py in <module>(); 13 from statsmodels.tools.sm_exceptions import ValueWarning, \; 14 HessianInversionWarning; ---> 15 from statsmodels.formula import handle_formula_data; 16 from statsmodels.compat.numpy import np_matrix_rank; 17 from statsmodels.base.optimizer import Optimizer. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\__init__.py in <module>(); 3 ; 4 ; ----> 5 from .formulatools import handle_formula_data. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\formulatools.py in <module>(); 1 from statsmodels.compat.python import iterkeys; 2 import statsmodels.tools.data as data_util; ----> 3 from patsy import dmatrices, NAAction; 4 import numpy as np; 5 . ModuleNotFoundError: No module named 'patsy'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:3219,Integrability,wrap,wrap,3219,"ding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools.tools import add_constant, categorical; ----> 5 from . import regression; 6 from .regression.linear_model import OLS, GLS, WLS, GLSAR; 7 from .regression.recursive_ls import RecursiveLS. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\__init__.py in <module>(); ----> 1 from .linear_model import yule_walker; 2 ; 3 from statsmodels import PytestTester; 4 test = PytestTester(); 5 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\linear_model.py in <module>(); 46 cache_readonly,; 47 cache_writable); ---> 48 import statsmodels.base.model as base; 49 import statsmodels.base.wrapper as wrap; 50 from statsmodels.emplike.elregress import _ELRegOpts. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\base\model.py in <module>(); 13 from statsmodels.tools.sm_exceptions import ValueWarning, \; 14 HessianInversionWarning; ---> 15 from statsmodels.formula import handle_formula_data; 16 from statsmodels.compat.numpy import np_matrix_rank; 17 from statsmodels.base.optimizer import Optimizer. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\__init__.py in <module>(); 3 ; 4 ; ----> 5 from .formulatools import handle_formula_data. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\formulatools.py in <module>(); 1 from statsmodels.compat.python import iterkeys; 2 import statsmodels.tools.data as data_util; ----> 3 from patsy import dmatrices, NAAction; 4 import numpy as np; 5 . ModuleNotFoundError: No module named 'patsy'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:3615,Performance,optimiz,optimizer,3615,"ding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools.tools import add_constant, categorical; ----> 5 from . import regression; 6 from .regression.linear_model import OLS, GLS, WLS, GLSAR; 7 from .regression.recursive_ls import RecursiveLS. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\__init__.py in <module>(); ----> 1 from .linear_model import yule_walker; 2 ; 3 from statsmodels import PytestTester; 4 test = PytestTester(); 5 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\linear_model.py in <module>(); 46 cache_readonly,; 47 cache_writable); ---> 48 import statsmodels.base.model as base; 49 import statsmodels.base.wrapper as wrap; 50 from statsmodels.emplike.elregress import _ELRegOpts. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\base\model.py in <module>(); 13 from statsmodels.tools.sm_exceptions import ValueWarning, \; 14 HessianInversionWarning; ---> 15 from statsmodels.formula import handle_formula_data; 16 from statsmodels.compat.numpy import np_matrix_rank; 17 from statsmodels.base.optimizer import Optimizer. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\__init__.py in <module>(); 3 ; 4 ; ----> 5 from .formulatools import handle_formula_data. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\formulatools.py in <module>(); 1 from statsmodels.compat.python import iterkeys; 2 import statsmodels.tools.data as data_util; ----> 3 from patsy import dmatrices, NAAction; 4 import numpy as np; 5 . ModuleNotFoundError: No module named 'patsy'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:3632,Performance,Optimiz,Optimizer,3632,"ding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools.tools import add_constant, categorical; ----> 5 from . import regression; 6 from .regression.linear_model import OLS, GLS, WLS, GLSAR; 7 from .regression.recursive_ls import RecursiveLS. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\__init__.py in <module>(); ----> 1 from .linear_model import yule_walker; 2 ; 3 from statsmodels import PytestTester; 4 test = PytestTester(); 5 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\linear_model.py in <module>(); 46 cache_readonly,; 47 cache_writable); ---> 48 import statsmodels.base.model as base; 49 import statsmodels.base.wrapper as wrap; 50 from statsmodels.emplike.elregress import _ELRegOpts. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\base\model.py in <module>(); 13 from statsmodels.tools.sm_exceptions import ValueWarning, \; 14 HessianInversionWarning; ---> 15 from statsmodels.formula import handle_formula_data; 16 from statsmodels.compat.numpy import np_matrix_rank; 17 from statsmodels.base.optimizer import Optimizer. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\__init__.py in <module>(); 3 ; 4 ; ----> 5 from .formulatools import handle_formula_data. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\formulatools.py in <module>(); 1 from statsmodels.compat.python import iterkeys; 2 import statsmodels.tools.data as data_util; ----> 3 from patsy import dmatrices, NAAction; 4 import numpy as np; 5 . ModuleNotFoundError: No module named 'patsy'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:725,Safety,timeout,timeout,725,"follow the normal pipeline and only meet problems when i use the function ""sc.pp.regress"". it works well on R; system is windows 8.1; and use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:759,Safety,timeout,timeout,759,"follow the normal pipeline and only meet problems when i use the function ""sc.pp.regress"". it works well on R; system is windows 8.1; and use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:793,Safety,timeout,timeout,793,"follow the normal pipeline and only meet problems when i use the function ""sc.pp.regress"". it works well on R; system is windows 8.1; and use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:839,Safety,Timeout,TimeoutError,839,"follow the normal pipeline and only meet problems when i use the function ""sc.pp.regress"". it works well on R; system is windows 8.1; and use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:935,Safety,timeout,timeout,935,"follow the normal pipeline and only meet problems when i use the function ""sc.pp.regress"". it works well on R; system is windows 8.1; and use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:970,Safety,timeout,timeout,970,"follow the normal pipeline and only meet problems when i use the function ""sc.pp.regress"". it works well on R; system is windows 8.1; and use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:1011,Safety,timeout,timeout,1011,"ly meet problems when i use the function ""sc.pp.regress"". it works well on R; system is windows 8.1; and use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:1045,Safety,timeout,timeout,1045,"ly meet problems when i use the function ""sc.pp.regress"". it works well on R; system is windows 8.1; and use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:1132,Safety,timeout,timeout,1132,"use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> 783 res = list(map(_regress_out_chunk, tasks)); 784 ; 785 # res is a list of vectors (each corresponding t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:1226,Safety,timeout,timeout,1226,"ython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> 783 res = list(map(_regress_out_chunk, tasks)); 784 ; 785 # res is a list of vectors (each corresponding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocess",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:1334,Safety,timeout,timeout,1334,"\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> 783 res = list(map(_regress_out_chunk, tasks)); 784 ; 785 # res is a list of vectors (each corresponding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:1362,Safety,timeout,timeout,1362,"\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> 783 res = list(map(_regress_out_chunk, tasks)); 784 ; 785 # res is a list of vectors (each corresponding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:1413,Safety,timeout,timeout,1413,"g\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> 783 res = list(map(_regress_out_chunk, tasks)); 784 ; 785 # res is a list of vectors (each corresponding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:1484,Safety,timeout,timeout,1484,"rt multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> 783 res = list(map(_regress_out_chunk, tasks)); 784 ; 785 # res is a list of vectors (each corresponding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-pac",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:2955,Testability,test,test,2955,"-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> 783 res = list(map(_regress_out_chunk, tasks)); 784 ; 785 # res is a list of vectors (each corresponding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools.tools import add_constant, categorical; ----> 5 from . import regression; 6 from .regression.linear_model import OLS, GLS, WLS, GLSAR; 7 from .regression.recursive_ls import RecursiveLS. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\__init__.py in <module>(); ----> 1 from .linear_model import yule_walker; 2 ; 3 from statsmodels import PytestTester; 4 test = PytestTester(); 5 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\linear_model.py in <module>(); 46 cache_readonly,; 47 cache_writable); ---> 48 import statsmodels.base.model as base; 49 import statsmodels.base.wrapper as wrap; 50 from statsmodels.emplike.elregress import _ELRegOpts. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\base\model.py in <module>(); 13 from statsmodels.tools.sm_exceptions import ValueWarning, \; 14 HessianInversionWarning; ---> 15 from statsmodels.formula import handle_formula_data; 16 from statsmodels.compat.numpy import np_matrix_rank; 17 from statsmodels.base.optimizer import Optimizer. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\__init__.py in <module>(); 3 ; 4 ; ----> 5 from .formulatools import handle_formula_data. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\formulatools.py in <module>(); 1 from st",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:428,Usability,simpl,simple,428,"follow the normal pipeline and only meet problems when i use the function ""sc.pp.regress"". it works well on R; system is windows 8.1; and use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:1959,Usability,simpl,simple,1959,"n wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> 783 res = list(map(_regress_out_chunk, tasks)); 784 ; 785 # res is a list of vectors (each corresponding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools.tools import add_constant, categorical; ----> 5 from . import regression; 6 from .regression.linear_model import OLS, GLS, WLS, GLSAR; 7 from .regression.recursive_ls import RecursiveLS. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\__init__.py in <module>(); ----> 1 from .linear_model import yule_walker; 2 ; 3 from sta",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/issues/212:2246,Usability,simpl,simple,2246," self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> 783 res = list(map(_regress_out_chunk, tasks)); 784 ; 785 # res is a list of vectors (each corresponding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools.tools import add_constant, categorical; ----> 5 from . import regression; 6 from .regression.linear_model import OLS, GLS, WLS, GLSAR; 7 from .regression.recursive_ls import RecursiveLS. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\__init__.py in <module>(); ----> 1 from .linear_model import yule_walker; 2 ; 3 from statsmodels import PytestTester; 4 test = PytestTester(); 5 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\linear_model.py in <module>(); 46 cache_readonly,; 47 cache_writable); ---> 48 import statsmodels.base.model as base; 49 import statsmodels.base.wr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212
https://github.com/scverse/scanpy/pull/214:77,Testability,log,logreg,77,Fixed the adata_raw issue and added functionality beyond just binary and 1v1 logreg,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/214
https://github.com/scverse/scanpy/issues/220:10,Availability,error,error,10,"I have an error when trying this function for my data:. ```; sc.pl.highest_expr_genes(adata); ```; results in. ```; filtered out 14139 cells that have less than 1 counts; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-11-1f5b130b9d4e> in <module>(); ----> 1 sc.pl.highest_expr_genes(adata). /usr/local/lib/python3.6/site-packages/scanpy-1.2.2+90.g47c579f-py3.6.egg/scanpy/plotting/qc.py in highest_expr_genes(adata, n_top, save, show, ax, **kwargs); 41 ; 42 # identify the genes with the highest mean; ---> 43 dat.var['mean_percent'] = dat.X.mean(axis=0).A1; 44 ; 45 top = dat.var.sort_values('mean_percent', ascending=False).index[:n_top]. AttributeError: 'numpy.ndarray' object has no attribute 'A1'; ```. It is not critical, but I wonder why this error happens. I have the most recent version from github. Maybe I'm not transforming my data to a certain format properly? Didn't find this documented anyway. However, a lot of other analysis that I do via Scanpy works for me, so generally the data is read and processed correctly.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/issues/220:845,Availability,error,error,845,"I have an error when trying this function for my data:. ```; sc.pl.highest_expr_genes(adata); ```; results in. ```; filtered out 14139 cells that have less than 1 counts; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-11-1f5b130b9d4e> in <module>(); ----> 1 sc.pl.highest_expr_genes(adata). /usr/local/lib/python3.6/site-packages/scanpy-1.2.2+90.g47c579f-py3.6.egg/scanpy/plotting/qc.py in highest_expr_genes(adata, n_top, save, show, ax, **kwargs); 41 ; 42 # identify the genes with the highest mean; ---> 43 dat.var['mean_percent'] = dat.X.mean(axis=0).A1; 44 ; 45 top = dat.var.sort_values('mean_percent', ascending=False).index[:n_top]. AttributeError: 'numpy.ndarray' object has no attribute 'A1'; ```. It is not critical, but I wonder why this error happens. I have the most recent version from github. Maybe I'm not transforming my data to a certain format properly? Didn't find this documented anyway. However, a lot of other analysis that I do via Scanpy works for me, so generally the data is read and processed correctly.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220
https://github.com/scverse/scanpy/pull/221:24,Testability,test,tested,24,"Should fix #218. I have tested it, both with all groups and with some restricted groups.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/221
https://github.com/scverse/scanpy/issues/222:909,Usability,learn,learn,909,"After pp.neighbors and tl.louvain, I've been calculating the silhouette index of the clustering arrangements to get an idea of how well the data is clustered: sil_avg = silhouette_score(adata.X, adata.obs['louvain'], metric = 'euclidean'). I'm using a for loop to go through many combinations of paramaters, (e.g. n_neighbors for pp.neighbors and resolution for tl.louvain), but it seems that the silhouette indices being generated aren't what I would expect. . However, I've noticed that there is an option, metric ='precomputed', which can be used if a distance matrix has already been calculated, which is done so after using pp.neighbors : adata.uns['neighbors']['distances']. . Any idea how to use silhouette_score with adata.uns['neighbors']['distances']? ; I've tried a couple different ways, e.g. : ; silhouette_score(adata.X, adata.uns['neighbors']['distances'], metric='precomputed'). http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222
https://github.com/scverse/scanpy/issues/223:543,Integrability,depend,depend,543,"Would you say that there is an optimal range to set n_neighbors usually? And maybe a max value that rarely should be exceeded?. I'm trying to optimize louvain clustering for several datasets, and I'm aiming to automate at least a portion of the process, by going through a range of neighbor values (tl.neighbors) and resolution values (for tl.louvain), while keeping n_pcs constant, and most of my highest scoring clustering arrangements (measured by the silhouette index) uses neighbor parameters ~ 22 - 30. I know that these parameters will depend on the dataset, but I'm wondering if I should set a lower upper limit (For now it's 30), then go in and try to optimize the clustering of specific clusters using the restrict_to parameter for the louvain function. The clustering arrangements I have don't seem to be adequate based on certain markers that I'm plotting across the cells. . Hope this makes sense. Best",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:142,Performance,optimiz,optimize,142,"Would you say that there is an optimal range to set n_neighbors usually? And maybe a max value that rarely should be exceeded?. I'm trying to optimize louvain clustering for several datasets, and I'm aiming to automate at least a portion of the process, by going through a range of neighbor values (tl.neighbors) and resolution values (for tl.louvain), while keeping n_pcs constant, and most of my highest scoring clustering arrangements (measured by the silhouette index) uses neighbor parameters ~ 22 - 30. I know that these parameters will depend on the dataset, but I'm wondering if I should set a lower upper limit (For now it's 30), then go in and try to optimize the clustering of specific clusters using the restrict_to parameter for the louvain function. The clustering arrangements I have don't seem to be adequate based on certain markers that I'm plotting across the cells. . Hope this makes sense. Best",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/223:661,Performance,optimiz,optimize,661,"Would you say that there is an optimal range to set n_neighbors usually? And maybe a max value that rarely should be exceeded?. I'm trying to optimize louvain clustering for several datasets, and I'm aiming to automate at least a portion of the process, by going through a range of neighbor values (tl.neighbors) and resolution values (for tl.louvain), while keeping n_pcs constant, and most of my highest scoring clustering arrangements (measured by the silhouette index) uses neighbor parameters ~ 22 - 30. I know that these parameters will depend on the dataset, but I'm wondering if I should set a lower upper limit (For now it's 30), then go in and try to optimize the clustering of specific clusters using the restrict_to parameter for the louvain function. The clustering arrangements I have don't seem to be adequate based on certain markers that I'm plotting across the cells. . Hope this makes sense. Best",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223
https://github.com/scverse/scanpy/issues/225:171,Availability,error,error,171,"I was trying to subset my dataset based on multiple louvain cluster IDs but it seems to only be possible with one cluster ID at a time. At least I'm getting the following error. `NotImplementedError: Slicing with two indices at the same time is not yet implemented. As a workaround, do row and column slicing succesively.`. I'm still new to python and scanpy but would there be a workaround or fix to this? To ease the process I'v inserted what I want to do. `adata_subset = adata[adata.obs['louvain'] == '2', '3']`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/225
https://github.com/scverse/scanpy/issues/227:435,Testability,log,logic,435,"I noticed that `pl.diffmap` interacts differently with the `show` and `save` options than most of the other plotting functions. Namely, if both `show` and `settings.autoshow` are true, `pl.diffmap` saves but does not show. Most of the other functions will both save and show. I looked in the code a bit and it looks like this is because other plotting functions use `utils.savefig_or_show` versus `pl.diffmap` has some custom plotting logic (I assume because `pl.diffmap` allows multiple plots to be generated by passing a list to `components`). Changing:; ```python; if not settings.autosave and show: pl.show(); ```; to just:; ```python; if show: pl.show(); ```; should make `pl.diffmap` more consistent with the other plotting functions.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/227
https://github.com/scverse/scanpy/pull/228:1402,Deployability,update,updates,1402,"uces a family of plots to validate gene markers obtained using `scanpy.api.tl.rank_genes_groups`. The plots work similarly as `scanpy.api.pl.rank_genes_groups`:; ; ![image](https://user-images.githubusercontent.com/4964309/43768141-efa3633a-9a36-11e8-856f-3970352c33a8.png). ![image](https://user-images.githubusercontent.com/4964309/43768035-b8ba9082-9a36-11e8-91e6-8c828f456981.png). ![image](https://user-images.githubusercontent.com/4964309/43768064-c78dc64c-9a36-11e8-95ed-351ef6fff99e.png). ![image](https://user-images.githubusercontent.com/4964309/43768099-d776a510-9a36-11e8-9708-959d5d0afe92.png). Those functions are wrappers around `scanpy.api.pl.heatmap`, `scanpy.api.pl.stacked_violin`, `scanpy.api.pl.dotplot` and `scanpy.api.pl.matrixplot`. `heatmap` and `dotplot` were modified to allow the 'brackets' on top of the images. This functionality can be used directly from those plots. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771788-83bbb604-9a40-11e8-90d6-51f084343a98.png). The `pl.stacked_violin` plot was before part of `pl.violin` but I thought that the code is cleaner by separating the two plots. Also, this PR adds the new plot `scanpy.api.pl.matrixplot` that plots the average gene expression per category. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771966-07c9bd92-9a41-11e8-818e-263dfae69b7f.png). This PR also updates several test for the plotting options and adds new ones. . I tried to update the documentation to reflect the changes. Also a new dataset called `pbmc68k_reduced` was added. This dataset is used for tests and for the example notebook [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). This dataset contains around 700 cells and 200 genes from the original 68k 10x genomics dataset and is saved as a small anndata object. It contains cell type annotations, UMAP coordinates and rank_gene_groups. The dataset can be accessed as; ```python; adata = sc.datasets.pbmc68k_reduced(); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:1480,Deployability,update,update,1480,"uces a family of plots to validate gene markers obtained using `scanpy.api.tl.rank_genes_groups`. The plots work similarly as `scanpy.api.pl.rank_genes_groups`:; ; ![image](https://user-images.githubusercontent.com/4964309/43768141-efa3633a-9a36-11e8-856f-3970352c33a8.png). ![image](https://user-images.githubusercontent.com/4964309/43768035-b8ba9082-9a36-11e8-91e6-8c828f456981.png). ![image](https://user-images.githubusercontent.com/4964309/43768064-c78dc64c-9a36-11e8-95ed-351ef6fff99e.png). ![image](https://user-images.githubusercontent.com/4964309/43768099-d776a510-9a36-11e8-9708-959d5d0afe92.png). Those functions are wrappers around `scanpy.api.pl.heatmap`, `scanpy.api.pl.stacked_violin`, `scanpy.api.pl.dotplot` and `scanpy.api.pl.matrixplot`. `heatmap` and `dotplot` were modified to allow the 'brackets' on top of the images. This functionality can be used directly from those plots. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771788-83bbb604-9a40-11e8-90d6-51f084343a98.png). The `pl.stacked_violin` plot was before part of `pl.violin` but I thought that the code is cleaner by separating the two plots. Also, this PR adds the new plot `scanpy.api.pl.matrixplot` that plots the average gene expression per category. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771966-07c9bd92-9a41-11e8-818e-263dfae69b7f.png). This PR also updates several test for the plotting options and adds new ones. . I tried to update the documentation to reflect the changes. Also a new dataset called `pbmc68k_reduced` was added. This dataset is used for tests and for the example notebook [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). This dataset contains around 700 cells and 200 genes from the original 68k 10x genomics dataset and is saved as a small anndata object. It contains cell type annotations, UMAP coordinates and rank_gene_groups. The dataset can be accessed as; ```python; adata = sc.datasets.pbmc68k_reduced(); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:642,Integrability,wrap,wrappers,642,"This PR introduces a family of plots to validate gene markers obtained using `scanpy.api.tl.rank_genes_groups`. The plots work similarly as `scanpy.api.pl.rank_genes_groups`:; ; ![image](https://user-images.githubusercontent.com/4964309/43768141-efa3633a-9a36-11e8-856f-3970352c33a8.png). ![image](https://user-images.githubusercontent.com/4964309/43768035-b8ba9082-9a36-11e8-91e6-8c828f456981.png). ![image](https://user-images.githubusercontent.com/4964309/43768064-c78dc64c-9a36-11e8-95ed-351ef6fff99e.png). ![image](https://user-images.githubusercontent.com/4964309/43768099-d776a510-9a36-11e8-9708-959d5d0afe92.png). Those functions are wrappers around `scanpy.api.pl.heatmap`, `scanpy.api.pl.stacked_violin`, `scanpy.api.pl.dotplot` and `scanpy.api.pl.matrixplot`. `heatmap` and `dotplot` were modified to allow the 'brackets' on top of the images. This functionality can be used directly from those plots. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771788-83bbb604-9a40-11e8-90d6-51f084343a98.png). The `pl.stacked_violin` plot was before part of `pl.violin` but I thought that the code is cleaner by separating the two plots. Also, this PR adds the new plot `scanpy.api.pl.matrixplot` that plots the average gene expression per category. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771966-07c9bd92-9a41-11e8-818e-263dfae69b7f.png). This PR also updates several test for the plotting options and adds new ones. . I tried to update the documentation to reflect the changes. Also a new dataset called `pbmc68k_reduced` was added. This dataset is used for tests and for the example notebook [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). This dataset contains around 700 cells and 200 genes from the original 68k 10x genomics dataset and is saved as a small anndata object. It contains cell type annotations, UMAP coordinates and rank_gene_groups. The dataset can be accessed as; ```python; adata = sc.datasets.pbmc68k_r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:40,Security,validat,validate,40,"This PR introduces a family of plots to validate gene markers obtained using `scanpy.api.tl.rank_genes_groups`. The plots work similarly as `scanpy.api.pl.rank_genes_groups`:; ; ![image](https://user-images.githubusercontent.com/4964309/43768141-efa3633a-9a36-11e8-856f-3970352c33a8.png). ![image](https://user-images.githubusercontent.com/4964309/43768035-b8ba9082-9a36-11e8-91e6-8c828f456981.png). ![image](https://user-images.githubusercontent.com/4964309/43768064-c78dc64c-9a36-11e8-95ed-351ef6fff99e.png). ![image](https://user-images.githubusercontent.com/4964309/43768099-d776a510-9a36-11e8-9708-959d5d0afe92.png). Those functions are wrappers around `scanpy.api.pl.heatmap`, `scanpy.api.pl.stacked_violin`, `scanpy.api.pl.dotplot` and `scanpy.api.pl.matrixplot`. `heatmap` and `dotplot` were modified to allow the 'brackets' on top of the images. This functionality can be used directly from those plots. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771788-83bbb604-9a40-11e8-90d6-51f084343a98.png). The `pl.stacked_violin` plot was before part of `pl.violin` but I thought that the code is cleaner by separating the two plots. Also, this PR adds the new plot `scanpy.api.pl.matrixplot` that plots the average gene expression per category. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771966-07c9bd92-9a41-11e8-818e-263dfae69b7f.png). This PR also updates several test for the plotting options and adds new ones. . I tried to update the documentation to reflect the changes. Also a new dataset called `pbmc68k_reduced` was added. This dataset is used for tests and for the example notebook [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). This dataset contains around 700 cells and 200 genes from the original 68k 10x genomics dataset and is saved as a small anndata object. It contains cell type annotations, UMAP coordinates and rank_gene_groups. The dataset can be accessed as; ```python; adata = sc.datasets.pbmc68k_r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:1948,Security,access,accessed,1948,"uces a family of plots to validate gene markers obtained using `scanpy.api.tl.rank_genes_groups`. The plots work similarly as `scanpy.api.pl.rank_genes_groups`:; ; ![image](https://user-images.githubusercontent.com/4964309/43768141-efa3633a-9a36-11e8-856f-3970352c33a8.png). ![image](https://user-images.githubusercontent.com/4964309/43768035-b8ba9082-9a36-11e8-91e6-8c828f456981.png). ![image](https://user-images.githubusercontent.com/4964309/43768064-c78dc64c-9a36-11e8-95ed-351ef6fff99e.png). ![image](https://user-images.githubusercontent.com/4964309/43768099-d776a510-9a36-11e8-9708-959d5d0afe92.png). Those functions are wrappers around `scanpy.api.pl.heatmap`, `scanpy.api.pl.stacked_violin`, `scanpy.api.pl.dotplot` and `scanpy.api.pl.matrixplot`. `heatmap` and `dotplot` were modified to allow the 'brackets' on top of the images. This functionality can be used directly from those plots. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771788-83bbb604-9a40-11e8-90d6-51f084343a98.png). The `pl.stacked_violin` plot was before part of `pl.violin` but I thought that the code is cleaner by separating the two plots. Also, this PR adds the new plot `scanpy.api.pl.matrixplot` that plots the average gene expression per category. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771966-07c9bd92-9a41-11e8-818e-263dfae69b7f.png). This PR also updates several test for the plotting options and adds new ones. . I tried to update the documentation to reflect the changes. Also a new dataset called `pbmc68k_reduced` was added. This dataset is used for tests and for the example notebook [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). This dataset contains around 700 cells and 200 genes from the original 68k 10x genomics dataset and is saved as a small anndata object. It contains cell type annotations, UMAP coordinates and rank_gene_groups. The dataset can be accessed as; ```python; adata = sc.datasets.pbmc68k_reduced(); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:1418,Testability,test,test,1418,"uces a family of plots to validate gene markers obtained using `scanpy.api.tl.rank_genes_groups`. The plots work similarly as `scanpy.api.pl.rank_genes_groups`:; ; ![image](https://user-images.githubusercontent.com/4964309/43768141-efa3633a-9a36-11e8-856f-3970352c33a8.png). ![image](https://user-images.githubusercontent.com/4964309/43768035-b8ba9082-9a36-11e8-91e6-8c828f456981.png). ![image](https://user-images.githubusercontent.com/4964309/43768064-c78dc64c-9a36-11e8-95ed-351ef6fff99e.png). ![image](https://user-images.githubusercontent.com/4964309/43768099-d776a510-9a36-11e8-9708-959d5d0afe92.png). Those functions are wrappers around `scanpy.api.pl.heatmap`, `scanpy.api.pl.stacked_violin`, `scanpy.api.pl.dotplot` and `scanpy.api.pl.matrixplot`. `heatmap` and `dotplot` were modified to allow the 'brackets' on top of the images. This functionality can be used directly from those plots. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771788-83bbb604-9a40-11e8-90d6-51f084343a98.png). The `pl.stacked_violin` plot was before part of `pl.violin` but I thought that the code is cleaner by separating the two plots. Also, this PR adds the new plot `scanpy.api.pl.matrixplot` that plots the average gene expression per category. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771966-07c9bd92-9a41-11e8-818e-263dfae69b7f.png). This PR also updates several test for the plotting options and adds new ones. . I tried to update the documentation to reflect the changes. Also a new dataset called `pbmc68k_reduced` was added. This dataset is used for tests and for the example notebook [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). This dataset contains around 700 cells and 200 genes from the original 68k 10x genomics dataset and is saved as a small anndata object. It contains cell type annotations, UMAP coordinates and rank_gene_groups. The dataset can be accessed as; ```python; adata = sc.datasets.pbmc68k_reduced(); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/pull/228:1609,Testability,test,tests,1609,"uces a family of plots to validate gene markers obtained using `scanpy.api.tl.rank_genes_groups`. The plots work similarly as `scanpy.api.pl.rank_genes_groups`:; ; ![image](https://user-images.githubusercontent.com/4964309/43768141-efa3633a-9a36-11e8-856f-3970352c33a8.png). ![image](https://user-images.githubusercontent.com/4964309/43768035-b8ba9082-9a36-11e8-91e6-8c828f456981.png). ![image](https://user-images.githubusercontent.com/4964309/43768064-c78dc64c-9a36-11e8-95ed-351ef6fff99e.png). ![image](https://user-images.githubusercontent.com/4964309/43768099-d776a510-9a36-11e8-9708-959d5d0afe92.png). Those functions are wrappers around `scanpy.api.pl.heatmap`, `scanpy.api.pl.stacked_violin`, `scanpy.api.pl.dotplot` and `scanpy.api.pl.matrixplot`. `heatmap` and `dotplot` were modified to allow the 'brackets' on top of the images. This functionality can be used directly from those plots. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771788-83bbb604-9a40-11e8-90d6-51f084343a98.png). The `pl.stacked_violin` plot was before part of `pl.violin` but I thought that the code is cleaner by separating the two plots. Also, this PR adds the new plot `scanpy.api.pl.matrixplot` that plots the average gene expression per category. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771966-07c9bd92-9a41-11e8-818e-263dfae69b7f.png). This PR also updates several test for the plotting options and adds new ones. . I tried to update the documentation to reflect the changes. Also a new dataset called `pbmc68k_reduced` was added. This dataset is used for tests and for the example notebook [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). This dataset contains around 700 cells and 200 genes from the original 68k 10x genomics dataset and is saved as a small anndata object. It contains cell type annotations, UMAP coordinates and rank_gene_groups. The dataset can be accessed as; ```python; adata = sc.datasets.pbmc68k_reduced(); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228
https://github.com/scverse/scanpy/issues/230:459,Usability,simpl,simple,459,"```py; >>> sc.pp.regress_out(adata, ['n_counts', 'percent_mito', 'S_score', 'G2M_score'], n_jobs = 1); regressing out ['n_counts', 'percent_mito', 'S_score', 'G2M_score']; sparse input is densified and may lead to high memory use; ... storing 'phase' as categorical; ```. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/simple.py"", line 783, in regress_out; res = list(map(_regress_out_chunk, tasks)); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/simple.py"", line 809, in _regress_out_chunk; result = sm.GLM(data_chunk[:, col_index], regres, family=sm.families.Gaussian()).fit(); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/statsmodels/genmod/generalized_linear_model.py"", line 1012, in fit; cov_kwds=cov_kwds, use_t=use_t, **kwargs); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/statsmodels/genmod/generalized_linear_model.py"", line 1109, in _fit_irls; raise ValueError(""The first guess on the deviance function ""; ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.; ```. Hello, after going through the data once and evaluating which clusters are potential unwanted cell types, I grabbed the barcodes (cell sample names or observation indices) and removed them from the dataset after freshly reloading the dataset so that I can do the preprocessing steps without those cells. When I go to regress out, this occurs, but it didn't before I removed those cell types. . Here's an example of how I removed those cell types:. ```py; keep_cells = [i for i in adata.obs.index if i not in e13_blood2.obs.index]; adata = adata[keep_cells, :]; adata; ```. Any help appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/230:646,Usability,simpl,simple,646,"```py; >>> sc.pp.regress_out(adata, ['n_counts', 'percent_mito', 'S_score', 'G2M_score'], n_jobs = 1); regressing out ['n_counts', 'percent_mito', 'S_score', 'G2M_score']; sparse input is densified and may lead to high memory use; ... storing 'phase' as categorical; ```. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/simple.py"", line 783, in regress_out; res = list(map(_regress_out_chunk, tasks)); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/simple.py"", line 809, in _regress_out_chunk; result = sm.GLM(data_chunk[:, col_index], regres, family=sm.families.Gaussian()).fit(); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/statsmodels/genmod/generalized_linear_model.py"", line 1012, in fit; cov_kwds=cov_kwds, use_t=use_t, **kwargs); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/statsmodels/genmod/generalized_linear_model.py"", line 1109, in _fit_irls; raise ValueError(""The first guess on the deviance function ""; ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.; ```. Hello, after going through the data once and evaluating which clusters are potential unwanted cell types, I grabbed the barcodes (cell sample names or observation indices) and removed them from the dataset after freshly reloading the dataset so that I can do the preprocessing steps without those cells. When I go to regress out, this occurs, but it didn't before I removed those cell types. . Here's an example of how I removed those cell types:. ```py; keep_cells = [i for i in adata.obs.index if i not in e13_blood2.obs.index]; adata = adata[keep_cells, :]; adata; ```. Any help appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230
https://github.com/scverse/scanpy/issues/231:2452,Availability,error,error,2452,"umap(adata, color, use_raw, edges, edges_width, edges_color, arrows, arrows_kwds, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, size, title, show, save, ax); 290 show=False,; 291 save=False,; --> 292 ax=ax); 293 if edges: utils.plot_edges(axs, adata, basis, edges_width, edges_color); 294 if arrows: utils.plot_arrows(axs, adata, basis, arrows_kwds). ~/software/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 104 show=show,; 105 save=save,; --> 106 ax=ax); 107 elif x is not None and y is not None:; 108 if ((x in adata.obs.keys() or x in adata.var.index). ~/software/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 409 raise ValueError('""' + name + '"" is invalid!'; 410 + ' specify valid name, one of '; --> 411 + str(adata.obs[key].cat.categories)); 412 else:; 413 iname = np.flatnonzero(adata.obs[key].cat.categories.values == name)[0]. ValueError: ""Z"" is invalid! specify valid name, one of Index(['Zero', '1', '2', '3', '4'], dtype='object'); ```; The last call `sc.pl.umap` gives and error but I would expect it to work. It seems that scanpy iterates over the string `'Zero'` in the last call of `sc.pl.umap`. Of course, it is easy to work around by explicitly passing a list with one element as in the second call, but it took me a while to figure this out. `sc.logging.print_versions()` prints `scanpy==1.2.2+96.g28f5034 anndata==0.6.4 numpy==1.15.0 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:2731,Testability,log,logging,2731,"umap(adata, color, use_raw, edges, edges_width, edges_color, arrows, arrows_kwds, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, size, title, show, save, ax); 290 show=False,; 291 save=False,; --> 292 ax=ax); 293 if edges: utils.plot_edges(axs, adata, basis, edges_width, edges_color); 294 if arrows: utils.plot_arrows(axs, adata, basis, arrows_kwds). ~/software/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 104 show=show,; 105 save=save,; --> 106 ax=ax); 107 elif x is not None and y is not None:; 108 if ((x in adata.obs.keys() or x in adata.var.index). ~/software/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 409 raise ValueError('""' + name + '"" is invalid!'; 410 + ' specify valid name, one of '; --> 411 + str(adata.obs[key].cat.categories)); 412 else:; 413 iname = np.flatnonzero(adata.obs[key].cat.categories.values == name)[0]. ValueError: ""Z"" is invalid! specify valid name, one of Index(['Zero', '1', '2', '3', '4'], dtype='object'); ```; The last call `sc.pl.umap` gives and error but I would expect it to work. It seems that scanpy iterates over the string `'Zero'` in the last call of `sc.pl.umap`. Of course, it is easy to work around by explicitly passing a list with one element as in the second call, but it took me a while to figure this out. `sc.logging.print_versions()` prints `scanpy==1.2.2+96.g28f5034 anndata==0.6.4 numpy==1.15.0 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/231:2855,Usability,learn,learn,2855,"umap(adata, color, use_raw, edges, edges_width, edges_color, arrows, arrows_kwds, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, size, title, show, save, ax); 290 show=False,; 291 save=False,; --> 292 ax=ax); 293 if edges: utils.plot_edges(axs, adata, basis, edges_width, edges_color); 294 if arrows: utils.plot_arrows(axs, adata, basis, arrows_kwds). ~/software/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 104 show=show,; 105 save=save,; --> 106 ax=ax); 107 elif x is not None and y is not None:; 108 if ((x in adata.obs.keys() or x in adata.var.index). ~/software/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 409 raise ValueError('""' + name + '"" is invalid!'; 410 + ' specify valid name, one of '; --> 411 + str(adata.obs[key].cat.categories)); 412 else:; 413 iname = np.flatnonzero(adata.obs[key].cat.categories.values == name)[0]. ValueError: ""Z"" is invalid! specify valid name, one of Index(['Zero', '1', '2', '3', '4'], dtype='object'); ```; The last call `sc.pl.umap` gives and error but I would expect it to work. It seems that scanpy iterates over the string `'Zero'` in the last call of `sc.pl.umap`. Of course, it is easy to work around by explicitly passing a list with one element as in the second call, but it took me a while to figure this out. `sc.logging.print_versions()` prints `scanpy==1.2.2+96.g28f5034 anndata==0.6.4 numpy==1.15.0 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231
https://github.com/scverse/scanpy/issues/233:130,Security,access,access,130,"I sucessfully used mnn to correct batch effect for my datasets but I would like to know when and how can I create a copy so as to access the raw data which will be used for differential genes expression analysis for instance. In one of the scanpy tutorial it shows how to create a copy of the raw data that can be used for latter analysis by typing adata.raw = sc.pp.log1p(adata, copy=True) but when can I do that with mnn. I used the following cmd to create the batch corrected files. How can I create the adata.raw copy?. corrected = sc.pp.mnn_correct(adata1,adata2,adata3,adata4, batch_categories=[""ba1"", ""ba2"",""ba3"",""ba4""]); adata_corrected = corrected[0]. adata_corrected.write('./write1/mnn_corrected.h5ad'). Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/233
https://github.com/scverse/scanpy/pull/234:114,Deployability,pipeline,pipeline,114,"This is the only way the README renders on PyPI. depends on github/markup#1222, which in turn depends on jch/html-pipeline#302",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/234
https://github.com/scverse/scanpy/pull/234:49,Integrability,depend,depends,49,"This is the only way the README renders on PyPI. depends on github/markup#1222, which in turn depends on jch/html-pipeline#302",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/234
https://github.com/scverse/scanpy/pull/234:94,Integrability,depend,depends,94,"This is the only way the README renders on PyPI. depends on github/markup#1222, which in turn depends on jch/html-pipeline#302",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/234
https://github.com/scverse/scanpy/issues/235:62,Testability,log,logreg,62,"```; sc.tl.rank_genes_groups(adata,groupby='louvain', method='logreg'); ```; ranking genes; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/tools/rank_genes_groups.py"", line 191, in rank_genes_groups; scores = scores_all[igroup]; IndexError: index 1 is out of bounds for axis 0 with size 1. Have tried finding similar situations online but can't seem to debug this one. ; Sorry if I'm on this issues page a lot, but I'm using scanpy relentlessly lately :P . Any suggestions appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/235
https://github.com/scverse/scanpy/issues/241:751,Availability,error,error,751,"If you use `knn=False` to calculate neighbors, any choice of metric is ignored. [Here's the offending line of code.](https://github.com/theislab/scanpy/blob/6c1daba7448be72de84dec16a038fcaeda1636ad/scanpy/neighbors/__init__.py#L706). A quick example:. ```python; import scanpy.api as sc; import numpy as np. adata = sc.datasets.krumsiek11(); adata.obs_names_make_unique(); sc.pp.pca(adata) # To get rid of warnings; adata_eucl = sc.pp.neighbors(adata, knn=False, method=""gauss"", copy=True); adata_spear = sc.pp.neighbors(adata, knn=False, method=""gauss"", copy=True, metric=""correlation""). assert np.all(adata_eucl.uns[""neighbors""][""connectivities""] == adata_spear.uns[""neighbors""][""connectivities""]); ```. Additionally, I suspect this should throw an error:. ```python; sc.pp.neighbors(adata, method=""gauss"", knn=False, metric=""not a real metric""); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/241
https://github.com/scverse/scanpy/issues/241:589,Testability,assert,assert,589,"If you use `knn=False` to calculate neighbors, any choice of metric is ignored. [Here's the offending line of code.](https://github.com/theislab/scanpy/blob/6c1daba7448be72de84dec16a038fcaeda1636ad/scanpy/neighbors/__init__.py#L706). A quick example:. ```python; import scanpy.api as sc; import numpy as np. adata = sc.datasets.krumsiek11(); adata.obs_names_make_unique(); sc.pp.pca(adata) # To get rid of warnings; adata_eucl = sc.pp.neighbors(adata, knn=False, method=""gauss"", copy=True); adata_spear = sc.pp.neighbors(adata, knn=False, method=""gauss"", copy=True, metric=""correlation""). assert np.all(adata_eucl.uns[""neighbors""][""connectivities""] == adata_spear.uns[""neighbors""][""connectivities""]); ```. Additionally, I suspect this should throw an error:. ```python; sc.pp.neighbors(adata, method=""gauss"", knn=False, metric=""not a real metric""); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/241
https://github.com/scverse/scanpy/issues/242:77,Availability,error,error,77,"I'm having some trouble getting the mitochondrial gene query to not throw an error. Here's an example:. ```python; import scanpy.api as sc; sc.queries.mitochondrial_genes(host=""www.ensembl.org"", org=""hsapiens""); ```; <details>; <summary>The output and traceback</summary>. ```python; You must set the host (e.g. f.host='www.ensembl.org' ; You must set the host (e.g. f.host='www.ensembl.org' ; You must set the host (e.g. f.host='www.ensembl.org' ; You must set the host (e.g. f.host='www.ensembl.org' ; You must set the host (e.g. f.host='www.ensembl.org' ; You must set the host (e.g. f.host='www.ensembl.org' ; ---------------------------------------------------------------------------; EmptyDataError Traceback (most recent call last); <ipython-input-14-a6967c88fd61> in <module>(); ----> 1 sc.queries.mitochondrial_genes(host=""www.ensembl.org"", org=""hsapiens""). /usr/local/lib/python3.6/site-packages/scanpy/queries/__init__.py in mitochondrial_genes(host, org); 40 ; 41 # parsing mitochondrial gene symbols; ---> 42 res = pd.read_csv(StringIO(s.query(xml)), sep='\t', header=None); 43 res.columns = ['symbol', 'chromosome_name']; 44 res = res.dropna(). /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision); 676 skip_blank_lines=skip_blank_lines); 677 ; --> 678 return _read(filepath_or_buffer, kwds); 679 ; 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3268,Deployability,install,installing,3268,"ues, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision); 676 skip_blank_lines=skip_blank_lines); 677 ; --> 678 return _read(filepath_or_buffer, kwds); 679 ; 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds); 438 ; 439 # Create the parser.; --> 440 parser = TextFileReader(filepath_or_buffer, **kwds); 441 ; 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds); 785 self.options['has_index_names'] = kwds['has_index_names']; 786 ; --> 787 self._make_engine(self.engine); 788 ; 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine); 1012 def _make_engine(self, engine='c'):; 1013 if engine == 'c':; -> 1014 self._engine = CParserWrapper(self.f, **self.options); 1015 else:; 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds); 1706 kwds['usecols'] = self.usecols; 1707 ; -> 1708 self._reader = parsers.TextReader(src, **kwds); 1709 ; 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file; ```. </details>. The arguments I've passed there are whats in the documentation for the function, so I'd figured I'd give them a shot first. I've also tried `host=""www.ensembl.org/biomart""`, but had no such luck. This is after installing the `bioservices` module via `pip` and it creating some config file the first time I tried to run the query.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/issues/242:3335,Modifiability,config,config,3335,"ues, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision); 676 skip_blank_lines=skip_blank_lines); 677 ; --> 678 return _read(filepath_or_buffer, kwds); 679 ; 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds); 438 ; 439 # Create the parser.; --> 440 parser = TextFileReader(filepath_or_buffer, **kwds); 441 ; 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds); 785 self.options['has_index_names'] = kwds['has_index_names']; 786 ; --> 787 self._make_engine(self.engine); 788 ; 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine); 1012 def _make_engine(self, engine='c'):; 1013 if engine == 'c':; -> 1014 self._engine = CParserWrapper(self.f, **self.options); 1015 else:; 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds); 1706 kwds['usecols'] = self.usecols; 1707 ; -> 1708 self._reader = parsers.TextReader(src, **kwds); 1709 ; 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file; ```. </details>. The arguments I've passed there are whats in the documentation for the function, so I'd figured I'd give them a shot first. I've also tried `host=""www.ensembl.org/biomart""`, but had no such luck. This is after installing the `bioservices` module via `pip` and it creating some config file the first time I tried to run the query.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242
https://github.com/scverse/scanpy/pull/244:292,Deployability,integrat,integrated,292,"I tried to collect in one file the code used for plotting functions that use matplotlib scatter like `sc.pl.tsne`, `sc.pl.pca` and `sc.pl.umap` and others. Also, I tried to annotate the code and improve the readability. . Currently, the code is on a separate file called `scatter.py` and not integrated into the API as this facilitates comparison with previous code. . Besides readability the proposed code can:; * Plot a large number of plots in multiple columms (instead of a long row of plots); * Pass arguments directly to `matplotlib.pyplot.scatter` like vmax and vmin to adjust the color scale. When plotting multiple plots, this is useful to have a consistent range of values). See cells 15 and 15 in this example: https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe ; If the admins would like to merge these changes I can replaced the previous functions. An example on how to use the code:. ```python; import scanpy.plotting.tools.scatter as spl; spl.tsne(adata, color='louvain'); ```. ![image](https://user-images.githubusercontent.com/4964309/44652273-c908b580-a9eb-11e8-86fa-aa1b55fa9b0a.png). Further examples [here](https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/244:292,Integrability,integrat,integrated,292,"I tried to collect in one file the code used for plotting functions that use matplotlib scatter like `sc.pl.tsne`, `sc.pl.pca` and `sc.pl.umap` and others. Also, I tried to annotate the code and improve the readability. . Currently, the code is on a separate file called `scatter.py` and not integrated into the API as this facilitates comparison with previous code. . Besides readability the proposed code can:; * Plot a large number of plots in multiple columms (instead of a long row of plots); * Pass arguments directly to `matplotlib.pyplot.scatter` like vmax and vmin to adjust the color scale. When plotting multiple plots, this is useful to have a consistent range of values). See cells 15 and 15 in this example: https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe ; If the admins would like to merge these changes I can replaced the previous functions. An example on how to use the code:. ```python; import scanpy.plotting.tools.scatter as spl; spl.tsne(adata, color='louvain'); ```. ![image](https://user-images.githubusercontent.com/4964309/44652273-c908b580-a9eb-11e8-86fa-aa1b55fa9b0a.png). Further examples [here](https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244
https://github.com/scverse/scanpy/pull/245:113,Testability,test,test,113,"Fixes #241. I've allowed the choice of metric for finding nearest neighbors when `knn=False`. I've added a small test to make sure it works, but would open to adding more. The fix I've made killed a few code paths, so I've removed the dead code. I also reorganized the test cases a bit (split one monolithic test into separate tests), as having more granular feedback helped with a little debugging.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:269,Testability,test,test,269,"Fixes #241. I've allowed the choice of metric for finding nearest neighbors when `knn=False`. I've added a small test to make sure it works, but would open to adding more. The fix I've made killed a few code paths, so I've removed the dead code. I also reorganized the test cases a bit (split one monolithic test into separate tests), as having more granular feedback helped with a little debugging.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:308,Testability,test,test,308,"Fixes #241. I've allowed the choice of metric for finding nearest neighbors when `knn=False`. I've added a small test to make sure it works, but would open to adding more. The fix I've made killed a few code paths, so I've removed the dead code. I also reorganized the test cases a bit (split one monolithic test into separate tests), as having more granular feedback helped with a little debugging.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:327,Testability,test,tests,327,"Fixes #241. I've allowed the choice of metric for finding nearest neighbors when `knn=False`. I've added a small test to make sure it works, but would open to adding more. The fix I've made killed a few code paths, so I've removed the dead code. I also reorganized the test cases a bit (split one monolithic test into separate tests), as having more granular feedback helped with a little debugging.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/pull/245:359,Usability,feedback,feedback,359,"Fixes #241. I've allowed the choice of metric for finding nearest neighbors when `knn=False`. I've added a small test to make sure it works, but would open to adding more. The fix I've made killed a few code paths, so I've removed the dead code. I also reorganized the test cases a bit (split one monolithic test into separate tests), as having more granular feedback helped with a little debugging.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/245
https://github.com/scverse/scanpy/issues/246:130,Availability,down,downstream,130,"I just updated scanpy and reran a script which is now giving different outputs. The clustering has changed slightly, and that has downstream effects on the results. The first place I noticed a difference is where the results of `sc.pp.filter_genes_dispersion()` are plotted. . In scanpy version 1.2.2+73.g1812406 and AnnData version 0.6.4 I get the following output:; ![screen shot 2018-08-28 at 14 05 53](https://user-images.githubusercontent.com/13019956/44722232-bade9600-aacc-11e8-88c6-3f4c17fd4e07.png). And with scanpy version 1.2.2+166.g6c1daba with Anndata version 0.6.9, I get higher dispersions:; ![screen shot 2018-08-28 at 14 06 15](https://user-images.githubusercontent.com/13019956/44722316-fda06e00-aacc-11e8-940f-1295b36eacf6.png). Previous results look the same, and the only two scanpy functions that were run in between were `sc.pp.log1p()` and `sc.pp.filter_genes_dispersion()`. I also ran ComBat, but that was not updated and can't really have changed on my system. I see sc.pp.log1p was changed in between, but it doesn't seem to have been anything can could have changed this... Or was there a change to the plotting that may have changed the plots I see?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/246
https://github.com/scverse/scanpy/issues/246:7,Deployability,update,updated,7,"I just updated scanpy and reran a script which is now giving different outputs. The clustering has changed slightly, and that has downstream effects on the results. The first place I noticed a difference is where the results of `sc.pp.filter_genes_dispersion()` are plotted. . In scanpy version 1.2.2+73.g1812406 and AnnData version 0.6.4 I get the following output:; ![screen shot 2018-08-28 at 14 05 53](https://user-images.githubusercontent.com/13019956/44722232-bade9600-aacc-11e8-88c6-3f4c17fd4e07.png). And with scanpy version 1.2.2+166.g6c1daba with Anndata version 0.6.9, I get higher dispersions:; ![screen shot 2018-08-28 at 14 06 15](https://user-images.githubusercontent.com/13019956/44722316-fda06e00-aacc-11e8-940f-1295b36eacf6.png). Previous results look the same, and the only two scanpy functions that were run in between were `sc.pp.log1p()` and `sc.pp.filter_genes_dispersion()`. I also ran ComBat, but that was not updated and can't really have changed on my system. I see sc.pp.log1p was changed in between, but it doesn't seem to have been anything can could have changed this... Or was there a change to the plotting that may have changed the plots I see?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/246
https://github.com/scverse/scanpy/issues/246:935,Deployability,update,updated,935,"I just updated scanpy and reran a script which is now giving different outputs. The clustering has changed slightly, and that has downstream effects on the results. The first place I noticed a difference is where the results of `sc.pp.filter_genes_dispersion()` are plotted. . In scanpy version 1.2.2+73.g1812406 and AnnData version 0.6.4 I get the following output:; ![screen shot 2018-08-28 at 14 05 53](https://user-images.githubusercontent.com/13019956/44722232-bade9600-aacc-11e8-88c6-3f4c17fd4e07.png). And with scanpy version 1.2.2+166.g6c1daba with Anndata version 0.6.9, I get higher dispersions:; ![screen shot 2018-08-28 at 14 06 15](https://user-images.githubusercontent.com/13019956/44722316-fda06e00-aacc-11e8-940f-1295b36eacf6.png). Previous results look the same, and the only two scanpy functions that were run in between were `sc.pp.log1p()` and `sc.pp.filter_genes_dispersion()`. I also ran ComBat, but that was not updated and can't really have changed on my system. I see sc.pp.log1p was changed in between, but it doesn't seem to have been anything can could have changed this... Or was there a change to the plotting that may have changed the plots I see?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/246
https://github.com/scverse/scanpy/issues/247:21,Availability,error,error,21,"Hi,. I am getting an error when loading my loom files, which did not happen before and I am not capable of understanding the error output to try to fix it. . ![screen shot 2018-08-29 at 10 58 23](https://user-images.githubusercontent.com/42487820/44760841-9b527680-ab7b-11e8-9e85-0d0235cee6db.png). Your help will be much appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:125,Availability,error,error,125,"Hi,. I am getting an error when loading my loom files, which did not happen before and I am not capable of understanding the error output to try to fix it. . ![screen shot 2018-08-29 at 10 58 23](https://user-images.githubusercontent.com/42487820/44760841-9b527680-ab7b-11e8-9e85-0d0235cee6db.png). Your help will be much appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/issues/247:32,Performance,load,loading,32,"Hi,. I am getting an error when loading my loom files, which did not happen before and I am not capable of understanding the error output to try to fix it. . ![screen shot 2018-08-29 at 10 58 23](https://user-images.githubusercontent.com/42487820/44760841-9b527680-ab7b-11e8-9e85-0d0235cee6db.png). Your help will be much appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247
https://github.com/scverse/scanpy/pull/248:133,Availability,error,error,133,"Previously discussed in #240. A few things left to discuss:. ## Tests. These are pretty simple, ""this doesn't intrinsically throw an error"" type tests. Should the tests cover more than that? Should they be more thorough is checking arguments won't throw errors? I'm open to suggestions on other things that could be checked. Also, is there a place they'd be more appropriate?. ## Allowing storage of multiple network representations . I think this would also be a pretty simple addition, but wanted to check again before implementing it. I'm thinking of adding a `use_network` argument which would allow key access to network stored in the AnnData object – similar to the `use_rep` argument. @LuckyMD mentioned there might be some storage concerns here, though I think the user is ultimately responsible for size in this case. The value added here is different representations are useful for different analysis, and it'd be useful to not have to have two objects when the rest of the data would be shared. ## Allow more choice of partition method for `louvain-igraph` package. I'm not too fussed on this one. It's just that `""RBConfiguration""` is hard coded when other methods are available, and I'm not aware of a reason it would be the best choice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:254,Availability,error,errors,254,"Previously discussed in #240. A few things left to discuss:. ## Tests. These are pretty simple, ""this doesn't intrinsically throw an error"" type tests. Should the tests cover more than that? Should they be more thorough is checking arguments won't throw errors? I'm open to suggestions on other things that could be checked. Also, is there a place they'd be more appropriate?. ## Allowing storage of multiple network representations . I think this would also be a pretty simple addition, but wanted to check again before implementing it. I'm thinking of adding a `use_network` argument which would allow key access to network stored in the AnnData object – similar to the `use_rep` argument. @LuckyMD mentioned there might be some storage concerns here, though I think the user is ultimately responsible for size in this case. The value added here is different representations are useful for different analysis, and it'd be useful to not have to have two objects when the rest of the data would be shared. ## Allow more choice of partition method for `louvain-igraph` package. I'm not too fussed on this one. It's just that `""RBConfiguration""` is hard coded when other methods are available, and I'm not aware of a reason it would be the best choice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:1181,Availability,avail,available,1181,"Previously discussed in #240. A few things left to discuss:. ## Tests. These are pretty simple, ""this doesn't intrinsically throw an error"" type tests. Should the tests cover more than that? Should they be more thorough is checking arguments won't throw errors? I'm open to suggestions on other things that could be checked. Also, is there a place they'd be more appropriate?. ## Allowing storage of multiple network representations . I think this would also be a pretty simple addition, but wanted to check again before implementing it. I'm thinking of adding a `use_network` argument which would allow key access to network stored in the AnnData object – similar to the `use_rep` argument. @LuckyMD mentioned there might be some storage concerns here, though I think the user is ultimately responsible for size in this case. The value added here is different representations are useful for different analysis, and it'd be useful to not have to have two objects when the rest of the data would be shared. ## Allow more choice of partition method for `louvain-igraph` package. I'm not too fussed on this one. It's just that `""RBConfiguration""` is hard coded when other methods are available, and I'm not aware of a reason it would be the best choice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:608,Security,access,access,608,"Previously discussed in #240. A few things left to discuss:. ## Tests. These are pretty simple, ""this doesn't intrinsically throw an error"" type tests. Should the tests cover more than that? Should they be more thorough is checking arguments won't throw errors? I'm open to suggestions on other things that could be checked. Also, is there a place they'd be more appropriate?. ## Allowing storage of multiple network representations . I think this would also be a pretty simple addition, but wanted to check again before implementing it. I'm thinking of adding a `use_network` argument which would allow key access to network stored in the AnnData object – similar to the `use_rep` argument. @LuckyMD mentioned there might be some storage concerns here, though I think the user is ultimately responsible for size in this case. The value added here is different representations are useful for different analysis, and it'd be useful to not have to have two objects when the rest of the data would be shared. ## Allow more choice of partition method for `louvain-igraph` package. I'm not too fussed on this one. It's just that `""RBConfiguration""` is hard coded when other methods are available, and I'm not aware of a reason it would be the best choice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:64,Testability,Test,Tests,64,"Previously discussed in #240. A few things left to discuss:. ## Tests. These are pretty simple, ""this doesn't intrinsically throw an error"" type tests. Should the tests cover more than that? Should they be more thorough is checking arguments won't throw errors? I'm open to suggestions on other things that could be checked. Also, is there a place they'd be more appropriate?. ## Allowing storage of multiple network representations . I think this would also be a pretty simple addition, but wanted to check again before implementing it. I'm thinking of adding a `use_network` argument which would allow key access to network stored in the AnnData object – similar to the `use_rep` argument. @LuckyMD mentioned there might be some storage concerns here, though I think the user is ultimately responsible for size in this case. The value added here is different representations are useful for different analysis, and it'd be useful to not have to have two objects when the rest of the data would be shared. ## Allow more choice of partition method for `louvain-igraph` package. I'm not too fussed on this one. It's just that `""RBConfiguration""` is hard coded when other methods are available, and I'm not aware of a reason it would be the best choice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:145,Testability,test,tests,145,"Previously discussed in #240. A few things left to discuss:. ## Tests. These are pretty simple, ""this doesn't intrinsically throw an error"" type tests. Should the tests cover more than that? Should they be more thorough is checking arguments won't throw errors? I'm open to suggestions on other things that could be checked. Also, is there a place they'd be more appropriate?. ## Allowing storage of multiple network representations . I think this would also be a pretty simple addition, but wanted to check again before implementing it. I'm thinking of adding a `use_network` argument which would allow key access to network stored in the AnnData object – similar to the `use_rep` argument. @LuckyMD mentioned there might be some storage concerns here, though I think the user is ultimately responsible for size in this case. The value added here is different representations are useful for different analysis, and it'd be useful to not have to have two objects when the rest of the data would be shared. ## Allow more choice of partition method for `louvain-igraph` package. I'm not too fussed on this one. It's just that `""RBConfiguration""` is hard coded when other methods are available, and I'm not aware of a reason it would be the best choice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:163,Testability,test,tests,163,"Previously discussed in #240. A few things left to discuss:. ## Tests. These are pretty simple, ""this doesn't intrinsically throw an error"" type tests. Should the tests cover more than that? Should they be more thorough is checking arguments won't throw errors? I'm open to suggestions on other things that could be checked. Also, is there a place they'd be more appropriate?. ## Allowing storage of multiple network representations . I think this would also be a pretty simple addition, but wanted to check again before implementing it. I'm thinking of adding a `use_network` argument which would allow key access to network stored in the AnnData object – similar to the `use_rep` argument. @LuckyMD mentioned there might be some storage concerns here, though I think the user is ultimately responsible for size in this case. The value added here is different representations are useful for different analysis, and it'd be useful to not have to have two objects when the rest of the data would be shared. ## Allow more choice of partition method for `louvain-igraph` package. I'm not too fussed on this one. It's just that `""RBConfiguration""` is hard coded when other methods are available, and I'm not aware of a reason it would be the best choice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:88,Usability,simpl,simple,88,"Previously discussed in #240. A few things left to discuss:. ## Tests. These are pretty simple, ""this doesn't intrinsically throw an error"" type tests. Should the tests cover more than that? Should they be more thorough is checking arguments won't throw errors? I'm open to suggestions on other things that could be checked. Also, is there a place they'd be more appropriate?. ## Allowing storage of multiple network representations . I think this would also be a pretty simple addition, but wanted to check again before implementing it. I'm thinking of adding a `use_network` argument which would allow key access to network stored in the AnnData object – similar to the `use_rep` argument. @LuckyMD mentioned there might be some storage concerns here, though I think the user is ultimately responsible for size in this case. The value added here is different representations are useful for different analysis, and it'd be useful to not have to have two objects when the rest of the data would be shared. ## Allow more choice of partition method for `louvain-igraph` package. I'm not too fussed on this one. It's just that `""RBConfiguration""` is hard coded when other methods are available, and I'm not aware of a reason it would be the best choice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/248:471,Usability,simpl,simple,471,"Previously discussed in #240. A few things left to discuss:. ## Tests. These are pretty simple, ""this doesn't intrinsically throw an error"" type tests. Should the tests cover more than that? Should they be more thorough is checking arguments won't throw errors? I'm open to suggestions on other things that could be checked. Also, is there a place they'd be more appropriate?. ## Allowing storage of multiple network representations . I think this would also be a pretty simple addition, but wanted to check again before implementing it. I'm thinking of adding a `use_network` argument which would allow key access to network stored in the AnnData object – similar to the `use_rep` argument. @LuckyMD mentioned there might be some storage concerns here, though I think the user is ultimately responsible for size in this case. The value added here is different representations are useful for different analysis, and it'd be useful to not have to have two objects when the rest of the data would be shared. ## Allow more choice of partition method for `louvain-igraph` package. I'm not too fussed on this one. It's just that `""RBConfiguration""` is hard coded when other methods are available, and I'm not aware of a reason it would be the best choice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248
https://github.com/scverse/scanpy/pull/250:78,Usability,undo,undocumented,78,"Was playing around with an incremental PCA earlier and saw the arguments were undocumented. Figured I could document it while I was waiting for it to run. I think I got all the style right, let me know if not.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/250
https://github.com/scverse/scanpy/issues/252:673,Availability,error,error,673,"Thanks for all the work in developing this package, it's truly fantastic. . I ran into what seems like a bug in the new plotting function sc.pl.rank_genes_groups_stacked_violin. It seems that when the ranked genes between 2 groups are similar (e.g. 'Tnf' is a highly ranked gene between two groups), then 'Tnf' is only plotted once on the first group, and any following groups with the same gene are truncated. You can see this in the toy example image I attached - when comparing groups M1 and M1+M2, 'Tnf' should be plotted for each group, but it is only plotted on group M1, therefore truncating group M2. When I plot the same data using rank_genes_groups_dotplot, this error doesn't happen and 'Tnf' is correctly plotted twice. I know this is a small bug that most people will probably not run across, but just in case you're comparing expression across similar groups this might be a useful fix. Thanks!. ![stacked_violin_global](https://user-images.githubusercontent.com/37122760/44924265-bd353000-ad18-11e8-84d0-a0136083dbdd.png). ![dotplot_global](https://user-images.githubusercontent.com/37122760/44924244-aa226000-ad18-11e8-9351-4b28d11a7ee5.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252
https://github.com/scverse/scanpy/issues/253:793,Deployability,integrat,integrating,793,"Hi @ivirshup!. We've discussed this in Aptos a couple of months ago. Adding an `interactive` parameter to all the scatter plots would be really useful for working with notebooks. Would you consider adding that functionality as you have a lot of experience with it? Importantly, it should be based on the restructured plotting code that @fidelram is currently working on in https://github.com/theislab/scanpy/pull/244 (we could move that branch to the scanpy repo?). Hence, this would be for post-Scanpy 1.3 and there is no great hurry. A solution that takes an `AnnData` and creates an interactive plot but totally ignores the current way scatter plots are generated and Fidel's restructured way would be what follows below (due to @NDKoehler). Hence, the task is to think about a good way of integrating this with how scatter plots are done in Scanpy (after Fidel's changes).; ```; from bokeh.plotting import figure, show, output_notebook, save#, output_file; from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource; from bokeh.palettes import viridis; output_notebook(). import matplotlib as mpl. def plot_interactive(data):. colors = [; ""#%02x%02x%02x"" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.viridis(mpl.colors.Normalize()(data.obs['CCS'].values)); ]. source = ColumnDataSource(dict(; x=data.obsm['X_umap'][:,0],; y=data.obsm['X_umap'][:,1],; color=colors,#data.obs['CCS'],; label=data.obs['Charge'],; #msize= p_df['marker_size'],; #topic_key= p_df['clusters'],; #title= p_df[u'Title'],; #content = p_df['Text_Rep']; seq=data.obs['seq'],; ccs=data.obs['CCS'],; charge=data.obs['Charge'],; )); #ax = sc.pl.umap(data, color=['Charge','CCS']); #sc.pl.umap(data, color=['CCS'], save='ccs'). title = 'T-SNE visualization of sequences'. plot_lda = figure(plot_width=800, plot_height=600,; title=title, tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",; x_axis_type=None, y_axis_type=None, min_border=1). plot_lda.scatter(x='x', y='y', legend='label', source=",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:1433,Energy Efficiency,Charge,Charge,1433,"ce with it? Importantly, it should be based on the restructured plotting code that @fidelram is currently working on in https://github.com/theislab/scanpy/pull/244 (we could move that branch to the scanpy repo?). Hence, this would be for post-Scanpy 1.3 and there is no great hurry. A solution that takes an `AnnData` and creates an interactive plot but totally ignores the current way scatter plots are generated and Fidel's restructured way would be what follows below (due to @NDKoehler). Hence, the task is to think about a good way of integrating this with how scatter plots are done in Scanpy (after Fidel's changes).; ```; from bokeh.plotting import figure, show, output_notebook, save#, output_file; from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource; from bokeh.palettes import viridis; output_notebook(). import matplotlib as mpl. def plot_interactive(data):. colors = [; ""#%02x%02x%02x"" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.viridis(mpl.colors.Normalize()(data.obs['CCS'].values)); ]. source = ColumnDataSource(dict(; x=data.obsm['X_umap'][:,0],; y=data.obsm['X_umap'][:,1],; color=colors,#data.obs['CCS'],; label=data.obs['Charge'],; #msize= p_df['marker_size'],; #topic_key= p_df['clusters'],; #title= p_df[u'Title'],; #content = p_df['Text_Rep']; seq=data.obs['seq'],; ccs=data.obs['CCS'],; charge=data.obs['Charge'],; )); #ax = sc.pl.umap(data, color=['Charge','CCS']); #sc.pl.umap(data, color=['CCS'], save='ccs'). title = 'T-SNE visualization of sequences'. plot_lda = figure(plot_width=800, plot_height=600,; title=title, tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",; x_axis_type=None, y_axis_type=None, min_border=1). plot_lda.scatter(x='x', y='y', legend='label', source=source, color='color',; alpha=0.8, size=5)#'msize', ). # hover tools; hover = plot_lda.select(dict(type=HoverTool)); hover.tooltips = {""content"": ""Sequence: @seq, CCS: @ccs, Charge: @charge ""}; plot_lda.legend.location = ""top_left"". show(plot_lda); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:1603,Energy Efficiency,charge,charge,1603,"ce with it? Importantly, it should be based on the restructured plotting code that @fidelram is currently working on in https://github.com/theislab/scanpy/pull/244 (we could move that branch to the scanpy repo?). Hence, this would be for post-Scanpy 1.3 and there is no great hurry. A solution that takes an `AnnData` and creates an interactive plot but totally ignores the current way scatter plots are generated and Fidel's restructured way would be what follows below (due to @NDKoehler). Hence, the task is to think about a good way of integrating this with how scatter plots are done in Scanpy (after Fidel's changes).; ```; from bokeh.plotting import figure, show, output_notebook, save#, output_file; from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource; from bokeh.palettes import viridis; output_notebook(). import matplotlib as mpl. def plot_interactive(data):. colors = [; ""#%02x%02x%02x"" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.viridis(mpl.colors.Normalize()(data.obs['CCS'].values)); ]. source = ColumnDataSource(dict(; x=data.obsm['X_umap'][:,0],; y=data.obsm['X_umap'][:,1],; color=colors,#data.obs['CCS'],; label=data.obs['Charge'],; #msize= p_df['marker_size'],; #topic_key= p_df['clusters'],; #title= p_df[u'Title'],; #content = p_df['Text_Rep']; seq=data.obs['seq'],; ccs=data.obs['CCS'],; charge=data.obs['Charge'],; )); #ax = sc.pl.umap(data, color=['Charge','CCS']); #sc.pl.umap(data, color=['CCS'], save='ccs'). title = 'T-SNE visualization of sequences'. plot_lda = figure(plot_width=800, plot_height=600,; title=title, tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",; x_axis_type=None, y_axis_type=None, min_border=1). plot_lda.scatter(x='x', y='y', legend='label', source=source, color='color',; alpha=0.8, size=5)#'msize', ). # hover tools; hover = plot_lda.select(dict(type=HoverTool)); hover.tooltips = {""content"": ""Sequence: @seq, CCS: @ccs, Charge: @charge ""}; plot_lda.legend.location = ""top_left"". show(plot_lda); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:1620,Energy Efficiency,Charge,Charge,1620,"ce with it? Importantly, it should be based on the restructured plotting code that @fidelram is currently working on in https://github.com/theislab/scanpy/pull/244 (we could move that branch to the scanpy repo?). Hence, this would be for post-Scanpy 1.3 and there is no great hurry. A solution that takes an `AnnData` and creates an interactive plot but totally ignores the current way scatter plots are generated and Fidel's restructured way would be what follows below (due to @NDKoehler). Hence, the task is to think about a good way of integrating this with how scatter plots are done in Scanpy (after Fidel's changes).; ```; from bokeh.plotting import figure, show, output_notebook, save#, output_file; from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource; from bokeh.palettes import viridis; output_notebook(). import matplotlib as mpl. def plot_interactive(data):. colors = [; ""#%02x%02x%02x"" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.viridis(mpl.colors.Normalize()(data.obs['CCS'].values)); ]. source = ColumnDataSource(dict(; x=data.obsm['X_umap'][:,0],; y=data.obsm['X_umap'][:,1],; color=colors,#data.obs['CCS'],; label=data.obs['Charge'],; #msize= p_df['marker_size'],; #topic_key= p_df['clusters'],; #title= p_df[u'Title'],; #content = p_df['Text_Rep']; seq=data.obs['seq'],; ccs=data.obs['CCS'],; charge=data.obs['Charge'],; )); #ax = sc.pl.umap(data, color=['Charge','CCS']); #sc.pl.umap(data, color=['CCS'], save='ccs'). title = 'T-SNE visualization of sequences'. plot_lda = figure(plot_width=800, plot_height=600,; title=title, tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",; x_axis_type=None, y_axis_type=None, min_border=1). plot_lda.scatter(x='x', y='y', legend='label', source=source, color='color',; alpha=0.8, size=5)#'msize', ). # hover tools; hover = plot_lda.select(dict(type=HoverTool)); hover.tooltips = {""content"": ""Sequence: @seq, CCS: @ccs, Charge: @charge ""}; plot_lda.legend.location = ""top_left"". show(plot_lda); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:1666,Energy Efficiency,Charge,Charge,1666,"ce with it? Importantly, it should be based on the restructured plotting code that @fidelram is currently working on in https://github.com/theislab/scanpy/pull/244 (we could move that branch to the scanpy repo?). Hence, this would be for post-Scanpy 1.3 and there is no great hurry. A solution that takes an `AnnData` and creates an interactive plot but totally ignores the current way scatter plots are generated and Fidel's restructured way would be what follows below (due to @NDKoehler). Hence, the task is to think about a good way of integrating this with how scatter plots are done in Scanpy (after Fidel's changes).; ```; from bokeh.plotting import figure, show, output_notebook, save#, output_file; from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource; from bokeh.palettes import viridis; output_notebook(). import matplotlib as mpl. def plot_interactive(data):. colors = [; ""#%02x%02x%02x"" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.viridis(mpl.colors.Normalize()(data.obs['CCS'].values)); ]. source = ColumnDataSource(dict(; x=data.obsm['X_umap'][:,0],; y=data.obsm['X_umap'][:,1],; color=colors,#data.obs['CCS'],; label=data.obs['Charge'],; #msize= p_df['marker_size'],; #topic_key= p_df['clusters'],; #title= p_df[u'Title'],; #content = p_df['Text_Rep']; seq=data.obs['seq'],; ccs=data.obs['CCS'],; charge=data.obs['Charge'],; )); #ax = sc.pl.umap(data, color=['Charge','CCS']); #sc.pl.umap(data, color=['CCS'], save='ccs'). title = 'T-SNE visualization of sequences'. plot_lda = figure(plot_width=800, plot_height=600,; title=title, tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",; x_axis_type=None, y_axis_type=None, min_border=1). plot_lda.scatter(x='x', y='y', legend='label', source=source, color='color',; alpha=0.8, size=5)#'msize', ). # hover tools; hover = plot_lda.select(dict(type=HoverTool)); hover.tooltips = {""content"": ""Sequence: @seq, CCS: @ccs, Charge: @charge ""}; plot_lda.legend.location = ""top_left"". show(plot_lda); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:2175,Energy Efficiency,Charge,Charge,2175,"ce with it? Importantly, it should be based on the restructured plotting code that @fidelram is currently working on in https://github.com/theislab/scanpy/pull/244 (we could move that branch to the scanpy repo?). Hence, this would be for post-Scanpy 1.3 and there is no great hurry. A solution that takes an `AnnData` and creates an interactive plot but totally ignores the current way scatter plots are generated and Fidel's restructured way would be what follows below (due to @NDKoehler). Hence, the task is to think about a good way of integrating this with how scatter plots are done in Scanpy (after Fidel's changes).; ```; from bokeh.plotting import figure, show, output_notebook, save#, output_file; from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource; from bokeh.palettes import viridis; output_notebook(). import matplotlib as mpl. def plot_interactive(data):. colors = [; ""#%02x%02x%02x"" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.viridis(mpl.colors.Normalize()(data.obs['CCS'].values)); ]. source = ColumnDataSource(dict(; x=data.obsm['X_umap'][:,0],; y=data.obsm['X_umap'][:,1],; color=colors,#data.obs['CCS'],; label=data.obs['Charge'],; #msize= p_df['marker_size'],; #topic_key= p_df['clusters'],; #title= p_df[u'Title'],; #content = p_df['Text_Rep']; seq=data.obs['seq'],; ccs=data.obs['CCS'],; charge=data.obs['Charge'],; )); #ax = sc.pl.umap(data, color=['Charge','CCS']); #sc.pl.umap(data, color=['CCS'], save='ccs'). title = 'T-SNE visualization of sequences'. plot_lda = figure(plot_width=800, plot_height=600,; title=title, tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",; x_axis_type=None, y_axis_type=None, min_border=1). plot_lda.scatter(x='x', y='y', legend='label', source=source, color='color',; alpha=0.8, size=5)#'msize', ). # hover tools; hover = plot_lda.select(dict(type=HoverTool)); hover.tooltips = {""content"": ""Sequence: @seq, CCS: @ccs, Charge: @charge ""}; plot_lda.legend.location = ""top_left"". show(plot_lda); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:2184,Energy Efficiency,charge,charge,2184,"ce with it? Importantly, it should be based on the restructured plotting code that @fidelram is currently working on in https://github.com/theislab/scanpy/pull/244 (we could move that branch to the scanpy repo?). Hence, this would be for post-Scanpy 1.3 and there is no great hurry. A solution that takes an `AnnData` and creates an interactive plot but totally ignores the current way scatter plots are generated and Fidel's restructured way would be what follows below (due to @NDKoehler). Hence, the task is to think about a good way of integrating this with how scatter plots are done in Scanpy (after Fidel's changes).; ```; from bokeh.plotting import figure, show, output_notebook, save#, output_file; from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource; from bokeh.palettes import viridis; output_notebook(). import matplotlib as mpl. def plot_interactive(data):. colors = [; ""#%02x%02x%02x"" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.viridis(mpl.colors.Normalize()(data.obs['CCS'].values)); ]. source = ColumnDataSource(dict(; x=data.obsm['X_umap'][:,0],; y=data.obsm['X_umap'][:,1],; color=colors,#data.obs['CCS'],; label=data.obs['Charge'],; #msize= p_df['marker_size'],; #topic_key= p_df['clusters'],; #title= p_df[u'Title'],; #content = p_df['Text_Rep']; seq=data.obs['seq'],; ccs=data.obs['CCS'],; charge=data.obs['Charge'],; )); #ax = sc.pl.umap(data, color=['Charge','CCS']); #sc.pl.umap(data, color=['CCS'], save='ccs'). title = 'T-SNE visualization of sequences'. plot_lda = figure(plot_width=800, plot_height=600,; title=title, tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",; x_axis_type=None, y_axis_type=None, min_border=1). plot_lda.scatter(x='x', y='y', legend='label', source=source, color='color',; alpha=0.8, size=5)#'msize', ). # hover tools; hover = plot_lda.select(dict(type=HoverTool)); hover.tooltips = {""content"": ""Sequence: @seq, CCS: @ccs, Charge: @charge ""}; plot_lda.legend.location = ""top_left"". show(plot_lda); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/253:793,Integrability,integrat,integrating,793,"Hi @ivirshup!. We've discussed this in Aptos a couple of months ago. Adding an `interactive` parameter to all the scatter plots would be really useful for working with notebooks. Would you consider adding that functionality as you have a lot of experience with it? Importantly, it should be based on the restructured plotting code that @fidelram is currently working on in https://github.com/theislab/scanpy/pull/244 (we could move that branch to the scanpy repo?). Hence, this would be for post-Scanpy 1.3 and there is no great hurry. A solution that takes an `AnnData` and creates an interactive plot but totally ignores the current way scatter plots are generated and Fidel's restructured way would be what follows below (due to @NDKoehler). Hence, the task is to think about a good way of integrating this with how scatter plots are done in Scanpy (after Fidel's changes).; ```; from bokeh.plotting import figure, show, output_notebook, save#, output_file; from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource; from bokeh.palettes import viridis; output_notebook(). import matplotlib as mpl. def plot_interactive(data):. colors = [; ""#%02x%02x%02x"" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.viridis(mpl.colors.Normalize()(data.obs['CCS'].values)); ]. source = ColumnDataSource(dict(; x=data.obsm['X_umap'][:,0],; y=data.obsm['X_umap'][:,1],; color=colors,#data.obs['CCS'],; label=data.obs['Charge'],; #msize= p_df['marker_size'],; #topic_key= p_df['clusters'],; #title= p_df[u'Title'],; #content = p_df['Text_Rep']; seq=data.obs['seq'],; ccs=data.obs['CCS'],; charge=data.obs['Charge'],; )); #ax = sc.pl.umap(data, color=['Charge','CCS']); #sc.pl.umap(data, color=['CCS'], save='ccs'). title = 'T-SNE visualization of sequences'. plot_lda = figure(plot_width=800, plot_height=600,; title=title, tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",; x_axis_type=None, y_axis_type=None, min_border=1). plot_lda.scatter(x='x', y='y', legend='label', source=",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253
https://github.com/scverse/scanpy/issues/254:286,Availability,error,error,286,"I try to use `sc.pl.pca` selecting components. According to the documentation the following should work:. ```python; import scanpy.api as sc; sc.logging.print_versions(); adata = sc.datasets.blobs(); sc.tl.pca(adata); sc.pl.pca(adata, components=['1,2', '2,3']); ```. However, I get an error. The output of the code above is:. ```python; scanpy==0+unknown anndata==0.6.9 numpy==1.14.5 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ... storing 'blobs' as categorical. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-4cd21e9edf25> in <module>(); 3 adata = sc.datasets.blobs(); 4 sc.tl.pca(adata); ----> 5 sc.pl.pca(adata, components=['1,2', '2,3']). ~/software/scanpy/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. ~/software/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). ~/software/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 291 if components is None: components = '1,2' if '2d' in projection els",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:1292,Modifiability,layers,layers,1292,"+unknown anndata==0.6.9 numpy==1.14.5 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ... storing 'blobs' as categorical. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-4cd21e9edf25> in <module>(); 3 adata = sc.datasets.blobs(); 4 sc.tl.pca(adata); ----> 5 sc.pl.pca(adata, components=['1,2', '2,3']). ~/software/scanpy/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. ~/software/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). ~/software/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 291 if components is None: components = '1,2' if '2d' in projection else '1,2,3'; 292 if isinstance(components, str): components = components.split(','); --> 293 components = np.array(components).astype(int) - 1; 294 keys = ['grey'] if color is None else [color] if isinstance(color, str) else color; 295 if title is not None and isinstance(title, str):. ValueError: invalid literal for int() with base 10: '1,2'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:1730,Modifiability,layers,layers,1730,"+unknown anndata==0.6.9 numpy==1.14.5 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ... storing 'blobs' as categorical. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-4cd21e9edf25> in <module>(); 3 adata = sc.datasets.blobs(); 4 sc.tl.pca(adata); ----> 5 sc.pl.pca(adata, components=['1,2', '2,3']). ~/software/scanpy/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. ~/software/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). ~/software/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 291 if components is None: components = '1,2' if '2d' in projection else '1,2,3'; 292 if isinstance(components, str): components = components.split(','); --> 293 components = np.array(components).astype(int) - 1; 294 keys = ['grey'] if color is None else [color] if isinstance(color, str) else color; 295 if title is not None and isinstance(title, str):. ValueError: invalid literal for int() with base 10: '1,2'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:145,Testability,log,logging,145,"I try to use `sc.pl.pca` selecting components. According to the documentation the following should work:. ```python; import scanpy.api as sc; sc.logging.print_versions(); adata = sc.datasets.blobs(); sc.tl.pca(adata); sc.pl.pca(adata, components=['1,2', '2,3']); ```. However, I get an error. The output of the code above is:. ```python; scanpy==0+unknown anndata==0.6.9 numpy==1.14.5 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ... storing 'blobs' as categorical. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-4cd21e9edf25> in <module>(); 3 adata = sc.datasets.blobs(); 4 sc.tl.pca(adata); ----> 5 sc.pl.pca(adata, components=['1,2', '2,3']). ~/software/scanpy/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. ~/software/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). ~/software/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 291 if components is None: components = '1,2' if '2d' in projection els",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/254:420,Usability,learn,learn,420,"I try to use `sc.pl.pca` selecting components. According to the documentation the following should work:. ```python; import scanpy.api as sc; sc.logging.print_versions(); adata = sc.datasets.blobs(); sc.tl.pca(adata); sc.pl.pca(adata, components=['1,2', '2,3']); ```. However, I get an error. The output of the code above is:. ```python; scanpy==0+unknown anndata==0.6.9 numpy==1.14.5 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ... storing 'blobs' as categorical. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-4cd21e9edf25> in <module>(); 3 adata = sc.datasets.blobs(); 4 sc.tl.pca(adata); ----> 5 sc.pl.pca(adata, components=['1,2', '2,3']). ~/software/scanpy/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. ~/software/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). ~/software/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 291 if components is None: components = '1,2' if '2d' in projection els",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254
https://github.com/scverse/scanpy/issues/255:86,Usability,guid,guide,86,Thanks for youre great job. But it is a bit diffcult for me to draw picture with your guide in https://scanpy.readthedocs.io/en/latest/basic_usage.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/255
https://github.com/scverse/scanpy/issues/256:192,Integrability,message,messages,192,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:352,Integrability,message,message,352,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:28,Testability,log,logging,28,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:63,Testability,log,logging,63,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:86,Testability,log,logging,86,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:188,Testability,log,log,188,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:216,Testability,log,logger,216,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:250,Testability,log,logging,250,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:331,Testability,log,logging,331,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:532,Testability,log,logging,532,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/256:559,Testability,log,logger,559,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256
https://github.com/scverse/scanpy/issues/257:282,Availability,error,error,282,"Hi,; I encountered a wired problem when I run UMAP with min_dist=0.1; `sc.tl.pca(adata_f,n_comps=250)`; `adata_f.obsm['X_pca'] *= -1 `; `sc.pp.neighbors(adata_f, n_neighbors=10)`; `scv.pp.moments(adata_f,renormalize=True,mode='connectivities')`; `sc.tl.umap(adata_f,min_dist=0.1)`; error is about produce NaN, and then I checked `adata_f.obs['X_umap']`, all NaN in array.; However, when I use min_dist=0.2, everything seems well. ; Could you help me to figure it out? Thank you~",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/257
https://github.com/scverse/scanpy/issues/258:216,Availability,error,error,216,"I just tried; ```python; import scanpy.api as sc; sc.queries.mitochondrial_genes('www.ensembl.org', 'strange_organism'); ```; I would expect scanpy complains that it does not know `'strange_organism'`, but I get the error ; ```python; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-13-6a41b361ab41> in <module>(); 1 import scanpy.api as sc; ----> 2 sc.queries.mitochondrial_genes('www.ensembl.org', 'drerio'). ~/software/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org); 34 s.add_attribute_to_xml('mgi_symbol'); 35 else:; ---> 36 logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True); 37 return None; 38 s.add_attribute_to_xml('chromosome_name'). NameError: name 'logg' is not defined; ```; It seems to me like `queries/__init__.py` misses an `from .. import logging as logg` statement. Would maybe also make sense to show the the message that an organism is not available at verbosity level 1 instead of 4?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/258
https://github.com/scverse/scanpy/issues/258:990,Availability,avail,available,990,"I just tried; ```python; import scanpy.api as sc; sc.queries.mitochondrial_genes('www.ensembl.org', 'strange_organism'); ```; I would expect scanpy complains that it does not know `'strange_organism'`, but I get the error ; ```python; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-13-6a41b361ab41> in <module>(); 1 import scanpy.api as sc; ----> 2 sc.queries.mitochondrial_genes('www.ensembl.org', 'drerio'). ~/software/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org); 34 s.add_attribute_to_xml('mgi_symbol'); 35 else:; ---> 36 logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True); 37 return None; 38 s.add_attribute_to_xml('chromosome_name'). NameError: name 'logg' is not defined; ```; It seems to me like `queries/__init__.py` misses an `from .. import logging as logg` statement. Would maybe also make sense to show the the message that an organism is not available at verbosity level 1 instead of 4?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/258
https://github.com/scverse/scanpy/issues/258:958,Integrability,message,message,958,"I just tried; ```python; import scanpy.api as sc; sc.queries.mitochondrial_genes('www.ensembl.org', 'strange_organism'); ```; I would expect scanpy complains that it does not know `'strange_organism'`, but I get the error ; ```python; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-13-6a41b361ab41> in <module>(); 1 import scanpy.api as sc; ----> 2 sc.queries.mitochondrial_genes('www.ensembl.org', 'drerio'). ~/software/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org); 34 s.add_attribute_to_xml('mgi_symbol'); 35 else:; ---> 36 logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True); 37 return None; 38 s.add_attribute_to_xml('chromosome_name'). NameError: name 'logg' is not defined; ```; It seems to me like `queries/__init__.py` misses an `from .. import logging as logg` statement. Would maybe also make sense to show the the message that an organism is not available at verbosity level 1 instead of 4?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/258
https://github.com/scverse/scanpy/issues/258:639,Testability,log,logg,639,"I just tried; ```python; import scanpy.api as sc; sc.queries.mitochondrial_genes('www.ensembl.org', 'strange_organism'); ```; I would expect scanpy complains that it does not know `'strange_organism'`, but I get the error ; ```python; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-13-6a41b361ab41> in <module>(); 1 import scanpy.api as sc; ----> 2 sc.queries.mitochondrial_genes('www.ensembl.org', 'drerio'). ~/software/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org); 34 s.add_attribute_to_xml('mgi_symbol'); 35 else:; ---> 36 logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True); 37 return None; 38 s.add_attribute_to_xml('chromosome_name'). NameError: name 'logg' is not defined; ```; It seems to me like `queries/__init__.py` misses an `from .. import logging as logg` statement. Would maybe also make sense to show the the message that an organism is not available at verbosity level 1 instead of 4?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/258
https://github.com/scverse/scanpy/issues/258:791,Testability,log,logg,791,"I just tried; ```python; import scanpy.api as sc; sc.queries.mitochondrial_genes('www.ensembl.org', 'strange_organism'); ```; I would expect scanpy complains that it does not know `'strange_organism'`, but I get the error ; ```python; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-13-6a41b361ab41> in <module>(); 1 import scanpy.api as sc; ----> 2 sc.queries.mitochondrial_genes('www.ensembl.org', 'drerio'). ~/software/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org); 34 s.add_attribute_to_xml('mgi_symbol'); 35 else:; ---> 36 logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True); 37 return None; 38 s.add_attribute_to_xml('chromosome_name'). NameError: name 'logg' is not defined; ```; It seems to me like `queries/__init__.py` misses an `from .. import logging as logg` statement. Would maybe also make sense to show the the message that an organism is not available at verbosity level 1 instead of 4?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/258
https://github.com/scverse/scanpy/issues/258:886,Testability,log,logging,886,"I just tried; ```python; import scanpy.api as sc; sc.queries.mitochondrial_genes('www.ensembl.org', 'strange_organism'); ```; I would expect scanpy complains that it does not know `'strange_organism'`, but I get the error ; ```python; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-13-6a41b361ab41> in <module>(); 1 import scanpy.api as sc; ----> 2 sc.queries.mitochondrial_genes('www.ensembl.org', 'drerio'). ~/software/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org); 34 s.add_attribute_to_xml('mgi_symbol'); 35 else:; ---> 36 logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True); 37 return None; 38 s.add_attribute_to_xml('chromosome_name'). NameError: name 'logg' is not defined; ```; It seems to me like `queries/__init__.py` misses an `from .. import logging as logg` statement. Would maybe also make sense to show the the message that an organism is not available at verbosity level 1 instead of 4?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/258
https://github.com/scverse/scanpy/issues/258:897,Testability,log,logg,897,"I just tried; ```python; import scanpy.api as sc; sc.queries.mitochondrial_genes('www.ensembl.org', 'strange_organism'); ```; I would expect scanpy complains that it does not know `'strange_organism'`, but I get the error ; ```python; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-13-6a41b361ab41> in <module>(); 1 import scanpy.api as sc; ----> 2 sc.queries.mitochondrial_genes('www.ensembl.org', 'drerio'). ~/software/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org); 34 s.add_attribute_to_xml('mgi_symbol'); 35 else:; ---> 36 logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True); 37 return None; 38 s.add_attribute_to_xml('chromosome_name'). NameError: name 'logg' is not defined; ```; It seems to me like `queries/__init__.py` misses an `from .. import logging as logg` statement. Would maybe also make sense to show the the message that an organism is not available at verbosity level 1 instead of 4?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/258
https://github.com/scverse/scanpy/pull/261:116,Deployability,install,installation,116,"Hey all,. I added scanpy as a bioconda package (https://github.com/bioconda/bioconda-recipes/pull/10863) along with installation instructions and a bioconda badge. The badge link on the README.rst doesn't work yet, it will 404 until the bioconda webpage gets auto-updated, but installing via bioconda does.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/261
https://github.com/scverse/scanpy/pull/261:264,Deployability,update,updated,264,"Hey all,. I added scanpy as a bioconda package (https://github.com/bioconda/bioconda-recipes/pull/10863) along with installation instructions and a bioconda badge. The badge link on the README.rst doesn't work yet, it will 404 until the bioconda webpage gets auto-updated, but installing via bioconda does.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/261
https://github.com/scverse/scanpy/pull/261:277,Deployability,install,installing,277,"Hey all,. I added scanpy as a bioconda package (https://github.com/bioconda/bioconda-recipes/pull/10863) along with installation instructions and a bioconda badge. The badge link on the README.rst doesn't work yet, it will 404 until the bioconda webpage gets auto-updated, but installing via bioconda does.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/261
https://github.com/scverse/scanpy/issues/263:76,Availability,error,error,76,"I'm trying to plot on a view of an on-disk AnnData object, and it throws an error. Not sure if this is meant to be a supported feature, but I gave it a go. Here's a little example to reproduce:. ```python; adata = sc.AnnData(X=np.random.binomial(100, .01, (100, 100))); adata.obs_names = adata.obs_names.astype(str); # Both these work; sc.pp.pca(adata); sc.pl.pca(adata[:, :5], color=""0""); adata.write(""tmp.h5ad""); adata_backed = sc.read(""tmp.h5ad"", backed=""r""); sc.pl.pca(adata_backed, color=""0"") # this works; sc.pl.pca(adata_backed[:, :5], color=""0"") # this throws an error; ```. <details>; <summary> traceback (pretty long) </summary>. ```python; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); h5py/_objects.pyx in h5py._objects.ObjectID.__dealloc__(). KeyError: 0. Exception ignored in: 'h5py._objects.ObjectID.__dealloc__'; Traceback (most recent call last):; File ""h5py/_objects.pyx"", line 200, in h5py._objects.ObjectID.__dealloc__; KeyError: 0; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-255f06b48663> in <module>(); 1 adata_backed = sc.read(""tmp.h5ad"", backed=""r""); 2 sc.pl.pca(adata_backed, color=""0""); ----> 3 sc.pl.pca(adata_backed[:, :5], color=""0""). /usr/local/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:571,Availability,error,error,571,"I'm trying to plot on a view of an on-disk AnnData object, and it throws an error. Not sure if this is meant to be a supported feature, but I gave it a go. Here's a little example to reproduce:. ```python; adata = sc.AnnData(X=np.random.binomial(100, .01, (100, 100))); adata.obs_names = adata.obs_names.astype(str); # Both these work; sc.pp.pca(adata); sc.pl.pca(adata[:, :5], color=""0""); adata.write(""tmp.h5ad""); adata_backed = sc.read(""tmp.h5ad"", backed=""r""); sc.pl.pca(adata_backed, color=""0"") # this works; sc.pl.pca(adata_backed[:, :5], color=""0"") # this throws an error; ```. <details>; <summary> traceback (pretty long) </summary>. ```python; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); h5py/_objects.pyx in h5py._objects.ObjectID.__dealloc__(). KeyError: 0. Exception ignored in: 'h5py._objects.ObjectID.__dealloc__'; Traceback (most recent call last):; File ""h5py/_objects.pyx"", line 200, in h5py._objects.ObjectID.__dealloc__; KeyError: 0; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-255f06b48663> in <module>(); 1 adata_backed = sc.read(""tmp.h5ad"", backed=""r""); 2 sc.pl.pca(adata_backed, color=""0""); ----> 3 sc.pl.pca(adata_backed[:, :5], color=""0""). /usr/local/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:4021,Deployability,update,updated,4021," _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 674 if not isinstance(X, AnnData):; 675 raise ValueError('`X` has to be an AnnData object.'); --> 676 self._init_as_view(X, oidx, vidx); 677 else:; 678 self._init_as_actual(. /usr/local/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 705 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')); 706 # hackish solution here, no copy should be necessary; --> 707 uns_new = deepcopy(self._adata_ref._uns); 708 # need to do the slicing before setting the updated self._n_obs, self._n_vars; 709 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/copy.py in deepcopy(x, memo, _nil); 178 y = x; 179 else:; --> 180 y = _reconstruct(x, memo, *rv); 181 ; 182 # If is its own copy, don't memoize. /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy); 278 if state is not None:; 279 if deep:; --> 280 state = deepcopy(state, memo); 281 if hasattr(y, '__setstate__'):; 282 y.__setstate__(state). /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/copy.py in deepcopy(x, memo, _nil); 148 copier = _deepcopy_dispatch.get(cls); 149 if copier:; --> 150 y = copier(x, memo); 151 else:; 152 try:. /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/copy.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:1868,Modifiability,layers,layers,1868,"__dealloc__; KeyError: 0; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-255f06b48663> in <module>(); 1 adata_backed = sc.read(""tmp.h5ad"", backed=""r""); 2 sc.pl.pca(adata_backed, color=""0""); ----> 3 sc.pl.pca(adata_backed[:, :5], color=""0""). /usr/local/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:2327,Modifiability,layers,layers,2327," pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:2621,Modifiability,layers,layers,2621,"--> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 674 if not isinstance(X, AnnData):; 675 raise ValueError('`X` has to be an AnnData object.'); --> 676 self._init_as_view(X, oidx, vidx);",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:2657,Modifiability,layers,layers,2657,"ls.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 674 if not isinstance(X, AnnData):; 675 raise ValueError('`X` has to be an AnnData object.'); --> 676 self._init_as_view(X, oidx, vidx); 677 else:; 678 self._init_as_actua",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:2664,Modifiability,layers,layers,2664,"ls.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 674 if not isinstance(X, AnnData):; 675 raise ValueError('`X` has to be an AnnData object.'); --> 676 self._init_as_view(X, oidx, vidx); 677 else:; 678 self._init_as_actua",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:3433,Modifiability,layers,layers,3433,"ft_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 674 if not isinstance(X, AnnData):; 675 raise ValueError('`X` has to be an AnnData object.'); --> 676 self._init_as_view(X, oidx, vidx); 677 else:; 678 self._init_as_actual(. /usr/local/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 705 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')); 706 # hackish solution here, no copy should be necessary; --> 707 uns_new = deepcopy(self._adata_ref._uns); 708 # need to do the slicing before setting the updated self._n_obs, self._n_vars; 709 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/copy.py in deepcopy(x, memo, _nil); 178 y = x; 179 else:; --> 180 y = _reconstruct(x, memo, *rv); 181 ; 182 # If is its own copy, don't memoize. /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/copy.py in _reconstruct(x, memo, func, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/264:123,Availability,error,error,123,"Hi,; I am trying to evaluate a PCA on a dataset with a very low number of Principal Components. I am getting the following error when trying to plot the PCA:; ```python; sc.pl.pca_variance_ratio(adata_h, log=True, save=True); ```; Result:; ```; /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py:530: RuntimeWarning: divide by zero encountered in log; if log: scores = np.log(scores). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-48cc676a34cc> in <module>(); 1 # log is natural logarithm; ----> 2 sc.pl.pca_variance_ratio(adata_h, log=True, save=True). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca_variance_ratio(adata, log, show, save); 157 default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.; 158 """"""; --> 159 ranking(adata, 'uns', 'variance_ratio', dictionary='pca', labels='PC', log=log); 160 utils.savefig_or_show('pca_variance_ratio', show=show, save=save); 161 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py in ranking(adata, attr, keys, dictionary, indices, labels, color, n_points, log, show); 555 score_min, score_max = np.min(score[indices]), np.max(score[indices]); 556 pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,; --> 557 (1.05 if score_max > 0 else 0.95) * score_max); 558 if show == False: return gs; 559 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py in ylim(*args, **kwargs); 1588 if not args and not kwargs:; 1589 return ax.get_ylim(); -> 1590 ret = ax.set_ylim(*args, **kwargs); 1591 return ret; 1592 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in set_ylim(self, bottom, top, emit, auto, **kw); 3455 bottom, top = bot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:6741,Deployability,update,update,6741," 127 if fmt == 'svg':. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs); 2261 orientation=orientation,; 2262 bbox_inches_restore=_bbox_inches_restore,; -> 2263 **kwargs); 2264 finally:; 2265 if bbox_inches and restore_bbox:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, *args, **kwargs); 515 ; 516 def print_png(self, filename_or_obj, *args, **kwargs):; --> 517 FigureCanvasAgg.draw(self); 518 renderer = self.get_renderer(); 519 original_dpi = renderer.dpi. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py in draw(self); 435 # if toolbar:; 436 # toolbar.set_cursor(cursors.WAIT); --> 437 self.figure.draw(self.renderer); 438 # A GUI class may be need to update a window using this draw, so; 439 # don't forget to call the superclass. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs); 53 renderer.start_filter(); 54 ; ---> 55 return draw(artist, renderer, *args, **kwargs); 56 finally:; 57 if artist.get_agg_filter() is not None:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/figure.py in draw(self, renderer); 1491 ; 1492 mimage._draw_list_compositing_images(; -> 1493 renderer, self, artists, self.suppressComposite); 1494 ; 1495 renderer.close_group('figure'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite); 139 if not_composite or not has_images:; 140 for a in artists:; --> 141 a.draw(renderer); 142 else:; 143 # Composite any adjacent images together. ~/.pyenv/versions/3.6.5/Py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:4127,Energy Efficiency,reduce,reduce,4127,"posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values. /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce; return ufunc.reduce(obj, axis, dtype, out, **passkwargs). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 339 pass; 340 else:; --> 341 return printer(obj); 342 # Finally look for special method names; 343 method = get_real_method(obj, self.print_method). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/pylabtools.py in <lambda>(fig); 241 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)); 242 if 'retina' in formats or 'png2x' in formats:; --> 243 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)); 244 if 'jpg' in formats or 'jpeg' in formats:; 245 jpg_formatter.for_type(Figure, lambda fig: print_figure(fig, 'jpg', **kwargs)). ~/.pyenv/versions/3.6.5/Python.framewor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:4148,Energy Efficiency,reduce,reduce,4148,"e values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values. /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce; return ufunc.reduce(obj, axis, dtype, out, **passkwargs). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 339 pass; 340 else:; --> 341 return printer(obj); 342 # Finally look for special method names; 343 method = get_real_method(obj, self.print_method). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/pylabtools.py in <lambda>(fig); 241 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)); 242 if 'retina' in formats or 'png2x' in formats:; --> 243 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)); 244 if 'jpg' in formats or 'jpeg' in formats:; 245 jpg_formatter.for_type(Figure, lambda fig: print_figure(fig, 'jpg', **kwargs)). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:204,Testability,log,log,204,"Hi,; I am trying to evaluate a PCA on a dataset with a very low number of Principal Components. I am getting the following error when trying to plot the PCA:; ```python; sc.pl.pca_variance_ratio(adata_h, log=True, save=True); ```; Result:; ```; /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py:530: RuntimeWarning: divide by zero encountered in log; if log: scores = np.log(scores). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-48cc676a34cc> in <module>(); 1 # log is natural logarithm; ----> 2 sc.pl.pca_variance_ratio(adata_h, log=True, save=True). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca_variance_ratio(adata, log, show, save); 157 default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.; 158 """"""; --> 159 ranking(adata, 'uns', 'variance_ratio', dictionary='pca', labels='PC', log=log); 160 utils.savefig_or_show('pca_variance_ratio', show=show, save=save); 161 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py in ranking(adata, attr, keys, dictionary, indices, labels, color, n_points, log, show); 555 score_min, score_max = np.min(score[indices]), np.max(score[indices]); 556 pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,; --> 557 (1.05 if score_max > 0 else 0.95) * score_max); 558 if show == False: return gs; 559 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py in ylim(*args, **kwargs); 1588 if not args and not kwargs:; 1589 return ax.get_ylim(); -> 1590 ret = ax.set_ylim(*args, **kwargs); 1591 return ret; 1592 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in set_ylim(self, bottom, top, emit, auto, **kw); 3455 bottom, top = bot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:415,Testability,log,log,415,"Hi,; I am trying to evaluate a PCA on a dataset with a very low number of Principal Components. I am getting the following error when trying to plot the PCA:; ```python; sc.pl.pca_variance_ratio(adata_h, log=True, save=True); ```; Result:; ```; /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py:530: RuntimeWarning: divide by zero encountered in log; if log: scores = np.log(scores). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-48cc676a34cc> in <module>(); 1 # log is natural logarithm; ----> 2 sc.pl.pca_variance_ratio(adata_h, log=True, save=True). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca_variance_ratio(adata, log, show, save); 157 default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.; 158 """"""; --> 159 ranking(adata, 'uns', 'variance_ratio', dictionary='pca', labels='PC', log=log); 160 utils.savefig_or_show('pca_variance_ratio', show=show, save=save); 161 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py in ranking(adata, attr, keys, dictionary, indices, labels, color, n_points, log, show); 555 score_min, score_max = np.min(score[indices]), np.max(score[indices]); 556 pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,; --> 557 (1.05 if score_max > 0 else 0.95) * score_max); 558 if show == False: return gs; 559 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py in ylim(*args, **kwargs); 1588 if not args and not kwargs:; 1589 return ax.get_ylim(); -> 1590 ret = ax.set_ylim(*args, **kwargs); 1591 return ret; 1592 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in set_ylim(self, bottom, top, emit, auto, **kw); 3455 bottom, top = bot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:423,Testability,log,log,423,"Hi,; I am trying to evaluate a PCA on a dataset with a very low number of Principal Components. I am getting the following error when trying to plot the PCA:; ```python; sc.pl.pca_variance_ratio(adata_h, log=True, save=True); ```; Result:; ```; /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py:530: RuntimeWarning: divide by zero encountered in log; if log: scores = np.log(scores). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-48cc676a34cc> in <module>(); 1 # log is natural logarithm; ----> 2 sc.pl.pca_variance_ratio(adata_h, log=True, save=True). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca_variance_ratio(adata, log, show, save); 157 default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.; 158 """"""; --> 159 ranking(adata, 'uns', 'variance_ratio', dictionary='pca', labels='PC', log=log); 160 utils.savefig_or_show('pca_variance_ratio', show=show, save=save); 161 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py in ranking(adata, attr, keys, dictionary, indices, labels, color, n_points, log, show); 555 score_min, score_max = np.min(score[indices]), np.max(score[indices]); 556 pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,; --> 557 (1.05 if score_max > 0 else 0.95) * score_max); 558 if show == False: return gs; 559 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py in ylim(*args, **kwargs); 1588 if not args and not kwargs:; 1589 return ax.get_ylim(); -> 1590 ret = ax.set_ylim(*args, **kwargs); 1591 return ret; 1592 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in set_ylim(self, bottom, top, emit, auto, **kw); 3455 bottom, top = bot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:440,Testability,log,log,440,"Hi,; I am trying to evaluate a PCA on a dataset with a very low number of Principal Components. I am getting the following error when trying to plot the PCA:; ```python; sc.pl.pca_variance_ratio(adata_h, log=True, save=True); ```; Result:; ```; /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py:530: RuntimeWarning: divide by zero encountered in log; if log: scores = np.log(scores). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-48cc676a34cc> in <module>(); 1 # log is natural logarithm; ----> 2 sc.pl.pca_variance_ratio(adata_h, log=True, save=True). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca_variance_ratio(adata, log, show, save); 157 default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.; 158 """"""; --> 159 ranking(adata, 'uns', 'variance_ratio', dictionary='pca', labels='PC', log=log); 160 utils.savefig_or_show('pca_variance_ratio', show=show, save=save); 161 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py in ranking(adata, attr, keys, dictionary, indices, labels, color, n_points, log, show); 555 score_min, score_max = np.min(score[indices]), np.max(score[indices]); 556 pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,; --> 557 (1.05 if score_max > 0 else 0.95) * score_max); 558 if show == False: return gs; 559 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py in ylim(*args, **kwargs); 1588 if not args and not kwargs:; 1589 return ax.get_ylim(); -> 1590 ret = ax.set_ylim(*args, **kwargs); 1591 return ret; 1592 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in set_ylim(self, bottom, top, emit, auto, **kw); 3455 bottom, top = bot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:626,Testability,log,log,626,"Hi,; I am trying to evaluate a PCA on a dataset with a very low number of Principal Components. I am getting the following error when trying to plot the PCA:; ```python; sc.pl.pca_variance_ratio(adata_h, log=True, save=True); ```; Result:; ```; /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py:530: RuntimeWarning: divide by zero encountered in log; if log: scores = np.log(scores). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-48cc676a34cc> in <module>(); 1 # log is natural logarithm; ----> 2 sc.pl.pca_variance_ratio(adata_h, log=True, save=True). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca_variance_ratio(adata, log, show, save); 157 default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.; 158 """"""; --> 159 ranking(adata, 'uns', 'variance_ratio', dictionary='pca', labels='PC', log=log); 160 utils.savefig_or_show('pca_variance_ratio', show=show, save=save); 161 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py in ranking(adata, attr, keys, dictionary, indices, labels, color, n_points, log, show); 555 score_min, score_max = np.min(score[indices]), np.max(score[indices]); 556 pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,; --> 557 (1.05 if score_max > 0 else 0.95) * score_max); 558 if show == False: return gs; 559 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py in ylim(*args, **kwargs); 1588 if not args and not kwargs:; 1589 return ax.get_ylim(); -> 1590 ret = ax.set_ylim(*args, **kwargs); 1591 return ret; 1592 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in set_ylim(self, bottom, top, emit, auto, **kw); 3455 bottom, top = bot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:641,Testability,log,logarithm,641,"Hi,; I am trying to evaluate a PCA on a dataset with a very low number of Principal Components. I am getting the following error when trying to plot the PCA:; ```python; sc.pl.pca_variance_ratio(adata_h, log=True, save=True); ```; Result:; ```; /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py:530: RuntimeWarning: divide by zero encountered in log; if log: scores = np.log(scores). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-48cc676a34cc> in <module>(); 1 # log is natural logarithm; ----> 2 sc.pl.pca_variance_ratio(adata_h, log=True, save=True). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca_variance_ratio(adata, log, show, save); 157 default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.; 158 """"""; --> 159 ranking(adata, 'uns', 'variance_ratio', dictionary='pca', labels='PC', log=log); 160 utils.savefig_or_show('pca_variance_ratio', show=show, save=save); 161 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py in ranking(adata, attr, keys, dictionary, indices, labels, color, n_points, log, show); 555 score_min, score_max = np.min(score[indices]), np.max(score[indices]); 556 pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,; --> 557 (1.05 if score_max > 0 else 0.95) * score_max); 558 if show == False: return gs; 559 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py in ylim(*args, **kwargs); 1588 if not args and not kwargs:; 1589 return ax.get_ylim(); -> 1590 ret = ax.set_ylim(*args, **kwargs); 1591 return ret; 1592 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in set_ylim(self, bottom, top, emit, auto, **kw); 3455 bottom, top = bot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:694,Testability,log,log,694,"Hi,; I am trying to evaluate a PCA on a dataset with a very low number of Principal Components. I am getting the following error when trying to plot the PCA:; ```python; sc.pl.pca_variance_ratio(adata_h, log=True, save=True); ```; Result:; ```; /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py:530: RuntimeWarning: divide by zero encountered in log; if log: scores = np.log(scores). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-48cc676a34cc> in <module>(); 1 # log is natural logarithm; ----> 2 sc.pl.pca_variance_ratio(adata_h, log=True, save=True). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca_variance_ratio(adata, log, show, save); 157 default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.; 158 """"""; --> 159 ranking(adata, 'uns', 'variance_ratio', dictionary='pca', labels='PC', log=log); 160 utils.savefig_or_show('pca_variance_ratio', show=show, save=save); 161 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py in ranking(adata, attr, keys, dictionary, indices, labels, color, n_points, log, show); 555 score_min, score_max = np.min(score[indices]), np.max(score[indices]); 556 pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,; --> 557 (1.05 if score_max > 0 else 0.95) * score_max); 558 if show == False: return gs; 559 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py in ylim(*args, **kwargs); 1588 if not args and not kwargs:; 1589 return ax.get_ylim(); -> 1590 ret = ax.set_ylim(*args, **kwargs); 1591 return ret; 1592 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in set_ylim(self, bottom, top, emit, auto, **kw); 3455 bottom, top = bot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:861,Testability,log,log,861,"Hi,; I am trying to evaluate a PCA on a dataset with a very low number of Principal Components. I am getting the following error when trying to plot the PCA:; ```python; sc.pl.pca_variance_ratio(adata_h, log=True, save=True); ```; Result:; ```; /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py:530: RuntimeWarning: divide by zero encountered in log; if log: scores = np.log(scores). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-48cc676a34cc> in <module>(); 1 # log is natural logarithm; ----> 2 sc.pl.pca_variance_ratio(adata_h, log=True, save=True). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca_variance_ratio(adata, log, show, save); 157 default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.; 158 """"""; --> 159 ranking(adata, 'uns', 'variance_ratio', dictionary='pca', labels='PC', log=log); 160 utils.savefig_or_show('pca_variance_ratio', show=show, save=save); 161 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py in ranking(adata, attr, keys, dictionary, indices, labels, color, n_points, log, show); 555 score_min, score_max = np.min(score[indices]), np.max(score[indices]); 556 pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,; --> 557 (1.05 if score_max > 0 else 0.95) * score_max); 558 if show == False: return gs; 559 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py in ylim(*args, **kwargs); 1588 if not args and not kwargs:; 1589 return ax.get_ylim(); -> 1590 ret = ax.set_ylim(*args, **kwargs); 1591 return ret; 1592 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in set_ylim(self, bottom, top, emit, auto, **kw); 3455 bottom, top = bot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:1050,Testability,log,log,1050,"g to evaluate a PCA on a dataset with a very low number of Principal Components. I am getting the following error when trying to plot the PCA:; ```python; sc.pl.pca_variance_ratio(adata_h, log=True, save=True); ```; Result:; ```; /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py:530: RuntimeWarning: divide by zero encountered in log; if log: scores = np.log(scores). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-48cc676a34cc> in <module>(); 1 # log is natural logarithm; ----> 2 sc.pl.pca_variance_ratio(adata_h, log=True, save=True). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca_variance_ratio(adata, log, show, save); 157 default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.; 158 """"""; --> 159 ranking(adata, 'uns', 'variance_ratio', dictionary='pca', labels='PC', log=log); 160 utils.savefig_or_show('pca_variance_ratio', show=show, save=save); 161 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py in ranking(adata, attr, keys, dictionary, indices, labels, color, n_points, log, show); 555 score_min, score_max = np.min(score[indices]), np.max(score[indices]); 556 pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,; --> 557 (1.05 if score_max > 0 else 0.95) * score_max); 558 if show == False: return gs; 559 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py in ylim(*args, **kwargs); 1588 if not args and not kwargs:; 1589 return ax.get_ylim(); -> 1590 ret = ax.set_ylim(*args, **kwargs); 1591 return ret; 1592 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in set_ylim(self, bottom, top, emit, auto, **kw); 3455 bottom, top = bottom; 3456 ; ->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:1054,Testability,log,log,1054,"g to evaluate a PCA on a dataset with a very low number of Principal Components. I am getting the following error when trying to plot the PCA:; ```python; sc.pl.pca_variance_ratio(adata_h, log=True, save=True); ```; Result:; ```; /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py:530: RuntimeWarning: divide by zero encountered in log; if log: scores = np.log(scores). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-48cc676a34cc> in <module>(); 1 # log is natural logarithm; ----> 2 sc.pl.pca_variance_ratio(adata_h, log=True, save=True). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca_variance_ratio(adata, log, show, save); 157 default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.; 158 """"""; --> 159 ranking(adata, 'uns', 'variance_ratio', dictionary='pca', labels='PC', log=log); 160 utils.savefig_or_show('pca_variance_ratio', show=show, save=save); 161 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py in ranking(adata, attr, keys, dictionary, indices, labels, color, n_points, log, show); 555 score_min, score_max = np.min(score[indices]), np.max(score[indices]); 556 pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,; --> 557 (1.05 if score_max > 0 else 0.95) * score_max); 558 if show == False: return gs; 559 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py in ylim(*args, **kwargs); 1588 if not args and not kwargs:; 1589 return ax.get_ylim(); -> 1590 ret = ax.set_ylim(*args, **kwargs); 1591 return ret; 1592 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in set_ylim(self, bottom, top, emit, auto, **kw); 3455 bottom, top = bottom; 3456 ; ->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:1322,Testability,log,log,1322,"s/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py:530: RuntimeWarning: divide by zero encountered in log; if log: scores = np.log(scores). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-48cc676a34cc> in <module>(); 1 # log is natural logarithm; ----> 2 sc.pl.pca_variance_ratio(adata_h, log=True, save=True). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca_variance_ratio(adata, log, show, save); 157 default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.; 158 """"""; --> 159 ranking(adata, 'uns', 'variance_ratio', dictionary='pca', labels='PC', log=log); 160 utils.savefig_or_show('pca_variance_ratio', show=show, save=save); 161 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/anndata.py in ranking(adata, attr, keys, dictionary, indices, labels, color, n_points, log, show); 555 score_min, score_max = np.min(score[indices]), np.max(score[indices]); 556 pl.ylim((0.95 if score_min > 0 else 1.05) * score_min,; --> 557 (1.05 if score_max > 0 else 0.95) * score_max); 558 if show == False: return gs; 559 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py in ylim(*args, **kwargs); 1588 if not args and not kwargs:; 1589 return ax.get_ylim(); -> 1590 ret = ax.set_ylim(*args, **kwargs); 1591 return ret; 1592 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in set_ylim(self, bottom, top, emit, auto, **kw); 3455 bottom, top = bottom; 3456 ; -> 3457 bottom = self._validate_converted_limits(bottom, self.convert_yunits); 3458 top = self._validate_converted_limits(top, self.convert_yunits); 3459 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py in _validate_converted_limit",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:11300,Testability,log,log,11300,"ist(self.iter_ticks()) # iter_ticks calls the locator; 1029 if self._smart_bounds and tick_tups:; 1030 # handle inverted limits. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axis.py in iter_ticks(self); 969 Iterate through all of the major and minor ticks.; 970 """"""; --> 971 majorLocs = self.major.locator(); 972 majorTicks = self.get_major_ticks(len(majorLocs)); 973 self.major.formatter.set_locs(majorLocs). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/ticker.py in __call__(self); 1952 def __call__(self):; 1953 vmin, vmax = self.axis.get_view_interval(); -> 1954 return self.tick_values(vmin, vmax); 1955 ; 1956 def tick_values(self, vmin, vmax):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/ticker.py in tick_values(self, vmin, vmax); 1960 vmin, vmax = mtransforms.nonsingular(; 1961 vmin, vmax, expander=1e-13, tiny=1e-14); -> 1962 locs = self._raw_ticks(vmin, vmax); 1963 ; 1964 prune = self._prune. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/ticker.py in _raw_ticks(self, vmin, vmax); 1906 if self._nbins == 'auto':; 1907 if self.axis is not None:; -> 1908 nbins = np.clip(self.axis.get_tick_space(),; 1909 max(1, self._min_n_ticks - 1), 9); 1910 else:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axis.py in get_tick_space(self); 2127 size = tick.label1.get_size() * 3; 2128 if size > 0:; -> 2129 return int(np.floor(length / size)); 2130 else:; 2131 return 2**31 - 1. ValueError: cannot convert float NaN to integer. <Figure size 800x800 with 1 Axes>; ```. I think the problem is that the variance explained for some components is 0, so taking the log (with `log=True` will ask scanpy to plot the variance explained starting from minus infinity. Would it be possible to plot only the non-zero variance components as a default?. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:11311,Testability,log,log,11311,"ist(self.iter_ticks()) # iter_ticks calls the locator; 1029 if self._smart_bounds and tick_tups:; 1030 # handle inverted limits. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axis.py in iter_ticks(self); 969 Iterate through all of the major and minor ticks.; 970 """"""; --> 971 majorLocs = self.major.locator(); 972 majorTicks = self.get_major_ticks(len(majorLocs)); 973 self.major.formatter.set_locs(majorLocs). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/ticker.py in __call__(self); 1952 def __call__(self):; 1953 vmin, vmax = self.axis.get_view_interval(); -> 1954 return self.tick_values(vmin, vmax); 1955 ; 1956 def tick_values(self, vmin, vmax):. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/ticker.py in tick_values(self, vmin, vmax); 1960 vmin, vmax = mtransforms.nonsingular(; 1961 vmin, vmax, expander=1e-13, tiny=1e-14); -> 1962 locs = self._raw_ticks(vmin, vmax); 1963 ; 1964 prune = self._prune. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/ticker.py in _raw_ticks(self, vmin, vmax); 1906 if self._nbins == 'auto':; 1907 if self.axis is not None:; -> 1908 nbins = np.clip(self.axis.get_tick_space(),; 1909 max(1, self._min_n_ticks - 1), 9); 1910 else:. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axis.py in get_tick_space(self); 2127 size = tick.label1.get_size() * 3; 2128 if size > 0:; -> 2129 return int(np.floor(length / size)); 2130 else:; 2131 return 2**31 - 1. ValueError: cannot convert float NaN to integer. <Figure size 800x800 with 1 Axes>; ```. I think the problem is that the variance explained for some components is 0, so taking the log (with `log=True` will ask scanpy to plot the variance explained starting from minus infinity. Would it be possible to plot only the non-zero variance components as a default?. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/265:26,Deployability,integrat,integrate,26,"Dear, . Is it possible to integrate scanpy with CCA and pyscenic?; CCA (canonical correlation analysis to alignment different datasets and batch effect correction):; https://satijalab.org/seurat/immune_alignment.html. pyscenic (single-cell regulatory network inference and clustering):; https://github.com/aertslab/pySCENIC",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:26,Integrability,integrat,integrate,26,"Dear, . Is it possible to integrate scanpy with CCA and pyscenic?; CCA (canonical correlation analysis to alignment different datasets and batch effect correction):; https://satijalab.org/seurat/immune_alignment.html. pyscenic (single-cell regulatory network inference and clustering):; https://github.com/aertslab/pySCENIC",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/pull/266:155,Usability,simpl,simply,155,"Else `argmax([])` is called later, which doesn’t make sense. This happens because `Dseg[tips[0]] + Dseg[tips[1]]` are all `inf` for some datasets. Is that simply a consequence of my data or is that a bug?. This PR assumes the former and results in such segments being skipped. In case of my data, this means that I end up with a single DPT group instead of multiple ones.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/266
https://github.com/scverse/scanpy/pull/270:2,Deployability,update,updated,2,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:655,Performance,optimiz,optimized,655,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:936,Safety,avoid,avoid,936,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:66,Testability,test,tests,66,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:98,Testability,test,test,98,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:185,Testability,test,test,185,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:226,Testability,test,test,226,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:300,Testability,test,test,300,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:407,Testability,test,test,407,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:595,Testability,test,tested,595,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:472,Usability,simpl,simplify,472,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/issues/271:450,Deployability,install,installed,450,"We should have a discussion about this separate from #265. There’s three options how to implement them. 1. Python has a built-in way of registering “entry points” which might be suitable. [A tutorial](https://amir.rachum.com/blog/2017/07/28/python-entry-points/). ```py; setup(; # ...; entry_points={; 'scanpy.extensions': ['myextension = my.extension.module'],; },; ); ```. 2. Some projects have a module like `scanpy.ext`. An extension is a module installed into there (which is possible in python):. ```py; setup(; name='scanpy-ext-myextension',; # ...; packages=['scanpy.ext.myextension'],; ); ```. 3. Some projects provide a config value like `scanpy.settings.extensions` to which module names can be `append`ed. Unlike the first two, this means the user has to enable them explicitly. Pytest uses [both 1 and 3](https://docs.pytest.org/en/latest/writing_plugins.html) for its plugins. Sphinx uses [both 1 and 3](http://www.sphinx-doc.org/en/master/extdev/index.html) for different things: they want their “builders” to be automatically discovered and the other extensions to be explicitly enabled. ---. Another option for 2. is to use [flask’s approach](http://flask.pocoo.org/docs/1.0/extensiondev/) of just naming modules `scanpy_*` which is too cute and magic to me. Veto.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:630,Modifiability,config,config,630,"We should have a discussion about this separate from #265. There’s three options how to implement them. 1. Python has a built-in way of registering “entry points” which might be suitable. [A tutorial](https://amir.rachum.com/blog/2017/07/28/python-entry-points/). ```py; setup(; # ...; entry_points={; 'scanpy.extensions': ['myextension = my.extension.module'],; },; ); ```. 2. Some projects have a module like `scanpy.ext`. An extension is a module installed into there (which is possible in python):. ```py; setup(; name='scanpy-ext-myextension',; # ...; packages=['scanpy.ext.myextension'],; ); ```. 3. Some projects provide a config value like `scanpy.settings.extensions` to which module names can be `append`ed. Unlike the first two, this means the user has to enable them explicitly. Pytest uses [both 1 and 3](https://docs.pytest.org/en/latest/writing_plugins.html) for its plugins. Sphinx uses [both 1 and 3](http://www.sphinx-doc.org/en/master/extdev/index.html) for different things: they want their “builders” to be automatically discovered and the other extensions to be explicitly enabled. ---. Another option for 2. is to use [flask’s approach](http://flask.pocoo.org/docs/1.0/extensiondev/) of just naming modules `scanpy_*` which is too cute and magic to me. Veto.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:882,Modifiability,plugin,plugins,882,"We should have a discussion about this separate from #265. There’s three options how to implement them. 1. Python has a built-in way of registering “entry points” which might be suitable. [A tutorial](https://amir.rachum.com/blog/2017/07/28/python-entry-points/). ```py; setup(; # ...; entry_points={; 'scanpy.extensions': ['myextension = my.extension.module'],; },; ); ```. 2. Some projects have a module like `scanpy.ext`. An extension is a module installed into there (which is possible in python):. ```py; setup(; name='scanpy-ext-myextension',; # ...; packages=['scanpy.ext.myextension'],; ); ```. 3. Some projects provide a config value like `scanpy.settings.extensions` to which module names can be `append`ed. Unlike the first two, this means the user has to enable them explicitly. Pytest uses [both 1 and 3](https://docs.pytest.org/en/latest/writing_plugins.html) for its plugins. Sphinx uses [both 1 and 3](http://www.sphinx-doc.org/en/master/extdev/index.html) for different things: they want their “builders” to be automatically discovered and the other extensions to be explicitly enabled. ---. Another option for 2. is to use [flask’s approach](http://flask.pocoo.org/docs/1.0/extensiondev/) of just naming modules `scanpy_*` which is too cute and magic to me. Veto.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/273:192,Testability,log,logreg,192,"Hi,. Reordering the categories of groups in obs leads to shuffling of marker genes to the wrong groups when using `sc.tl.rank_genes_groups`. Interestingly, this only happens if I use `method='logreg`. It works fine with `method='t-test`. I am not sure how relevant this is. Maybe, one should not reorder categories. However, this was helpful to me when I wanted to plot the categories in a certain order. Here comes a minimal example:. ```; import pandas as pd; import scanpy.api as sc. adata = sc.datasets.blobs(640, 3); sc.tl.pca(adata); sc.pp.neighbors(adata); sc.tl.louvain(adata). method='logreg'. sc.tl.rank_genes_groups(adata, 'louvain', method=method); print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). adata.obs['louvain'].cat.reorder_categories(['2', '1', '0'], inplace = True). sc.tl.rank_genes_groups(adata, 'louvain', method=method); print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)); ```; The result looks like this:; ```python; 0 1 2; 0 570 63 126; 2 1 0; 0 570 63 126; ```; The expected result would be that the marker genes end up in the same groups as above (watch the change in the lowest row):; ```python; 0 1 2; 0 570 63 126; 2 1 0; 0 126 63 570; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273
https://github.com/scverse/scanpy/issues/273:231,Testability,test,test,231,"Hi,. Reordering the categories of groups in obs leads to shuffling of marker genes to the wrong groups when using `sc.tl.rank_genes_groups`. Interestingly, this only happens if I use `method='logreg`. It works fine with `method='t-test`. I am not sure how relevant this is. Maybe, one should not reorder categories. However, this was helpful to me when I wanted to plot the categories in a certain order. Here comes a minimal example:. ```; import pandas as pd; import scanpy.api as sc. adata = sc.datasets.blobs(640, 3); sc.tl.pca(adata); sc.pp.neighbors(adata); sc.tl.louvain(adata). method='logreg'. sc.tl.rank_genes_groups(adata, 'louvain', method=method); print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). adata.obs['louvain'].cat.reorder_categories(['2', '1', '0'], inplace = True). sc.tl.rank_genes_groups(adata, 'louvain', method=method); print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)); ```; The result looks like this:; ```python; 0 1 2; 0 570 63 126; 2 1 0; 0 570 63 126; ```; The expected result would be that the marker genes end up in the same groups as above (watch the change in the lowest row):; ```python; 0 1 2; 0 570 63 126; 2 1 0; 0 126 63 570; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273
https://github.com/scverse/scanpy/issues/273:594,Testability,log,logreg,594,"Hi,. Reordering the categories of groups in obs leads to shuffling of marker genes to the wrong groups when using `sc.tl.rank_genes_groups`. Interestingly, this only happens if I use `method='logreg`. It works fine with `method='t-test`. I am not sure how relevant this is. Maybe, one should not reorder categories. However, this was helpful to me when I wanted to plot the categories in a certain order. Here comes a minimal example:. ```; import pandas as pd; import scanpy.api as sc. adata = sc.datasets.blobs(640, 3); sc.tl.pca(adata); sc.pp.neighbors(adata); sc.tl.louvain(adata). method='logreg'. sc.tl.rank_genes_groups(adata, 'louvain', method=method); print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). adata.obs['louvain'].cat.reorder_categories(['2', '1', '0'], inplace = True). sc.tl.rank_genes_groups(adata, 'louvain', method=method); print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)); ```; The result looks like this:; ```python; 0 1 2; 0 570 63 126; 2 1 0; 0 570 63 126; ```; The expected result would be that the marker genes end up in the same groups as above (watch the change in the lowest row):; ```python; 0 1 2; 0 570 63 126; 2 1 0; 0 126 63 570; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273
https://github.com/scverse/scanpy/issues/275:629,Modifiability,variab,variables,629,"I found a minor bug in this tutorial; [Clustering 3k PBMCs following a Seurat Tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). I hope this is the correct venue to post to regarding this. I'm currently going through this to learn how to use scanpy. In the first section; ```; path = './data/pbmc3k_filtered_gene_bc_matrices/hg19/'; adata = sc.read(path + 'matrix.mtx', cache=True).T # transpose the data; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; adata.obs_names = pd.read_csv(path + 'barcodes.tsv', header=None)[0]; ```. Due to how pandas dataframes indexes this part; ```; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ```; does not yield the expected results. As `var_names` becomes the index of `var` adding `genes[0]` will try to merge a data frame with unmatching index resulting in a `NaN` column in `var` for `'gene_ids'`. The solution should be either; ```; genes = genes.set_index(1); adata.var = genes; ```; or; ```; adata.var_names = genes[1]; genes = genes.set_index(1); adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ``` . It does probably not have any effect on the tutorial but I thought I'd mention it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275
https://github.com/scverse/scanpy/issues/275:939,Modifiability,variab,variables,939,"I found a minor bug in this tutorial; [Clustering 3k PBMCs following a Seurat Tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). I hope this is the correct venue to post to regarding this. I'm currently going through this to learn how to use scanpy. In the first section; ```; path = './data/pbmc3k_filtered_gene_bc_matrices/hg19/'; adata = sc.read(path + 'matrix.mtx', cache=True).T # transpose the data; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; adata.obs_names = pd.read_csv(path + 'barcodes.tsv', header=None)[0]; ```. Due to how pandas dataframes indexes this part; ```; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ```; does not yield the expected results. As `var_names` becomes the index of `var` adding `genes[0]` will try to merge a data frame with unmatching index resulting in a `NaN` column in `var` for `'gene_ids'`. The solution should be either; ```; genes = genes.set_index(1); adata.var = genes; ```; or; ```; adata.var_names = genes[1]; genes = genes.set_index(1); adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ``` . It does probably not have any effect on the tutorial but I thought I'd mention it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275
https://github.com/scverse/scanpy/issues/275:1392,Modifiability,variab,variables,1392,"I found a minor bug in this tutorial; [Clustering 3k PBMCs following a Seurat Tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). I hope this is the correct venue to post to regarding this. I'm currently going through this to learn how to use scanpy. In the first section; ```; path = './data/pbmc3k_filtered_gene_bc_matrices/hg19/'; adata = sc.read(path + 'matrix.mtx', cache=True).T # transpose the data; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; adata.obs_names = pd.read_csv(path + 'barcodes.tsv', header=None)[0]; ```. Due to how pandas dataframes indexes this part; ```; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ```; does not yield the expected results. As `var_names` becomes the index of `var` adding `genes[0]` will try to merge a data frame with unmatching index resulting in a `NaN` column in `var` for `'gene_ids'`. The solution should be either; ```; genes = genes.set_index(1); adata.var = genes; ```; or; ```; adata.var_names = genes[1]; genes = genes.set_index(1); adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ``` . It does probably not have any effect on the tutorial but I thought I'd mention it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275
https://github.com/scverse/scanpy/issues/275:428,Performance,cache,cache,428,"I found a minor bug in this tutorial; [Clustering 3k PBMCs following a Seurat Tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). I hope this is the correct venue to post to regarding this. I'm currently going through this to learn how to use scanpy. In the first section; ```; path = './data/pbmc3k_filtered_gene_bc_matrices/hg19/'; adata = sc.read(path + 'matrix.mtx', cache=True).T # transpose the data; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; adata.obs_names = pd.read_csv(path + 'barcodes.tsv', header=None)[0]; ```. Due to how pandas dataframes indexes this part; ```; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ```; does not yield the expected results. As `var_names` becomes the index of `var` adding `genes[0]` will try to merge a data frame with unmatching index resulting in a `NaN` column in `var` for `'gene_ids'`. The solution should be either; ```; genes = genes.set_index(1); adata.var = genes; ```; or; ```; adata.var_names = genes[1]; genes = genes.set_index(1); adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ``` . It does probably not have any effect on the tutorial but I thought I'd mention it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275
https://github.com/scverse/scanpy/issues/275:283,Usability,learn,learn,283,"I found a minor bug in this tutorial; [Clustering 3k PBMCs following a Seurat Tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). I hope this is the correct venue to post to regarding this. I'm currently going through this to learn how to use scanpy. In the first section; ```; path = './data/pbmc3k_filtered_gene_bc_matrices/hg19/'; adata = sc.read(path + 'matrix.mtx', cache=True).T # transpose the data; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; adata.obs_names = pd.read_csv(path + 'barcodes.tsv', header=None)[0]; ```. Due to how pandas dataframes indexes this part; ```; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ```; does not yield the expected results. As `var_names` becomes the index of `var` adding `genes[0]` will try to merge a data frame with unmatching index resulting in a `NaN` column in `var` for `'gene_ids'`. The solution should be either; ```; genes = genes.set_index(1); adata.var = genes; ```; or; ```; adata.var_names = genes[1]; genes = genes.set_index(1); adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ``` . It does probably not have any effect on the tutorial but I thought I'd mention it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275
https://github.com/scverse/scanpy/issues/276:337,Availability,error,errors,337,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276
https://github.com/scverse/scanpy/issues/276:386,Availability,error,errors,386,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276
https://github.com/scverse/scanpy/issues/276:463,Availability,error,errors,463,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276
https://github.com/scverse/scanpy/issues/276:521,Availability,error,errors,521,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276
https://github.com/scverse/scanpy/issues/276:97,Deployability,install,installing,97,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276
https://github.com/scverse/scanpy/issues/276:276,Deployability,install,install,276,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276
https://github.com/scverse/scanpy/issues/276:441,Deployability,install,install,441,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276
https://github.com/scverse/scanpy/issues/276:44,Energy Efficiency,power,powerful,44,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276
https://github.com/scverse/scanpy/issues/276:131,Testability,log,login-,131,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276
https://github.com/scverse/scanpy/issues/276:505,Usability,clear,clearly,505,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276
https://github.com/scverse/scanpy/issues/280:190,Availability,error,error,190,"Hi, I am reproducing this tutorial https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. the line sc.pp.neighbors(adata) produces the following error:. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!. Ubuntu 18.04; Python 3.6.6. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Can you help me? Thank You",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:212,Safety,detect,detected,212,"Hi, I am reproducing this tutorial https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. the line sc.pp.neighbors(adata) produces the following error:. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!. Ubuntu 18.04; Python 3.6.6. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Can you help me? Thank You",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:274,Testability,Assert,Assertion,274,"Hi, I am reproducing this tutorial https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. the line sc.pp.neighbors(adata) produces the following error:. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!. Ubuntu 18.04; Python 3.6.6. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Can you help me? Thank You",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:417,Usability,learn,learn,417,"Hi, I am reproducing this tutorial https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. the line sc.pp.neighbors(adata) produces the following error:. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!. Ubuntu 18.04; Python 3.6.6. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Can you help me? Thank You",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/281:995,Deployability,install,installed,995,"Hi there! Thanks for such a great tool. We are working with a group of people at EBI, Sanger, U. of Freiburg, Earlham (Norwich), the US and Australia in trying to bring different single cell analysis tools to Galaxy and other workflow environments. For tools like Seurat, SC3 or Scanpy, which are normally used as libraries, we are writing an scripting layer for each of these to facilitate their use directly from the shell for well defined functionality (to be called then from whatever workflow environment people want to use). You can get an idea based on what we have here for Seurat for instance https://github.com/ebi-gene-expression-group/r-seurat-scripts. We normally make a conda package out of this called \<tool\>-scripts. This probably fits well in the initial development phase where we want to be agile in the generation of these, but in the mid and definitely longer term, we would really like to contribute those scripts to the main tool repos, so that they are distributed and installed with them. Would you welcome such contribution (a set of scripts for high level functionality that can be used as executables from a shell) in this git repo?. This is where we are discussing things a bit with this community:; https://github.com/galaxyproject/tools-iuc/issues/2057. Current WIP for scanpy scripts (see feature branches):; https://github.com/ebi-gene-expression-group/scanpy-scripts/tree/feature/read10x",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/pull/282:87,Integrability,wrap,wrapper,87,"Hi,. I corrected these small mistakes while checking the documentation to write Galaxy wrapper. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/282
https://github.com/scverse/scanpy/pull/283:121,Deployability,integrat,integration,121,These changes enable Scanpy's pre-processing functions to run on distributed engines including Dask and Spark. The Spark integration itself relies on [Zap](https://github.com/lasersonlab/zap) for a distributed version of NumPy. . The main change is the `materialize_as_ndarray` function that is used at certain points of the computation to materialize intermediate results (not the full matrix). This is a no-op in the non-distributed case.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/283
https://github.com/scverse/scanpy/pull/283:121,Integrability,integrat,integration,121,These changes enable Scanpy's pre-processing functions to run on distributed engines including Dask and Spark. The Spark integration itself relies on [Zap](https://github.com/lasersonlab/zap) for a distributed version of NumPy. . The main change is the `materialize_as_ndarray` function that is used at certain points of the computation to materialize intermediate results (not the full matrix). This is a no-op in the non-distributed case.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/283
https://github.com/scverse/scanpy/pull/284:56,Modifiability,variab,variable,56,add option to keep genes and store bool array of highly variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/issues/285:60,Deployability,install,install,60,"I tried seveal times in both win and linux， python2 can not install scany, si there a solution for python2, thanks a lot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/286:2,Deployability,Update,Update,2,"**Update** Note sure if it is a problem with our script. However, if colors are not found, can it use default color-map and let us plot after raising a warning?. Log is following. ~Working on a PR.~ This issue is for reference. Could not upload source file since it depends on data file which are huge. ```; (Py36) pragati@wasabi-simons ~/Work/scanpy_exp $ python planaria.py ; scanpy==1.3.2+4.g7c9fb1a anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:39.15); Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 158, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('grey80', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/axes/_axes.py"", line 4210, in scatter; colors = mcolors.to_rgba_array(c); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 259, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 160, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 204, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'grey80'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home/pragati/Py36/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne; return plot_scatter(adata, basis='tsne', **k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:266,Integrability,depend,depends,266,"**Update** Note sure if it is a problem with our script. However, if colors are not found, can it use default color-map and let us plot after raising a warning?. Log is following. ~Working on a PR.~ This issue is for reference. Could not upload source file since it depends on data file which are huge. ```; (Py36) pragati@wasabi-simons ~/Work/scanpy_exp $ python planaria.py ; scanpy==1.3.2+4.g7c9fb1a anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:39.15); Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 158, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('grey80', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/axes/_axes.py"", line 4210, in scatter; colors = mcolors.to_rgba_array(c); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 259, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 160, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 204, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'grey80'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home/pragati/Py36/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne; return plot_scatter(adata, basis='tsne', **k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:822,Performance,cache,cache,822,"**Update** Note sure if it is a problem with our script. However, if colors are not found, can it use default color-map and let us plot after raising a warning?. Log is following. ~Working on a PR.~ This issue is for reference. Could not upload source file since it depends on data file which are huge. ```; (Py36) pragati@wasabi-simons ~/Work/scanpy_exp $ python planaria.py ; scanpy==1.3.2+4.g7c9fb1a anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:39.15); Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 158, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('grey80', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/axes/_axes.py"", line 4210, in scatter; colors = mcolors.to_rgba_array(c); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 259, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 160, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 204, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'grey80'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home/pragati/Py36/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne; return plot_scatter(adata, basis='tsne', **k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:162,Testability,Log,Log,162,"**Update** Note sure if it is a problem with our script. However, if colors are not found, can it use default color-map and let us plot after raising a warning?. Log is following. ~Working on a PR.~ This issue is for reference. Could not upload source file since it depends on data file which are huge. ```; (Py36) pragati@wasabi-simons ~/Work/scanpy_exp $ python planaria.py ; scanpy==1.3.2+4.g7c9fb1a anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:39.15); Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 158, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('grey80', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/axes/_axes.py"", line 4210, in scatter; colors = mcolors.to_rgba_array(c); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 259, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 160, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 204, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'grey80'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home/pragati/Py36/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne; return plot_scatter(adata, basis='tsne', **k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:468,Usability,learn,learn,468,"**Update** Note sure if it is a problem with our script. However, if colors are not found, can it use default color-map and let us plot after raising a warning?. Log is following. ~Working on a PR.~ This issue is for reference. Could not upload source file since it depends on data file which are huge. ```; (Py36) pragati@wasabi-simons ~/Work/scanpy_exp $ python planaria.py ; scanpy==1.3.2+4.g7c9fb1a anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:39.15); Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 158, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('grey80', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/axes/_axes.py"", line 4210, in scatter; colors = mcolors.to_rgba_array(c); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 259, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 160, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 204, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'grey80'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home/pragati/Py36/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne; return plot_scatter(adata, basis='tsne', **k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/pull/289:858,Energy Efficiency,efficient,efficient,858,"Added p-values output to Wilcoxon tests, as discussed in #270. Left most of the code the same (including the chunking). Wilcoxon test now outputs pvals, adjusted pvals (Bonferroni correction), and log fold changes. The change in speed should be much smaller than before:; Original time:; <img width=""468"" alt=""screen shot 2018-10-08 at 4 04 02 pm"" src=""https://user-images.githubusercontent.com/37122760/46632615-43fbda80-cb19-11e8-9a6b-9f6e06dd656c.png"">. New time:; <img width=""481"" alt=""screen shot 2018-10-08 at 4 02 47 pm"" src=""https://user-images.githubusercontent.com/37122760/46632624-4bbb7f00-cb19-11e8-916a-db518ff60791.png"">. I also fixed a bug where the Wilcoxon test would output the absolute values of the scores when rankby_abs=True, which shouldn't happen according to the function description. . As a side note, I was unable to find a quick/efficient way to rank the full sparse matrix for the wilcoxon test, which is why I kept the chunking along genes as suggested. Hope this helps!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:34,Testability,test,tests,34,"Added p-values output to Wilcoxon tests, as discussed in #270. Left most of the code the same (including the chunking). Wilcoxon test now outputs pvals, adjusted pvals (Bonferroni correction), and log fold changes. The change in speed should be much smaller than before:; Original time:; <img width=""468"" alt=""screen shot 2018-10-08 at 4 04 02 pm"" src=""https://user-images.githubusercontent.com/37122760/46632615-43fbda80-cb19-11e8-9a6b-9f6e06dd656c.png"">. New time:; <img width=""481"" alt=""screen shot 2018-10-08 at 4 02 47 pm"" src=""https://user-images.githubusercontent.com/37122760/46632624-4bbb7f00-cb19-11e8-916a-db518ff60791.png"">. I also fixed a bug where the Wilcoxon test would output the absolute values of the scores when rankby_abs=True, which shouldn't happen according to the function description. . As a side note, I was unable to find a quick/efficient way to rank the full sparse matrix for the wilcoxon test, which is why I kept the chunking along genes as suggested. Hope this helps!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:129,Testability,test,test,129,"Added p-values output to Wilcoxon tests, as discussed in #270. Left most of the code the same (including the chunking). Wilcoxon test now outputs pvals, adjusted pvals (Bonferroni correction), and log fold changes. The change in speed should be much smaller than before:; Original time:; <img width=""468"" alt=""screen shot 2018-10-08 at 4 04 02 pm"" src=""https://user-images.githubusercontent.com/37122760/46632615-43fbda80-cb19-11e8-9a6b-9f6e06dd656c.png"">. New time:; <img width=""481"" alt=""screen shot 2018-10-08 at 4 02 47 pm"" src=""https://user-images.githubusercontent.com/37122760/46632624-4bbb7f00-cb19-11e8-916a-db518ff60791.png"">. I also fixed a bug where the Wilcoxon test would output the absolute values of the scores when rankby_abs=True, which shouldn't happen according to the function description. . As a side note, I was unable to find a quick/efficient way to rank the full sparse matrix for the wilcoxon test, which is why I kept the chunking along genes as suggested. Hope this helps!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:197,Testability,log,log,197,"Added p-values output to Wilcoxon tests, as discussed in #270. Left most of the code the same (including the chunking). Wilcoxon test now outputs pvals, adjusted pvals (Bonferroni correction), and log fold changes. The change in speed should be much smaller than before:; Original time:; <img width=""468"" alt=""screen shot 2018-10-08 at 4 04 02 pm"" src=""https://user-images.githubusercontent.com/37122760/46632615-43fbda80-cb19-11e8-9a6b-9f6e06dd656c.png"">. New time:; <img width=""481"" alt=""screen shot 2018-10-08 at 4 02 47 pm"" src=""https://user-images.githubusercontent.com/37122760/46632624-4bbb7f00-cb19-11e8-916a-db518ff60791.png"">. I also fixed a bug where the Wilcoxon test would output the absolute values of the scores when rankby_abs=True, which shouldn't happen according to the function description. . As a side note, I was unable to find a quick/efficient way to rank the full sparse matrix for the wilcoxon test, which is why I kept the chunking along genes as suggested. Hope this helps!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:675,Testability,test,test,675,"Added p-values output to Wilcoxon tests, as discussed in #270. Left most of the code the same (including the chunking). Wilcoxon test now outputs pvals, adjusted pvals (Bonferroni correction), and log fold changes. The change in speed should be much smaller than before:; Original time:; <img width=""468"" alt=""screen shot 2018-10-08 at 4 04 02 pm"" src=""https://user-images.githubusercontent.com/37122760/46632615-43fbda80-cb19-11e8-9a6b-9f6e06dd656c.png"">. New time:; <img width=""481"" alt=""screen shot 2018-10-08 at 4 02 47 pm"" src=""https://user-images.githubusercontent.com/37122760/46632624-4bbb7f00-cb19-11e8-916a-db518ff60791.png"">. I also fixed a bug where the Wilcoxon test would output the absolute values of the scores when rankby_abs=True, which shouldn't happen according to the function description. . As a side note, I was unable to find a quick/efficient way to rank the full sparse matrix for the wilcoxon test, which is why I kept the chunking along genes as suggested. Hope this helps!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:920,Testability,test,test,920,"Added p-values output to Wilcoxon tests, as discussed in #270. Left most of the code the same (including the chunking). Wilcoxon test now outputs pvals, adjusted pvals (Bonferroni correction), and log fold changes. The change in speed should be much smaller than before:; Original time:; <img width=""468"" alt=""screen shot 2018-10-08 at 4 04 02 pm"" src=""https://user-images.githubusercontent.com/37122760/46632615-43fbda80-cb19-11e8-9a6b-9f6e06dd656c.png"">. New time:; <img width=""481"" alt=""screen shot 2018-10-08 at 4 02 47 pm"" src=""https://user-images.githubusercontent.com/37122760/46632624-4bbb7f00-cb19-11e8-916a-db518ff60791.png"">. I also fixed a bug where the Wilcoxon test would output the absolute values of the scores when rankby_abs=True, which shouldn't happen according to the function description. . As a side note, I was unable to find a quick/efficient way to rank the full sparse matrix for the wilcoxon test, which is why I kept the chunking along genes as suggested. Hope this helps!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/issues/290:351,Availability,robust,robust,351,"A frequent problem that I am faced with is to identify clusters using known lists of markers. I think that one existing approach is to first identify markers and then check if the known markers are among those identified markers. Also, I have used the `score_genes` function, originally used to identify cell cycle genes. However, I think that a more robust method is possible and probably I am just not aware of it. Does anyone has some experience with this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/pull/291:80,Testability,test,test,80,This PR addresses #286 and adds an extra check for valid color names. I added a test for the new functionality and took the opportunity to modify the plotting tests to mimic the way they are done for the pbmc3k tests.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/291
https://github.com/scverse/scanpy/pull/291:159,Testability,test,tests,159,This PR addresses #286 and adds an extra check for valid color names. I added a test for the new functionality and took the opportunity to modify the plotting tests to mimic the way they are done for the pbmc3k tests.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/291
https://github.com/scverse/scanpy/pull/291:211,Testability,test,tests,211,This PR addresses #286 and adds an extra check for valid color names. I added a test for the new functionality and took the opportunity to modify the plotting tests to mimic the way they are done for the pbmc3k tests.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/291
https://github.com/scverse/scanpy/pull/292:30,Integrability,wrap,wrapper,30,"Hi @falexwolf . Added a small wrapper for phenograph clustering, similar to phate.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/issues/300:491,Performance,perform,perform,491,"Need a file `highly_variable_genes.py` in `scanpy.preprocessing` with a function `highly_variable_genes`. This function is very similar to `filter_genes_dispersion`. However, by default, it assumes data has been logarithmized using `sc.pp.log1p`. Hence, in the “Seurat” method, an exponentiation with `expm1` is necessary (the current way in which the parameter `log` treats things is inconsistent as it doesn’t properly transform back using `expm`). Also, the new function doesn’t actually perform the filtering but simply annotates the data (`subset=False`). No need in an option `subset` in the new function. Of course, the old function remains for backwards compatibility.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/300
https://github.com/scverse/scanpy/issues/300:212,Testability,log,logarithmized,212,"Need a file `highly_variable_genes.py` in `scanpy.preprocessing` with a function `highly_variable_genes`. This function is very similar to `filter_genes_dispersion`. However, by default, it assumes data has been logarithmized using `sc.pp.log1p`. Hence, in the “Seurat” method, an exponentiation with `expm1` is necessary (the current way in which the parameter `log` treats things is inconsistent as it doesn’t properly transform back using `expm`). Also, the new function doesn’t actually perform the filtering but simply annotates the data (`subset=False`). No need in an option `subset` in the new function. Of course, the old function remains for backwards compatibility.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/300
https://github.com/scverse/scanpy/issues/300:363,Testability,log,log,363,"Need a file `highly_variable_genes.py` in `scanpy.preprocessing` with a function `highly_variable_genes`. This function is very similar to `filter_genes_dispersion`. However, by default, it assumes data has been logarithmized using `sc.pp.log1p`. Hence, in the “Seurat” method, an exponentiation with `expm1` is necessary (the current way in which the parameter `log` treats things is inconsistent as it doesn’t properly transform back using `expm`). Also, the new function doesn’t actually perform the filtering but simply annotates the data (`subset=False`). No need in an option `subset` in the new function. Of course, the old function remains for backwards compatibility.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/300
https://github.com/scverse/scanpy/issues/300:517,Usability,simpl,simply,517,"Need a file `highly_variable_genes.py` in `scanpy.preprocessing` with a function `highly_variable_genes`. This function is very similar to `filter_genes_dispersion`. However, by default, it assumes data has been logarithmized using `sc.pp.log1p`. Hence, in the “Seurat” method, an exponentiation with `expm1` is necessary (the current way in which the parameter `log` treats things is inconsistent as it doesn’t properly transform back using `expm`). Also, the new function doesn’t actually perform the filtering but simply annotates the data (`subset=False`). No need in an option `subset` in the new function. Of course, the old function remains for backwards compatibility.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/300
https://github.com/scverse/scanpy/issues/301:443,Integrability,wrap,wraps,443,"Need a file `normalization.py` in `scanpy.preprocessing`. In that, there should be cleaned-up versions of `normalize_per_cell` and `normalize_per_cell_weinreib17` without any recursive calls and no calls to `filter_cells`. The new names of these function should be `normalize_total` and `normalize_quantile`. There should be a backup version of `normalize_per_cell` that behaves exactly as it currently does for backwards compatibility, which wraps `filter_cells` and `normalize_per_cell`. In the ideal case, `normalize_total` would wrap `normalize_quantile(quantile=1)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:533,Integrability,wrap,wrap,533,"Need a file `normalization.py` in `scanpy.preprocessing`. In that, there should be cleaned-up versions of `normalize_per_cell` and `normalize_per_cell_weinreib17` without any recursive calls and no calls to `filter_cells`. The new names of these function should be `normalize_total` and `normalize_quantile`. There should be a backup version of `normalize_per_cell` that behaves exactly as it currently does for backwards compatibility, which wraps `filter_cells` and `normalize_per_cell`. In the ideal case, `normalize_total` would wrap `normalize_quantile(quantile=1)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/302:240,Deployability,integrat,integrated,240,"We want notebooks that automatically run through on readthedocs: https://nbsphinx.readthedocs.io/en/0.3.5. So that the basic tutorial (https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) can be integrated into the scanpy repository in `docs/tutorials` as a notebook with all output cleared (no images etc.). It is run on the readthedocs server and will produce exactly the same output, as is guaranteed by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/pbmc3k.py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:240,Integrability,integrat,integrated,240,"We want notebooks that automatically run through on readthedocs: https://nbsphinx.readthedocs.io/en/0.3.5. So that the basic tutorial (https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) can be integrated into the scanpy repository in `docs/tutorials` as a notebook with all output cleared (no images etc.). It is run on the readthedocs server and will produce exactly the same output, as is guaranteed by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/pbmc3k.py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:457,Testability,test,test,457,"We want notebooks that automatically run through on readthedocs: https://nbsphinx.readthedocs.io/en/0.3.5. So that the basic tutorial (https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) can be integrated into the scanpy repository in `docs/tutorials` as a notebook with all output cleared (no images etc.). It is run on the readthedocs server and will produce exactly the same output, as is guaranteed by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/pbmc3k.py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:517,Testability,test,tests,517,"We want notebooks that automatically run through on readthedocs: https://nbsphinx.readthedocs.io/en/0.3.5. So that the basic tutorial (https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) can be integrated into the scanpy repository in `docs/tutorials` as a notebook with all output cleared (no images etc.). It is run on the readthedocs server and will produce exactly the same output, as is guaranteed by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/pbmc3k.py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:328,Usability,clear,cleared,328,"We want notebooks that automatically run through on readthedocs: https://nbsphinx.readthedocs.io/en/0.3.5. So that the basic tutorial (https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) can be integrated into the scanpy repository in `docs/tutorials` as a notebook with all output cleared (no images etc.). It is run on the readthedocs server and will produce exactly the same output, as is guaranteed by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/pbmc3k.py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/pull/304:11,Deployability,update,updated,11,Tests were updated to include instances of the requirements.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/304
https://github.com/scverse/scanpy/pull/304:0,Testability,Test,Tests,0,Tests were updated to include instances of the requirements.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/304
https://github.com/scverse/scanpy/pull/305:11,Testability,test,testing,11,"Hi,. I was testing the different functions and found out that some requirements were missing and so in the `requirements.txt`. I tried to add the one I could find on Pypi. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/308:1023,Availability,avail,available,1023,"This PR computes a hierarchical clustering and draws a dendrogram to some of the plots. This is particularly useful when looking at marker genes from `scanpy.tl.rank_genes_groups` because the categories and marker genes are reordered according to the dendrogram. Eg:. ```PYTHON; pbmc = sc.datasets.pbmc68k_reduced(); ```; ![image](https://user-images.githubusercontent.com/4964309/47084320-71304300-d213-11e8-9a02-4cb2558f4cd7.png). ![image](https://user-images.githubusercontent.com/4964309/47084465-d97f2480-d213-11e8-838e-4407e4b854c7.png). Currently, for all rank_genes_groups_* plots, the dendrogram is set to `True` by default as I think this results in more meaningful images. For `heatmap`, `dotplot`, `matrixplot` and `stacked_violin` plots, I added a new parameter to activate the dendrogram:. ```PYTHON; sc.pl.dotplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True); ```; ![image](https://user-images.githubusercontent.com/4964309/47084622-50b4b880-d214-11e8-9179-6a9d5ecff306.png). More example are available at https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/issues/310:149,Availability,error,error,149,"Hi,. I am testing the `scanpy.api.tl.sandbag` and `scanpy.api.tl.cyclone` functions, after installing the latest version of `pypairs`. . 1. I got an error: `TypeError: 'module' object is not callable`. It seems that it could be solved by calling `pypairs.cyclone.cyclone` or `pypairs.sandbag.sandbag`. 2. In `sandbag`: the argument `fraction` is not used to call the `pypairs` `sandbag` function; 3. In `cyclone` and `sandbag`, `gene_names` and `sample_names` should not be `None`. I can fix these issues in a PR. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:91,Deployability,install,installing,91,"Hi,. I am testing the `scanpy.api.tl.sandbag` and `scanpy.api.tl.cyclone` functions, after installing the latest version of `pypairs`. . 1. I got an error: `TypeError: 'module' object is not callable`. It seems that it could be solved by calling `pypairs.cyclone.cyclone` or `pypairs.sandbag.sandbag`. 2. In `sandbag`: the argument `fraction` is not used to call the `pypairs` `sandbag` function; 3. In `cyclone` and `sandbag`, `gene_names` and `sample_names` should not be `None`. I can fix these issues in a PR. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:10,Testability,test,testing,10,"Hi,. I am testing the `scanpy.api.tl.sandbag` and `scanpy.api.tl.cyclone` functions, after installing the latest version of `pypairs`. . 1. I got an error: `TypeError: 'module' object is not callable`. It seems that it could be solved by calling `pypairs.cyclone.cyclone` or `pypairs.sandbag.sandbag`. 2. In `sandbag`: the argument `fraction` is not used to call the `pypairs` `sandbag` function; 3. In `cyclone` and `sandbag`, `gene_names` and `sample_names` should not be `None`. I can fix these issues in a PR. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/311:157,Availability,error,error,157,"Hi,. I am testing `pl.scatter` and it seems that:; - `color` cannot be a list (contrary to what the documentation mentions); - `components='all'` raises the error: `ValueError: invalid literal for int() with base 10: 'all'`. Any idea how to fix that?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:10,Testability,test,testing,10,"Hi,. I am testing `pl.scatter` and it seems that:; - `color` cannot be a list (contrary to what the documentation mentions); - `components='all'` raises the error: `ValueError: invalid literal for int() with base 10: 'all'`. Any idea how to fix that?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/pull/312:152,Testability,test,tests,152,Fix issue #310 ; - Import sandbag and cyclone; - Align parameters to the one in `pypairs`; - Add documentation for the parameters. I wanted to add some tests but I was not sure which dataset makes sense. Any idea?. Bérénice,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/312
https://github.com/scverse/scanpy/issues/313:20,Deployability,pipeline,pipeline,20,"In my preprocessing pipeline, I filtered for the most variable genes and also regress out n_counts and cell cycle. However, it seems like every time I restart my Jupyter notebook and rerun the preprocessing pipeline, I get a different neighborhood graph and UMAP, and therefore different clustering. If I use the same preprocessed data, I can reproduce the same PCA (using svd_solver='arpack'), UMAP, and clusters. So I was wondering if there is randomness introduced during filtering process or regress_out methods? What should I do to ensure reproducibility?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:207,Deployability,pipeline,pipeline,207,"In my preprocessing pipeline, I filtered for the most variable genes and also regress out n_counts and cell cycle. However, it seems like every time I restart my Jupyter notebook and rerun the preprocessing pipeline, I get a different neighborhood graph and UMAP, and therefore different clustering. If I use the same preprocessed data, I can reproduce the same PCA (using svd_solver='arpack'), UMAP, and clusters. So I was wondering if there is randomness introduced during filtering process or regress_out methods? What should I do to ensure reproducibility?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:54,Modifiability,variab,variable,54,"In my preprocessing pipeline, I filtered for the most variable genes and also regress out n_counts and cell cycle. However, it seems like every time I restart my Jupyter notebook and rerun the preprocessing pipeline, I get a different neighborhood graph and UMAP, and therefore different clustering. If I use the same preprocessed data, I can reproduce the same PCA (using svd_solver='arpack'), UMAP, and clusters. So I was wondering if there is randomness introduced during filtering process or regress_out methods? What should I do to ensure reproducibility?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/315:17,Modifiability,variab,variable,17,"After the highly variable genes information was added to .var `pl.pca_loadings` no longer works. This is an example that reproduces the problem:; ```PYTHON; import scanpy.api as sc; import numpy as np; import pandas as pd. N = 1000; M = 2000. adata = sc.AnnData(; X=np.random.random_sample((N, M)); ). sc.pp.filter_genes_dispersion(adata, subset=False); sc.tl.pca(adata); sc.pl.pca_loadings(adata); ```. If ` subset=True` then the pca_loadings works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/315
https://github.com/scverse/scanpy/pull/316:91,Deployability,release,release,91,"I like the `calculateQCMetrics` function from [`scater`](https://bioconductor.org/packages/release/bioc/html/scater.html), and have had a half finished python version drifting between my notebooks for a while. Is there is interest in adding this to scanpy?. I'm aiming to mostly copy the interface of `calculateQCMetrics` while being memory efficient, since this is likely run before filtering. # Todo. - [x] Figure out how I want to deal with more types of sparse matrix; - [x] Add `feature_control` argument, possibly `variable_control`; - [x] Clean up and expand tests; - [x] Expand documentation. # Questions. * What's up with sparse matrix choice in scanpy? Is `adata.X` expected to be any of the matrix types, or is it more limited? Just trying to figure out how much effort I should put into that.; * Are there any additional metrics that could be useful?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:341,Energy Efficiency,efficient,efficient,341,"I like the `calculateQCMetrics` function from [`scater`](https://bioconductor.org/packages/release/bioc/html/scater.html), and have had a half finished python version drifting between my notebooks for a while. Is there is interest in adding this to scanpy?. I'm aiming to mostly copy the interface of `calculateQCMetrics` while being memory efficient, since this is likely run before filtering. # Todo. - [x] Figure out how I want to deal with more types of sparse matrix; - [x] Add `feature_control` argument, possibly `variable_control`; - [x] Clean up and expand tests; - [x] Expand documentation. # Questions. * What's up with sparse matrix choice in scanpy? Is `adata.X` expected to be any of the matrix types, or is it more limited? Just trying to figure out how much effort I should put into that.; * Are there any additional metrics that could be useful?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:288,Integrability,interface,interface,288,"I like the `calculateQCMetrics` function from [`scater`](https://bioconductor.org/packages/release/bioc/html/scater.html), and have had a half finished python version drifting between my notebooks for a while. Is there is interest in adding this to scanpy?. I'm aiming to mostly copy the interface of `calculateQCMetrics` while being memory efficient, since this is likely run before filtering. # Todo. - [x] Figure out how I want to deal with more types of sparse matrix; - [x] Add `feature_control` argument, possibly `variable_control`; - [x] Clean up and expand tests; - [x] Expand documentation. # Questions. * What's up with sparse matrix choice in scanpy? Is `adata.X` expected to be any of the matrix types, or is it more limited? Just trying to figure out how much effort I should put into that.; * Are there any additional metrics that could be useful?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:566,Testability,test,tests,566,"I like the `calculateQCMetrics` function from [`scater`](https://bioconductor.org/packages/release/bioc/html/scater.html), and have had a half finished python version drifting between my notebooks for a while. Is there is interest in adding this to scanpy?. I'm aiming to mostly copy the interface of `calculateQCMetrics` while being memory efficient, since this is likely run before filtering. # Todo. - [x] Figure out how I want to deal with more types of sparse matrix; - [x] Add `feature_control` argument, possibly `variable_control`; - [x] Clean up and expand tests; - [x] Expand documentation. # Questions. * What's up with sparse matrix choice in scanpy? Is `adata.X` expected to be any of the matrix types, or is it more limited? Just trying to figure out how much effort I should put into that.; * Are there any additional metrics that could be useful?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/issues/317:20,Testability,test,tests,20,"All of the plotting tests are failing for me. Largely this seems based on font rendering. ![highest_expr_genes-failed-diff](https://user-images.githubusercontent.com/8238804/47278908-46c9f580-d618-11e8-99c0-ac5512994ea3.png). Though there might be something more with the violin plots. ![master_violin_multi_panel-failed-diff](https://user-images.githubusercontent.com/8238804/47278943-990b1680-d618-11e8-8e04-8e987c40b90e.png). Is there some setup required to get these tests to pass? My `matplotlibrc` is empty, so I don't think that's the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:471,Testability,test,tests,471,"All of the plotting tests are failing for me. Largely this seems based on font rendering. ![highest_expr_genes-failed-diff](https://user-images.githubusercontent.com/8238804/47278908-46c9f580-d618-11e8-99c0-ac5512994ea3.png). Though there might be something more with the violin plots. ![master_violin_multi_panel-failed-diff](https://user-images.githubusercontent.com/8238804/47278943-990b1680-d618-11e8-8e04-8e987c40b90e.png). Is there some setup required to get these tests to pass? My `matplotlibrc` is empty, so I don't think that's the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/318:637,Availability,error,error,637,"Hi,. I am testing `sc.pl.stacked_violin` and I had an issue with running it on an AnnData variable that is imported from a `h5ad` file. I explain. If I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. No problem, but if I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> adata.write('anndata.h5ad'); >>> adata = sc.read_h5ad('anndata.h5ad'); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. then I got the error:. ```; Traceback (most recent call last):; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/anndata.py"", line 896, in stacked_violin; orient='vertical', scale=scale, ax=ax, **kwds); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 2387, in violinplot; color, palette, saturation); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 562, in __init__; self.establish_variables(x, y, hue, data, orient, order, hue_order); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 155, in establish_variables; raise ValueError(err); ValueError: Could not interpret input 'variable'; ```. I tested it with versions 1.3.1 and 1.3.2, installed with bioconda. Any idea?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/318
https://github.com/scverse/scanpy/issues/318:1474,Deployability,install,installed,1474,"Hi,. I am testing `sc.pl.stacked_violin` and I had an issue with running it on an AnnData variable that is imported from a `h5ad` file. I explain. If I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. No problem, but if I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> adata.write('anndata.h5ad'); >>> adata = sc.read_h5ad('anndata.h5ad'); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. then I got the error:. ```; Traceback (most recent call last):; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/anndata.py"", line 896, in stacked_violin; orient='vertical', scale=scale, ax=ax, **kwds); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 2387, in violinplot; color, palette, saturation); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 562, in __init__; self.establish_variables(x, y, hue, data, orient, order, hue_order); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 155, in establish_variables; raise ValueError(err); ValueError: Could not interpret input 'variable'; ```. I tested it with versions 1.3.1 and 1.3.2, installed with bioconda. Any idea?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/318
https://github.com/scverse/scanpy/issues/318:90,Modifiability,variab,variable,90,"Hi,. I am testing `sc.pl.stacked_violin` and I had an issue with running it on an AnnData variable that is imported from a `h5ad` file. I explain. If I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. No problem, but if I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> adata.write('anndata.h5ad'); >>> adata = sc.read_h5ad('anndata.h5ad'); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. then I got the error:. ```; Traceback (most recent call last):; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/anndata.py"", line 896, in stacked_violin; orient='vertical', scale=scale, ax=ax, **kwds); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 2387, in violinplot; color, palette, saturation); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 562, in __init__; self.establish_variables(x, y, hue, data, orient, order, hue_order); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 155, in establish_variables; raise ValueError(err); ValueError: Could not interpret input 'variable'; ```. I tested it with versions 1.3.1 and 1.3.2, installed with bioconda. Any idea?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/318
https://github.com/scverse/scanpy/issues/318:1415,Modifiability,variab,variable,1415,"Hi,. I am testing `sc.pl.stacked_violin` and I had an issue with running it on an AnnData variable that is imported from a `h5ad` file. I explain. If I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. No problem, but if I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> adata.write('anndata.h5ad'); >>> adata = sc.read_h5ad('anndata.h5ad'); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. then I got the error:. ```; Traceback (most recent call last):; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/anndata.py"", line 896, in stacked_violin; orient='vertical', scale=scale, ax=ax, **kwds); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 2387, in violinplot; color, palette, saturation); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 562, in __init__; self.establish_variables(x, y, hue, data, orient, order, hue_order); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 155, in establish_variables; raise ValueError(err); ValueError: Could not interpret input 'variable'; ```. I tested it with versions 1.3.1 and 1.3.2, installed with bioconda. Any idea?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/318
https://github.com/scverse/scanpy/issues/318:10,Testability,test,testing,10,"Hi,. I am testing `sc.pl.stacked_violin` and I had an issue with running it on an AnnData variable that is imported from a `h5ad` file. I explain. If I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. No problem, but if I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> adata.write('anndata.h5ad'); >>> adata = sc.read_h5ad('anndata.h5ad'); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. then I got the error:. ```; Traceback (most recent call last):; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/anndata.py"", line 896, in stacked_violin; orient='vertical', scale=scale, ax=ax, **kwds); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 2387, in violinplot; color, palette, saturation); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 562, in __init__; self.establish_variables(x, y, hue, data, orient, order, hue_order); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 155, in establish_variables; raise ValueError(err); ValueError: Could not interpret input 'variable'; ```. I tested it with versions 1.3.1 and 1.3.2, installed with bioconda. Any idea?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/318
https://github.com/scverse/scanpy/issues/318:1433,Testability,test,tested,1433,"Hi,. I am testing `sc.pl.stacked_violin` and I had an issue with running it on an AnnData variable that is imported from a `h5ad` file. I explain. If I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. No problem, but if I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> adata.write('anndata.h5ad'); >>> adata = sc.read_h5ad('anndata.h5ad'); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. then I got the error:. ```; Traceback (most recent call last):; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/anndata.py"", line 896, in stacked_violin; orient='vertical', scale=scale, ax=ax, **kwds); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 2387, in violinplot; color, palette, saturation); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 562, in __init__; self.establish_variables(x, y, hue, data, orient, order, hue_order); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 155, in establish_variables; raise ValueError(err); ValueError: Could not interpret input 'variable'; ```. I tested it with versions 1.3.1 and 1.3.2, installed with bioconda. Any idea?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/318
https://github.com/scverse/scanpy/issues/319:5,Testability,test,test,5,"On a test dataset I compute neighbors and then immediately compute/plot both tSNE and UMAP and show them next to each other. Sometimes, we get pretty dramatic differences such as the one attached. Is this an algorithmic difference or something wrong with my approach?. ```; sc.pp.neighbors(adata, n_pcs=n_pcs, n_neighbors=n_neighbors); sc.tl.tsne(adata, n_pcs=n_pcs, random_state=random_state); sc.tl.umap(adata). sc.pl.tsne(adata, color=genes_to_color, color_map='RdBu_r', use_raw=False, save="".png""); sc.pl.umap(adata, color=genes_to_color, color_map='RdBu_r', use_raw=False, save="".png""); ```. ![screenshot from 2018-10-22 11-57-49](https://user-images.githubusercontent.com/330899/47306454-25561300-d5f2-11e8-98e9-939703dcf61b.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/320:476,Availability,error,error,476,"Hi scanpy team,; I am having trouble reading in a loom file using `read_loom`. I started a jupyter lab session in a conda environment and tried to read in a loom file from http://scope.aertslab.org/#/Bernard_Thienpont , under the User Uploaded -> Uncategorized tab (Thienpont_T-cell_v4_R_fixed.loom). Here are the relevant parts of my conda environment; ```; loompy 2.0.15 <pip>; python 3.6.6 h5001a0f_0 conda-forge; anndata 0.6.11 <pip>; scanpy 1.3.2 <pip>; ```; Here is the error below, I don't know if this is an issue with the latest loompy version? Thanks so much for any help! -Orr; ```; import scanpy.api as sc ; sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-3e86e297513a> in <module>; 1 import scanpy.api as sc; ----> 2 sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names); 142 filename = fspath(filename) # allow passing pathlib.Path objects; 143 from loompy import connect; --> 144 with connect(filename, 'r') as lc:; 145 ; 146 if X_name not in lc.layers.keys(): X_name = ''. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in connect(filename, mode, validate, spec_version); 1151 Note: if validation is requested, an exception is raised if validation fails.; 1152 	""""""; -> 1153 return LoomConnection(filename, mode, validate=validate, spec_version=spec_version). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in __init__(self, filename, mode, validate, spec_version); 82 lv = loompy.LoomValidator(version=spec_version); 83 if not lv.validate(filename):; ---> 84 raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); 85 ; 86 self._file = h5py.File(filename, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:1843,Availability,error,errors,1843,"v4_R_fixed.loom'). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names); 142 filename = fspath(filename) # allow passing pathlib.Path objects; 143 from loompy import connect; --> 144 with connect(filename, 'r') as lc:; 145 ; 146 if X_name not in lc.layers.keys(): X_name = ''. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in connect(filename, mode, validate, spec_version); 1151 Note: if validation is requested, an exception is raised if validation fails.; 1152 	""""""; -> 1153 return LoomConnection(filename, mode, validate=validate, spec_version=spec_version). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in __init__(self, filename, mode, validate, spec_version); 82 lv = loompy.LoomValidator(version=spec_version); 83 if not lv.validate(filename):; ---> 84 raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); 85 ; 86 self._file = h5py.File(filename, mode). ValueError: Row attribute 'Gene' dtype object is not allowed; Row attribute 'Regulons' dtype [('SPI1_extended_(1805g)', 'u1'), ('SPI1_(1756g)', 'u1'), ('KLF5_extended_(1521g)', 'u1'), ('EHF_extended_(1513g)', 'u1'), ('STAT1_extended_(1443g)', 'u1'), ('ELF3_extended_(1249g)', 'u1'), ('STAT1_(1212g)', 'u1'), ('CEBPB_extended_(1162g)', 'u1'), ('CEBPB_(1045g)', 'u1'), ('KLF5_(1038g)', 'u1'), ('ELF1_extended_(987g)', 'u1'), ('ETS2_extended_(932g)', 'u1'), ('IRF8_extended_(923g)', 'u1'), ('JUND_extended_(789g)', 'u1'), ('EHF_(762g)', 'u1'), ('ELF1_(753g)', 'u1'), ('TFEC_extended_(694g)', 'u1'), ('ETS2_(688g)', 'u1'), ('IRF7_extended_(688g)', 'u1'), ('IRF8_(687g)', 'u1'), ('XBP1_extended_(669g)', 'u1'), ('IRF1_extended_(662g)', 'u1'), ('ETS1_extended_(647g)', 'u1'), ('IRF1_(617g)', 'u1'), ('IRF7_(616g)', 'u1'), ('XBP1_(587g)', 'u1'), ('ETS1_(577g)', 'u1'), ('UQCRB_(573g)', 'u1'), ('PRDM1_extende",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:1252,Modifiability,layers,layers,1252,"rized tab (Thienpont_T-cell_v4_R_fixed.loom). Here are the relevant parts of my conda environment; ```; loompy 2.0.15 <pip>; python 3.6.6 h5001a0f_0 conda-forge; anndata 0.6.11 <pip>; scanpy 1.3.2 <pip>; ```; Here is the error below, I don't know if this is an issue with the latest loompy version? Thanks so much for any help! -Orr; ```; import scanpy.api as sc ; sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-3e86e297513a> in <module>; 1 import scanpy.api as sc; ----> 2 sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names); 142 filename = fspath(filename) # allow passing pathlib.Path objects; 143 from loompy import connect; --> 144 with connect(filename, 'r') as lc:; 145 ; 146 if X_name not in lc.layers.keys(): X_name = ''. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in connect(filename, mode, validate, spec_version); 1151 Note: if validation is requested, an exception is raised if validation fails.; 1152 	""""""; -> 1153 return LoomConnection(filename, mode, validate=validate, spec_version=spec_version). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in __init__(self, filename, mode, validate, spec_version); 82 lv = loompy.LoomValidator(version=spec_version); 83 if not lv.validate(filename):; ---> 84 raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); 85 ; 86 self._file = h5py.File(filename, mode). ValueError: Row attribute 'Gene' dtype object is not allowed; Row attribute 'Regulons' dtype [('SPI1_extended_(1805g)', 'u1'), ('SPI1_(1756g)', 'u1'), ('KLF5_extended_(1521g)', 'u1'), ('EHF_extended_(1513g)', 'u1'), ('STAT1_extended_(1443g)', 'u1'),",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:1377,Security,validat,validate,1377,"; anndata 0.6.11 <pip>; scanpy 1.3.2 <pip>; ```; Here is the error below, I don't know if this is an issue with the latest loompy version? Thanks so much for any help! -Orr; ```; import scanpy.api as sc ; sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-3e86e297513a> in <module>; 1 import scanpy.api as sc; ----> 2 sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names); 142 filename = fspath(filename) # allow passing pathlib.Path objects; 143 from loompy import connect; --> 144 with connect(filename, 'r') as lc:; 145 ; 146 if X_name not in lc.layers.keys(): X_name = ''. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in connect(filename, mode, validate, spec_version); 1151 Note: if validation is requested, an exception is raised if validation fails.; 1152 	""""""; -> 1153 return LoomConnection(filename, mode, validate=validate, spec_version=spec_version). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in __init__(self, filename, mode, validate, spec_version); 82 lv = loompy.LoomValidator(version=spec_version); 83 if not lv.validate(filename):; ---> 84 raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); 85 ; 86 self._file = h5py.File(filename, mode). ValueError: Row attribute 'Gene' dtype object is not allowed; Row attribute 'Regulons' dtype [('SPI1_extended_(1805g)', 'u1'), ('SPI1_(1756g)', 'u1'), ('KLF5_extended_(1521g)', 'u1'), ('EHF_extended_(1513g)', 'u1'), ('STAT1_extended_(1443g)', 'u1'), ('ELF3_extended_(1249g)', 'u1'), ('STAT1_(1212g)', 'u1'), ('CEBPB_extended_(1162g)', 'u1'), ('CEBPB_(1045g)', 'u1'), ('KLF5_(1038g)', 'u1'), ('ELF1_extended_(9",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:1416,Security,validat,validation,1416,"; anndata 0.6.11 <pip>; scanpy 1.3.2 <pip>; ```; Here is the error below, I don't know if this is an issue with the latest loompy version? Thanks so much for any help! -Orr; ```; import scanpy.api as sc ; sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-3e86e297513a> in <module>; 1 import scanpy.api as sc; ----> 2 sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names); 142 filename = fspath(filename) # allow passing pathlib.Path objects; 143 from loompy import connect; --> 144 with connect(filename, 'r') as lc:; 145 ; 146 if X_name not in lc.layers.keys(): X_name = ''. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in connect(filename, mode, validate, spec_version); 1151 Note: if validation is requested, an exception is raised if validation fails.; 1152 	""""""; -> 1153 return LoomConnection(filename, mode, validate=validate, spec_version=spec_version). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in __init__(self, filename, mode, validate, spec_version); 82 lv = loompy.LoomValidator(version=spec_version); 83 if not lv.validate(filename):; ---> 84 raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); 85 ; 86 self._file = h5py.File(filename, mode). ValueError: Row attribute 'Gene' dtype object is not allowed; Row attribute 'Regulons' dtype [('SPI1_extended_(1805g)', 'u1'), ('SPI1_(1756g)', 'u1'), ('KLF5_extended_(1521g)', 'u1'), ('EHF_extended_(1513g)', 'u1'), ('STAT1_extended_(1443g)', 'u1'), ('ELF3_extended_(1249g)', 'u1'), ('STAT1_(1212g)', 'u1'), ('CEBPB_extended_(1162g)', 'u1'), ('CEBPB_(1045g)', 'u1'), ('KLF5_(1038g)', 'u1'), ('ELF1_extended_(9",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:1467,Security,validat,validation,1467,"; anndata 0.6.11 <pip>; scanpy 1.3.2 <pip>; ```; Here is the error below, I don't know if this is an issue with the latest loompy version? Thanks so much for any help! -Orr; ```; import scanpy.api as sc ; sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-3e86e297513a> in <module>; 1 import scanpy.api as sc; ----> 2 sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names); 142 filename = fspath(filename) # allow passing pathlib.Path objects; 143 from loompy import connect; --> 144 with connect(filename, 'r') as lc:; 145 ; 146 if X_name not in lc.layers.keys(): X_name = ''. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in connect(filename, mode, validate, spec_version); 1151 Note: if validation is requested, an exception is raised if validation fails.; 1152 	""""""; -> 1153 return LoomConnection(filename, mode, validate=validate, spec_version=spec_version). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in __init__(self, filename, mode, validate, spec_version); 82 lv = loompy.LoomValidator(version=spec_version); 83 if not lv.validate(filename):; ---> 84 raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); 85 ; 86 self._file = h5py.File(filename, mode). ValueError: Row attribute 'Gene' dtype object is not allowed; Row attribute 'Regulons' dtype [('SPI1_extended_(1805g)', 'u1'), ('SPI1_(1756g)', 'u1'), ('KLF5_extended_(1521g)', 'u1'), ('EHF_extended_(1513g)', 'u1'), ('STAT1_extended_(1443g)', 'u1'), ('ELF3_extended_(1249g)', 'u1'), ('STAT1_(1212g)', 'u1'), ('CEBPB_extended_(1162g)', 'u1'), ('CEBPB_(1045g)', 'u1'), ('KLF5_(1038g)', 'u1'), ('ELF1_extended_(9",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:1543,Security,validat,validate,1543,"t loompy version? Thanks so much for any help! -Orr; ```; import scanpy.api as sc ; sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-3e86e297513a> in <module>; 1 import scanpy.api as sc; ----> 2 sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names); 142 filename = fspath(filename) # allow passing pathlib.Path objects; 143 from loompy import connect; --> 144 with connect(filename, 'r') as lc:; 145 ; 146 if X_name not in lc.layers.keys(): X_name = ''. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in connect(filename, mode, validate, spec_version); 1151 Note: if validation is requested, an exception is raised if validation fails.; 1152 	""""""; -> 1153 return LoomConnection(filename, mode, validate=validate, spec_version=spec_version). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in __init__(self, filename, mode, validate, spec_version); 82 lv = loompy.LoomValidator(version=spec_version); 83 if not lv.validate(filename):; ---> 84 raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); 85 ; 86 self._file = h5py.File(filename, mode). ValueError: Row attribute 'Gene' dtype object is not allowed; Row attribute 'Regulons' dtype [('SPI1_extended_(1805g)', 'u1'), ('SPI1_(1756g)', 'u1'), ('KLF5_extended_(1521g)', 'u1'), ('EHF_extended_(1513g)', 'u1'), ('STAT1_extended_(1443g)', 'u1'), ('ELF3_extended_(1249g)', 'u1'), ('STAT1_(1212g)', 'u1'), ('CEBPB_extended_(1162g)', 'u1'), ('CEBPB_(1045g)', 'u1'), ('KLF5_(1038g)', 'u1'), ('ELF1_extended_(987g)', 'u1'), ('ETS2_extended_(932g)', 'u1'), ('IRF8_extended_(923g)', 'u1'), ('JUND_extended_(789g)', 'u1'), ('EHF_(762g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:1552,Security,validat,validate,1552,"t loompy version? Thanks so much for any help! -Orr; ```; import scanpy.api as sc ; sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-3e86e297513a> in <module>; 1 import scanpy.api as sc; ----> 2 sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names); 142 filename = fspath(filename) # allow passing pathlib.Path objects; 143 from loompy import connect; --> 144 with connect(filename, 'r') as lc:; 145 ; 146 if X_name not in lc.layers.keys(): X_name = ''. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in connect(filename, mode, validate, spec_version); 1151 Note: if validation is requested, an exception is raised if validation fails.; 1152 	""""""; -> 1153 return LoomConnection(filename, mode, validate=validate, spec_version=spec_version). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in __init__(self, filename, mode, validate, spec_version); 82 lv = loompy.LoomValidator(version=spec_version); 83 if not lv.validate(filename):; ---> 84 raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); 85 ; 86 self._file = h5py.File(filename, mode). ValueError: Row attribute 'Gene' dtype object is not allowed; Row attribute 'Regulons' dtype [('SPI1_extended_(1805g)', 'u1'), ('SPI1_(1756g)', 'u1'), ('KLF5_extended_(1521g)', 'u1'), ('EHF_extended_(1513g)', 'u1'), ('STAT1_extended_(1443g)', 'u1'), ('ELF3_extended_(1249g)', 'u1'), ('STAT1_(1212g)', 'u1'), ('CEBPB_extended_(1162g)', 'u1'), ('CEBPB_(1045g)', 'u1'), ('KLF5_(1038g)', 'u1'), ('ELF1_extended_(987g)', 'u1'), ('ETS2_extended_(932g)', 'u1'), ('IRF8_extended_(923g)', 'u1'), ('JUND_extended_(789g)', 'u1'), ('EHF_(762g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:1694,Security,validat,validate,1694,"----------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-3e86e297513a> in <module>; 1 import scanpy.api as sc; ----> 2 sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names); 142 filename = fspath(filename) # allow passing pathlib.Path objects; 143 from loompy import connect; --> 144 with connect(filename, 'r') as lc:; 145 ; 146 if X_name not in lc.layers.keys(): X_name = ''. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in connect(filename, mode, validate, spec_version); 1151 Note: if validation is requested, an exception is raised if validation fails.; 1152 	""""""; -> 1153 return LoomConnection(filename, mode, validate=validate, spec_version=spec_version). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in __init__(self, filename, mode, validate, spec_version); 82 lv = loompy.LoomValidator(version=spec_version); 83 if not lv.validate(filename):; ---> 84 raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); 85 ; 86 self._file = h5py.File(filename, mode). ValueError: Row attribute 'Gene' dtype object is not allowed; Row attribute 'Regulons' dtype [('SPI1_extended_(1805g)', 'u1'), ('SPI1_(1756g)', 'u1'), ('KLF5_extended_(1521g)', 'u1'), ('EHF_extended_(1513g)', 'u1'), ('STAT1_extended_(1443g)', 'u1'), ('ELF3_extended_(1249g)', 'u1'), ('STAT1_(1212g)', 'u1'), ('CEBPB_extended_(1162g)', 'u1'), ('CEBPB_(1045g)', 'u1'), ('KLF5_(1038g)', 'u1'), ('ELF1_extended_(987g)', 'u1'), ('ETS2_extended_(932g)', 'u1'), ('IRF8_extended_(923g)', 'u1'), ('JUND_extended_(789g)', 'u1'), ('EHF_(762g)', 'u1'), ('ELF1_(753g)', 'u1'), ('TFEC_extended_(694g)', 'u1'), ('ETS2_(688g)', 'u1'), ('IRF7_extended_(688g)', 'u1'), ('IRF8_(687g)', 'u1'), ('XBP1_extended",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:1784,Security,validat,validate,1784,"-2-3e86e297513a> in <module>; 1 import scanpy.api as sc; ----> 2 sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names); 142 filename = fspath(filename) # allow passing pathlib.Path objects; 143 from loompy import connect; --> 144 with connect(filename, 'r') as lc:; 145 ; 146 if X_name not in lc.layers.keys(): X_name = ''. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in connect(filename, mode, validate, spec_version); 1151 Note: if validation is requested, an exception is raised if validation fails.; 1152 	""""""; -> 1153 return LoomConnection(filename, mode, validate=validate, spec_version=spec_version). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in __init__(self, filename, mode, validate, spec_version); 82 lv = loompy.LoomValidator(version=spec_version); 83 if not lv.validate(filename):; ---> 84 raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); 85 ; 86 self._file = h5py.File(filename, mode). ValueError: Row attribute 'Gene' dtype object is not allowed; Row attribute 'Regulons' dtype [('SPI1_extended_(1805g)', 'u1'), ('SPI1_(1756g)', 'u1'), ('KLF5_extended_(1521g)', 'u1'), ('EHF_extended_(1513g)', 'u1'), ('STAT1_extended_(1443g)', 'u1'), ('ELF3_extended_(1249g)', 'u1'), ('STAT1_(1212g)', 'u1'), ('CEBPB_extended_(1162g)', 'u1'), ('CEBPB_(1045g)', 'u1'), ('KLF5_(1038g)', 'u1'), ('ELF1_extended_(987g)', 'u1'), ('ETS2_extended_(932g)', 'u1'), ('IRF8_extended_(923g)', 'u1'), ('JUND_extended_(789g)', 'u1'), ('EHF_(762g)', 'u1'), ('ELF1_(753g)', 'u1'), ('TFEC_extended_(694g)', 'u1'), ('ETS2_(688g)', 'u1'), ('IRF7_extended_(688g)', 'u1'), ('IRF8_(687g)', 'u1'), ('XBP1_extended_(669g)', 'u1'), ('IRF1_extended_(662g)', 'u1'), ('ETS1_extended_(647g)', 'u1'), ('IRF1_(617g)', 'u1'), ('IRF7_(61",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/323:156,Availability,error,error,156,"Simple test case; ```; data = sc.read(""pbmc3k.h5ad""); logical_ar = data.var[""name""] == ""RER1""; df = data[:, logical_ar]; df.uns = data.uns # this causes an error ; ```. Causes this error; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-8b2cadedfe9b> in <module>(); 1 l = data.var[""name""] == ""RER1""; 2 df = data[:, l]; ----> 3 df.uns = data.uns. /usr/local/lib/python3.6/site-packages/anndata/base.py in uns(self, value); 987 # here, we directly generate the copy; 988 adata = self._adata_ref._getitem_copy((self._oidx, self._vidx)); --> 989 self._init_as_actual(adata); 990 self._uns = value; 991 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, dtype, shape, filename, filemode); 816 self._varm = BoundRecArr(varm, self, 'varm'); 817 ; --> 818 self._check_dimensions(); 819 self._check_uniqueness(); 820 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _check_dimensions(self, key); 1692 raise ValueError('Observations annot. `obs` must have number of '; 1693 'rows of `X` ({}), but has {} rows.'; -> 1694 .format(self._n_obs, self._obs.shape[0])); 1695 if 'var' in key and len(self._var) != self._n_vars:; 1696 raise ValueError('Variables annot. `var` must have number of '. ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:181,Availability,error,error,181,"Simple test case; ```; data = sc.read(""pbmc3k.h5ad""); logical_ar = data.var[""name""] == ""RER1""; df = data[:, logical_ar]; df.uns = data.uns # this causes an error ; ```. Causes this error; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-8b2cadedfe9b> in <module>(); 1 l = data.var[""name""] == ""RER1""; 2 df = data[:, l]; ----> 3 df.uns = data.uns. /usr/local/lib/python3.6/site-packages/anndata/base.py in uns(self, value); 987 # here, we directly generate the copy; 988 adata = self._adata_ref._getitem_copy((self._oidx, self._vidx)); --> 989 self._init_as_actual(adata); 990 self._uns = value; 991 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, dtype, shape, filename, filemode); 816 self._varm = BoundRecArr(varm, self, 'varm'); 817 ; --> 818 self._check_dimensions(); 819 self._check_uniqueness(); 820 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _check_dimensions(self, key); 1692 raise ValueError('Observations annot. `obs` must have number of '; 1693 'rows of `X` ({}), but has {} rows.'; -> 1694 .format(self._n_obs, self._obs.shape[0])); 1695 if 'var' in key and len(self._var) != self._n_vars:; 1696 raise ValueError('Variables annot. `var` must have number of '. ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1308,Modifiability,Variab,Variables,1308,"Simple test case; ```; data = sc.read(""pbmc3k.h5ad""); logical_ar = data.var[""name""] == ""RER1""; df = data[:, logical_ar]; df.uns = data.uns # this causes an error ; ```. Causes this error; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-8b2cadedfe9b> in <module>(); 1 l = data.var[""name""] == ""RER1""; 2 df = data[:, l]; ----> 3 df.uns = data.uns. /usr/local/lib/python3.6/site-packages/anndata/base.py in uns(self, value); 987 # here, we directly generate the copy; 988 adata = self._adata_ref._getitem_copy((self._oidx, self._vidx)); --> 989 self._init_as_actual(adata); 990 self._uns = value; 991 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, dtype, shape, filename, filemode); 816 self._varm = BoundRecArr(varm, self, 'varm'); 817 ; --> 818 self._check_dimensions(); 819 self._check_uniqueness(); 820 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _check_dimensions(self, key); 1692 raise ValueError('Observations annot. `obs` must have number of '; 1693 'rows of `X` ({}), but has {} rows.'; -> 1694 .format(self._n_obs, self._obs.shape[0])); 1695 if 'var' in key and len(self._var) != self._n_vars:; 1696 raise ValueError('Variables annot. `var` must have number of '. ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:7,Testability,test,test,7,"Simple test case; ```; data = sc.read(""pbmc3k.h5ad""); logical_ar = data.var[""name""] == ""RER1""; df = data[:, logical_ar]; df.uns = data.uns # this causes an error ; ```. Causes this error; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-8b2cadedfe9b> in <module>(); 1 l = data.var[""name""] == ""RER1""; 2 df = data[:, l]; ----> 3 df.uns = data.uns. /usr/local/lib/python3.6/site-packages/anndata/base.py in uns(self, value); 987 # here, we directly generate the copy; 988 adata = self._adata_ref._getitem_copy((self._oidx, self._vidx)); --> 989 self._init_as_actual(adata); 990 self._uns = value; 991 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, dtype, shape, filename, filemode); 816 self._varm = BoundRecArr(varm, self, 'varm'); 817 ; --> 818 self._check_dimensions(); 819 self._check_uniqueness(); 820 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _check_dimensions(self, key); 1692 raise ValueError('Observations annot. `obs` must have number of '; 1693 'rows of `X` ({}), but has {} rows.'; -> 1694 .format(self._n_obs, self._obs.shape[0])); 1695 if 'var' in key and len(self._var) != self._n_vars:; 1696 raise ValueError('Variables annot. `var` must have number of '. ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:0,Usability,Simpl,Simple,0,"Simple test case; ```; data = sc.read(""pbmc3k.h5ad""); logical_ar = data.var[""name""] == ""RER1""; df = data[:, logical_ar]; df.uns = data.uns # this causes an error ; ```. Causes this error; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-8b2cadedfe9b> in <module>(); 1 l = data.var[""name""] == ""RER1""; 2 df = data[:, l]; ----> 3 df.uns = data.uns. /usr/local/lib/python3.6/site-packages/anndata/base.py in uns(self, value); 987 # here, we directly generate the copy; 988 adata = self._adata_ref._getitem_copy((self._oidx, self._vidx)); --> 989 self._init_as_actual(adata); 990 self._uns = value; 991 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, dtype, shape, filename, filemode); 816 self._varm = BoundRecArr(varm, self, 'varm'); 817 ; --> 818 self._check_dimensions(); 819 self._check_uniqueness(); 820 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _check_dimensions(self, key); 1692 raise ValueError('Observations annot. `obs` must have number of '; 1693 'rows of `X` ({}), but has {} rows.'; -> 1694 .format(self._n_obs, self._obs.shape[0])); 1695 if 'var' in key and len(self._var) != self._n_vars:; 1696 raise ValueError('Variables annot. `var` must have number of '. ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/324:298,Testability,log,log,298,"To elaborate a bit on my comment on pull request #284 that `sc.pp.pca(adata, use_highly_variable=True)` does not reproduce the same umap embedding as subsetting the genes. I have done the following:. ```; disp_filter = sc.pp.filter_genes_dispersion(adata.X, flavor='cell_ranger', n_top_genes=4000, log=False); adata_hvg = adata.copy(); adata_hvg = adata_hvg[:, disp_filter['gene_subset']]; sc.pp.filter_genes_dispersion(adata, flavor='cell_ranger', n_top_genes=4000, log=False, subset=False). sc.pp.pca(adata, n_comps=50, use_highly_variable=True, svd_solver='arpack'); sc.pp.neighbors(adata); sc.tl.umap(adata). sc.pp.pca(adata_hvg, n_comps=50, svd_solver='arpack'); sc.pp.neighbors(adata_hvg); sc.tl.umap(adata_hvg). sc.pl.umap(adata, color='n_counts', use_raw=False); sc.pl.umap(adata_hvg, color='n_counts', use_raw=False); ```. The umap output is:; ![screen shot 2018-10-25 at 10 37 36](https://user-images.githubusercontent.com/13019956/47487680-29c53a80-d843-11e8-8535-de3c84f3a3b3.png). There is clearly a difference between the two cases also in the tsne, draw_graph and diffmap visualisations. Only pca seems to be consistent judging by eye.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/324:467,Testability,log,log,467,"To elaborate a bit on my comment on pull request #284 that `sc.pp.pca(adata, use_highly_variable=True)` does not reproduce the same umap embedding as subsetting the genes. I have done the following:. ```; disp_filter = sc.pp.filter_genes_dispersion(adata.X, flavor='cell_ranger', n_top_genes=4000, log=False); adata_hvg = adata.copy(); adata_hvg = adata_hvg[:, disp_filter['gene_subset']]; sc.pp.filter_genes_dispersion(adata, flavor='cell_ranger', n_top_genes=4000, log=False, subset=False). sc.pp.pca(adata, n_comps=50, use_highly_variable=True, svd_solver='arpack'); sc.pp.neighbors(adata); sc.tl.umap(adata). sc.pp.pca(adata_hvg, n_comps=50, svd_solver='arpack'); sc.pp.neighbors(adata_hvg); sc.tl.umap(adata_hvg). sc.pl.umap(adata, color='n_counts', use_raw=False); sc.pl.umap(adata_hvg, color='n_counts', use_raw=False); ```. The umap output is:; ![screen shot 2018-10-25 at 10 37 36](https://user-images.githubusercontent.com/13019956/47487680-29c53a80-d843-11e8-8535-de3c84f3a3b3.png). There is clearly a difference between the two cases also in the tsne, draw_graph and diffmap visualisations. Only pca seems to be consistent judging by eye.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/324:1003,Usability,clear,clearly,1003,"To elaborate a bit on my comment on pull request #284 that `sc.pp.pca(adata, use_highly_variable=True)` does not reproduce the same umap embedding as subsetting the genes. I have done the following:. ```; disp_filter = sc.pp.filter_genes_dispersion(adata.X, flavor='cell_ranger', n_top_genes=4000, log=False); adata_hvg = adata.copy(); adata_hvg = adata_hvg[:, disp_filter['gene_subset']]; sc.pp.filter_genes_dispersion(adata, flavor='cell_ranger', n_top_genes=4000, log=False, subset=False). sc.pp.pca(adata, n_comps=50, use_highly_variable=True, svd_solver='arpack'); sc.pp.neighbors(adata); sc.tl.umap(adata). sc.pp.pca(adata_hvg, n_comps=50, svd_solver='arpack'); sc.pp.neighbors(adata_hvg); sc.tl.umap(adata_hvg). sc.pl.umap(adata, color='n_counts', use_raw=False); sc.pl.umap(adata_hvg, color='n_counts', use_raw=False); ```. The umap output is:; ![screen shot 2018-10-25 at 10 37 36](https://user-images.githubusercontent.com/13019956/47487680-29c53a80-d843-11e8-8535-de3c84f3a3b3.png). There is clearly a difference between the two cases also in the tsne, draw_graph and diffmap visualisations. Only pca seems to be consistent judging by eye.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/325:1335,Availability,avail,available,1335,"Are Louvain clustering results supposed to be reproducible? I ran the clustering on the 1.3 mln dataset, exactly following the code provided here https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py as follows:. ```; import scanpy.api as sc; sc.settings.verbosity = 2; adata = sc.read_10x_h5('1M_neurons_filtered_gene_bc_matrices_h5.h5') ; sc.pp.recipe_zheng17(adata) ; sc.pp.neighbors(adata) ; sc.tl.louvain(adata) ; adata.obs['louvain'].to_csv('clustering-scanpy.csv'); ```. This works fine and I get the clustering result that makes sense when visualized, however it differs quite strongly from the clustering result that Alex sent me some time ago (as I understood him, that were exactly the clustering results used for the visualisation in the Scanpy paper). My questions:; 1. Is re-running the above snippet supposed to give me the exact same results every time, or are there some random seeds used along the way?; 2. If yes, how can one modify the code to ensure reproducibility?; 3. Is there any way to modify the snippet to get the exact same results as shown here https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells and in the published Scanpy paper?; 4. If the answer to the previous question is no, could you make those results publicly available somewhere?. I noticed the following warning from `sc.pp.neighbors` that might be relevant:. > WARNING: You're trying to run this on 1000 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params.; Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1631,Usability,learn,learn,1631,"Are Louvain clustering results supposed to be reproducible? I ran the clustering on the 1.3 mln dataset, exactly following the code provided here https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py as follows:. ```; import scanpy.api as sc; sc.settings.verbosity = 2; adata = sc.read_10x_h5('1M_neurons_filtered_gene_bc_matrices_h5.h5') ; sc.pp.recipe_zheng17(adata) ; sc.pp.neighbors(adata) ; sc.tl.louvain(adata) ; adata.obs['louvain'].to_csv('clustering-scanpy.csv'); ```. This works fine and I get the clustering result that makes sense when visualized, however it differs quite strongly from the clustering result that Alex sent me some time ago (as I understood him, that were exactly the clustering results used for the visualisation in the Scanpy paper). My questions:; 1. Is re-running the above snippet supposed to give me the exact same results every time, or are there some random seeds used along the way?; 2. If yes, how can one modify the code to ensure reproducibility?; 3. Is there any way to modify the snippet to get the exact same results as shown here https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells and in the published Scanpy paper?; 4. If the answer to the previous question is no, could you make those results publicly available somewhere?. I noticed the following warning from `sc.pp.neighbors` that might be relevant:. > WARNING: You're trying to run this on 1000 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params.; Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/326:105,Modifiability,config,config,105,"Currently, test collection takes about 11 seconds. This seemed a little long so I played around with the config a bit, and found if all the test files names are prepended with `test_`, and I set `python_files = test_*.py`, collection takes about 1 second. Mind if I make that change?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/326
https://github.com/scverse/scanpy/issues/326:11,Testability,test,test,11,"Currently, test collection takes about 11 seconds. This seemed a little long so I played around with the config a bit, and found if all the test files names are prepended with `test_`, and I set `python_files = test_*.py`, collection takes about 1 second. Mind if I make that change?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/326
https://github.com/scverse/scanpy/issues/326:140,Testability,test,test,140,"Currently, test collection takes about 11 seconds. This seemed a little long so I played around with the config a bit, and found if all the test files names are prepended with `test_`, and I set `python_files = test_*.py`, collection takes about 1 second. Mind if I make that change?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/326
https://github.com/scverse/scanpy/pull/327:122,Integrability,depend,dependent,122,"Test collection was taking a while, this should cut that time by about 90%. For some reason, pytest test collection seems dependent on the number of files the `python_files` parameter matches. Having a more specific path under `testpaths` doesn't – which would be a simpler change. Should close #326.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/327
https://github.com/scverse/scanpy/pull/327:0,Testability,Test,Test,0,"Test collection was taking a while, this should cut that time by about 90%. For some reason, pytest test collection seems dependent on the number of files the `python_files` parameter matches. Having a more specific path under `testpaths` doesn't – which would be a simpler change. Should close #326.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/327
https://github.com/scverse/scanpy/pull/327:100,Testability,test,test,100,"Test collection was taking a while, this should cut that time by about 90%. For some reason, pytest test collection seems dependent on the number of files the `python_files` parameter matches. Having a more specific path under `testpaths` doesn't – which would be a simpler change. Should close #326.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/327
https://github.com/scverse/scanpy/pull/327:228,Testability,test,testpaths,228,"Test collection was taking a while, this should cut that time by about 90%. For some reason, pytest test collection seems dependent on the number of files the `python_files` parameter matches. Having a more specific path under `testpaths` doesn't – which would be a simpler change. Should close #326.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/327
https://github.com/scverse/scanpy/pull/327:266,Usability,simpl,simpler,266,"Test collection was taking a while, this should cut that time by about 90%. For some reason, pytest test collection seems dependent on the number of files the `python_files` parameter matches. Having a more specific path under `testpaths` doesn't – which would be a simpler change. Should close #326.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/327
https://github.com/scverse/scanpy/issues/328:288,Availability,error,error,288,"Hi,. I am writing the Galaxy integration for `pl.paga_path`. From the documentation, I understand that if we can provide an annotation (other than `dpt_pseudotime`) that are keys of `adata.obs`.; I tested with a dataset in which `dpt_pseudotime` is not a key of `adata.obs` and I got the error:. ```; Traceback (most recent call last):; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:29,Deployability,integrat,integration,29,"Hi,. I am writing the Galaxy integration for `pl.paga_path`. From the documentation, I understand that if we can provide an annotation (other than `dpt_pseudotime`) that are keys of `adata.obs`.; I tested with a dataset in which `dpt_pseudotime` is not a key of `adata.obs` and I got the error:. ```; Traceback (most recent call last):; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:2123,Energy Efficiency,adapt,adapted,2123,"e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item; KeyError: 'dpt_pseudotime'; ```. My test dataset was probably not an adapted one (I did not find any test to reproduce) but is it an expected behavior? Should we provide as input always a `adata` with `dpt_pseudotime`? Maybe it would be interesting to detail that in the documentation.; Anything I can do to help there?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:29,Integrability,integrat,integration,29,"Hi,. I am writing the Galaxy integration for `pl.paga_path`. From the documentation, I understand that if we can provide an annotation (other than `dpt_pseudotime`) that are keys of `adata.obs`.; I tested with a dataset in which `dpt_pseudotime` is not a key of `adata.obs` and I got the error:. ```; Traceback (most recent call last):; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:2123,Modifiability,adapt,adapted,2123,"e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item; KeyError: 'dpt_pseudotime'; ```. My test dataset was probably not an adapted one (I did not find any test to reproduce) but is it an expected behavior? Should we provide as input always a `adata` with `dpt_pseudotime`? Maybe it would be interesting to detail that in the documentation.; Anything I can do to help there?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:1903,Security,hash,hashtable,1903,"e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item; KeyError: 'dpt_pseudotime'; ```. My test dataset was probably not an adapted one (I did not find any test to reproduce) but is it an expected behavior? Should we provide as input always a `adata` with `dpt_pseudotime`? Maybe it would be interesting to detail that in the documentation.; Anything I can do to help there?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:2016,Security,hash,hashtable,2016,"e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item; KeyError: 'dpt_pseudotime'; ```. My test dataset was probably not an adapted one (I did not find any test to reproduce) but is it an expected behavior? Should we provide as input always a `adata` with `dpt_pseudotime`? Maybe it would be interesting to detail that in the documentation.; Anything I can do to help there?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:198,Testability,test,tested,198,"Hi,. I am writing the Galaxy integration for `pl.paga_path`. From the documentation, I understand that if we can provide an annotation (other than `dpt_pseudotime`) that are keys of `adata.obs`.; I tested with a dataset in which `dpt_pseudotime` is not a key of `adata.obs` and I got the error:. ```; Traceback (most recent call last):; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:2090,Testability,test,test,2090,"e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item; KeyError: 'dpt_pseudotime'; ```. My test dataset was probably not an adapted one (I did not find any test to reproduce) but is it an expected behavior? Should we provide as input always a `adata` with `dpt_pseudotime`? Maybe it would be interesting to detail that in the documentation.; Anything I can do to help there?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:2155,Testability,test,test,2155,"e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item; KeyError: 'dpt_pseudotime'; ```. My test dataset was probably not an adapted one (I did not find any test to reproduce) but is it an expected behavior? Should we provide as input always a `adata` with `dpt_pseudotime`? Maybe it would be interesting to detail that in the documentation.; Anything I can do to help there?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/331:864,Availability,error,error,864,"I'm doing the same pathway I've done on hundreds of datasets but this particular one fails when I try to calculate PCA with:. ```; /opt/Python-3.7.0/lib/python3.7/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py in svds(A, k, ncv, tol, which, v0, maxiter, return_singular_vectors); 1768 ; 1769 if k <= 0 or k >= min(n, m):; -> 1770 raise ValueError(""k must be between 1 and min(A.shape), k={0}, A.shape={1}"".format(k, A.shape)); 1771 ; 1772 if isinstance(A, LinearOperator):. ValueError: k must be between 1 and min(A.shape), k=50, A.shape=(48, 2066); ```; Looking into this, I looped through adata.var['n_cells'] and no values were greater than 48, so I'm not sure why this is happening. Dropbox link with both the input test [H5AD file and notebook here](https://www.dropbox.com/sh/t2qb7ffz5msyc5e/AAD256Vs6HqLwNBjcBeCsGVGa?dl=0). Am I missing as simple error?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/331
https://github.com/scverse/scanpy/issues/331:730,Testability,test,test,730,"I'm doing the same pathway I've done on hundreds of datasets but this particular one fails when I try to calculate PCA with:. ```; /opt/Python-3.7.0/lib/python3.7/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py in svds(A, k, ncv, tol, which, v0, maxiter, return_singular_vectors); 1768 ; 1769 if k <= 0 or k >= min(n, m):; -> 1770 raise ValueError(""k must be between 1 and min(A.shape), k={0}, A.shape={1}"".format(k, A.shape)); 1771 ; 1772 if isinstance(A, LinearOperator):. ValueError: k must be between 1 and min(A.shape), k=50, A.shape=(48, 2066); ```; Looking into this, I looped through adata.var['n_cells'] and no values were greater than 48, so I'm not sure why this is happening. Dropbox link with both the input test [H5AD file and notebook here](https://www.dropbox.com/sh/t2qb7ffz5msyc5e/AAD256Vs6HqLwNBjcBeCsGVGa?dl=0). Am I missing as simple error?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/331
https://github.com/scverse/scanpy/issues/331:857,Usability,simpl,simple,857,"I'm doing the same pathway I've done on hundreds of datasets but this particular one fails when I try to calculate PCA with:. ```; /opt/Python-3.7.0/lib/python3.7/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py in svds(A, k, ncv, tol, which, v0, maxiter, return_singular_vectors); 1768 ; 1769 if k <= 0 or k >= min(n, m):; -> 1770 raise ValueError(""k must be between 1 and min(A.shape), k={0}, A.shape={1}"".format(k, A.shape)); 1771 ; 1772 if isinstance(A, LinearOperator):. ValueError: k must be between 1 and min(A.shape), k=50, A.shape=(48, 2066); ```; Looking into this, I looped through adata.var['n_cells'] and no values were greater than 48, so I'm not sure why this is happening. Dropbox link with both the input test [H5AD file and notebook here](https://www.dropbox.com/sh/t2qb7ffz5msyc5e/AAD256Vs6HqLwNBjcBeCsGVGa?dl=0). Am I missing as simple error?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/331
https://github.com/scverse/scanpy/issues/332:248,Testability,Test,Test,248,"Slicing causes inconsistent state when either n_obs or n_vars == 1, resulting in a slice that can no longer be sliced again on the other axis, due to shape inconsistencies between the AnnData and X object. anndata 0.6.11, scanpy 1.3.2, WSL/Ubuntu. Test case:; ```; import scanpy.api as sc; data = sc.read('example-dataset/data.h5ad'); >>> data.shape, data.X.shape, data._X.shape; ((2638, 1838), (2638, 1838), (2638, 1838)); >>> slice = data[0, :]; >>> slice.shape, slice.X.shape, slice._X.shape; ((1, 1838), (1838,), (1, 1838)); >>> slice[:, 0]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__; self._init_as_view(X, oidx, vidx); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view; self._init_X_as_view(); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view; X = self._adata_ref.X[self._oidx, self._vidx]; IndexError: too many indices for array; >>> slice[0]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__; self._init_as_view(X, oidx, vidx); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view; self._init_X_as_view(); File ""/cellxgene/venv/lib/python3.6/site-packages/an",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/333:48,Availability,error,error,48,"After updating scanpy to the v 1.3.2 I got this error when doing the subplots as in the ""PAGA for hematopoiesis in mouse"" tutorial:; _, axs = pl.subplots(ncols=6, figsize=(16, 30), gridspec_kw={'wspace': 0.05, 'left': 0.12}); pl.subplots_adjust(left=0.05, right=0.98, top=0.82, bottom=0.2); for ipath, (descr, path) in enumerate(paths):; _, data = sc.pl.paga_path(; adata, path, gene_names_3, ; show_node_names=False,; ax=axs[ipath],; ytick_fontsize=12,; left_margin=0.15,; n_avg=50,; annotations=['distance'],; show_yticks=True if ipath==0 else False,; show_colorbar=False,; color_map='Greys',; color_maps_annotations={'distance': 'viridis'},; title='{} path'.format(descr),; return_data=True,; show=False); #data.to_csv('./write/paga_path_{}.csv'.format(descr)); #pl.savefig('./figures/paga_path.png'); pl.show(). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-8-c59dfbccf885> in <module>(); 16 title='{} path'.format(descr),; 17 return_data=True,; ---> 18 show=False); 19 #data.to_csv('./write/paga_path_{}.csv'.format(descr)); 20 #pl.savefig('./figures/paga_path.png'). ~\Anaconda3\lib\site-packages\scanpy\plotting\tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 932 idcs = idcs[idcs_group]; 933 if key in adata.obs_keys(): x += list(adata.obs[key].values[idcs]); --> 934 else: x += list(adata_X[:, key].X[idcs]); 935 if ikey == 0:; 936 groups += [group for i in range(len(idcs))]. ~\Anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 1301 def __getitem__(self, index):; 1302 """"""Returns a sliced view of the object.""""""; -> 1303 return self._getitem_view(index); 1304 ; 1305 def _getitem_vi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:2404,Modifiability,layers,layers,2404,"tations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 932 idcs = idcs[idcs_group]; 933 if key in adata.obs_keys(): x += list(adata.obs[key].values[idcs]); --> 934 else: x += list(adata_X[:, key].X[idcs]); 935 if ikey == 0:; 936 groups += [group for i in range(len(idcs))]. ~\Anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 1301 def __getitem__(self, index):; 1302 """"""Returns a sliced view of the object.""""""; -> 1303 return self._getitem_view(index); 1304 ; 1305 def _getitem_view(self, index):. ~\Anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index); 1305 def _getitem_view(self, index):; 1306 oidx, vidx = self._normalize_indices(index); -> 1307 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1308 ; 1309 # this is used in the setter for uns, if a view. ~\Anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 667 if not isinstance(X, AnnData):; 668 raise ValueError('`X` has to be an AnnData object.'); --> 669 self._init_as_view(X, oidx, vidx); 670 else:; 671 self._init_as_actual(. ~\Anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx); 724 self._X = None; 725 else:; --> 726 self._init_X_as_view(); 727 ; 728 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\Anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self); 751 shape = (; 752 get_n_items_idx(self._oidx, self._adata_ref.n_obs),; --> 753 get_n_items_idx(self._vidx, self._adata_ref.n_vars); 754 ); 755 if np.isscalar(X):. ~\Anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l); 141 return 1; 142 else:; --> 143 return len(idx). TypeError: object of type 'numpy.int64' has no len()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/pull/334:217,Deployability,Update,Updated,217,"Hi @falexwolf,. 10X is releasing a new version of CellRanger that is changing the output format. This pull request makes Scanpy forward compatible with the new version. In particular, the following changes are made:. Updated `read_10x_h5`:; - Renamed the original `read_10x_h5` as `_read_legacy_10x_h5`;; - Added `_read_v3_10x_h5` to read the new Cell Ranger output format;; - The new `read_10x_h5` determines the version of HDF5 input by the presence of the matrix key, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is True (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` and `genome` were added into the outcome object as new attributes. Updated `read_10x_mtx`:; - Renamed the original `read_10x_mtx` as `_read_legacy_10x_mtx`;; - Added `_read_v3_10x_mtx` to read the new Cell Ranger output format;; - The new `read_10x_mtx` determines the version of matrix input by the presence of the `genes.tsv` file under the input directory, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is `True` (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` was added into the outcome object as a new attribute. Added small test datasets and code for the revised functions to verify the expected behavior. Note for the `genome` argument:; - There is a genome argument in Scanpy's `read_10x_h5` function but not in `read_10x_mtx` as the genome was already specified by the path of input directory. The outcome object of the two functions should be the same which always take one genome at a time.; - In this PR, when there are multiple genomes (e.g. Barnyard), `read_10x_mtx` always read them all, whereas `read_10x_h5` always need to specify one of them (mm10 by default). However, when `gex_only == False`, the `genome` argument ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/334
https://github.com/scverse/scanpy/pull/334:786,Deployability,Update,Updated,786,"Hi @falexwolf,. 10X is releasing a new version of CellRanger that is changing the output format. This pull request makes Scanpy forward compatible with the new version. In particular, the following changes are made:. Updated `read_10x_h5`:; - Renamed the original `read_10x_h5` as `_read_legacy_10x_h5`;; - Added `_read_v3_10x_h5` to read the new Cell Ranger output format;; - The new `read_10x_h5` determines the version of HDF5 input by the presence of the matrix key, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is True (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` and `genome` were added into the outcome object as new attributes. Updated `read_10x_mtx`:; - Renamed the original `read_10x_mtx` as `_read_legacy_10x_mtx`;; - Added `_read_v3_10x_mtx` to read the new Cell Ranger output format;; - The new `read_10x_mtx` determines the version of matrix input by the presence of the `genes.tsv` file under the input directory, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is `True` (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` was added into the outcome object as a new attribute. Added small test datasets and code for the revised functions to verify the expected behavior. Note for the `genome` argument:; - There is a genome argument in Scanpy's `read_10x_h5` function but not in `read_10x_mtx` as the genome was already specified by the path of input directory. The outcome object of the two functions should be the same which always take one genome at a time.; - In this PR, when there are multiple genomes (e.g. Barnyard), `read_10x_mtx` always read them all, whereas `read_10x_h5` always need to specify one of them (mm10 by default). However, when `gex_only == False`, the `genome` argument ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/334
https://github.com/scverse/scanpy/pull/334:475,Integrability,wrap,wraps,475,"Hi @falexwolf,. 10X is releasing a new version of CellRanger that is changing the output format. This pull request makes Scanpy forward compatible with the new version. In particular, the following changes are made:. Updated `read_10x_h5`:; - Renamed the original `read_10x_h5` as `_read_legacy_10x_h5`;; - Added `_read_v3_10x_h5` to read the new Cell Ranger output format;; - The new `read_10x_h5` determines the version of HDF5 input by the presence of the matrix key, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is True (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` and `genome` were added into the outcome object as new attributes. Updated `read_10x_mtx`:; - Renamed the original `read_10x_mtx` as `_read_legacy_10x_mtx`;; - Added `_read_v3_10x_mtx` to read the new Cell Ranger output format;; - The new `read_10x_mtx` determines the version of matrix input by the presence of the `genes.tsv` file under the input directory, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is `True` (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` was added into the outcome object as a new attribute. Added small test datasets and code for the revised functions to verify the expected behavior. Note for the `genome` argument:; - There is a genome argument in Scanpy's `read_10x_h5` function but not in `read_10x_mtx` as the genome was already specified by the path of input directory. The outcome object of the two functions should be the same which always take one genome at a time.; - In this PR, when there are multiple genomes (e.g. Barnyard), `read_10x_mtx` always read them all, whereas `read_10x_h5` always need to specify one of them (mm10 by default). However, when `gex_only == False`, the `genome` argument ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/334
https://github.com/scverse/scanpy/pull/334:1083,Integrability,wrap,wraps,1083,"ellRanger that is changing the output format. This pull request makes Scanpy forward compatible with the new version. In particular, the following changes are made:. Updated `read_10x_h5`:; - Renamed the original `read_10x_h5` as `_read_legacy_10x_h5`;; - Added `_read_v3_10x_h5` to read the new Cell Ranger output format;; - The new `read_10x_h5` determines the version of HDF5 input by the presence of the matrix key, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is True (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` and `genome` were added into the outcome object as new attributes. Updated `read_10x_mtx`:; - Renamed the original `read_10x_mtx` as `_read_legacy_10x_mtx`;; - Added `_read_v3_10x_mtx` to read the new Cell Ranger output format;; - The new `read_10x_mtx` determines the version of matrix input by the presence of the `genes.tsv` file under the input directory, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is `True` (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` was added into the outcome object as a new attribute. Added small test datasets and code for the revised functions to verify the expected behavior. Note for the `genome` argument:; - There is a genome argument in Scanpy's `read_10x_h5` function but not in `read_10x_mtx` as the genome was already specified by the path of input directory. The outcome object of the two functions should be the same which always take one genome at a time.; - In this PR, when there are multiple genomes (e.g. Barnyard), `read_10x_mtx` always read them all, whereas `read_10x_h5` always need to specify one of them (mm10 by default). However, when `gex_only == False`, the `genome` argument will be ignored and the whole matrix will be read.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/334
https://github.com/scverse/scanpy/pull/334:1395,Testability,test,test,1395,"ellRanger that is changing the output format. This pull request makes Scanpy forward compatible with the new version. In particular, the following changes are made:. Updated `read_10x_h5`:; - Renamed the original `read_10x_h5` as `_read_legacy_10x_h5`;; - Added `_read_v3_10x_h5` to read the new Cell Ranger output format;; - The new `read_10x_h5` determines the version of HDF5 input by the presence of the matrix key, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is True (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` and `genome` were added into the outcome object as new attributes. Updated `read_10x_mtx`:; - Renamed the original `read_10x_mtx` as `_read_legacy_10x_mtx`;; - Added `_read_v3_10x_mtx` to read the new Cell Ranger output format;; - The new `read_10x_mtx` determines the version of matrix input by the presence of the `genes.tsv` file under the input directory, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is `True` (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` was added into the outcome object as a new attribute. Added small test datasets and code for the revised functions to verify the expected behavior. Note for the `genome` argument:; - There is a genome argument in Scanpy's `read_10x_h5` function but not in `read_10x_mtx` as the genome was already specified by the path of input directory. The outcome object of the two functions should be the same which always take one genome at a time.; - In this PR, when there are multiple genomes (e.g. Barnyard), `read_10x_mtx` always read them all, whereas `read_10x_h5` always need to specify one of them (mm10 by default). However, when `gex_only == False`, the `genome` argument will be ignored and the whole matrix will be read.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/334
https://github.com/scverse/scanpy/issues/335:160,Performance,cache,cached,160,The previous `sc.pl.umap` etc. had an option to export legend positions via 'on data export'. We need a solution in the docs... Presumably just by exposing the cached positions to the user.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/335
https://github.com/scverse/scanpy/pull/340:251,Performance,optimiz,optimized,251,"On master (37851434b2) from the base of the repo, I haven't seen the following code finish running:. ```python; import scanpy.api as sc; adata = sc.read(""./data/pbmc3k_raw.h5ad""); %time sc.pp.downsample_counts(adata, 1500); ```. This PR implements an optimized version of the same thing, which gives:. ```python; %time sc.pp.downsample_counts(adata, 1500) ; CPU times: user 2.25 s, sys: 44.7 ms, total: 2.29 s; Wall time: 2.32 s; ```. ## What's changed. * I've rewritten the function to use numba along with fewer allocations; * Added a test for the function; * Added argument `replace`, which indicates whether subsampling should happen with replacement. ## Notes. To me, it makes more sense to sample without replacement, since for small changes in total counts you'll have more similar profiles. However, I've set the default for replacement to `True` to preserve the current behavior. Neither this or the previous method scale well with sampling depth, and it's maybe worth using a call to sample a multinomial or multivariate hypergeometric distribution instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:537,Testability,test,test,537,"On master (37851434b2) from the base of the repo, I haven't seen the following code finish running:. ```python; import scanpy.api as sc; adata = sc.read(""./data/pbmc3k_raw.h5ad""); %time sc.pp.downsample_counts(adata, 1500); ```. This PR implements an optimized version of the same thing, which gives:. ```python; %time sc.pp.downsample_counts(adata, 1500) ; CPU times: user 2.25 s, sys: 44.7 ms, total: 2.29 s; Wall time: 2.32 s; ```. ## What's changed. * I've rewritten the function to use numba along with fewer allocations; * Added a test for the function; * Added argument `replace`, which indicates whether subsampling should happen with replacement. ## Notes. To me, it makes more sense to sample without replacement, since for small changes in total counts you'll have more similar profiles. However, I've set the default for replacement to `True` to preserve the current behavior. Neither this or the previous method scale well with sampling depth, and it's maybe worth using a call to sample a multinomial or multivariate hypergeometric distribution instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/issues/342:158,Availability,error,error,158,"Hi,; I'm attempting to run scvelo on my scanpy processed 10x data.; Initially I provided the .h5ad file from scanpy as the input file for scvelo but ran into error KeyError: 'unspliced', I'm assuming unspliced data can't be found and also that ""adata.layers"" doesn't exist in my scanpy processed .h5ad file.; I also attempted providing the unprocessed data directly into scvelo but again came up with the error KeyError: 'unspliced'.; I've attempted adata.layers.keys() which returns odict_keys([]) and scv.pp.show_proportions(adata) which returns Abundance of []: []. (Currently running version 1.3.2 of scanpy and 0.1.11 of scvelo).; My adata file doesn't seem to contain adata.layers information, is there a way to add this information via scanpy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/342
https://github.com/scverse/scanpy/issues/342:405,Availability,error,error,405,"Hi,; I'm attempting to run scvelo on my scanpy processed 10x data.; Initially I provided the .h5ad file from scanpy as the input file for scvelo but ran into error KeyError: 'unspliced', I'm assuming unspliced data can't be found and also that ""adata.layers"" doesn't exist in my scanpy processed .h5ad file.; I also attempted providing the unprocessed data directly into scvelo but again came up with the error KeyError: 'unspliced'.; I've attempted adata.layers.keys() which returns odict_keys([]) and scv.pp.show_proportions(adata) which returns Abundance of []: []. (Currently running version 1.3.2 of scanpy and 0.1.11 of scvelo).; My adata file doesn't seem to contain adata.layers information, is there a way to add this information via scanpy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/342
https://github.com/scverse/scanpy/issues/342:251,Modifiability,layers,layers,251,"Hi,; I'm attempting to run scvelo on my scanpy processed 10x data.; Initially I provided the .h5ad file from scanpy as the input file for scvelo but ran into error KeyError: 'unspliced', I'm assuming unspliced data can't be found and also that ""adata.layers"" doesn't exist in my scanpy processed .h5ad file.; I also attempted providing the unprocessed data directly into scvelo but again came up with the error KeyError: 'unspliced'.; I've attempted adata.layers.keys() which returns odict_keys([]) and scv.pp.show_proportions(adata) which returns Abundance of []: []. (Currently running version 1.3.2 of scanpy and 0.1.11 of scvelo).; My adata file doesn't seem to contain adata.layers information, is there a way to add this information via scanpy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/342
https://github.com/scverse/scanpy/issues/342:456,Modifiability,layers,layers,456,"Hi,; I'm attempting to run scvelo on my scanpy processed 10x data.; Initially I provided the .h5ad file from scanpy as the input file for scvelo but ran into error KeyError: 'unspliced', I'm assuming unspliced data can't be found and also that ""adata.layers"" doesn't exist in my scanpy processed .h5ad file.; I also attempted providing the unprocessed data directly into scvelo but again came up with the error KeyError: 'unspliced'.; I've attempted adata.layers.keys() which returns odict_keys([]) and scv.pp.show_proportions(adata) which returns Abundance of []: []. (Currently running version 1.3.2 of scanpy and 0.1.11 of scvelo).; My adata file doesn't seem to contain adata.layers information, is there a way to add this information via scanpy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/342
https://github.com/scverse/scanpy/issues/342:680,Modifiability,layers,layers,680,"Hi,; I'm attempting to run scvelo on my scanpy processed 10x data.; Initially I provided the .h5ad file from scanpy as the input file for scvelo but ran into error KeyError: 'unspliced', I'm assuming unspliced data can't be found and also that ""adata.layers"" doesn't exist in my scanpy processed .h5ad file.; I also attempted providing the unprocessed data directly into scvelo but again came up with the error KeyError: 'unspliced'.; I've attempted adata.layers.keys() which returns odict_keys([]) and scv.pp.show_proportions(adata) which returns Abundance of []: []. (Currently running version 1.3.2 of scanpy and 0.1.11 of scvelo).; My adata file doesn't seem to contain adata.layers information, is there a way to add this information via scanpy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/342
https://github.com/scverse/scanpy/pull/343:40,Deployability,update,updated,40,"Added test for filter_genes_dispersion, updated docs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/343
https://github.com/scverse/scanpy/pull/343:6,Testability,test,test,6,"Added test for filter_genes_dispersion, updated docs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/343
https://github.com/scverse/scanpy/issues/344:250,Availability,error,error,250,"It seems like the dendrogram feature has been merged into the master branch according to this thread #308 and it is documented in the API. However, after updating my scanpy to 1.3.2, I still cannot access the dendrogram feature and get the following error when I set `dendrogram=True` when using `sc.pl.heatmap`:; `AttributeError: Unknown property dendrogram`. Here are the versions that I am using:; scanpy==1.3.2 anndata==0.6.10 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. Do you plan on releasing the dendrogram feature? Which package version currently has it?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344
https://github.com/scverse/scanpy/issues/344:198,Security,access,access,198,"It seems like the dendrogram feature has been merged into the master branch according to this thread #308 and it is documented in the API. However, after updating my scanpy to 1.3.2, I still cannot access the dendrogram feature and get the following error when I set `dendrogram=True` when using `sc.pl.heatmap`:; `AttributeError: Unknown property dendrogram`. Here are the versions that I am using:; scanpy==1.3.2 anndata==0.6.10 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. Do you plan on releasing the dendrogram feature? Which package version currently has it?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344
https://github.com/scverse/scanpy/issues/344:480,Usability,learn,learn,480,"It seems like the dendrogram feature has been merged into the master branch according to this thread #308 and it is documented in the API. However, after updating my scanpy to 1.3.2, I still cannot access the dendrogram feature and get the following error when I set `dendrogram=True` when using `sc.pl.heatmap`:; `AttributeError: Unknown property dendrogram`. Here are the versions that I am using:; scanpy==1.3.2 anndata==0.6.10 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. Do you plan on releasing the dendrogram feature? Which package version currently has it?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344
https://github.com/scverse/scanpy/issues/346:116,Availability,error,error,116,"When I run `sc.tl.rank_genes_groups` and I set the `reference` argument to one of my groups I am testing, I get the error. ```pytb; 91 groups_order = [str(n) for n in groups_order]; 92 if reference != 'rest' and reference not in set(groups_order):; ---> 93 groups_order += [reference]; 94 if (reference != 'rest'; 95 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list; ```. That's because 'groups_order' is not a list at this point of the code when the argument `groups` is at its default `all`. When I change this and I pass a list, e.g. ['0', '1'], then the error vanished, because `groups order` will then be a list that we can add to. This should be changed, because the setting 'all' is the default value.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:605,Availability,error,error,605,"When I run `sc.tl.rank_genes_groups` and I set the `reference` argument to one of my groups I am testing, I get the error. ```pytb; 91 groups_order = [str(n) for n in groups_order]; 92 if reference != 'rest' and reference not in set(groups_order):; ---> 93 groups_order += [reference]; 94 if (reference != 'rest'; 95 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list; ```. That's because 'groups_order' is not a list at this point of the code when the argument `groups` is at its default `all`. When I change this and I pass a list, e.g. ['0', '1'], then the error vanished, because `groups order` will then be a list that we can add to. This should be changed, because the setting 'all' is the default value.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:97,Testability,test,testing,97,"When I run `sc.tl.rank_genes_groups` and I set the `reference` argument to one of my groups I am testing, I get the error. ```pytb; 91 groups_order = [str(n) for n in groups_order]; 92 if reference != 'rest' and reference not in set(groups_order):; ---> 93 groups_order += [reference]; 94 if (reference != 'rest'; 95 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list; ```. That's because 'groups_order' is not a list at this point of the code when the argument `groups` is at its default `all`. When I change this and I pass a list, e.g. ['0', '1'], then the error vanished, because `groups order` will then be a list that we can add to. This should be changed, because the setting 'all' is the default value.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/348:83,Availability,error,errors,83,"Hi, . **Issue**: When I try to use the multipanel option, the command runs without errors but the plot never shows up:. `sc.pl.violin(adata, keys = ['IL10', 'IFNG', 'CD3G'], multi_panel = None, groupby=None)`. **Question**: Is there a way to split the violin plots form the sc.pl.violin() into a specific number of rows? Similar to the `ncols=` in R?; ; Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/348
https://github.com/scverse/scanpy/issues/350:1677,Performance,perform,performed,1677,"Hello,. It would appear that `louvain-igraph` has been obsoleted in favour of `leidenalg`, and the author makes a [persuasive case](https://arxiv.org/abs/1810.08473) as to the superiority of the new approach. To my untrained eye, the algorithm is conceptually similar to the Louvain modification used by Seurat, but introduces an extra collapsed network refinement step. it should be easy to support this in Scanpy - the syntax appears to be identical to the old `louvain` innards, and I was able to construct a very minimal dummy function for testing by taking the key bits of `sc.tl.louvain()` and replacing `louvain.` with `leidenalg.`:. ```py; import leidenalg; import numpy as np; import pandas as pd; from scanpy import utils; from natsort import natsorted. def leiden(adata, use_weights=False, resolution=1, iterations=-1):; 	g = utils.get_igraph_from_adjacency(adata.uns['neighbors']['connectivities'], directed=True); 	weights = None; 	if use_weights:; 		weights = np.array(g.es[""weight""]).astype(np.float64); 	part = leidenalg.find_partition(; 		g, leidenalg.RBConfigurationVertexPartition, ; 		resolution_parameter = resolution, weights = weights, ; 		n_iterations = iterations,; 	); 	groups = np.array(part.membership); 	adata.obs['louvain'] = pd.Categorical(; 		values=groups.astype('U'),; 		categories=natsorted(np.unique(groups).astype('U')),; 	); ```. As such, replacing any `louvain.` with `leidenalg.` in `sc.tl.louvain()` would do most of the work. Probably the only new thing that would need support would the the `n_iterations` parameter in `leidenalg.find_partition()`. The default value is 2, positive values control how many passes of the algorithm are performed. -1 just makes it run until it fails to improve the clustering.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:544,Testability,test,testing,544,"Hello,. It would appear that `louvain-igraph` has been obsoleted in favour of `leidenalg`, and the author makes a [persuasive case](https://arxiv.org/abs/1810.08473) as to the superiority of the new approach. To my untrained eye, the algorithm is conceptually similar to the Louvain modification used by Seurat, but introduces an extra collapsed network refinement step. it should be easy to support this in Scanpy - the syntax appears to be identical to the old `louvain` innards, and I was able to construct a very minimal dummy function for testing by taking the key bits of `sc.tl.louvain()` and replacing `louvain.` with `leidenalg.`:. ```py; import leidenalg; import numpy as np; import pandas as pd; from scanpy import utils; from natsort import natsorted. def leiden(adata, use_weights=False, resolution=1, iterations=-1):; 	g = utils.get_igraph_from_adjacency(adata.uns['neighbors']['connectivities'], directed=True); 	weights = None; 	if use_weights:; 		weights = np.array(g.es[""weight""]).astype(np.float64); 	part = leidenalg.find_partition(; 		g, leidenalg.RBConfigurationVertexPartition, ; 		resolution_parameter = resolution, weights = weights, ; 		n_iterations = iterations,; 	); 	groups = np.array(part.membership); 	adata.obs['louvain'] = pd.Categorical(; 		values=groups.astype('U'),; 		categories=natsorted(np.unique(groups).astype('U')),; 	); ```. As such, replacing any `louvain.` with `leidenalg.` in `sc.tl.louvain()` would do most of the work. Probably the only new thing that would need support would the the `n_iterations` parameter in `leidenalg.find_partition()`. The default value is 2, positive values control how many passes of the algorithm are performed. -1 just makes it run until it fails to improve the clustering.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/353:189,Availability,down,downstream,189,"I want to remove mitochondrial genes from the data before normalization. I already have used mitochondrial genes to calculate ""pct_counts_mito"", but I don't want them to be in the data for downstream analysis. Is there a function to achieve this in scanpy.api? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/353
https://github.com/scverse/scanpy/issues/355:287,Availability,down,downloaded,287,"This is in an Ubuntu 16..04 Docker container:; ```; docker run --rm -it ubuntu:16.04; ```; Then I ran:; ```; apt-get update && apt-get install -y \; python3-pip \; python3-setuptools; python3-wheel. pip3 install scanpy; ```. I get the following output (after all of the dependencies are downloaded):. ```; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/wheel/bdist_wheel.py"", line 179, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:3388,Availability,error,error,3388,"d/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; creating build/lib/scanpy/preprocessing; copying scanpy/preprocessing/recipes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:11778,Availability,error,error,11778,"t.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-33o4crd7/scanpy/; You are using pip version 8.1.1, however version 18.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command.; ```. Same error message after upgrading pip. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:11884,Availability,avail,available,11884,"t.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-33o4crd7/scanpy/; You are using pip version 8.1.1, however version 18.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command.; ```. Same error message after upgrading pip. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:11982,Availability,error,error,11982,"t.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-33o4crd7/scanpy/; You are using pip version 8.1.1, however version 18.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command.; ```. Same error message after upgrading pip. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:117,Deployability,update,update,117,"This is in an Ubuntu 16..04 Docker container:; ```; docker run --rm -it ubuntu:16.04; ```; Then I ran:; ```; apt-get update && apt-get install -y \; python3-pip \; python3-setuptools; python3-wheel. pip3 install scanpy; ```. I get the following output (after all of the dependencies are downloaded):. ```; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/wheel/bdist_wheel.py"", line 179, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:135,Deployability,install,install,135,"This is in an Ubuntu 16..04 Docker container:; ```; docker run --rm -it ubuntu:16.04; ```; Then I ran:; ```; apt-get update && apt-get install -y \; python3-pip \; python3-setuptools; python3-wheel. pip3 install scanpy; ```. I get the following output (after all of the dependencies are downloaded):. ```; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/wheel/bdist_wheel.py"", line 179, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:204,Deployability,install,install,204,"This is in an Ubuntu 16..04 Docker container:; ```; docker run --rm -it ubuntu:16.04; ```; Then I ran:; ```; apt-get update && apt-get install -y \; python3-pip \; python3-setuptools; python3-wheel. pip3 install scanpy; ```. I get the following output (after all of the dependencies are downloaded):. ```; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/wheel/bdist_wheel.py"", line 179, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:3086,Deployability,Install,Installing,3086,"ne 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> bui",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:3365,Deployability,install,install,3365,"ta_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; creating build/lib/scanpy/preprocessing; copying scanpy/preproc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:3632,Deployability,install,install,3632,"; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; creating build/lib/scanpy/preprocessing; copying scanpy/preprocessing/recipes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/highly_variable_genes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/__init__.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/magic.py -> build/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:3674,Deployability,install,install-record,3674,"; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; creating build/lib/scanpy/preprocessing; copying scanpy/preprocessing/recipes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/highly_variable_genes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/__init__.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/magic.py -> build/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:3872,Deployability,install,install,3872,"n directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; creating build/lib/scanpy/preprocessing; copying scanpy/preprocessing/recipes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/highly_variable_genes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/__init__.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/magic.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/dca.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/mnn_correct.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/qc.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/simple.py -> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:9717,Deployability,install,install,9717,"opying scanpy/neighbors/umap/utils.py -> build/lib/scanpy/neighbors/umap; running egg_info; writing dependency_links to scanpy.egg-info/dependency_links.txt; writing scanpy.egg-info/PKG-INFO; writing top-level names to scanpy.egg-info/top_level.txt; writing requirements to scanpy.egg-info/requires.txt; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/setuptools/command/install.py"", line 61, in run; return orig.install.run(self); File ""/usr/lib/python3.5/distutils/command/install.py"", line 583, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for packa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:9759,Deployability,install,install,9759,"running egg_info; writing dependency_links to scanpy.egg-info/dependency_links.txt; writing scanpy.egg-info/PKG-INFO; writing top-level names to scanpy.egg-info/top_level.txt; writing requirements to scanpy.egg-info/requires.txt; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/setuptools/command/install.py"", line 61, in run; return orig.install.run(self); File ""/usr/lib/python3.5/distutils/command/install.py"", line 583, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/pyt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:9821,Deployability,install,install,9821,".egg-info/dependency_links.txt; writing scanpy.egg-info/PKG-INFO; writing top-level names to scanpy.egg-info/top_level.txt; writing requirements to scanpy.egg-info/requires.txt; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/setuptools/command/install.py"", line 61, in run; return orig.install.run(self); File ""/usr/lib/python3.5/distutils/command/install.py"", line 583, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:11658,Deployability,install,install,11658,"t.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-33o4crd7/scanpy/; You are using pip version 8.1.1, however version 18.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command.; ```. Same error message after upgrading pip. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:11700,Deployability,install,install-record,11700,"t.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-33o4crd7/scanpy/; You are using pip version 8.1.1, however version 18.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command.; ```. Same error message after upgrading pip. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:11939,Deployability,install,install,11939,"t.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-33o4crd7/scanpy/; You are using pip version 8.1.1, however version 18.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command.; ```. Same error message after upgrading pip. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:11949,Deployability,upgrade,upgrade,11949,"t.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-33o4crd7/scanpy/; You are using pip version 8.1.1, however version 18.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command.; ```. Same error message after upgrading pip. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:270,Integrability,depend,dependencies,270,"This is in an Ubuntu 16..04 Docker container:; ```; docker run --rm -it ubuntu:16.04; ```; Then I ran:; ```; apt-get update && apt-get install -y \; python3-pip \; python3-setuptools; python3-wheel. pip3 install scanpy; ```. I get the following output (after all of the dependencies are downloaded):. ```; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/wheel/bdist_wheel.py"", line 179, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:11988,Integrability,message,message,11988,"t.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-33o4crd7/scanpy/; You are using pip version 8.1.1, however version 18.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command.; ```. Same error message after upgrading pip. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:2789,Performance,cache,cache,2789,"uild_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:2947,Performance,cache,cache,2947,"ne 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> bui",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:4230,Testability,log,logging,4230,"solver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; creating build/lib/scanpy/preprocessing; copying scanpy/preprocessing/recipes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/highly_variable_genes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/__init__.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/magic.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/dca.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/mnn_correct.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/qc.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/simple.py -> build/lib/scanpy/preprocessing; creating build/lib/scanpy/datasets; copying scanpy/datasets/__init__.py -> build/lib/scanpy/datasets; copying scanpy/datasets/api_without_datasets.py -> build/lib/scanpy/datasets; creating build/lib/scanpy/queries; copying scanpy/queries/__init__.py -> bui",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:5612,Testability,log,logging,5612,copying scanpy/preprocessing/magic.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/dca.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/mnn_correct.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/qc.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/simple.py -> build/lib/scanpy/preprocessing; creating build/lib/scanpy/datasets; copying scanpy/datasets/__init__.py -> build/lib/scanpy/datasets; copying scanpy/datasets/api_without_datasets.py -> build/lib/scanpy/datasets; creating build/lib/scanpy/queries; copying scanpy/queries/__init__.py -> build/lib/scanpy/queries; creating build/lib/scanpy/api; copying scanpy/api/__init__.py -> build/lib/scanpy/api; copying scanpy/api/datasets.py -> build/lib/scanpy/api; copying scanpy/api/export_to.py -> build/lib/scanpy/api; copying scanpy/api/tl.py -> build/lib/scanpy/api; copying scanpy/api/pl.py -> build/lib/scanpy/api; copying scanpy/api/queries.py -> build/lib/scanpy/api; copying scanpy/api/logging.py -> build/lib/scanpy/api; copying scanpy/api/pp.py -> build/lib/scanpy/api; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/phate.py -> build/lib/scanpy/tools; copying scanpy/tools/diffmap.py -> build/lib/scanpy/tools; copying scanpy/tools/score_genes.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/rank_genes_groups.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/pypairs.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_tsne_fix.py -> build/lib/scanpy/tools; copying scanpy/tools/draw_graph.py -> b,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:3267,Usability,learn,learn,3267,"ne 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> bui",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:4914,Usability,simpl,simple,4914,ning build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; creating build/lib/scanpy/preprocessing; copying scanpy/preprocessing/recipes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/highly_variable_genes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/__init__.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/magic.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/dca.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/mnn_correct.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/qc.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/simple.py -> build/lib/scanpy/preprocessing; creating build/lib/scanpy/datasets; copying scanpy/datasets/__init__.py -> build/lib/scanpy/datasets; copying scanpy/datasets/api_without_datasets.py -> build/lib/scanpy/datasets; creating build/lib/scanpy/queries; copying scanpy/queries/__init__.py -> build/lib/scanpy/queries; creating build/lib/scanpy/api; copying scanpy/api/__init__.py -> build/lib/scanpy/api; copying scanpy/api/datasets.py -> build/lib/scanpy/api; copying scanpy/api/export_to.py -> build/lib/scanpy/api; copying scanpy/api/tl.py -> build/lib/scanpy/api; copying scanpy/api/pl.py -> build/lib/scanpy/api; copying scanpy/api/queries.py -> build/lib/scanpy/api; copying scanpy/api/logging.py -> build/lib/scanpy/api; copying scanpy/api/pp.py -> build/lib/scanpy/api; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/phate.py -> build/lib/scanpy/tools; copying scanpy/tools/diffmap.py -> build/,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/356:63,Availability,error,error,63,"Hey,. Was just trying to plot a cluster map when I ran into an error:. ```python; import scanpy.api as sc; from scipy import sparse; A = sparse.rand(100, 100, 0.1, ""csr""). # This works; sc.pl.clustermap(sc.AnnData(A.toarray())). # This throws an error; sc.pl.clustermap(sc.AnnData(sparse.rand(100, 100, 0.1, ""csr""))); ```. <details>; <summary> Traceback: </summary>. ```python; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-13-f65606c38b22> in <module>; ----> 1 sc.pl.clustermap(sc.AnnData(A)). ~/github/scanpy/scanpy/plotting/anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds); 750 g = sns.clustermap(df, row_colors=row_colors, **kwds); 751 else:; --> 752 g = sns.clustermap(df, **kwds); 753 show = settings.autoshow if show is None else show; 754 if show: pl.show(). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs); 1299 row_cluster=row_cluster, col_cluster=col_cluster,; 1300 row_linkage=row_linkage, col_linkage=col_linkage,; -> 1301 **kwargs). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in plot(self, metric, method, colorbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, **kws); 1126 colorbar_kws = {} if colorbar_kws is None else colorbar_kws; 1127 self.plot_dendrograms(row_cluster, col_cluster, metric, method,; -> 1128 row_linkage=row_linkage, col_linkage=col_linkage); 1129 try:; 1130 xind = self.dendrogram_col.reordered_ind. /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in plot_dendrograms(self, row_cluster, col_cluster, metric, method, row_linkage, col_linkage); 1019 self.dendrogram_row = dendrogram(; 1020 self.data2d, metric=metric, method=method, label=False, axis=0,; -> 1021 ax=self.ax_row_dendrogram, rotate=True, linkage=row_linkage); 10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/356
https://github.com/scverse/scanpy/issues/356:246,Availability,error,error,246,"Hey,. Was just trying to plot a cluster map when I ran into an error:. ```python; import scanpy.api as sc; from scipy import sparse; A = sparse.rand(100, 100, 0.1, ""csr""). # This works; sc.pl.clustermap(sc.AnnData(A.toarray())). # This throws an error; sc.pl.clustermap(sc.AnnData(sparse.rand(100, 100, 0.1, ""csr""))); ```. <details>; <summary> Traceback: </summary>. ```python; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-13-f65606c38b22> in <module>; ----> 1 sc.pl.clustermap(sc.AnnData(A)). ~/github/scanpy/scanpy/plotting/anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds); 750 g = sns.clustermap(df, row_colors=row_colors, **kwds); 751 else:; --> 752 g = sns.clustermap(df, **kwds); 753 show = settings.autoshow if show is None else show; 754 if show: pl.show(). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs); 1299 row_cluster=row_cluster, col_cluster=col_cluster,; 1300 row_linkage=row_linkage, col_linkage=col_linkage,; -> 1301 **kwargs). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in plot(self, metric, method, colorbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, **kws); 1126 colorbar_kws = {} if colorbar_kws is None else colorbar_kws; 1127 self.plot_dendrograms(row_cluster, col_cluster, metric, method,; -> 1128 row_linkage=row_linkage, col_linkage=col_linkage); 1129 try:; 1130 xind = self.dendrogram_col.reordered_ind. /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in plot_dendrograms(self, row_cluster, col_cluster, metric, method, row_linkage, col_linkage); 1019 self.dendrogram_row = dendrogram(; 1020 self.data2d, metric=metric, method=method, label=False, axis=0,; -> 1021 ax=self.ax_row_dendrogram, rotate=True, linkage=row_linkage); 10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/356
https://github.com/scverse/scanpy/issues/356:1105,Availability,mask,mask,1105,"py.api as sc; from scipy import sparse; A = sparse.rand(100, 100, 0.1, ""csr""). # This works; sc.pl.clustermap(sc.AnnData(A.toarray())). # This throws an error; sc.pl.clustermap(sc.AnnData(sparse.rand(100, 100, 0.1, ""csr""))); ```. <details>; <summary> Traceback: </summary>. ```python; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-13-f65606c38b22> in <module>; ----> 1 sc.pl.clustermap(sc.AnnData(A)). ~/github/scanpy/scanpy/plotting/anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds); 750 g = sns.clustermap(df, row_colors=row_colors, **kwds); 751 else:; --> 752 g = sns.clustermap(df, **kwds); 753 show = settings.autoshow if show is None else show; 754 if show: pl.show(). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs); 1299 row_cluster=row_cluster, col_cluster=col_cluster,; 1300 row_linkage=row_linkage, col_linkage=col_linkage,; -> 1301 **kwargs). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in plot(self, metric, method, colorbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, **kws); 1126 colorbar_kws = {} if colorbar_kws is None else colorbar_kws; 1127 self.plot_dendrograms(row_cluster, col_cluster, metric, method,; -> 1128 row_linkage=row_linkage, col_linkage=col_linkage); 1129 try:; 1130 xind = self.dendrogram_col.reordered_ind. /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in plot_dendrograms(self, row_cluster, col_cluster, metric, method, row_linkage, col_linkage); 1019 self.dendrogram_row = dendrogram(; 1020 self.data2d, metric=metric, method=method, label=False, axis=0,; -> 1021 ax=self.ax_row_dendrogram, rotate=True, linkage=row_linkage); 1022 else:; 1023 self.ax_row_dendrogram.set_xticks([]). /usr/local/lib/python3.6/site-packages/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/356
https://github.com/scverse/scanpy/issues/356:4729,Availability,error,error,4729,"-packages/scipy/spatial/distance.py in pdist(X, metric, *args, **kwargs); 1932 if metric_name is not None:; 1933 X, typ, kwargs = _validate_pdist_input(X, m, n,; -> 1934 metric_name, **kwargs); 1935 ; 1936 # get pdist wrapper. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _validate_pdist_input(X, m, n, metric_name, **kwargs); 287 typ = types[types.index(X.dtype)] if X.dtype in types else types[0]; 288 # validate data; --> 289 X = _convert_to_type(X, out_type=typ); 290 ; 291 # validate kwargs. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _convert_to_type(X, out_type); 182 ; 183 def _convert_to_type(X, out_type):; --> 184 return np.ascontiguousarray(X, dtype=out_type); 185 ; 186 . /usr/local/lib/python3.6/site-packages/numpy/core/numeric.py in ascontiguousarray(a, dtype); 588 ; 589 """"""; --> 590 return array(a, dtype, copy=False, order='C', ndmin=1); 591 ; 592 . ValueError: setting an array element with a sequence.; ```. </details>. -------------------. I think this is happening because `pandas` doesn't like being passed a sparse array ([which can happen in `sc.pl.clustermap`](https://github.com/theislab/scanpy/blob/7de1f5159c91d6d2243bb7866d9495ee6747c750/scanpy/plotting/anndata.py#L741)):. ```python; import pandas as pd; adata = sc.AnnData(A) # from above; pd.DataFrame(A) # Throws error; pd.DataFrame(adata.X, index=adata.obs_name, columns=adata.var_name).head() # returns malformed dataframe; 0 ... 99; 0 (0, 9)\t0.76754415\n (0, 28)\t0.16313511\n ... ... (0, 9)\t0.76754415\n (0, 28)\t0.16313511\n ...; 1 (0, 9)\t0.76754415\n (0, 28)\t0.16313511\n ... ... (0, 9)\t0.76754415\n (0, 28)\t0.16313511\n ...; 2 (0, 9)\t0.76754415\n (0, 28)\t0.16313511\n ... ... (0, 9)\t0.76754415\n (0, 28)\t0.16313511\n ...; 3 (0, 9)\t0.76754415\n (0, 28)\t0.16313511\n ... ... (0, 9)\t0.76754415\n (0, 28)\t0.16313511\n ...; 4 (0, 9)\t0.76754415\n (0, 28)\t0.16313511\n ... ... (0, 9)\t0.76754415\n (0, 28)\t0.16313511\n ... [5 rows x 100 columns]. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/356
https://github.com/scverse/scanpy/issues/356:3601,Integrability,wrap,wrapper,3601,"is None:; --> 564 self.linkage = self.calculated_linkage; 565 else:; 566 self.linkage = linkage. /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in calculated_linkage(self); 624 def calculated_linkage(self):; 625 try:; --> 626 return self._calculate_linkage_fastcluster(); 627 except ImportError:; 628 return self._calculate_linkage_scipy(). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in _calculate_linkage_fastcluster(self); 618 else:; 619 linkage = fastcluster.linkage(self.array, method=self.method,; --> 620 metric=self.metric); 621 return linkage; 622 . /usr/local/lib/python3.6/site-packages/fastcluster.py in linkage(X, method, metric, preserve_input); 241 assert X.ndim==2; 242 N = len(X); --> 243 X = pdist(X, metric); 244 X = array(X, dtype=double, copy=False, order='C', subok=True); 245 Z = empty((N-1,4)). /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in pdist(X, metric, *args, **kwargs); 1932 if metric_name is not None:; 1933 X, typ, kwargs = _validate_pdist_input(X, m, n,; -> 1934 metric_name, **kwargs); 1935 ; 1936 # get pdist wrapper. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _validate_pdist_input(X, m, n, metric_name, **kwargs); 287 typ = types[types.index(X.dtype)] if X.dtype in types else types[0]; 288 # validate data; --> 289 X = _convert_to_type(X, out_type=typ); 290 ; 291 # validate kwargs. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _convert_to_type(X, out_type); 182 ; 183 def _convert_to_type(X, out_type):; --> 184 return np.ascontiguousarray(X, dtype=out_type); 185 ; 186 . /usr/local/lib/python3.6/site-packages/numpy/core/numeric.py in ascontiguousarray(a, dtype); 588 ; 589 """"""; --> 590 return array(a, dtype, copy=False, order='C', ndmin=1); 591 ; 592 . ValueError: setting an array element with a sequence.; ```. </details>. -------------------. I think this is happening because `pandas` doesn't like being passed a sparse array ([which can happen in `sc.pl.cluster",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/356
https://github.com/scverse/scanpy/issues/356:3812,Security,validat,validate,3812,"culate_linkage_scipy(). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in _calculate_linkage_fastcluster(self); 618 else:; 619 linkage = fastcluster.linkage(self.array, method=self.method,; --> 620 metric=self.metric); 621 return linkage; 622 . /usr/local/lib/python3.6/site-packages/fastcluster.py in linkage(X, method, metric, preserve_input); 241 assert X.ndim==2; 242 N = len(X); --> 243 X = pdist(X, metric); 244 X = array(X, dtype=double, copy=False, order='C', subok=True); 245 Z = empty((N-1,4)). /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in pdist(X, metric, *args, **kwargs); 1932 if metric_name is not None:; 1933 X, typ, kwargs = _validate_pdist_input(X, m, n,; -> 1934 metric_name, **kwargs); 1935 ; 1936 # get pdist wrapper. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _validate_pdist_input(X, m, n, metric_name, **kwargs); 287 typ = types[types.index(X.dtype)] if X.dtype in types else types[0]; 288 # validate data; --> 289 X = _convert_to_type(X, out_type=typ); 290 ; 291 # validate kwargs. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _convert_to_type(X, out_type); 182 ; 183 def _convert_to_type(X, out_type):; --> 184 return np.ascontiguousarray(X, dtype=out_type); 185 ; 186 . /usr/local/lib/python3.6/site-packages/numpy/core/numeric.py in ascontiguousarray(a, dtype); 588 ; 589 """"""; --> 590 return array(a, dtype, copy=False, order='C', ndmin=1); 591 ; 592 . ValueError: setting an array element with a sequence.; ```. </details>. -------------------. I think this is happening because `pandas` doesn't like being passed a sparse array ([which can happen in `sc.pl.clustermap`](https://github.com/theislab/scanpy/blob/7de1f5159c91d6d2243bb7866d9495ee6747c750/scanpy/plotting/anndata.py#L741)):. ```python; import pandas as pd; adata = sc.AnnData(A) # from above; pd.DataFrame(A) # Throws error; pd.DataFrame(adata.X, index=adata.obs_name, columns=adata.var_name).head() # returns malformed datafram",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/356
https://github.com/scverse/scanpy/issues/356:3886,Security,validat,validate,3886,"culate_linkage_scipy(). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in _calculate_linkage_fastcluster(self); 618 else:; 619 linkage = fastcluster.linkage(self.array, method=self.method,; --> 620 metric=self.metric); 621 return linkage; 622 . /usr/local/lib/python3.6/site-packages/fastcluster.py in linkage(X, method, metric, preserve_input); 241 assert X.ndim==2; 242 N = len(X); --> 243 X = pdist(X, metric); 244 X = array(X, dtype=double, copy=False, order='C', subok=True); 245 Z = empty((N-1,4)). /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in pdist(X, metric, *args, **kwargs); 1932 if metric_name is not None:; 1933 X, typ, kwargs = _validate_pdist_input(X, m, n,; -> 1934 metric_name, **kwargs); 1935 ; 1936 # get pdist wrapper. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _validate_pdist_input(X, m, n, metric_name, **kwargs); 287 typ = types[types.index(X.dtype)] if X.dtype in types else types[0]; 288 # validate data; --> 289 X = _convert_to_type(X, out_type=typ); 290 ; 291 # validate kwargs. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _convert_to_type(X, out_type); 182 ; 183 def _convert_to_type(X, out_type):; --> 184 return np.ascontiguousarray(X, dtype=out_type); 185 ; 186 . /usr/local/lib/python3.6/site-packages/numpy/core/numeric.py in ascontiguousarray(a, dtype); 588 ; 589 """"""; --> 590 return array(a, dtype, copy=False, order='C', ndmin=1); 591 ; 592 . ValueError: setting an array element with a sequence.; ```. </details>. -------------------. I think this is happening because `pandas` doesn't like being passed a sparse array ([which can happen in `sc.pl.clustermap`](https://github.com/theislab/scanpy/blob/7de1f5159c91d6d2243bb7866d9495ee6747c750/scanpy/plotting/anndata.py#L741)):. ```python; import pandas as pd; adata = sc.AnnData(A) # from above; pd.DataFrame(A) # Throws error; pd.DataFrame(adata.X, index=adata.obs_name, columns=adata.var_name).head() # returns malformed datafram",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/356
https://github.com/scverse/scanpy/issues/356:3199,Testability,assert,assert,3199,", ax); 745 plotter = _DendrogramPlotter(data, linkage=linkage, axis=axis,; 746 metric=metric, method=method,; --> 747 label=label, rotate=rotate); 748 if ax is None:; 749 ax = plt.gca(). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in __init__(self, data, linkage, metric, method, axis, label, rotate); 562 ; 563 if linkage is None:; --> 564 self.linkage = self.calculated_linkage; 565 else:; 566 self.linkage = linkage. /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in calculated_linkage(self); 624 def calculated_linkage(self):; 625 try:; --> 626 return self._calculate_linkage_fastcluster(); 627 except ImportError:; 628 return self._calculate_linkage_scipy(). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in _calculate_linkage_fastcluster(self); 618 else:; 619 linkage = fastcluster.linkage(self.array, method=self.method,; --> 620 metric=self.metric); 621 return linkage; 622 . /usr/local/lib/python3.6/site-packages/fastcluster.py in linkage(X, method, metric, preserve_input); 241 assert X.ndim==2; 242 N = len(X); --> 243 X = pdist(X, metric); 244 X = array(X, dtype=double, copy=False, order='C', subok=True); 245 Z = empty((N-1,4)). /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in pdist(X, metric, *args, **kwargs); 1932 if metric_name is not None:; 1933 X, typ, kwargs = _validate_pdist_input(X, m, n,; -> 1934 metric_name, **kwargs); 1935 ; 1936 # get pdist wrapper. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _validate_pdist_input(X, m, n, metric_name, **kwargs); 287 typ = types[types.index(X.dtype)] if X.dtype in types else types[0]; 288 # validate data; --> 289 X = _convert_to_type(X, out_type=typ); 290 ; 291 # validate kwargs. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _convert_to_type(X, out_type); 182 ; 183 def _convert_to_type(X, out_type):; --> 184 return np.ascontiguousarray(X, dtype=out_type); 185 ; 186 . /usr/local/lib/python3.6/site-packages/numpy/core/numeric.py ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/356
https://github.com/scverse/scanpy/pull/357:83,Testability,test,test,83,"Fixes #356 for me. It's a pretty simple change. It's a little hard for me to add a test at the moment, but I'm pretty sure this works.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/357
https://github.com/scverse/scanpy/pull/357:33,Usability,simpl,simple,33,"Fixes #356 for me. It's a pretty simple change. It's a little hard for me to add a test at the moment, but I'm pretty sure this works.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/357
https://github.com/scverse/scanpy/pull/358:903,Modifiability,variab,variables,903,"Following on discussion from #316, I've renamed a number of arguments and metrics. Additionally I've optimized a bit for memory usage. Changes to naming can be summarized as follows:. | current | proposed |; | ------- | -------- |; |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|; |`total_{expr_values}` | `total_{expr_type}`|; |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|; |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{qc_var}`|; |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{qc_var}`|; | | |; |`total_{expr_values}` | `total_{expr_type}`|; |`mean_{expr_values}` | `mean_{expr_type}`|; |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|; |`pct_dropout_by_{expr_values}` | `pct_dropout_by_{expr_type}`|. I went with `qc_vars` over `control_vars` on the recommendation of a lab mate, since they are presumably variables which are important for quality control, but were not necessarily controlled.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358
https://github.com/scverse/scanpy/pull/358:101,Performance,optimiz,optimized,101,"Following on discussion from #316, I've renamed a number of arguments and metrics. Additionally I've optimized a bit for memory usage. Changes to naming can be summarized as follows:. | current | proposed |; | ------- | -------- |; |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|; |`total_{expr_values}` | `total_{expr_type}`|; |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|; |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{qc_var}`|; |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{qc_var}`|; | | |; |`total_{expr_values}` | `total_{expr_type}`|; |`mean_{expr_values}` | `mean_{expr_type}`|; |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|; |`pct_dropout_by_{expr_values}` | `pct_dropout_by_{expr_type}`|. I went with `qc_vars` over `control_vars` on the recommendation of a lab mate, since they are presumably variables which are important for quality control, but were not necessarily controlled.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358
https://github.com/scverse/scanpy/issues/363:1024,Modifiability,layers,layers,1024," object with n_obs × n_vars = 29322 × 19860. ```python; >>> tiss[tiss.obs['cell_ontology_class']=='B cell']; ```. ```pytb; IndexError Traceback (most recent call last); <ipython-input-269-28b4524131cb> in <module>(); ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1299 def __getitem__(self, index):; 1300 """"""Returns a sliced view of the object.""""""; -> 1301 return self._getitem_view(index); 1302 ; 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 713 raise KeyError('Unknown Index type'); 714 # fix categories; --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new); 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new); 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns); 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[; 1319 np.where(np.in1d(; -> 1320 all_categories, df_sub[k].cat.categories))[0]]; 1321 ; 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7; ```. even though it's part of the set:; ```py; >>> set(tiss.obs['cell_ontology_class']); {'B cell',; 'NA',; '",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/pull/364:324,Energy Efficiency,Adapt,Adapt,324,"Currently it’s almost a MVP. What do you people think (especially @mbuttner)? Possible improvements:. - [x] **Heuristic for neighborhood size**; - [x] Expose more data: E.g. a column in `.obs` with the corrected p values; - [ ] **Better docs**; - [ ] **groupby**; - [ ] Mean of multiple samples instead of whole data; - [ ] Adapt to cells that appear in no neighborhoods; - [ ] Speedups?. I guess at least the heuristic and the docs have to go in!. I also don’t know if relying on the previous `neighbors` call is a good idea. Its default is 15 Neighbors, and even for a small (200 cells) dataset, the number the heuristic determined was 75.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:324,Modifiability,Adapt,Adapt,324,"Currently it’s almost a MVP. What do you people think (especially @mbuttner)? Possible improvements:. - [x] **Heuristic for neighborhood size**; - [x] Expose more data: E.g. a column in `.obs` with the corrected p values; - [ ] **Better docs**; - [ ] **groupby**; - [ ] Mean of multiple samples instead of whole data; - [ ] Adapt to cells that appear in no neighborhoods; - [ ] Speedups?. I guess at least the heuristic and the docs have to go in!. I also don’t know if relying on the previous `neighbors` call is a good idea. Its default is 15 Neighbors, and even for a small (200 cells) dataset, the number the heuristic determined was 75.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:151,Security,Expose,Expose,151,"Currently it’s almost a MVP. What do you people think (especially @mbuttner)? Possible improvements:. - [x] **Heuristic for neighborhood size**; - [x] Expose more data: E.g. a column in `.obs` with the corrected p values; - [ ] **Better docs**; - [ ] **groupby**; - [ ] Mean of multiple samples instead of whole data; - [ ] Adapt to cells that appear in no neighborhoods; - [ ] Speedups?. I guess at least the heuristic and the docs have to go in!. I also don’t know if relying on the previous `neighbors` call is a good idea. Its default is 15 Neighbors, and even for a small (200 cells) dataset, the number the heuristic determined was 75.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/issues/365:324,Availability,error,error,324,"Hi,. I have been Scanpy for a short time and I find it really great!; However, I tried recently to use it for differential expression using rank_genes_groups and I could not make it work. I have a single-cell RNA-seq dataset with cell types.; When running `rank_genes_groups(adata, groupby=""celltype"")`, I get the following error:. ```; ~/.py3Env/lib/python3.5/site-packages/scanpy/tools/rank_genes_groups.py` in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 374 adata.uns[key_added]['names'] = np.rec.fromarrays(; 375 [n for n in rankings_gene_names],; --> 376 dtype=[(rn, 'U50') for rn in groups_order_save]); 377; 378 if method in {'t-test', 't-test_overestim_var', 'wilcoxon'}:. ~/.py3Env/lib/python3.5/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder); 632 # populate the record array (makes a copy); 633 for i in range(len(arrayList)):; --> 634 _array[_names[i]] = arrayList[i]; 635; 636 return _array. ValueError: setting an array element with a sequence; ```. Do you have any idea of what could cause this error?. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:1167,Availability,error,error,1167,"Hi,. I have been Scanpy for a short time and I find it really great!; However, I tried recently to use it for differential expression using rank_genes_groups and I could not make it work. I have a single-cell RNA-seq dataset with cell types.; When running `rank_genes_groups(adata, groupby=""celltype"")`, I get the following error:. ```; ~/.py3Env/lib/python3.5/site-packages/scanpy/tools/rank_genes_groups.py` in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 374 adata.uns[key_added]['names'] = np.rec.fromarrays(; 375 [n for n in rankings_gene_names],; --> 376 dtype=[(rn, 'U50') for rn in groups_order_save]); 377; 378 if method in {'t-test', 't-test_overestim_var', 'wilcoxon'}:. ~/.py3Env/lib/python3.5/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder); 632 # populate the record array (makes a copy); 633 for i in range(len(arrayList)):; --> 634 _array[_names[i]] = arrayList[i]; 635; 636 return _array. ValueError: setting an array element with a sequence; ```. Do you have any idea of what could cause this error?. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:722,Testability,test,test,722,"Hi,. I have been Scanpy for a short time and I find it really great!; However, I tried recently to use it for differential expression using rank_genes_groups and I could not make it work. I have a single-cell RNA-seq dataset with cell types.; When running `rank_genes_groups(adata, groupby=""celltype"")`, I get the following error:. ```; ~/.py3Env/lib/python3.5/site-packages/scanpy/tools/rank_genes_groups.py` in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 374 adata.uns[key_added]['names'] = np.rec.fromarrays(; 375 [n for n in rankings_gene_names],; --> 376 dtype=[(rn, 'U50') for rn in groups_order_save]); 377; 378 if method in {'t-test', 't-test_overestim_var', 'wilcoxon'}:. ~/.py3Env/lib/python3.5/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder); 632 # populate the record array (makes a copy); 633 for i in range(len(arrayList)):; --> 634 _array[_names[i]] = arrayList[i]; 635; 636 return _array. ValueError: setting an array element with a sequence; ```. Do you have any idea of what could cause this error?. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/366:120,Deployability,pipeline,pipeline,120,Hi . We have purchased the Nadia dolomite machine which is the automated version of dropseq. I am using the dropseqpipe pipeline for demultiplexing and generate count matrix. Analysis with Seurat is fine but how do you load a count matrix in scanpy?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/366
https://github.com/scverse/scanpy/issues/366:219,Performance,load,load,219,Hi . We have purchased the Nadia dolomite machine which is the automated version of dropseq. I am using the dropseqpipe pipeline for demultiplexing and generate count matrix. Analysis with Seurat is fine but how do you load a count matrix in scanpy?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/366
https://github.com/scverse/scanpy/issues/367:470,Availability,mask,mask,470,"I calculated `sc.tl.paga(adata, groups='cell_ontology_class')` without problems but I couldn't run `sc.tl.paga_expression_entropies(adata)`. I've modified the original code and it now runs - if this looks good you can perhaps update the original code? also, if it doesn't let me know so I don't carry over the mistakes!. ```; from scipy.stats import entropy; groups_order, groups_masks = sc.utils.select_groups(tiss, key=tiss.uns['paga']['groups']); entropies = []; for mask in groups_masks:; X_mask = tiss.X[mask].todense(); x_median = np.nanmedian(X_mask, axis=1,overwrite_input=True); x_probs = (x_median - np.nanmin(x_median)) / (np.nanmax(x_median) - np.nanmin(x_median)); entropies.append(entropy(x_probs)); entropies; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/367
https://github.com/scverse/scanpy/issues/367:509,Availability,mask,mask,509,"I calculated `sc.tl.paga(adata, groups='cell_ontology_class')` without problems but I couldn't run `sc.tl.paga_expression_entropies(adata)`. I've modified the original code and it now runs - if this looks good you can perhaps update the original code? also, if it doesn't let me know so I don't carry over the mistakes!. ```; from scipy.stats import entropy; groups_order, groups_masks = sc.utils.select_groups(tiss, key=tiss.uns['paga']['groups']); entropies = []; for mask in groups_masks:; X_mask = tiss.X[mask].todense(); x_median = np.nanmedian(X_mask, axis=1,overwrite_input=True); x_probs = (x_median - np.nanmin(x_median)) / (np.nanmax(x_median) - np.nanmin(x_median)); entropies.append(entropy(x_probs)); entropies; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/367
https://github.com/scverse/scanpy/issues/367:226,Deployability,update,update,226,"I calculated `sc.tl.paga(adata, groups='cell_ontology_class')` without problems but I couldn't run `sc.tl.paga_expression_entropies(adata)`. I've modified the original code and it now runs - if this looks good you can perhaps update the original code? also, if it doesn't let me know so I don't carry over the mistakes!. ```; from scipy.stats import entropy; groups_order, groups_masks = sc.utils.select_groups(tiss, key=tiss.uns['paga']['groups']); entropies = []; for mask in groups_masks:; X_mask = tiss.X[mask].todense(); x_median = np.nanmedian(X_mask, axis=1,overwrite_input=True); x_probs = (x_median - np.nanmin(x_median)) / (np.nanmax(x_median) - np.nanmin(x_median)); entropies.append(entropy(x_probs)); entropies; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/367
https://github.com/scverse/scanpy/pull/369:763,Energy Efficiency,reduce,reduced,763,"This PR introduces the following changes:. * `swap_axes` option was added to `pl.matrixplot` and `pl.heatmap`. When `swap_axes=True`, the x axis contains cells and the y axis contains genes (#349).; * added `show_genes_labels` to `pl.heatmap`. This allows to have compact heatmaps without overlapping gene labels.; * added lines to separate categories in `pl.heatmap`.; * changed categories colors in `pl.heatmap` by the colors found in `adata.uns`; * removed empty space that was present in different plots; * added a `layer` option to specify which layer to use for plotting. ; * added a new visualization called `pl.tracksplot`. ; * changed `if <variable> is True` by `if <variable>` after @flying-sheep remarks. ; * added `setup()` from matplotlib.testing; * reduced dpi of test images to 40.; * added var_groups plot for stacked_violin when `swap_axes=True` ; * improved layout of stacked_violin (default width and linewidth of violin plots). As an example, here is how some of the changes look like. I am using the `rank_genes_groups_*` plots because they contain more visual elements. Further examples can be seen here: https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c. **heatmap** with `swap_axes=True, show_gene_labels=False`:; ![image](https://user-images.githubusercontent.com/4964309/48777204-46358500-ecd2-11e8-8ced-e772e0987f95.png). **matrixplot** with `swap_axes=True`:. ![image](https://user-images.githubusercontent.com/4964309/48777447-dffd3200-ecd2-11e8-9720-31e084eec0f4.png). **new *tracksplot***: Each *track* contains the var (genes) values sorted and colored according to the categories used:; ![image](https://user-images.githubusercontent.com/4964309/48777284-7c730480-ecd2-11e8-8e3b-ab4a02969311.png). **tracksplot** using the results of `sc.tl.rank_genes_groups`:; ![image](https://user-images.githubusercontent.com/4964309/48777641-6fa2e080-ecd3-11e8-90be-e3742058eb99.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:649,Modifiability,variab,variable,649,"This PR introduces the following changes:. * `swap_axes` option was added to `pl.matrixplot` and `pl.heatmap`. When `swap_axes=True`, the x axis contains cells and the y axis contains genes (#349).; * added `show_genes_labels` to `pl.heatmap`. This allows to have compact heatmaps without overlapping gene labels.; * added lines to separate categories in `pl.heatmap`.; * changed categories colors in `pl.heatmap` by the colors found in `adata.uns`; * removed empty space that was present in different plots; * added a `layer` option to specify which layer to use for plotting. ; * added a new visualization called `pl.tracksplot`. ; * changed `if <variable> is True` by `if <variable>` after @flying-sheep remarks. ; * added `setup()` from matplotlib.testing; * reduced dpi of test images to 40.; * added var_groups plot for stacked_violin when `swap_axes=True` ; * improved layout of stacked_violin (default width and linewidth of violin plots). As an example, here is how some of the changes look like. I am using the `rank_genes_groups_*` plots because they contain more visual elements. Further examples can be seen here: https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c. **heatmap** with `swap_axes=True, show_gene_labels=False`:; ![image](https://user-images.githubusercontent.com/4964309/48777204-46358500-ecd2-11e8-8ced-e772e0987f95.png). **matrixplot** with `swap_axes=True`:. ![image](https://user-images.githubusercontent.com/4964309/48777447-dffd3200-ecd2-11e8-9720-31e084eec0f4.png). **new *tracksplot***: Each *track* contains the var (genes) values sorted and colored according to the categories used:; ![image](https://user-images.githubusercontent.com/4964309/48777284-7c730480-ecd2-11e8-8e3b-ab4a02969311.png). **tracksplot** using the results of `sc.tl.rank_genes_groups`:; ![image](https://user-images.githubusercontent.com/4964309/48777641-6fa2e080-ecd3-11e8-90be-e3742058eb99.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:676,Modifiability,variab,variable,676,"This PR introduces the following changes:. * `swap_axes` option was added to `pl.matrixplot` and `pl.heatmap`. When `swap_axes=True`, the x axis contains cells and the y axis contains genes (#349).; * added `show_genes_labels` to `pl.heatmap`. This allows to have compact heatmaps without overlapping gene labels.; * added lines to separate categories in `pl.heatmap`.; * changed categories colors in `pl.heatmap` by the colors found in `adata.uns`; * removed empty space that was present in different plots; * added a `layer` option to specify which layer to use for plotting. ; * added a new visualization called `pl.tracksplot`. ; * changed `if <variable> is True` by `if <variable>` after @flying-sheep remarks. ; * added `setup()` from matplotlib.testing; * reduced dpi of test images to 40.; * added var_groups plot for stacked_violin when `swap_axes=True` ; * improved layout of stacked_violin (default width and linewidth of violin plots). As an example, here is how some of the changes look like. I am using the `rank_genes_groups_*` plots because they contain more visual elements. Further examples can be seen here: https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c. **heatmap** with `swap_axes=True, show_gene_labels=False`:; ![image](https://user-images.githubusercontent.com/4964309/48777204-46358500-ecd2-11e8-8ced-e772e0987f95.png). **matrixplot** with `swap_axes=True`:. ![image](https://user-images.githubusercontent.com/4964309/48777447-dffd3200-ecd2-11e8-9720-31e084eec0f4.png). **new *tracksplot***: Each *track* contains the var (genes) values sorted and colored according to the categories used:; ![image](https://user-images.githubusercontent.com/4964309/48777284-7c730480-ecd2-11e8-8e3b-ab4a02969311.png). **tracksplot** using the results of `sc.tl.rank_genes_groups`:; ![image](https://user-images.githubusercontent.com/4964309/48777641-6fa2e080-ecd3-11e8-90be-e3742058eb99.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:752,Testability,test,testing,752,"This PR introduces the following changes:. * `swap_axes` option was added to `pl.matrixplot` and `pl.heatmap`. When `swap_axes=True`, the x axis contains cells and the y axis contains genes (#349).; * added `show_genes_labels` to `pl.heatmap`. This allows to have compact heatmaps without overlapping gene labels.; * added lines to separate categories in `pl.heatmap`.; * changed categories colors in `pl.heatmap` by the colors found in `adata.uns`; * removed empty space that was present in different plots; * added a `layer` option to specify which layer to use for plotting. ; * added a new visualization called `pl.tracksplot`. ; * changed `if <variable> is True` by `if <variable>` after @flying-sheep remarks. ; * added `setup()` from matplotlib.testing; * reduced dpi of test images to 40.; * added var_groups plot for stacked_violin when `swap_axes=True` ; * improved layout of stacked_violin (default width and linewidth of violin plots). As an example, here is how some of the changes look like. I am using the `rank_genes_groups_*` plots because they contain more visual elements. Further examples can be seen here: https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c. **heatmap** with `swap_axes=True, show_gene_labels=False`:; ![image](https://user-images.githubusercontent.com/4964309/48777204-46358500-ecd2-11e8-8ced-e772e0987f95.png). **matrixplot** with `swap_axes=True`:. ![image](https://user-images.githubusercontent.com/4964309/48777447-dffd3200-ecd2-11e8-9720-31e084eec0f4.png). **new *tracksplot***: Each *track* contains the var (genes) values sorted and colored according to the categories used:; ![image](https://user-images.githubusercontent.com/4964309/48777284-7c730480-ecd2-11e8-8e3b-ab4a02969311.png). **tracksplot** using the results of `sc.tl.rank_genes_groups`:; ![image](https://user-images.githubusercontent.com/4964309/48777641-6fa2e080-ecd3-11e8-90be-e3742058eb99.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:778,Testability,test,test,778,"This PR introduces the following changes:. * `swap_axes` option was added to `pl.matrixplot` and `pl.heatmap`. When `swap_axes=True`, the x axis contains cells and the y axis contains genes (#349).; * added `show_genes_labels` to `pl.heatmap`. This allows to have compact heatmaps without overlapping gene labels.; * added lines to separate categories in `pl.heatmap`.; * changed categories colors in `pl.heatmap` by the colors found in `adata.uns`; * removed empty space that was present in different plots; * added a `layer` option to specify which layer to use for plotting. ; * added a new visualization called `pl.tracksplot`. ; * changed `if <variable> is True` by `if <variable>` after @flying-sheep remarks. ; * added `setup()` from matplotlib.testing; * reduced dpi of test images to 40.; * added var_groups plot for stacked_violin when `swap_axes=True` ; * improved layout of stacked_violin (default width and linewidth of violin plots). As an example, here is how some of the changes look like. I am using the `rank_genes_groups_*` plots because they contain more visual elements. Further examples can be seen here: https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c. **heatmap** with `swap_axes=True, show_gene_labels=False`:; ![image](https://user-images.githubusercontent.com/4964309/48777204-46358500-ecd2-11e8-8ced-e772e0987f95.png). **matrixplot** with `swap_axes=True`:. ![image](https://user-images.githubusercontent.com/4964309/48777447-dffd3200-ecd2-11e8-9720-31e084eec0f4.png). **new *tracksplot***: Each *track* contains the var (genes) values sorted and colored according to the categories used:; ![image](https://user-images.githubusercontent.com/4964309/48777284-7c730480-ecd2-11e8-8e3b-ab4a02969311.png). **tracksplot** using the results of `sc.tl.rank_genes_groups`:; ![image](https://user-images.githubusercontent.com/4964309/48777641-6fa2e080-ecd3-11e8-90be-e3742058eb99.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/issues/373:388,Usability,simpl,simply,388,"@falexwolf wrote:. > > Wow, this looks great! One remark for future PRs: We’re migrating to a new documentation style using type annotations; > ; > I'm still not convinced that we should use type annotations for Scanpy toplevel functions. People use Scanpy in Jupyter Lab and notebooks and not in Pycharm. Hence, there is no gain in the annotations, by contrast, the function annotations simply look super complicated and it's no longer feasible to grasp at first sight what's going on. This also regards the output of the help in Jupyter Lab and Notebooks.; > ; > So, while I think that for AnnData and everything in the background, type annotations may make sense for a few developers (not for me, as I'm doing everything on remote servers using emacs), it doesn't make sense for the Scanpy user.; > ; > Also, all the other big packages I work with all the time simply don't have it (numpy, seaborn, pandas, tensorflow) and it makes it harder and lengthier for contributors to contribute if they need to go through it.; > ; > Finally, I'm still not happy about how the automatically generated docs from the type annotation look:; > ![image](https://user-images.githubusercontent.com/16916678/48796750-6ebb8000-ecce-11e8-9cdc-33b6056d8957.png); > which is from; > ![image](https://user-images.githubusercontent.com/16916678/48796824-a0344b80-ecce-11e8-8570-e4754f4ccd96.png); > Clearly, the automatically generated line with `Union[...]` is just way too complicated for a human to make sense of. The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous.; > ; > So, please let's stay away from having more type annotations and corresponding docstrings at this stage and let's simply continue imitating what all the major packages are doing.; > ; > Also: regarding your comment about the use of '``' vs. '`' in the docs: again, I think it leads to an inhomogeneous appearance to have *two* types of markup for code-related things. I agree that the read-the-docs i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:864,Usability,simpl,simply,864,"@falexwolf wrote:. > > Wow, this looks great! One remark for future PRs: We’re migrating to a new documentation style using type annotations; > ; > I'm still not convinced that we should use type annotations for Scanpy toplevel functions. People use Scanpy in Jupyter Lab and notebooks and not in Pycharm. Hence, there is no gain in the annotations, by contrast, the function annotations simply look super complicated and it's no longer feasible to grasp at first sight what's going on. This also regards the output of the help in Jupyter Lab and Notebooks.; > ; > So, while I think that for AnnData and everything in the background, type annotations may make sense for a few developers (not for me, as I'm doing everything on remote servers using emacs), it doesn't make sense for the Scanpy user.; > ; > Also, all the other big packages I work with all the time simply don't have it (numpy, seaborn, pandas, tensorflow) and it makes it harder and lengthier for contributors to contribute if they need to go through it.; > ; > Finally, I'm still not happy about how the automatically generated docs from the type annotation look:; > ![image](https://user-images.githubusercontent.com/16916678/48796750-6ebb8000-ecce-11e8-9cdc-33b6056d8957.png); > which is from; > ![image](https://user-images.githubusercontent.com/16916678/48796824-a0344b80-ecce-11e8-8570-e4754f4ccd96.png); > Clearly, the automatically generated line with `Union[...]` is just way too complicated for a human to make sense of. The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous.; > ; > So, please let's stay away from having more type annotations and corresponding docstrings at this stage and let's simply continue imitating what all the major packages are doing.; > ; > Also: regarding your comment about the use of '``' vs. '`' in the docs: again, I think it leads to an inhomogeneous appearance to have *two* types of markup for code-related things. I agree that the read-the-docs i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1379,Usability,Clear,Clearly,1379,"n Pycharm. Hence, there is no gain in the annotations, by contrast, the function annotations simply look super complicated and it's no longer feasible to grasp at first sight what's going on. This also regards the output of the help in Jupyter Lab and Notebooks.; > ; > So, while I think that for AnnData and everything in the background, type annotations may make sense for a few developers (not for me, as I'm doing everything on remote servers using emacs), it doesn't make sense for the Scanpy user.; > ; > Also, all the other big packages I work with all the time simply don't have it (numpy, seaborn, pandas, tensorflow) and it makes it harder and lengthier for contributors to contribute if they need to go through it.; > ; > Finally, I'm still not happy about how the automatically generated docs from the type annotation look:; > ![image](https://user-images.githubusercontent.com/16916678/48796750-6ebb8000-ecce-11e8-9cdc-33b6056d8957.png); > which is from; > ![image](https://user-images.githubusercontent.com/16916678/48796824-a0344b80-ecce-11e8-8570-e4754f4ccd96.png); > Clearly, the automatically generated line with `Union[...]` is just way too complicated for a human to make sense of. The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous.; > ; > So, please let's stay away from having more type annotations and corresponding docstrings at this stage and let's simply continue imitating what all the major packages are doing.; > ; > Also: regarding your comment about the use of '``' vs. '`' in the docs: again, I think it leads to an inhomogeneous appearance to have *two* types of markup for code-related things. I agree that the read-the-docs italicized default style for '`' might be supoptimal, and I'll work on that if there is some time. But in general, I think there should be essentially one markup for code, as it's done in the tensorflow docs and a couple of other examples.; > ; > Happy to also discuss offline, @flying-sheep ;).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1715,Usability,simpl,simply,1715,"n Pycharm. Hence, there is no gain in the annotations, by contrast, the function annotations simply look super complicated and it's no longer feasible to grasp at first sight what's going on. This also regards the output of the help in Jupyter Lab and Notebooks.; > ; > So, while I think that for AnnData and everything in the background, type annotations may make sense for a few developers (not for me, as I'm doing everything on remote servers using emacs), it doesn't make sense for the Scanpy user.; > ; > Also, all the other big packages I work with all the time simply don't have it (numpy, seaborn, pandas, tensorflow) and it makes it harder and lengthier for contributors to contribute if they need to go through it.; > ; > Finally, I'm still not happy about how the automatically generated docs from the type annotation look:; > ![image](https://user-images.githubusercontent.com/16916678/48796750-6ebb8000-ecce-11e8-9cdc-33b6056d8957.png); > which is from; > ![image](https://user-images.githubusercontent.com/16916678/48796824-a0344b80-ecce-11e8-8570-e4754f4ccd96.png); > Clearly, the automatically generated line with `Union[...]` is just way too complicated for a human to make sense of. The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous.; > ; > So, please let's stay away from having more type annotations and corresponding docstrings at this stage and let's simply continue imitating what all the major packages are doing.; > ; > Also: regarding your comment about the use of '``' vs. '`' in the docs: again, I think it leads to an inhomogeneous appearance to have *two* types of markup for code-related things. I agree that the read-the-docs italicized default style for '`' might be supoptimal, and I'll work on that if there is some time. But in general, I think there should be essentially one markup for code, as it's done in the tensorflow docs and a couple of other examples.; > ; > Happy to also discuss offline, @flying-sheep ;).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/375:86,Availability,error,error,86,"Hey! I thought plotting `.var` columns in `sc.pl.violin()` worked previously (and the error message seems to suggest this as well). I am doing the following:; ```; adata_counts.var['dropout_per_gene'] = (adata_counts.X > 0).mean(0); adata_counts.obs['dropout_per_cell'] = (adata_counts.X > 0).mean(1). sc.pl.violin(adata_counts, keys='dropout_per_gene'); ```. and I get this error:; ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-8-463060c90a0b> in <module>(); ----> 1 sc.pl.violin(adata_counts, keys='dropout_per_gene'). ~/scanpy/scanpy/plotting/anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 630 X_col = adata.raw[:, key].X; 631 else:; --> 632 X_col = adata[:, key].X; 633 obs_df[key] = X_col; 634 if groupby is None:. ~/anndata/anndata/base.py in __getitem__(self, index); 1303 def __getitem__(self, index):; 1304 """"""Returns a sliced view of the object.""""""; -> 1305 return self._getitem_view(index); 1306 ; 1307 def _getitem_view(self, index):. ~/anndata/anndata/base.py in _getitem_view(self, index); 1306 ; 1307 def _getitem_view(self, index):; -> 1308 oidx, vidx = self._normalize_indices(index); 1309 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1310 . ~/anndata/anndata/base.py in _normalize_indices(self, index); 1283 obs, var = super(AnnData, self)._unpack_index(index); 1284 obs = _normalize_index(obs, self.obs_names); -> 1285 var = _normalize_index(var, self.var_names); 1286 return obs, var; 1287 . ~/anndata/anndata/base.py in _normalize_index(index, names); 261 return slice(start, stop, step); 262 elif isinstance(index, (int, str)):; --> 263 return name_idx(index); 264 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 265 # here, we replaced the implementation based on name_idx with this. ~/anndata/anndata/base.py in name_idx(i); 248 raise IndexError(; 249 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:375,Availability,error,error,375,"Hey! I thought plotting `.var` columns in `sc.pl.violin()` worked previously (and the error message seems to suggest this as well). I am doing the following:; ```; adata_counts.var['dropout_per_gene'] = (adata_counts.X > 0).mean(0); adata_counts.obs['dropout_per_cell'] = (adata_counts.X > 0).mean(1). sc.pl.violin(adata_counts, keys='dropout_per_gene'); ```. and I get this error:; ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-8-463060c90a0b> in <module>(); ----> 1 sc.pl.violin(adata_counts, keys='dropout_per_gene'). ~/scanpy/scanpy/plotting/anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 630 X_col = adata.raw[:, key].X; 631 else:; --> 632 X_col = adata[:, key].X; 633 obs_df[key] = X_col; 634 if groupby is None:. ~/anndata/anndata/base.py in __getitem__(self, index); 1303 def __getitem__(self, index):; 1304 """"""Returns a sliced view of the object.""""""; -> 1305 return self._getitem_view(index); 1306 ; 1307 def _getitem_view(self, index):. ~/anndata/anndata/base.py in _getitem_view(self, index); 1306 ; 1307 def _getitem_view(self, index):; -> 1308 oidx, vidx = self._normalize_indices(index); 1309 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1310 . ~/anndata/anndata/base.py in _normalize_indices(self, index); 1283 obs, var = super(AnnData, self)._unpack_index(index); 1284 obs = _normalize_index(obs, self.obs_names); -> 1285 var = _normalize_index(var, self.var_names); 1286 return obs, var; 1287 . ~/anndata/anndata/base.py in _normalize_index(index, names); 261 return slice(start, stop, step); 262 elif isinstance(index, (int, str)):; --> 263 return name_idx(index); 264 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 265 # here, we replaced the implementation based on name_idx with this. ~/anndata/anndata/base.py in name_idx(i); 248 raise IndexError(; 249 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:92,Integrability,message,message,92,"Hey! I thought plotting `.var` columns in `sc.pl.violin()` worked previously (and the error message seems to suggest this as well). I am doing the following:; ```; adata_counts.var['dropout_per_gene'] = (adata_counts.X > 0).mean(0); adata_counts.obs['dropout_per_cell'] = (adata_counts.X > 0).mean(1). sc.pl.violin(adata_counts, keys='dropout_per_gene'); ```. and I get this error:; ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-8-463060c90a0b> in <module>(); ----> 1 sc.pl.violin(adata_counts, keys='dropout_per_gene'). ~/scanpy/scanpy/plotting/anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 630 X_col = adata.raw[:, key].X; 631 else:; --> 632 X_col = adata[:, key].X; 633 obs_df[key] = X_col; 634 if groupby is None:. ~/anndata/anndata/base.py in __getitem__(self, index); 1303 def __getitem__(self, index):; 1304 """"""Returns a sliced view of the object.""""""; -> 1305 return self._getitem_view(index); 1306 ; 1307 def _getitem_view(self, index):. ~/anndata/anndata/base.py in _getitem_view(self, index); 1306 ; 1307 def _getitem_view(self, index):; -> 1308 oidx, vidx = self._normalize_indices(index); 1309 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1310 . ~/anndata/anndata/base.py in _normalize_indices(self, index); 1283 obs, var = super(AnnData, self)._unpack_index(index); 1284 obs = _normalize_index(obs, self.obs_names); -> 1285 var = _normalize_index(var, self.var_names); 1286 return obs, var; 1287 . ~/anndata/anndata/base.py in _normalize_index(index, names); 261 return slice(start, stop, step); 262 elif isinstance(index, (int, str)):; --> 263 return name_idx(index); 264 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 265 # here, we replaced the implementation based on name_idx with this. ~/anndata/anndata/base.py in name_idx(i); 248 raise IndexError(; 249 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:2036,Modifiability,variab,variable,2036,"aceback (most recent call last); <ipython-input-8-463060c90a0b> in <module>(); ----> 1 sc.pl.violin(adata_counts, keys='dropout_per_gene'). ~/scanpy/scanpy/plotting/anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 630 X_col = adata.raw[:, key].X; 631 else:; --> 632 X_col = adata[:, key].X; 633 obs_df[key] = X_col; 634 if groupby is None:. ~/anndata/anndata/base.py in __getitem__(self, index); 1303 def __getitem__(self, index):; 1304 """"""Returns a sliced view of the object.""""""; -> 1305 return self._getitem_view(index); 1306 ; 1307 def _getitem_view(self, index):. ~/anndata/anndata/base.py in _getitem_view(self, index); 1306 ; 1307 def _getitem_view(self, index):; -> 1308 oidx, vidx = self._normalize_indices(index); 1309 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1310 . ~/anndata/anndata/base.py in _normalize_indices(self, index); 1283 obs, var = super(AnnData, self)._unpack_index(index); 1284 obs = _normalize_index(obs, self.obs_names); -> 1285 var = _normalize_index(var, self.var_names); 1286 return obs, var; 1287 . ~/anndata/anndata/base.py in _normalize_index(index, names); 261 return slice(start, stop, step); 262 elif isinstance(index, (int, str)):; --> 263 return name_idx(index); 264 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 265 # here, we replaced the implementation based on name_idx with this. ~/anndata/anndata/base.py in name_idx(i); 248 raise IndexError(; 249 'Key ""{}"" is not valid observation/variable name/index.'; --> 250 .format(i)); 251 i = i_found[0]; 252 return i. IndexError: Key ""dropout_per_gene"" is not valid observation/variable name/index.; ```. The whole thing works for:; ```; sc.pl.violin(adata_counts.T, keys='dropout_per_gene'); sc.pl.violin(adata_counts, keys='dropout_per_cell); ```. So it's clearly just not taking `.var` columns for `sc.pl.violing()`. . I've also reproduced this with `adata = sc.datasets.blob()`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:2174,Modifiability,variab,variable,2174,"aceback (most recent call last); <ipython-input-8-463060c90a0b> in <module>(); ----> 1 sc.pl.violin(adata_counts, keys='dropout_per_gene'). ~/scanpy/scanpy/plotting/anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 630 X_col = adata.raw[:, key].X; 631 else:; --> 632 X_col = adata[:, key].X; 633 obs_df[key] = X_col; 634 if groupby is None:. ~/anndata/anndata/base.py in __getitem__(self, index); 1303 def __getitem__(self, index):; 1304 """"""Returns a sliced view of the object.""""""; -> 1305 return self._getitem_view(index); 1306 ; 1307 def _getitem_view(self, index):. ~/anndata/anndata/base.py in _getitem_view(self, index); 1306 ; 1307 def _getitem_view(self, index):; -> 1308 oidx, vidx = self._normalize_indices(index); 1309 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1310 . ~/anndata/anndata/base.py in _normalize_indices(self, index); 1283 obs, var = super(AnnData, self)._unpack_index(index); 1284 obs = _normalize_index(obs, self.obs_names); -> 1285 var = _normalize_index(var, self.var_names); 1286 return obs, var; 1287 . ~/anndata/anndata/base.py in _normalize_index(index, names); 261 return slice(start, stop, step); 262 elif isinstance(index, (int, str)):; --> 263 return name_idx(index); 264 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 265 # here, we replaced the implementation based on name_idx with this. ~/anndata/anndata/base.py in name_idx(i); 248 raise IndexError(; 249 'Key ""{}"" is not valid observation/variable name/index.'; --> 250 .format(i)); 251 i = i_found[0]; 252 return i. IndexError: Key ""dropout_per_gene"" is not valid observation/variable name/index.; ```. The whole thing works for:; ```; sc.pl.violin(adata_counts.T, keys='dropout_per_gene'); sc.pl.violin(adata_counts, keys='dropout_per_cell); ```. So it's clearly just not taking `.var` columns for `sc.pl.violing()`. . I've also reproduced this with `adata = sc.datasets.blob()`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:686,Testability,log,log,686,"Hey! I thought plotting `.var` columns in `sc.pl.violin()` worked previously (and the error message seems to suggest this as well). I am doing the following:; ```; adata_counts.var['dropout_per_gene'] = (adata_counts.X > 0).mean(0); adata_counts.obs['dropout_per_cell'] = (adata_counts.X > 0).mean(1). sc.pl.violin(adata_counts, keys='dropout_per_gene'); ```. and I get this error:; ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-8-463060c90a0b> in <module>(); ----> 1 sc.pl.violin(adata_counts, keys='dropout_per_gene'). ~/scanpy/scanpy/plotting/anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 630 X_col = adata.raw[:, key].X; 631 else:; --> 632 X_col = adata[:, key].X; 633 obs_df[key] = X_col; 634 if groupby is None:. ~/anndata/anndata/base.py in __getitem__(self, index); 1303 def __getitem__(self, index):; 1304 """"""Returns a sliced view of the object.""""""; -> 1305 return self._getitem_view(index); 1306 ; 1307 def _getitem_view(self, index):. ~/anndata/anndata/base.py in _getitem_view(self, index); 1306 ; 1307 def _getitem_view(self, index):; -> 1308 oidx, vidx = self._normalize_indices(index); 1309 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1310 . ~/anndata/anndata/base.py in _normalize_indices(self, index); 1283 obs, var = super(AnnData, self)._unpack_index(index); 1284 obs = _normalize_index(obs, self.obs_names); -> 1285 var = _normalize_index(var, self.var_names); 1286 return obs, var; 1287 . ~/anndata/anndata/base.py in _normalize_index(index, names); 261 return slice(start, stop, step); 262 elif isinstance(index, (int, str)):; --> 263 return name_idx(index); 264 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 265 # here, we replaced the implementation based on name_idx with this. ~/anndata/anndata/base.py in name_idx(i); 248 raise IndexError(; 249 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/375:2354,Usability,clear,clearly,2354,"aceback (most recent call last); <ipython-input-8-463060c90a0b> in <module>(); ----> 1 sc.pl.violin(adata_counts, keys='dropout_per_gene'). ~/scanpy/scanpy/plotting/anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 630 X_col = adata.raw[:, key].X; 631 else:; --> 632 X_col = adata[:, key].X; 633 obs_df[key] = X_col; 634 if groupby is None:. ~/anndata/anndata/base.py in __getitem__(self, index); 1303 def __getitem__(self, index):; 1304 """"""Returns a sliced view of the object.""""""; -> 1305 return self._getitem_view(index); 1306 ; 1307 def _getitem_view(self, index):. ~/anndata/anndata/base.py in _getitem_view(self, index); 1306 ; 1307 def _getitem_view(self, index):; -> 1308 oidx, vidx = self._normalize_indices(index); 1309 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1310 . ~/anndata/anndata/base.py in _normalize_indices(self, index); 1283 obs, var = super(AnnData, self)._unpack_index(index); 1284 obs = _normalize_index(obs, self.obs_names); -> 1285 var = _normalize_index(var, self.var_names); 1286 return obs, var; 1287 . ~/anndata/anndata/base.py in _normalize_index(index, names); 261 return slice(start, stop, step); 262 elif isinstance(index, (int, str)):; --> 263 return name_idx(index); 264 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 265 # here, we replaced the implementation based on name_idx with this. ~/anndata/anndata/base.py in name_idx(i); 248 raise IndexError(; 249 'Key ""{}"" is not valid observation/variable name/index.'; --> 250 .format(i)); 251 i = i_found[0]; 252 return i. IndexError: Key ""dropout_per_gene"" is not valid observation/variable name/index.; ```. The whole thing works for:; ```; sc.pl.violin(adata_counts.T, keys='dropout_per_gene'); sc.pl.violin(adata_counts, keys='dropout_per_cell); ```. So it's clearly just not taking `.var` columns for `sc.pl.violing()`. . I've also reproduced this with `adata = sc.datasets.blob()`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375
https://github.com/scverse/scanpy/issues/381:167,Availability,error,error,167,"Dear, ; When I use sc.pl.paga(adata) to show my single cell data with 16 clusters, it works well. However, when I show another data with only 4 cluster, the following error occurs:. ValueError Traceback (most recent call last); ~/bin/miniconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py in scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs); 4226 valid_shape = False; -> 4227 raise ValueError; 4228 except ValueError:. ValueError: . During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last); <ipython-input-29-c0e8bf06937e> in <module>(); ----> 1 sc.pl.paga(adata,color=['louvain','ID']). ~/bin/miniconda3/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, show, save, ax); 396 single_component=single_component,; 397 arrowsize=arrowsize,; --> 398 pos=pos); 399 if colorbars[icolor]:; 400 bottom = panel_pos[0][0]. ~/bin/miniconda3/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py in _paga_graph(adata, ax, layout, layout_kwds, init_pos, solid_edges, dashed_edges, transitions, threshold, root, colors, labels, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, title, pos, cmap, frameon, min_edge_width, max_edge_width, export_to_gexf, cax, colorbar, use_raw, cb_kwds, single_component, arrowsize, random_state); 746 sct = ax.scatter(; 747 pos_array[:, 0], pos_array[:, 1],; --> 748 c=colors, edgecolors='face', s=groups_sizes, cmap=cmap); 749 if fontsize is None:; 750 fontsize = rcParams['legend.fontsize']. ~/bin/miniconda3/lib/pytho",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381
https://github.com/scverse/scanpy/pull/382:473,Deployability,install,install,473,"This is inspired by the Spring exporter. It also fills a directory with output files, but in addition to the matrix, adds cluster markers, meta data and embeddings. I keep the conversion code in the cellbrowser package, so I can change it easily in the future. We could also copy the whole function over, but I don't know if this makes a lot of sense, as people will need the cellbrowser package anyways and currently it doesn't have any dependencies, so should be easy to install. This function can optionally convert to html files, start a webserver and serve the result.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/pull/382:438,Integrability,depend,dependencies,438,"This is inspired by the Spring exporter. It also fills a directory with output files, but in addition to the matrix, adds cluster markers, meta data and embeddings. I keep the conversion code in the cellbrowser package, so I can change it easily in the future. We could also copy the whole function over, but I don't know if this makes a lot of sense, as people will need the cellbrowser package anyways and currently it doesn't have any dependencies, so should be easy to install. This function can optionally convert to html files, start a webserver and serve the result.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382
https://github.com/scverse/scanpy/issues/383:95,Modifiability,variab,variables,95,"I have an adata object and multiple CD4+ and CD8+ cells as observations and gene expression as variables. I want to draw a PCA with each dot corresponding to one cell and colored by cell type (e.g. all CD4 cells in blue, CD8 in orange). Each cell have a unique identifier, and I have a list cd4, which contains identifiers of only CD4 cells, and the same for CD8. I tried the following:; `scn.pl.pca(adata_needed, groups={""cd4"":cd4, ""cd8"":cd8}, palette = [""red"", ""blue""])`; But the result is the PCA plot in grey; ![image](https://user-images.githubusercontent.com/45490688/49294731-db036400-f4c4-11e8-9b8f-00b3d20e5c41.png). I also tried to visualize only one sample:; `scn.pl.pca(adata_needed, groups=[""SRR1551000""])`. But got exactly the same result. I decided to use ""color"" parameter in pca plot:; `scn.pl.pca(adata_needed, color=""SRR1551000"")`; But got the following mistake:. > ValueError: key 'SRR1551000' is invalid! pass valid observation annotation, one of [] or a gene name Index(['ENSG....', 'ENSG...', ). The observation names are there (adata_needed.obs_names outputs the observation I try to give to the function). Here's the plot I need, but drawn with pandas. ; ![image](https://user-images.githubusercontent.com/45490688/49296023-3be06b80-f4c8-11e8-8a21-f1aa9e9b4a46.png). How can I draw a PCA plot like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383
https://github.com/scverse/scanpy/issues/386:45,Modifiability,variab,variable,45,"I would like to use stacked_violin plot with variable y-axis limits, particularly when swap_axes=True. Examples [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c), particularly code in line 7, show this. How do I do this? When I use it now with my code, it always chooses a uniform y-axis limit for all genes. Which option do I use for variable y-axis limits? Maybe this aspect of scanpy.api.pl.stacked_violin() should be better documented.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/386:359,Modifiability,variab,variable,359,"I would like to use stacked_violin plot with variable y-axis limits, particularly when swap_axes=True. Examples [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c), particularly code in line 7, show this. How do I do this? When I use it now with my code, it always chooses a uniform y-axis limit for all genes. Which option do I use for variable y-axis limits? Maybe this aspect of scanpy.api.pl.stacked_violin() should be better documented.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386
https://github.com/scverse/scanpy/issues/387:568,Availability,avail,available,568,"Matplotlib 3 came and went without https://github.com/matplotlib/matplotlib/issues/9460 being fixed. @ftheis still can’t properly distinguish clusters made by scanpy. The two designated colorblind-friendly colormaps in matplotlib look [like this](https://gist.github.com/flying-sheep/5172a1f1f9757d4ece239e742ba41b08). An alternative would be to push red or green to the end of the current color cycle, so it only appears for 10+ clusters. @mpetroff created a [neat tool](https://colorcyclepicker.mpetroff.net/) that could help to design a custom one if we decide the available ones are too ugly.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/387:358,Energy Efficiency,green,green,358,"Matplotlib 3 came and went without https://github.com/matplotlib/matplotlib/issues/9460 being fixed. @ftheis still can’t properly distinguish clusters made by scanpy. The two designated colorblind-friendly colormaps in matplotlib look [like this](https://gist.github.com/flying-sheep/5172a1f1f9757d4ece239e742ba41b08). An alternative would be to push red or green to the end of the current color cycle, so it only appears for 10+ clusters. @mpetroff created a [neat tool](https://colorcyclepicker.mpetroff.net/) that could help to design a custom one if we decide the available ones are too ugly.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387
https://github.com/scverse/scanpy/issues/388:727,Modifiability,variab,variable,727,"First, very nice job on this package! It is a welcome replacement to seurat for python programmers! Yay!. I have a suggestion to improve sc.pl.dotplot(). I would like to be able to adjust the limits of the color_map used for sc.pl.dotplot(). The documentation states that kwargs get passed to matplotlib.pyplot.scatter(). But the dotplot does not change when vmin and vmax are included as kwargs. In the source code for anndata.dotplot includes the following lines:. import matplotlib.colors; normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)); colors = [cmap(normalize(value)) for value in mean_flat]. I believe these lines are the issue. Because vmin and vmax are calculated from the mean_flat variable the vmin and vmax provided as parameters are ignored. Perhaps rather than determining the scale using mean_flat, the vmin and vmax should be used when provided by the user. This would allow the user to set the color_bar scale as they need. Currently, there is no way to change the color_bar and the documentation is misleading, since it suggests that it can be done.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388
https://github.com/scverse/scanpy/pull/390:320,Modifiability,variab,variable,320,"Hi Fidel, ; Here is the pull request for vmin vmax in dotplot...; I am guessing that there could be similar issues with other plotting functions and other plotting keywords.; In general, it would be best to check plotting methods for plotting keywords and use what is provided in kwds by default rather than setting the variable without checking whether it was provided. ; Thanks for your groups great work!; Tim",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390
https://github.com/scverse/scanpy/issues/391:88,Availability,error,error,88,"Hi there,. While running ```sc.pp.highly_variable_genes(adata.X)``` I got the following error:. ```AttributeError: X not found```. I then ran ```sc.pp.highly_variable_genes(adata)``` and got the following:. ```ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]). You can drop duplicate edges by setting the duplicates kwarg ```. The older ```sc.pp.filter_genes_dispersion(adata.X)``` works fine. Do you know how to fix this?. Thank you!. **Info**: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/391:625,Usability,learn,learn,625,"Hi there,. While running ```sc.pp.highly_variable_genes(adata.X)``` I got the following error:. ```AttributeError: X not found```. I then ran ```sc.pp.highly_variable_genes(adata)``` and got the following:. ```ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]). You can drop duplicate edges by setting the duplicates kwarg ```. The older ```sc.pp.filter_genes_dispersion(adata.X)``` works fine. Do you know how to fix this?. Thank you!. **Info**: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391
https://github.com/scverse/scanpy/issues/393:1189,Integrability,depend,depend,1189,"Right now we get different PCs when using sparse data (applying TruncatedSVD) compared to using dense data (applying PCA). . `TruncatedSVD(X-X.mean(0))` would be equivalent to `PCA(X)`. `X-X.mean(0)` would obviously not be sparse anymore, which is why it is currently implemented as `TruncatedSVD(X)`. The first PC will be mainly representing the vector of means, thus be very different from zero-centered PCA. The following components would approximately resemble PCA. However, since all subsequent PCs are orthogonal to the first PC, we will never get to the exact solution. Hence, the PCs are questionable, in particular when the very first ones are quite misleading. That's not desirable. I think we should obtain the same PCA representation regardless of the data type. Don't we have to densify `X` at some point anyways, as we would have to compute `X.dot(X.T)`. Thus it might be worth thinking of some EM approach?. Whatsoever, I think as long the data is manageable and fits into the RAM, we should just use the densified `X`. . Line 486 in preprocessing/simple I don't quite understand:; ```; if zero_center is not None:; zero_center = not issparse(adata_comp.X); ```; It doesn't depend on the actual value of the attribute `zero_center` anymore. Is that a bug, or what is the rationale behind this?. For now, we can change that into something like; ```; zero_center = zero_center if zero_center is not None else False if issparse(adata.X) and adata.X.shape[0] > 1e4 else True; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/393:1063,Usability,simpl,simple,1063,"Right now we get different PCs when using sparse data (applying TruncatedSVD) compared to using dense data (applying PCA). . `TruncatedSVD(X-X.mean(0))` would be equivalent to `PCA(X)`. `X-X.mean(0)` would obviously not be sparse anymore, which is why it is currently implemented as `TruncatedSVD(X)`. The first PC will be mainly representing the vector of means, thus be very different from zero-centered PCA. The following components would approximately resemble PCA. However, since all subsequent PCs are orthogonal to the first PC, we will never get to the exact solution. Hence, the PCs are questionable, in particular when the very first ones are quite misleading. That's not desirable. I think we should obtain the same PCA representation regardless of the data type. Don't we have to densify `X` at some point anyways, as we would have to compute `X.dot(X.T)`. Thus it might be worth thinking of some EM approach?. Whatsoever, I think as long the data is manageable and fits into the RAM, we should just use the densified `X`. . Line 486 in preprocessing/simple I don't quite understand:; ```; if zero_center is not None:; zero_center = not issparse(adata_comp.X); ```; It doesn't depend on the actual value of the attribute `zero_center` anymore. Is that a bug, or what is the rationale behind this?. For now, we can change that into something like; ```; zero_center = zero_center if zero_center is not None else False if issparse(adata.X) and adata.X.shape[0] > 1e4 else True; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393
https://github.com/scverse/scanpy/issues/394:43,Availability,recover,recover,43,"Maybe I am doing something wrong. I try to recover the observation from annData as a dataframe, and I am playing with some of the commands. This is an example. ```; import scanpy.api as sc; data = sc.read_text('/media/SETH_DATA/SETH_Alex/Fibroblasts.txt').T. print(data.data); ```. Then this error happens:. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-14-1f44700b9ea5> in <module>(); ----> 1 print(data.data). ~/anaconda3_6/lib/python3.6/site-packages/anndata/utils.py in new_func(*args, **kwargs); 104 def new_func(*args, **kwargs):; 105 warnings.simplefilter('always', DeprecationWarning) # turn off filter; --> 106 warnings.warning(; 107 'Use {0} instead of {1}, {1} will be removed in the future.'; 108 .format(new_name, func.__name__),. AttributeError: module 'warnings' has no attribute 'warning'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/394
https://github.com/scverse/scanpy/issues/394:292,Availability,error,error,292,"Maybe I am doing something wrong. I try to recover the observation from annData as a dataframe, and I am playing with some of the commands. This is an example. ```; import scanpy.api as sc; data = sc.read_text('/media/SETH_DATA/SETH_Alex/Fibroblasts.txt').T. print(data.data); ```. Then this error happens:. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-14-1f44700b9ea5> in <module>(); ----> 1 print(data.data). ~/anaconda3_6/lib/python3.6/site-packages/anndata/utils.py in new_func(*args, **kwargs); 104 def new_func(*args, **kwargs):; 105 warnings.simplefilter('always', DeprecationWarning) # turn off filter; --> 106 warnings.warning(; 107 'Use {0} instead of {1}, {1} will be removed in the future.'; 108 .format(new_name, func.__name__),. AttributeError: module 'warnings' has no attribute 'warning'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/394
https://github.com/scverse/scanpy/issues/394:43,Safety,recover,recover,43,"Maybe I am doing something wrong. I try to recover the observation from annData as a dataframe, and I am playing with some of the commands. This is an example. ```; import scanpy.api as sc; data = sc.read_text('/media/SETH_DATA/SETH_Alex/Fibroblasts.txt').T. print(data.data); ```. Then this error happens:. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-14-1f44700b9ea5> in <module>(); ----> 1 print(data.data). ~/anaconda3_6/lib/python3.6/site-packages/anndata/utils.py in new_func(*args, **kwargs); 104 def new_func(*args, **kwargs):; 105 warnings.simplefilter('always', DeprecationWarning) # turn off filter; --> 106 warnings.warning(; 107 'Use {0} instead of {1}, {1} will be removed in the future.'; 108 .format(new_name, func.__name__),. AttributeError: module 'warnings' has no attribute 'warning'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/394
https://github.com/scverse/scanpy/issues/394:651,Usability,simpl,simplefilter,651,"Maybe I am doing something wrong. I try to recover the observation from annData as a dataframe, and I am playing with some of the commands. This is an example. ```; import scanpy.api as sc; data = sc.read_text('/media/SETH_DATA/SETH_Alex/Fibroblasts.txt').T. print(data.data); ```. Then this error happens:. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-14-1f44700b9ea5> in <module>(); ----> 1 print(data.data). ~/anaconda3_6/lib/python3.6/site-packages/anndata/utils.py in new_func(*args, **kwargs); 104 def new_func(*args, **kwargs):; 105 warnings.simplefilter('always', DeprecationWarning) # turn off filter; --> 106 warnings.warning(; 107 'Use {0} instead of {1}, {1} will be removed in the future.'; 108 .format(new_name, func.__name__),. AttributeError: module 'warnings' has no attribute 'warning'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/394
https://github.com/scverse/scanpy/pull/398:875,Energy Efficiency,power,power,875,"Hi all,. this is a re-implementation of the ComBat function in python for batch effect removal by Brent Pedersen at the University of Utah which I slightly modified to work with AnnData objects. I asked Brent for permission and he would be happy with us using this. Originally, the code was written in R for the SVA package:; Jeffrey T. Leek, W. Evan Johnson, Hilary S. Parker, Andrew E. Jaffe; and John D. Storey (). sva: Surrogate Variable Analysis. R package; version 3.4.0. The idea is taken from this paper:; Johnson WE, Rabinovic A, Li C (2007). Adjusting batch effects in microarray; expression data using Empirical Bayes methods. Biostatistics 8:118-127. . Originally, the method was developed to adjust for batch effects in microarray data, however, it is commonly applied to scRNA-seq data nowadays. The method fits linear models to the genes and pools statistical power by means of EB to estimate per gene correction factors. . I understand that @mbuttner also has an implementation of this - maybe we can combine and get the best of both approaches?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/398:433,Modifiability,Variab,Variable,433,"Hi all,. this is a re-implementation of the ComBat function in python for batch effect removal by Brent Pedersen at the University of Utah which I slightly modified to work with AnnData objects. I asked Brent for permission and he would be happy with us using this. Originally, the code was written in R for the SVA package:; Jeffrey T. Leek, W. Evan Johnson, Hilary S. Parker, Andrew E. Jaffe; and John D. Storey (). sva: Surrogate Variable Analysis. R package; version 3.4.0. The idea is taken from this paper:; Johnson WE, Rabinovic A, Li C (2007). Adjusting batch effects in microarray; expression data using Empirical Bayes methods. Biostatistics 8:118-127. . Originally, the method was developed to adjust for batch effects in microarray data, however, it is commonly applied to scRNA-seq data nowadays. The method fits linear models to the genes and pools statistical power by means of EB to estimate per gene correction factors. . I understand that @mbuttner also has an implementation of this - maybe we can combine and get the best of both approaches?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398
https://github.com/scverse/scanpy/pull/403:58,Testability,Benchmark,Benchmarks,58,For this - https://github.com/theislab/scanpy/issues/393. Benchmarks; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/pca_for_sparse.ipynb,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/pull/403:112,Testability,benchmark,benchmarks,112,For this - https://github.com/theislab/scanpy/issues/393. Benchmarks; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/pca_for_sparse.ipynb,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403
https://github.com/scverse/scanpy/issues/405:549,Availability,error,error,549,"Scanpy vs. 1.3.6; installed using pip3; OSX 10.10.5; Jupyter lab. code:; `list_of_list_of_marker_genes = [mg1, mg2, mg3]; for mg in list_of_list_of_marker_genes:; sc.pl.stacked_violin(adata, mg, groupby = 'louvain’, rotation=90); print(mg)`. This works for some mg's but not for all; sc.pl.matrixplot works for all of them; the same for sc.pl.violin; it works also if I would combine all marker genes into one list and then run; sc.pl.stacked_violin. When I say it does not work: it typically generates one stacked plot and then; 'hangs up' with an error message for subsequent plots:. Error message:. IndexError: list index out of range. When I run sc.pl.stacked_violin for the individual mg’s, some work others don’t. It is a reproducible results. I explicitly tested whether the genes in the respective marker gene lists are present in ’n_cells’. If not, I update the lists. What am I missing here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:586,Availability,Error,Error,586,"Scanpy vs. 1.3.6; installed using pip3; OSX 10.10.5; Jupyter lab. code:; `list_of_list_of_marker_genes = [mg1, mg2, mg3]; for mg in list_of_list_of_marker_genes:; sc.pl.stacked_violin(adata, mg, groupby = 'louvain’, rotation=90); print(mg)`. This works for some mg's but not for all; sc.pl.matrixplot works for all of them; the same for sc.pl.violin; it works also if I would combine all marker genes into one list and then run; sc.pl.stacked_violin. When I say it does not work: it typically generates one stacked plot and then; 'hangs up' with an error message for subsequent plots:. Error message:. IndexError: list index out of range. When I run sc.pl.stacked_violin for the individual mg’s, some work others don’t. It is a reproducible results. I explicitly tested whether the genes in the respective marker gene lists are present in ’n_cells’. If not, I update the lists. What am I missing here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:18,Deployability,install,installed,18,"Scanpy vs. 1.3.6; installed using pip3; OSX 10.10.5; Jupyter lab. code:; `list_of_list_of_marker_genes = [mg1, mg2, mg3]; for mg in list_of_list_of_marker_genes:; sc.pl.stacked_violin(adata, mg, groupby = 'louvain’, rotation=90); print(mg)`. This works for some mg's but not for all; sc.pl.matrixplot works for all of them; the same for sc.pl.violin; it works also if I would combine all marker genes into one list and then run; sc.pl.stacked_violin. When I say it does not work: it typically generates one stacked plot and then; 'hangs up' with an error message for subsequent plots:. Error message:. IndexError: list index out of range. When I run sc.pl.stacked_violin for the individual mg’s, some work others don’t. It is a reproducible results. I explicitly tested whether the genes in the respective marker gene lists are present in ’n_cells’. If not, I update the lists. What am I missing here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:860,Deployability,update,update,860,"Scanpy vs. 1.3.6; installed using pip3; OSX 10.10.5; Jupyter lab. code:; `list_of_list_of_marker_genes = [mg1, mg2, mg3]; for mg in list_of_list_of_marker_genes:; sc.pl.stacked_violin(adata, mg, groupby = 'louvain’, rotation=90); print(mg)`. This works for some mg's but not for all; sc.pl.matrixplot works for all of them; the same for sc.pl.violin; it works also if I would combine all marker genes into one list and then run; sc.pl.stacked_violin. When I say it does not work: it typically generates one stacked plot and then; 'hangs up' with an error message for subsequent plots:. Error message:. IndexError: list index out of range. When I run sc.pl.stacked_violin for the individual mg’s, some work others don’t. It is a reproducible results. I explicitly tested whether the genes in the respective marker gene lists are present in ’n_cells’. If not, I update the lists. What am I missing here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:555,Integrability,message,message,555,"Scanpy vs. 1.3.6; installed using pip3; OSX 10.10.5; Jupyter lab. code:; `list_of_list_of_marker_genes = [mg1, mg2, mg3]; for mg in list_of_list_of_marker_genes:; sc.pl.stacked_violin(adata, mg, groupby = 'louvain’, rotation=90); print(mg)`. This works for some mg's but not for all; sc.pl.matrixplot works for all of them; the same for sc.pl.violin; it works also if I would combine all marker genes into one list and then run; sc.pl.stacked_violin. When I say it does not work: it typically generates one stacked plot and then; 'hangs up' with an error message for subsequent plots:. Error message:. IndexError: list index out of range. When I run sc.pl.stacked_violin for the individual mg’s, some work others don’t. It is a reproducible results. I explicitly tested whether the genes in the respective marker gene lists are present in ’n_cells’. If not, I update the lists. What am I missing here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:592,Integrability,message,message,592,"Scanpy vs. 1.3.6; installed using pip3; OSX 10.10.5; Jupyter lab. code:; `list_of_list_of_marker_genes = [mg1, mg2, mg3]; for mg in list_of_list_of_marker_genes:; sc.pl.stacked_violin(adata, mg, groupby = 'louvain’, rotation=90); print(mg)`. This works for some mg's but not for all; sc.pl.matrixplot works for all of them; the same for sc.pl.violin; it works also if I would combine all marker genes into one list and then run; sc.pl.stacked_violin. When I say it does not work: it typically generates one stacked plot and then; 'hangs up' with an error message for subsequent plots:. Error message:. IndexError: list index out of range. When I run sc.pl.stacked_violin for the individual mg’s, some work others don’t. It is a reproducible results. I explicitly tested whether the genes in the respective marker gene lists are present in ’n_cells’. If not, I update the lists. What am I missing here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/issues/405:763,Testability,test,tested,763,"Scanpy vs. 1.3.6; installed using pip3; OSX 10.10.5; Jupyter lab. code:; `list_of_list_of_marker_genes = [mg1, mg2, mg3]; for mg in list_of_list_of_marker_genes:; sc.pl.stacked_violin(adata, mg, groupby = 'louvain’, rotation=90); print(mg)`. This works for some mg's but not for all; sc.pl.matrixplot works for all of them; the same for sc.pl.violin; it works also if I would combine all marker genes into one list and then run; sc.pl.stacked_violin. When I say it does not work: it typically generates one stacked plot and then; 'hangs up' with an error message for subsequent plots:. Error message:. IndexError: list index out of range. When I run sc.pl.stacked_violin for the individual mg’s, some work others don’t. It is a reproducible results. I explicitly tested whether the genes in the respective marker gene lists are present in ’n_cells’. If not, I update the lists. What am I missing here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405
https://github.com/scverse/scanpy/pull/406:259,Usability,simpl,simply,259,"This distributes the reexports in `scanpy/api/` in two modules: the core functionality goes into the root (`scanpy/__init__.py`) and the external functionality goes into `scanpy/ext/`. The structure of the latter mirrors the one of `scanpy.api` so that users simply need to adjust their imports; ```; import scanpy as sc; import scanpy.ext as sce; ```; instead of previously; ```; import scanpy.api as sc; ```. Backward compatibility for use of `scanpy.api` is fully maintained, but from now on, its use and development are deprecated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406
https://github.com/scverse/scanpy/issues/407:646,Usability,clear,clear,646,"Hello, I have a minor suggestion that I think will be very helpful for people to navigate and read the documentation. Right now Read the Docs does not display the full table of contents on the left (i.e. one link for every page).; For example, to go to the page on `scanpy.api.pp.calculate_qc_metrics` (which is https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.calculate_qc_metrics.html) I have two options:. First, I could go to the API page under prepocessing (https://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp) , and click the link to `pp.calculate_qc_metrics` link highlighted in the picture. However, it's not clear from the font used that it's clickable, because it looks like just markdown. I'd suggest making links blue, or underlined, or both, because that's what will convey that they are clickable. ![image](https://user-images.githubusercontent.com/12504176/50525392-065f7b80-0a90-11e9-8731-35dde488a456.png). Second, at the bottom of the page I could click `next`, which will take me to the next page. This only allows me to navigate the pages sequentially, and is not very practical because I don't know what comes next. . It would be extremely helpful for people navigating the doc if the table of contents unfurled into links for the individual pages contained in the section they are at. I made an example below of what I think this could look like, in it each page under `Preprocessing: PP` gets it's own link in the sidebar. ![image](https://user-images.githubusercontent.com/12504176/50525689-f052ba80-0a91-11e9-8ddc-9cdb86a0a7f5.png). Thanks. Eduardo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407
https://github.com/scverse/scanpy/issues/408:39,Usability,simpl,simple,39,"I'm probably not seeing something very simple:; https://scanpy.readthedocs.io/en/latest/api/scanpy.external.html#module-scanpy.external; renders fine whereas; https://scanpy.readthedocs.io/en/latest/api/scanpy.api.html#module-scanpy.api; https://scanpy.readthedocs.io/en/latest/api/scanpy.plotting.html#module-scanpy.plotting; both append an auto-generated list of all functions and classes defined in the module. As there is an annotated manual list already, we obviously don't want this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408
https://github.com/scverse/scanpy/issues/411:158,Integrability,message,message,158,"Hi,. when using the command ; ```python; sc.preprocessing.highly_variable_genes.highly_variable_genes(all_data,n_top_genes=5000); ```. I get this informative message:; ```; --> added; 'highly_variable', boolean vector (adata.var); 'means', boolean vector (adata.var); 'dispersions', boolean vector (adata.var); 'dispersions_norm', boolean vector (adata.var); ```. Even though not all of them are booleans! I guess it is just a copy-paste residue from implementation :); Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/411
https://github.com/scverse/scanpy/pull/412:186,Security,access,access,186,"Now that we changed from the reexports in `scanpy.api` whose sole purpose were to reexport top-level functions to the user in a clean way, we run into two problems;; * one can no longer access the submodules as the names are occupied by the functions; * the users sees private modules such as `pp.distributed` and similar. Therefore, I'd change everything that is not intended to be used by the user to a private submodule. This actually was the very first design before `scanpy.api` (inspired by `statsmodels.api` and `matplotlib.pyplot`) was introduced. Comments are welcome: @flying-sheep and everyone else.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/412
https://github.com/scverse/scanpy/issues/415:28,Modifiability,variab,variable,28,"Hi,. Using Seurat, in their variable gene function I've had some success using the `equal_frequency` option, where each bin contains an equal number of genes. Would it possible to implement this option in scanpy? . If you'd like I could submit a PR to implement this feature. I think it could be as simple as using `pd.qcut` instead of `pd.cut` or you could use a similar style as in the `cell_ranger` flavor with `pd.cut(df['mean'], np.r_[-np.inf,; np.percentile(df['mean'], np.arange(10, 105, 5)), np.inf])`. I don't know how useful it would be, but I could also add the option to have more bins in the `cell_ranger` flavor by replacing `np.arange(10,105,5)` with `np.linspace(10, 100, n_bins - 1)`. Best,; David",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415
https://github.com/scverse/scanpy/issues/415:299,Usability,simpl,simple,299,"Hi,. Using Seurat, in their variable gene function I've had some success using the `equal_frequency` option, where each bin contains an equal number of genes. Would it possible to implement this option in scanpy? . If you'd like I could submit a PR to implement this feature. I think it could be as simple as using `pd.qcut` instead of `pd.cut` or you could use a similar style as in the `cell_ranger` flavor with `pd.cut(df['mean'], np.r_[-np.inf,; np.percentile(df['mean'], np.arange(10, 105, 5)), np.inf])`. I don't know how useful it would be, but I could also add the option to have more bins in the `cell_ranger` flavor by replacing `np.arange(10,105,5)` with `np.linspace(10, 100, n_bins - 1)`. Best,; David",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415
https://github.com/scverse/scanpy/issues/416:5,Deployability,install,install,5,"`pip install -e .`; gives:; ```; Obtaining file:///apps/gau/scanpy; Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/apps/gau/scanpy/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""/apps/gau/scanpy/scanpy/__init__.py"", line 31, in <module>; from . import tools as tl; File ""/apps/gau/scanpy/scanpy/tools/__init__.py"", line 1, in <module>; from ..preprocessing._simple import pca; File ""/apps/gau/scanpy/scanpy/preprocessing/__init__.py"", line 7, in <module>; from ._combat import combat; File ""/apps/gau/scanpy/scanpy/preprocessing/_combat.py"", line 111, in <module>; def combat(adata: AnnData, key: str = 'batch', inplace: bool = True):; NameError: name 'AnnData' is not defined; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/416
https://github.com/scverse/scanpy/issues/418:802,Modifiability,extend,extended,802,"scanpy version 1.3.7. NOTE!: During typing this issue out i figured out how to solve it and save figures with all elements showing with the help of ; `fig1.savefig(""name.svg"" bbox_inches='tight')`, which solves 90% of the issue I had. I also noticed that these functions have a built in save function where I found the answer. I added a question at the bottom which is still of interest to me. However, feel free to rank this as non essential.; --------------------------------------. First of all, the new figure plotting functions looks amazing.; I just have a few issues that I hope I can get some help with.; I seem to often get the behavior of figures from scanpy after I plot that the elements like xtick labels and other important features are hidden due to figure boarders or that boarders are extended far beyond the plotting are. Due to the way the axes is constructed I can't simply do a fig.tight_layout(). Even with the grid_spec specific tight_layout https://matplotlib.org/users/tight_layout_guide.html#use-with-gridspec I get the same result.; I get the sense that the figures looks ok in a notebook, perhaps, where these elements can be seen, but that does unfortunate not translate to my workflow of manually saving figures and plotting with qt or tk backends to be able to get a quick overview. Below is an example of a fig.savefig(""test.png""); for the command; ```; sc.pl.matrixplot(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True); ```; ![test](https://user-images.githubusercontent.com/715716/50937406-77644300-1441-11e9-8713-8bebdf94c26b.png). The over extending bounders can't be seen here due to the white background but the missing x-tics are clear. (Note I realized how to at least save figures with a 'tight' bounding box so that issue is solved.). Q: Is it possible to get an interactive figure (as in plotting with qt or tk) where elements are visible as with fig.tight_layout() for ordinary axes using yo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:1642,Modifiability,extend,extending,1642,"ve figures with all elements showing with the help of ; `fig1.savefig(""name.svg"" bbox_inches='tight')`, which solves 90% of the issue I had. I also noticed that these functions have a built in save function where I found the answer. I added a question at the bottom which is still of interest to me. However, feel free to rank this as non essential.; --------------------------------------. First of all, the new figure plotting functions looks amazing.; I just have a few issues that I hope I can get some help with.; I seem to often get the behavior of figures from scanpy after I plot that the elements like xtick labels and other important features are hidden due to figure boarders or that boarders are extended far beyond the plotting are. Due to the way the axes is constructed I can't simply do a fig.tight_layout(). Even with the grid_spec specific tight_layout https://matplotlib.org/users/tight_layout_guide.html#use-with-gridspec I get the same result.; I get the sense that the figures looks ok in a notebook, perhaps, where these elements can be seen, but that does unfortunate not translate to my workflow of manually saving figures and plotting with qt or tk backends to be able to get a quick overview. Below is an example of a fig.savefig(""test.png""); for the command; ```; sc.pl.matrixplot(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True); ```; ![test](https://user-images.githubusercontent.com/715716/50937406-77644300-1441-11e9-8713-8bebdf94c26b.png). The over extending bounders can't be seen here due to the white background but the missing x-tics are clear. (Note I realized how to at least save figures with a 'tight' bounding box so that issue is solved.). Q: Is it possible to get an interactive figure (as in plotting with qt or tk) where elements are visible as with fig.tight_layout() for ordinary axes using your plotting functions? @fidelram tagging you here as I assume you might be able to know this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:1352,Testability,test,test,1352,"ve figures with all elements showing with the help of ; `fig1.savefig(""name.svg"" bbox_inches='tight')`, which solves 90% of the issue I had. I also noticed that these functions have a built in save function where I found the answer. I added a question at the bottom which is still of interest to me. However, feel free to rank this as non essential.; --------------------------------------. First of all, the new figure plotting functions looks amazing.; I just have a few issues that I hope I can get some help with.; I seem to often get the behavior of figures from scanpy after I plot that the elements like xtick labels and other important features are hidden due to figure boarders or that boarders are extended far beyond the plotting are. Due to the way the axes is constructed I can't simply do a fig.tight_layout(). Even with the grid_spec specific tight_layout https://matplotlib.org/users/tight_layout_guide.html#use-with-gridspec I get the same result.; I get the sense that the figures looks ok in a notebook, perhaps, where these elements can be seen, but that does unfortunate not translate to my workflow of manually saving figures and plotting with qt or tk backends to be able to get a quick overview. Below is an example of a fig.savefig(""test.png""); for the command; ```; sc.pl.matrixplot(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True); ```; ![test](https://user-images.githubusercontent.com/715716/50937406-77644300-1441-11e9-8713-8bebdf94c26b.png). The over extending bounders can't be seen here due to the white background but the missing x-tics are clear. (Note I realized how to at least save figures with a 'tight' bounding box so that issue is solved.). Q: Is it possible to get an interactive figure (as in plotting with qt or tk) where elements are visible as with fig.tight_layout() for ordinary axes using your plotting functions? @fidelram tagging you here as I assume you might be able to know this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:1526,Testability,test,test,1526,"ve figures with all elements showing with the help of ; `fig1.savefig(""name.svg"" bbox_inches='tight')`, which solves 90% of the issue I had. I also noticed that these functions have a built in save function where I found the answer. I added a question at the bottom which is still of interest to me. However, feel free to rank this as non essential.; --------------------------------------. First of all, the new figure plotting functions looks amazing.; I just have a few issues that I hope I can get some help with.; I seem to often get the behavior of figures from scanpy after I plot that the elements like xtick labels and other important features are hidden due to figure boarders or that boarders are extended far beyond the plotting are. Due to the way the axes is constructed I can't simply do a fig.tight_layout(). Even with the grid_spec specific tight_layout https://matplotlib.org/users/tight_layout_guide.html#use-with-gridspec I get the same result.; I get the sense that the figures looks ok in a notebook, perhaps, where these elements can be seen, but that does unfortunate not translate to my workflow of manually saving figures and plotting with qt or tk backends to be able to get a quick overview. Below is an example of a fig.savefig(""test.png""); for the command; ```; sc.pl.matrixplot(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True); ```; ![test](https://user-images.githubusercontent.com/715716/50937406-77644300-1441-11e9-8713-8bebdf94c26b.png). The over extending bounders can't be seen here due to the white background but the missing x-tics are clear. (Note I realized how to at least save figures with a 'tight' bounding box so that issue is solved.). Q: Is it possible to get an interactive figure (as in plotting with qt or tk) where elements are visible as with fig.tight_layout() for ordinary axes using your plotting functions? @fidelram tagging you here as I assume you might be able to know this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:887,Usability,simpl,simply,887,"scanpy version 1.3.7. NOTE!: During typing this issue out i figured out how to solve it and save figures with all elements showing with the help of ; `fig1.savefig(""name.svg"" bbox_inches='tight')`, which solves 90% of the issue I had. I also noticed that these functions have a built in save function where I found the answer. I added a question at the bottom which is still of interest to me. However, feel free to rank this as non essential.; --------------------------------------. First of all, the new figure plotting functions looks amazing.; I just have a few issues that I hope I can get some help with.; I seem to often get the behavior of figures from scanpy after I plot that the elements like xtick labels and other important features are hidden due to figure boarders or that boarders are extended far beyond the plotting are. Due to the way the axes is constructed I can't simply do a fig.tight_layout(). Even with the grid_spec specific tight_layout https://matplotlib.org/users/tight_layout_guide.html#use-with-gridspec I get the same result.; I get the sense that the figures looks ok in a notebook, perhaps, where these elements can be seen, but that does unfortunate not translate to my workflow of manually saving figures and plotting with qt or tk backends to be able to get a quick overview. Below is an example of a fig.savefig(""test.png""); for the command; ```; sc.pl.matrixplot(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True); ```; ![test](https://user-images.githubusercontent.com/715716/50937406-77644300-1441-11e9-8713-8bebdf94c26b.png). The over extending bounders can't be seen here due to the white background but the missing x-tics are clear. (Note I realized how to at least save figures with a 'tight' bounding box so that issue is solved.). Q: Is it possible to get an interactive figure (as in plotting with qt or tk) where elements are visible as with fig.tight_layout() for ordinary axes using yo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/418:1735,Usability,clear,clear,1735,"ve figures with all elements showing with the help of ; `fig1.savefig(""name.svg"" bbox_inches='tight')`, which solves 90% of the issue I had. I also noticed that these functions have a built in save function where I found the answer. I added a question at the bottom which is still of interest to me. However, feel free to rank this as non essential.; --------------------------------------. First of all, the new figure plotting functions looks amazing.; I just have a few issues that I hope I can get some help with.; I seem to often get the behavior of figures from scanpy after I plot that the elements like xtick labels and other important features are hidden due to figure boarders or that boarders are extended far beyond the plotting are. Due to the way the axes is constructed I can't simply do a fig.tight_layout(). Even with the grid_spec specific tight_layout https://matplotlib.org/users/tight_layout_guide.html#use-with-gridspec I get the same result.; I get the sense that the figures looks ok in a notebook, perhaps, where these elements can be seen, but that does unfortunate not translate to my workflow of manually saving figures and plotting with qt or tk backends to be able to get a quick overview. Below is an example of a fig.savefig(""test.png""); for the command; ```; sc.pl.matrixplot(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True); ```; ![test](https://user-images.githubusercontent.com/715716/50937406-77644300-1441-11e9-8713-8bebdf94c26b.png). The over extending bounders can't be seen here due to the white background but the missing x-tics are clear. (Note I realized how to at least save figures with a 'tight' bounding box so that issue is solved.). Q: Is it possible to get an interactive figure (as in plotting with qt or tk) where elements are visible as with fig.tight_layout() for ordinary axes using your plotting functions? @fidelram tagging you here as I assume you might be able to know this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418
https://github.com/scverse/scanpy/issues/419:514,Modifiability,variab,variable,514,"scanpy version 1.3.7,. I noticed that some figures does not return an ax handle.; Specifically rank_genes_groups_heatmap does not return an axis becouse _rank_genes_groups_plot does not return an axis from the matrix plot. Only tracksplot and stacked_violin seems to return an axis in [_rank_genes_groups_plot](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L258). Also I'm wondering why not all plotting functions return an axes handle (list) always.; It's easy to capture in a variable if not wished to print to screen and easier to keep track of than having to both capture and determine the show=False state, and it would mirror behavior of other figure packages. Any specific reason you do not do this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419
https://github.com/scverse/scanpy/issues/421:262,Availability,error,error,262,"The documentation says:. > percent_top : Container[int], optional (default: (50, 100, 200, 500)); > Which proportions of top genes to cover. If empty or None don’t calculate. However, supplying `percent_top=None` or `percent_top=[]` leads to index-out-of-bounds error, due to `proportions = top_segment_proportions(X, percent_top)` being always executed and `top_segment_proportions()` not handling a None or empty second argument.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421
https://github.com/scverse/scanpy/issues/422:1211,Testability,log,log,1211,"ata.varm['PCs'][:, c])[::-1]; genes_ranked_by_loading_in_PC = adata.var_names[sort_idcs]; ; sc.pl.heatmap(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True, dendrogram=dendrogram); fig = plt.gcf(); ```; and I get the flollowing traceback; ```; TypeErrorTraceback (most recent call last); <ipython-input-641-33e0c59aae76> in <module>; 10 cells_ranked_by_loading_in_PC = adata.obs_names[sort_cell]; 11 ; ---> 12 sc.pl.heatmap(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True, dendrogram=dendrogram); 13 fig1 = plt.gcf(); 14 # sc.pl.heatmap(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='louvain', use_raw=None, cmap='RdBu_r', swap_axes=True, dendrogram=dendrogram). ~/.virtualenvs/default/lib/python3.6/site-packages/scanpy/plotting/anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show_gene_labels, show, save, figsize, **kwds); 1274 groupby_ax = fig.add_subplot(axs[2, 0]); 1275 ticks, labels, groupby_cmap = _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors=groupby_colors,; -> 1276 orientation='bottom'); 1277 # add lines to main heatmap; 1278 line_positions = np.cumsum(obs_tidy.index.value_counts(sort=False))[:-1]. ~/.virtualenvs/default/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name); 2382 if len(labels) > 1:; 2383 groupby_ax.set_xticks(ticks); -> 2384 if max([len(x) for x in labels]) < 3:; 2385 # if the labels are small do not rotate them; 2386 rotation = 0. ~/.virtualenvs/default/lib/python3.6/site-packages/scanpy/plotting/anndata.py in <listcomp>(.0); 2382 if len(labels) > 1:; 2383 groupby_ax.set_xticks(ticks); -> 2384 if max([len(x) for x in labels]) < 3:; 2385 # if the labels are small do not rotate them; 23",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422
https://github.com/scverse/scanpy/pull/423:175,Modifiability,extend,extending,175,"I'm not sure if `sc.write` is forgotten or left out on purpose. Leaving it out make sense as it only offers `sc.write('file.csv', adata)` over `adata.write()`. Alternatively, extending `sc.write` functionality to loom and zarr and keeping it in the new API might make it more useful. I was using it just to make the code more symmetric :) i.e. `sc.read` and `sc.write`, but I don't mind if it's removed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/423
https://github.com/scverse/scanpy/issues/428:747,Availability,down,download,747,"Possibly related to #275. The provided `sc.datasets.pbmc3k` does not have values for `""gene_ids""`. It'd be useful to have a standard dataset with ensembl ids and gene symbols for testing visualization functions with the `gene_symbols` argument. ```python; In [1]: import scanpy.api as sc ; ...: sc.datasets.pbmc3k().var.head() ; Out[1]: ; gene_ids; index ; MIR1302-10 NaN; FAM138A NaN; OR4F5 NaN; RP11-34P13.7 NaN; RP11-34P13.8 NaN. In [2]: import h5py ; ...: with h5py.File(""./data/pbmc3k_raw.h5ad"") as f: ; ...: print(repr(f[""var""][:])) ; ...: ; array([(b'MIR1302-10', -1), (b'FAM138A', -1), (b'OR4F5', -1), ...,; (b'CU459201.1', -1), (b'AC002321.2', -1), (b'AC002321.1', -1)],; dtype=[('index', 'S19'), ('gene_ids', 'i1')]); ```. However, if I download the source files, everything works out fine:. ```python. In [3]: !wget http://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz ; ...; In [4]: !tar xzf pbmc3k_filtered_gene_bc_matrices.tar.gz; ...; In [5]: sc.read_10x_mtx(""./filtered_gene_bc_matrices/hg19"").var.head() ; Out[5]: ; gene_ids; MIR1302-10 ENSG00000243485; FAM138A ENSG00000237613; OR4F5 ENSG00000186092; RP11-34P13.7 ENSG00000238009; RP11-34P13.8 ENSG00000239945; ```. Which makes me think the file hosted at `http://falexwolf.de/data/pbmc3k_raw.h5ad` was created when there was a bug in the `read_10x_mtx` code.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/428
https://github.com/scverse/scanpy/issues/428:179,Testability,test,testing,179,"Possibly related to #275. The provided `sc.datasets.pbmc3k` does not have values for `""gene_ids""`. It'd be useful to have a standard dataset with ensembl ids and gene symbols for testing visualization functions with the `gene_symbols` argument. ```python; In [1]: import scanpy.api as sc ; ...: sc.datasets.pbmc3k().var.head() ; Out[1]: ; gene_ids; index ; MIR1302-10 NaN; FAM138A NaN; OR4F5 NaN; RP11-34P13.7 NaN; RP11-34P13.8 NaN. In [2]: import h5py ; ...: with h5py.File(""./data/pbmc3k_raw.h5ad"") as f: ; ...: print(repr(f[""var""][:])) ; ...: ; array([(b'MIR1302-10', -1), (b'FAM138A', -1), (b'OR4F5', -1), ...,; (b'CU459201.1', -1), (b'AC002321.2', -1), (b'AC002321.1', -1)],; dtype=[('index', 'S19'), ('gene_ids', 'i1')]); ```. However, if I download the source files, everything works out fine:. ```python. In [3]: !wget http://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz ; ...; In [4]: !tar xzf pbmc3k_filtered_gene_bc_matrices.tar.gz; ...; In [5]: sc.read_10x_mtx(""./filtered_gene_bc_matrices/hg19"").var.head() ; Out[5]: ; gene_ids; MIR1302-10 ENSG00000243485; FAM138A ENSG00000237613; OR4F5 ENSG00000186092; RP11-34P13.7 ENSG00000238009; RP11-34P13.8 ENSG00000239945; ```. Which makes me think the file hosted at `http://falexwolf.de/data/pbmc3k_raw.h5ad` was created when there was a bug in the `read_10x_mtx` code.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/428
https://github.com/scverse/scanpy/issues/431:21,Availability,error,error,21,"Hi,. I am getting an error when the list of components is greater than 5 in `sc.pl.pca_loadings`. The error is a NamedError, `NameError: name 'count' is not defined`, coming from [this line](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L554).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/431
https://github.com/scverse/scanpy/issues/431:102,Availability,error,error,102,"Hi,. I am getting an error when the list of components is greater than 5 in `sc.pl.pca_loadings`. The error is a NamedError, `NameError: name 'count' is not defined`, coming from [this line](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L554).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/431
https://github.com/scverse/scanpy/issues/432:67,Availability,error,error,67,"Minor bug I assume. Using fewer than 50 cells raises the following error when trying to run `sc.tl.pca`. The code handles this when the `n_vars < 50` but not when `n_obs` is. ```pytb; ValueErrorTraceback (most recent call last); <ipython-input-823-4c11b9b62e6d> in <module>; ----> 1 sc.tl.pca(bla). ~/.virtualenvs/default/lib/python3.6/site-packages/scanpy/preprocessing/simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 504 pca_ = TruncatedSVD(n_components=n_comps, random_state=random_state); 505 X = adata_comp.X; --> 506 X_pca = pca_.fit_transform(X); 507 ; 508 if X_pca.dtype.descr != np.dtype(dtype).descr: X_pca = X_pca.astype(dtype). ~/.virtualenvs/default/lib/python3.6/site-packages/sklearn/decomposition/pca.py in fit_transform(self, X, y); 357 ; 358 """"""; --> 359 U, S, V = self._fit(X); 360 U = U[:, :self.n_components_]; 361 . ~/.virtualenvs/default/lib/python3.6/site-packages/sklearn/decomposition/pca.py in _fit(self, X); 404 # Call different fits for either full or truncated SVD; 405 if self._fit_svd_solver == 'full':; --> 406 return self._fit_full(X, n_components); 407 elif self._fit_svd_solver in ['arpack', 'randomized']:; 408 return self._fit_truncated(X, n_components, self._fit_svd_solver). ~/.virtualenvs/default/lib/python3.6/site-packages/sklearn/decomposition/pca.py in _fit_full(self, X, n_components); 423 ""min(n_samples, n_features)=%r with ""; 424 ""svd_solver='full'""; --> 425 % (n_components, min(n_samples, n_features))); 426 elif n_components >= 1:; 427 if not isinstance(n_components, (numbers.Integral, np.integer)):. ValueError: n_components=50 must be between 0 and min(n_samples, n_features)=38 with svd_solver='full'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/432:371,Usability,simpl,simple,371,"Minor bug I assume. Using fewer than 50 cells raises the following error when trying to run `sc.tl.pca`. The code handles this when the `n_vars < 50` but not when `n_obs` is. ```pytb; ValueErrorTraceback (most recent call last); <ipython-input-823-4c11b9b62e6d> in <module>; ----> 1 sc.tl.pca(bla). ~/.virtualenvs/default/lib/python3.6/site-packages/scanpy/preprocessing/simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 504 pca_ = TruncatedSVD(n_components=n_comps, random_state=random_state); 505 X = adata_comp.X; --> 506 X_pca = pca_.fit_transform(X); 507 ; 508 if X_pca.dtype.descr != np.dtype(dtype).descr: X_pca = X_pca.astype(dtype). ~/.virtualenvs/default/lib/python3.6/site-packages/sklearn/decomposition/pca.py in fit_transform(self, X, y); 357 ; 358 """"""; --> 359 U, S, V = self._fit(X); 360 U = U[:, :self.n_components_]; 361 . ~/.virtualenvs/default/lib/python3.6/site-packages/sklearn/decomposition/pca.py in _fit(self, X); 404 # Call different fits for either full or truncated SVD; 405 if self._fit_svd_solver == 'full':; --> 406 return self._fit_full(X, n_components); 407 elif self._fit_svd_solver in ['arpack', 'randomized']:; 408 return self._fit_truncated(X, n_components, self._fit_svd_solver). ~/.virtualenvs/default/lib/python3.6/site-packages/sklearn/decomposition/pca.py in _fit_full(self, X, n_components); 423 ""min(n_samples, n_features)=%r with ""; 424 ""svd_solver='full'""; --> 425 % (n_components, min(n_samples, n_features))); 426 elif n_components >= 1:; 427 if not isinstance(n_components, (numbers.Integral, np.integer)):. ValueError: n_components=50 must be between 0 and min(n_samples, n_features)=38 with svd_solver='full'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432
https://github.com/scverse/scanpy/issues/434:47,Testability,test,test,47,"Whether I read the data as:; `adata = sc.read('test.h5ad', backed='r')`. or:; `adata = sc.read('test.h5ad', backed='r+')`. The amount of memory used is the same (I'm measuring memory usage with `/usr/bin/time -v` and looking at `Maximum resident set size`). In my particular case, I have a very large data set and I'm only interested in `adata.obs`. My current solution is to use the `h5py` package and read only the `obs` group from the `h5ad` file.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/434:96,Testability,test,test,96,"Whether I read the data as:; `adata = sc.read('test.h5ad', backed='r')`. or:; `adata = sc.read('test.h5ad', backed='r+')`. The amount of memory used is the same (I'm measuring memory usage with `/usr/bin/time -v` and looking at `Maximum resident set size`). In my particular case, I have a very large data set and I'm only interested in `adata.obs`. My current solution is to use the `h5py` package and read only the `obs` group from the `h5ad` file.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434
https://github.com/scverse/scanpy/issues/435:169,Availability,error,error,169,"Hi there,. I seem to have trouble analyzing a dataset. https://www.dropbox.com/s/og5lw42chh2qujm/Trial_data1.csv?dl=0. If I run sc.pp.log1p (adata), I get the following error. . TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. I could normalize it on my own and say if I do a PCA analysis and try to plot the results, I get the following error. sc.pl.pca(adata, color = 'DAPI'); TypeError: object of type 'numpy.int64' has no len(). If I plot it without the color, it does work though. . Below is how I go from CSV to AnnData. # Read File; x = pd.read_csv('Trial_data1.csv', delimiter=',', index_col=0). # Convert to AnnData; file_url = 'https://raw.githubusercontent.com/ajitjohnson/Jupyter-Notebooks/master/py_scripts/mi_pp_anndata.py'; exec(open(wget.download(file_url)).read()); adata = mi_pp_anndata (x). Any help is appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:446,Availability,error,error,446,"Hi there,. I seem to have trouble analyzing a dataset. https://www.dropbox.com/s/og5lw42chh2qujm/Trial_data1.csv?dl=0. If I run sc.pp.log1p (adata), I get the following error. . TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. I could normalize it on my own and say if I do a PCA analysis and try to plot the results, I get the following error. sc.pl.pca(adata, color = 'DAPI'); TypeError: object of type 'numpy.int64' has no len(). If I plot it without the color, it does work though. . Below is how I go from CSV to AnnData. # Read File; x = pd.read_csv('Trial_data1.csv', delimiter=',', index_col=0). # Convert to AnnData; file_url = 'https://raw.githubusercontent.com/ajitjohnson/Jupyter-Notebooks/master/py_scripts/mi_pp_anndata.py'; exec(open(wget.download(file_url)).read()); adata = mi_pp_anndata (x). Any help is appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/435:862,Availability,down,download,862,"Hi there,. I seem to have trouble analyzing a dataset. https://www.dropbox.com/s/og5lw42chh2qujm/Trial_data1.csv?dl=0. If I run sc.pp.log1p (adata), I get the following error. . TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''. I could normalize it on my own and say if I do a PCA analysis and try to plot the results, I get the following error. sc.pl.pca(adata, color = 'DAPI'); TypeError: object of type 'numpy.int64' has no len(). If I plot it without the color, it does work though. . Below is how I go from CSV to AnnData. # Read File; x = pd.read_csv('Trial_data1.csv', delimiter=',', index_col=0). # Convert to AnnData; file_url = 'https://raw.githubusercontent.com/ajitjohnson/Jupyter-Notebooks/master/py_scripts/mi_pp_anndata.py'; exec(open(wget.download(file_url)).read()); adata = mi_pp_anndata (x). Any help is appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435
https://github.com/scverse/scanpy/issues/436:131,Testability,test,test,131,"Hello, I would like to know how the scores are calculated in the case of the differential expression analysis using the wilcoxon's test.; This is what I found in the code of the function _rank_genes_groups.py:; `scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(; (n_active * m_active * (n_active + m_active + 1) / 12)); scores[np.isnan(scores)] = 0; pvals = 2 * stats.distributions.norm.sf(np.abs(scores))` . Is this the version of the test that works with not paired samples?; Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/436
https://github.com/scverse/scanpy/issues/436:457,Testability,test,test,457,"Hello, I would like to know how the scores are calculated in the case of the differential expression analysis using the wilcoxon's test.; This is what I found in the code of the function _rank_genes_groups.py:; `scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(; (n_active * m_active * (n_active + m_active + 1) / 12)); scores[np.isnan(scores)] = 0; pvals = 2 * stats.distributions.norm.sf(np.abs(scores))` . Is this the version of the test that works with not paired samples?; Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/436
https://github.com/scverse/scanpy/issues/438:415,Availability,error,error,415,"Hej,. I have been trying to plot the ranked gene groups with a dotplot in this way. ```; sc.plotting.tools.rank_genes_groups_dotplot(all_data_flt_clst, n_genes=5, save='.pdf', groupby='batch', dendrogram=False, layer='imputed') ; ```. or from non-raw data; ```; sc.plotting.tools.rank_genes_groups_dotplot(all_data_flt_clst, n_genes=5, save='.pdf', groupby='batch', dendrogram=False, use_raw=False) ; ```. I get an error regarding the index names of the genes. Other commands for plotting gene rank groups (such as heatmap) show the same problem. The error goes as follows:; ```; KeyError: 'Indices ""[\'PRM2\', \'ACAP1\', \'SPEM1\', \'SPATA3\', \'C10orf62\', \'TNP1\', \'MIR193BHG\', \'PRM1\', \'CCDC179\', \'AC007557.1\', \'SPACA1\', \'ERICH2\', \'RP11-360D2.1\', \'TIPARP-AS1\', \'GS1-124K5.4\', \'DCN\', \'C1S\', \'SERPING1\', \'C1R\', \'SERPINF1\', \'GAGE2A\', \'PTMA\', \'HMGB1\', \'VCX2\', \'ERP29\', \'ZCWPW1\', \'SMC1B\', \'DPH7\', \'SCML1\', \'CLSPN\', \'CSAD\', \'C1QBP\', \'DNAJB6\', \'TCF3\', \'RRBP1\', \'HSP90AA1\', \'TMED10\', \'ART3\', \'BUB1\', \'KRBOX1\', \'B2M\', \'IFITM3\', \'GNG11\', \'IFITM2\', \'IFI27\', \'TYROBP\', \'S100A4\', \'FCER1G\', \'CD163\', \'CYBA\', \'CALD1\', \'IGFBP7\', \'TIMP3\', \'PTGDS\', \'TSHZ2\', \'MT-ND3\', \'MT-ND1\', \'MT-ND2\', \'MT-ATP6\', \'MT-ND4\', \'MT-ND1\', \'HMGN5\', \'MT-ND3\', \'ALDH1A1\', \'MT-ATP6\']"" contain invalid observation/variables names/indices.'; ```. I checked the var_names and obs_names of my object, but they seem totally fine. Do you have any ideas about the origin of the problem? I can post the backtracking of the error if needed :). Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:551,Availability,error,error,551,"Hej,. I have been trying to plot the ranked gene groups with a dotplot in this way. ```; sc.plotting.tools.rank_genes_groups_dotplot(all_data_flt_clst, n_genes=5, save='.pdf', groupby='batch', dendrogram=False, layer='imputed') ; ```. or from non-raw data; ```; sc.plotting.tools.rank_genes_groups_dotplot(all_data_flt_clst, n_genes=5, save='.pdf', groupby='batch', dendrogram=False, use_raw=False) ; ```. I get an error regarding the index names of the genes. Other commands for plotting gene rank groups (such as heatmap) show the same problem. The error goes as follows:; ```; KeyError: 'Indices ""[\'PRM2\', \'ACAP1\', \'SPEM1\', \'SPATA3\', \'C10orf62\', \'TNP1\', \'MIR193BHG\', \'PRM1\', \'CCDC179\', \'AC007557.1\', \'SPACA1\', \'ERICH2\', \'RP11-360D2.1\', \'TIPARP-AS1\', \'GS1-124K5.4\', \'DCN\', \'C1S\', \'SERPING1\', \'C1R\', \'SERPINF1\', \'GAGE2A\', \'PTMA\', \'HMGB1\', \'VCX2\', \'ERP29\', \'ZCWPW1\', \'SMC1B\', \'DPH7\', \'SCML1\', \'CLSPN\', \'CSAD\', \'C1QBP\', \'DNAJB6\', \'TCF3\', \'RRBP1\', \'HSP90AA1\', \'TMED10\', \'ART3\', \'BUB1\', \'KRBOX1\', \'B2M\', \'IFITM3\', \'GNG11\', \'IFITM2\', \'IFI27\', \'TYROBP\', \'S100A4\', \'FCER1G\', \'CD163\', \'CYBA\', \'CALD1\', \'IGFBP7\', \'TIMP3\', \'PTGDS\', \'TSHZ2\', \'MT-ND3\', \'MT-ND1\', \'MT-ND2\', \'MT-ATP6\', \'MT-ND4\', \'MT-ND1\', \'HMGN5\', \'MT-ND3\', \'ALDH1A1\', \'MT-ATP6\']"" contain invalid observation/variables names/indices.'; ```. I checked the var_names and obs_names of my object, but they seem totally fine. Do you have any ideas about the origin of the problem? I can post the backtracking of the error if needed :). Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1595,Availability,error,error,1595,"Hej,. I have been trying to plot the ranked gene groups with a dotplot in this way. ```; sc.plotting.tools.rank_genes_groups_dotplot(all_data_flt_clst, n_genes=5, save='.pdf', groupby='batch', dendrogram=False, layer='imputed') ; ```. or from non-raw data; ```; sc.plotting.tools.rank_genes_groups_dotplot(all_data_flt_clst, n_genes=5, save='.pdf', groupby='batch', dendrogram=False, use_raw=False) ; ```. I get an error regarding the index names of the genes. Other commands for plotting gene rank groups (such as heatmap) show the same problem. The error goes as follows:; ```; KeyError: 'Indices ""[\'PRM2\', \'ACAP1\', \'SPEM1\', \'SPATA3\', \'C10orf62\', \'TNP1\', \'MIR193BHG\', \'PRM1\', \'CCDC179\', \'AC007557.1\', \'SPACA1\', \'ERICH2\', \'RP11-360D2.1\', \'TIPARP-AS1\', \'GS1-124K5.4\', \'DCN\', \'C1S\', \'SERPING1\', \'C1R\', \'SERPINF1\', \'GAGE2A\', \'PTMA\', \'HMGB1\', \'VCX2\', \'ERP29\', \'ZCWPW1\', \'SMC1B\', \'DPH7\', \'SCML1\', \'CLSPN\', \'CSAD\', \'C1QBP\', \'DNAJB6\', \'TCF3\', \'RRBP1\', \'HSP90AA1\', \'TMED10\', \'ART3\', \'BUB1\', \'KRBOX1\', \'B2M\', \'IFITM3\', \'GNG11\', \'IFITM2\', \'IFI27\', \'TYROBP\', \'S100A4\', \'FCER1G\', \'CD163\', \'CYBA\', \'CALD1\', \'IGFBP7\', \'TIMP3\', \'PTGDS\', \'TSHZ2\', \'MT-ND3\', \'MT-ND1\', \'MT-ND2\', \'MT-ATP6\', \'MT-ND4\', \'MT-ND1\', \'HMGN5\', \'MT-ND3\', \'ALDH1A1\', \'MT-ATP6\']"" contain invalid observation/variables names/indices.'; ```. I checked the var_names and obs_names of my object, but they seem totally fine. Do you have any ideas about the origin of the problem? I can post the backtracking of the error if needed :). Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/issues/438:1393,Modifiability,variab,variables,1393,"Hej,. I have been trying to plot the ranked gene groups with a dotplot in this way. ```; sc.plotting.tools.rank_genes_groups_dotplot(all_data_flt_clst, n_genes=5, save='.pdf', groupby='batch', dendrogram=False, layer='imputed') ; ```. or from non-raw data; ```; sc.plotting.tools.rank_genes_groups_dotplot(all_data_flt_clst, n_genes=5, save='.pdf', groupby='batch', dendrogram=False, use_raw=False) ; ```. I get an error regarding the index names of the genes. Other commands for plotting gene rank groups (such as heatmap) show the same problem. The error goes as follows:; ```; KeyError: 'Indices ""[\'PRM2\', \'ACAP1\', \'SPEM1\', \'SPATA3\', \'C10orf62\', \'TNP1\', \'MIR193BHG\', \'PRM1\', \'CCDC179\', \'AC007557.1\', \'SPACA1\', \'ERICH2\', \'RP11-360D2.1\', \'TIPARP-AS1\', \'GS1-124K5.4\', \'DCN\', \'C1S\', \'SERPING1\', \'C1R\', \'SERPINF1\', \'GAGE2A\', \'PTMA\', \'HMGB1\', \'VCX2\', \'ERP29\', \'ZCWPW1\', \'SMC1B\', \'DPH7\', \'SCML1\', \'CLSPN\', \'CSAD\', \'C1QBP\', \'DNAJB6\', \'TCF3\', \'RRBP1\', \'HSP90AA1\', \'TMED10\', \'ART3\', \'BUB1\', \'KRBOX1\', \'B2M\', \'IFITM3\', \'GNG11\', \'IFITM2\', \'IFI27\', \'TYROBP\', \'S100A4\', \'FCER1G\', \'CD163\', \'CYBA\', \'CALD1\', \'IGFBP7\', \'TIMP3\', \'PTGDS\', \'TSHZ2\', \'MT-ND3\', \'MT-ND1\', \'MT-ND2\', \'MT-ATP6\', \'MT-ND4\', \'MT-ND1\', \'HMGN5\', \'MT-ND3\', \'ALDH1A1\', \'MT-ATP6\']"" contain invalid observation/variables names/indices.'; ```. I checked the var_names and obs_names of my object, but they seem totally fine. Do you have any ideas about the origin of the problem? I can post the backtracking of the error if needed :). Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438
https://github.com/scverse/scanpy/pull/439:255,Deployability,release,released,255,"This uses the `__array__` method on ndarray-like classes to convert from; a distributed array to a regular NumPy ndarray when `materialize_as_ndarray` is called. This was prompted by an [improvement in Zappy](https://github.com/lasersonlab/zappy/pull/7) (released in 0.2.0) that makes Zappy arrays implement the `__array__` method. As another benefit, this change should make it easier to use other implementations of the ndarray interface in Scanpy in the future. Note that there is still a Dask-specific call for the case of a sequence of arrays, since Dask can materialize these in one call to `compute`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/pull/439:430,Integrability,interface,interface,430,"This uses the `__array__` method on ndarray-like classes to convert from; a distributed array to a regular NumPy ndarray when `materialize_as_ndarray` is called. This was prompted by an [improvement in Zappy](https://github.com/lasersonlab/zappy/pull/7) (released in 0.2.0) that makes Zappy arrays implement the `__array__` method. As another benefit, this change should make it easier to use other implementations of the ndarray interface in Scanpy in the future. Note that there is still a Dask-specific call for the case of a sequence of arrays, since Dask can materialize these in one call to `compute`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439
https://github.com/scverse/scanpy/issues/440:403,Availability,error,error,403,"Hej all,. I am using a backed dataset because, when I run the umap scatterplot, the RAM go pretty much crazy on our server. When I use not-backed data, I can do UMAP scatterplots without any problem apart from the memory usage. But now that the data is backed, when running the following:; ```; sc.plotting.tools.scatterplots.umap(all_data_flt_clst, color=markers, cmap=""Blues"", ncols=5); ```. I get an error message that seems related to the `h5py` package. Here is the whole trace back. ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-a78575d924b7> in <module>; 24 if len(markers) > 0:; 25 print(""Expression plots of "", names, "" markers: "", markers); ---> 26 sc.plotting.tools.scatterplots.umap(all_data_flt_clst, color=markers, cmap=""Blues"", ncols=5); 27 ; 28 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selectio",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:409,Integrability,message,message,409,"Hej all,. I am using a backed dataset because, when I run the umap scatterplot, the RAM go pretty much crazy on our server. When I use not-backed data, I can do UMAP scatterplots without any problem apart from the memory usage. But now that the data is backed, when running the following:; ```; sc.plotting.tools.scatterplots.umap(all_data_flt_clst, color=markers, cmap=""Blues"", ncols=5); ```. I get an error message that seems related to the `h5py` package. Here is the whole trace back. ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-a78575d924b7> in <module>; 24 if len(markers) > 0:; 25 print(""Expression plots of "", names, "" markers: "", markers); ---> 26 sc.plotting.tools.scatterplots.umap(all_data_flt_clst, color=markers, cmap=""Blues"", ncols=5); 27 ; 28 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selectio",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:1772,Integrability,wrap,wrapper,1772,"atterplots.umap(all_data_flt_clst, color=markers, cmap=""Blues"", ncols=5); 27 ; 28 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selection = sel.select(self.shape, args, dsid=self.id); 477 ; 478 if selection.nselect == 0:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in select(shape, args, dsid); 70 elif isinstance(arg, np.ndarray):; 71 sel = PointSelection(shape); ---> 72 sel[arg]; 73 return sel; 74 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in __getitem__(self, arg); 210 """""" Perform point-wise selection from a NumPy boolean array """"""; 211 if not (isinstance(arg, np.ndarray) and arg.dtype.kind == 'b'):; --> 212 raise TypeError(""PointSelection __getitem__ only works with bool arrays""); 213 if not arg.shape == self.shape:; 214 raise TypeError(""Boolean indexing array has incompatible shape""). TypeError: PointSelection __getitem__ only work",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:1828,Integrability,wrap,wrapper,1828,"Blues"", ncols=5); 27 ; 28 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selection = sel.select(self.shape, args, dsid=self.id); 477 ; 478 if selection.nselect == 0:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in select(shape, args, dsid); 70 elif isinstance(arg, np.ndarray):; 71 sel = PointSelection(shape); ---> 72 sel[arg]; 73 return sel; 74 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in __getitem__(self, arg); 210 """""" Perform point-wise selection from a NumPy boolean array """"""; 211 if not (isinstance(arg, np.ndarray) and arg.dtype.kind == 'b'):; --> 212 raise TypeError(""PointSelection __getitem__ only works with bool arrays""); 213 if not arg.shape == self.shape:; 214 raise TypeError(""Boolean indexing array has incompatible shape""). TypeError: PointSelection __getitem__ only works with bool arrays. <Figure size 1978.56x288 with 0 Axes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:1951,Performance,Perform,Perform,1951,"ts.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selection = sel.select(self.shape, args, dsid=self.id); 477 ; 478 if selection.nselect == 0:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in select(shape, args, dsid); 70 elif isinstance(arg, np.ndarray):; 71 sel = PointSelection(shape); ---> 72 sel[arg]; 73 return sel; 74 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in __getitem__(self, arg); 210 """""" Perform point-wise selection from a NumPy boolean array """"""; 211 if not (isinstance(arg, np.ndarray) and arg.dtype.kind == 'b'):; --> 212 raise TypeError(""PointSelection __getitem__ only works with bool arrays""); 213 if not arg.shape == self.shape:; 214 raise TypeError(""Boolean indexing array has incompatible shape""). TypeError: PointSelection __getitem__ only works with bool arrays. <Figure size 1978.56x288 with 0 Axes>; ``` . Is it something implicit in the format of the backed file that cannot be solved? Also, do you think the ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/issues/440:2410,Performance,Perform,Perform,2410,"adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selection = sel.select(self.shape, args, dsid=self.id); 477 ; 478 if selection.nselect == 0:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in select(shape, args, dsid); 70 elif isinstance(arg, np.ndarray):; 71 sel = PointSelection(shape); ---> 72 sel[arg]; 73 return sel; 74 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in __getitem__(self, arg); 210 """""" Perform point-wise selection from a NumPy boolean array """"""; 211 if not (isinstance(arg, np.ndarray) and arg.dtype.kind == 'b'):; --> 212 raise TypeError(""PointSelection __getitem__ only works with bool arrays""); 213 if not arg.shape == self.shape:; 214 raise TypeError(""Boolean indexing array has incompatible shape""). TypeError: PointSelection __getitem__ only works with bool arrays. <Figure size 1978.56x288 with 0 Axes>; ``` . Is it something implicit in the format of the backed file that cannot be solved? Also, do you think the memory usage is due to something else than the data not being backed? It is only 8000 cells and 15000 genes. Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440
https://github.com/scverse/scanpy/pull/441:107,Testability,log,logg,107,"Define `.uns['neighbors']['distances']` as the distance matrix instead of the weighted adjacency matrix in logg.hint. By the way, when `knn=True`, distances to non-neighbor data points are stored as zero in this matrix to keep it sparse, right? But technically these are not zeros, but rather ""unknown values"". Does it make sense to add a note in the documentation about that? Because for `knn=False`, the semantics of zeros change completely.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441
https://github.com/scverse/scanpy/pull/442:670,Availability,error,error,670,"Removes the need to specify a genome string in 10x h5 files by default. This removes a personal annoyance of mine, where I had to figure out what the reference was called when there's often only one reference used per file. For cellranger `v3.0.0+` files, specifying a genome acts as a filter on input, as it did already. However, it doesn't only act if `gex_only` is `True`. Additionally, the behavior of `gex_only` has been changed to fit the documentation, i.e. it just filters for gene expression variables. For legacy files:. * If the file only has one genome group, that one is used by default; * If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. As the behavior of the function has meaningfully changed, this is a breaking change (though I'd be surprised if it affected many people). Personally, I haven't seen many 10x files which contain multiple genomes, so I'd appreciate feedback or examples from people who have.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:501,Modifiability,variab,variables,501,"Removes the need to specify a genome string in 10x h5 files by default. This removes a personal annoyance of mine, where I had to figure out what the reference was called when there's often only one reference used per file. For cellranger `v3.0.0+` files, specifying a genome acts as a filter on input, as it did already. However, it doesn't only act if `gex_only` is `True`. Additionally, the behavior of `gex_only` has been changed to fit the documentation, i.e. it just filters for gene expression variables. For legacy files:. * If the file only has one genome group, that one is used by default; * If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. As the behavior of the function has meaningfully changed, this is a breaking change (though I'd be surprised if it affected many people). Personally, I haven't seen many 10x files which contain multiple genomes, so I'd appreciate feedback or examples from people who have.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/442:1017,Usability,feedback,feedback,1017,"Removes the need to specify a genome string in 10x h5 files by default. This removes a personal annoyance of mine, where I had to figure out what the reference was called when there's often only one reference used per file. For cellranger `v3.0.0+` files, specifying a genome acts as a filter on input, as it did already. However, it doesn't only act if `gex_only` is `True`. Additionally, the behavior of `gex_only` has been changed to fit the documentation, i.e. it just filters for gene expression variables. For legacy files:. * If the file only has one genome group, that one is used by default; * If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. As the behavior of the function has meaningfully changed, this is a breaking change (though I'd be surprised if it affected many people). Personally, I haven't seen many 10x files which contain multiple genomes, so I'd appreciate feedback or examples from people who have.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442
https://github.com/scverse/scanpy/pull/444:94,Availability,error,error,94,"An addendum to #442, now if you try to read a 10x file and pass a genome it doesn't have, the error tells you which file it was and the genomes it does have:. ```python; In [1]: import scanpy as sc ; ...: sc.read_10x_h5(""scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5"", ""not a genome"") ; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last). ... ValueError: Could not find genome 'not a genome' in 'scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5'. Available genomes are: ['hg19_chr21']; ```. Previous behavior:. ```python; In [1]: import scanpy as sc ; ...: sc.read_10x_h5(""scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5"", ""not a genome"") ; ---------------------------------------------------------------------------; NoSuchNodeError Traceback (most recent call last). ... Exception: Genome not a genome does not exist in this file.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/444
https://github.com/scverse/scanpy/pull/444:555,Availability,Avail,Available,555,"An addendum to #442, now if you try to read a 10x file and pass a genome it doesn't have, the error tells you which file it was and the genomes it does have:. ```python; In [1]: import scanpy as sc ; ...: sc.read_10x_h5(""scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5"", ""not a genome"") ; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last). ... ValueError: Could not find genome 'not a genome' in 'scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5'. Available genomes are: ['hg19_chr21']; ```. Previous behavior:. ```python; In [1]: import scanpy as sc ; ...: sc.read_10x_h5(""scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5"", ""not a genome"") ; ---------------------------------------------------------------------------; NoSuchNodeError Traceback (most recent call last). ... Exception: Genome not a genome does not exist in this file.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/444
https://github.com/scverse/scanpy/pull/444:228,Testability,test,tests,228,"An addendum to #442, now if you try to read a 10x file and pass a genome it doesn't have, the error tells you which file it was and the genomes it does have:. ```python; In [1]: import scanpy as sc ; ...: sc.read_10x_h5(""scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5"", ""not a genome"") ; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last). ... ValueError: Could not find genome 'not a genome' in 'scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5'. Available genomes are: ['hg19_chr21']; ```. Previous behavior:. ```python; In [1]: import scanpy as sc ; ...: sc.read_10x_h5(""scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5"", ""not a genome"") ; ---------------------------------------------------------------------------; NoSuchNodeError Traceback (most recent call last). ... Exception: Genome not a genome does not exist in this file.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/444
https://github.com/scverse/scanpy/pull/444:494,Testability,test,tests,494,"An addendum to #442, now if you try to read a 10x file and pass a genome it doesn't have, the error tells you which file it was and the genomes it does have:. ```python; In [1]: import scanpy as sc ; ...: sc.read_10x_h5(""scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5"", ""not a genome"") ; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last). ... ValueError: Could not find genome 'not a genome' in 'scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5'. Available genomes are: ['hg19_chr21']; ```. Previous behavior:. ```python; In [1]: import scanpy as sc ; ...: sc.read_10x_h5(""scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5"", ""not a genome"") ; ---------------------------------------------------------------------------; NoSuchNodeError Traceback (most recent call last). ... Exception: Genome not a genome does not exist in this file.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/444
https://github.com/scverse/scanpy/pull/444:688,Testability,test,tests,688,"An addendum to #442, now if you try to read a 10x file and pass a genome it doesn't have, the error tells you which file it was and the genomes it does have:. ```python; In [1]: import scanpy as sc ; ...: sc.read_10x_h5(""scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5"", ""not a genome"") ; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last). ... ValueError: Could not find genome 'not a genome' in 'scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5'. Available genomes are: ['hg19_chr21']; ```. Previous behavior:. ```python; In [1]: import scanpy as sc ; ...: sc.read_10x_h5(""scanpy/tests/_data/10x_data/1.2.0/filtered_gene_bc_matrices_h5.h5"", ""not a genome"") ; ---------------------------------------------------------------------------; NoSuchNodeError Traceback (most recent call last). ... Exception: Genome not a genome does not exist in this file.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/444
https://github.com/scverse/scanpy/issues/445:2350,Modifiability,layers,layers,2350," node_size_scale, node_size_power, edge_width_scale, normalize_to_color, title, pos, cmap, frameon, min_edge_width, max_edge_width, export_to_gexf, cax, colorbar, use_raw, cb_kwds, single_component, arrowsize); 612 adata_gene = adata.raw[:, colors]; 613 else:; --> 614 adata_gene = adata[:, colors]; 615 x_color.append(np.mean(adata_gene.X[subset])); 616 colors = x_color. C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 1307 def __getitem__(self, index):; 1308 """"""Returns a sliced view of the object.""""""; -> 1309 return self._getitem_view(index); 1310 ; 1311 def _getitem_view(self, index):. C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index); 1311 def _getitem_view(self, index):; 1312 oidx, vidx = self._normalize_indices(index); -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1314 ; 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx); 723 self._X = None; 724 else:; --> 725 self._init_X_as_view(); 726 ; 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self); 750 shape = (; 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),; --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars); 753 ); 754 if np.isscalar(X):. C:\ProgramData\Anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l); 148 return 1; 149 else:; --> 150 return len(idx). TypeError: object of type 'numpy.int64' has no len(); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445
https://github.com/scverse/scanpy/issues/446:87,Testability,log,logfoldchange,87,"Hi, . Thanks for the great tools included in scanpy. . I was searching to see what the logfoldchange numbers in the `rank_genes_groups` are. i.e. is it natural log/log base 10 or log2? The code in https://github.com/theislab/scanpy/blob/master/scanpy/tools/_rank_genes_groups.py shows that it is log2foldchange. . Perhaps this should be made clear in the documentation? Or `logfoldchange` should be chnaged to `log2foldchange`. . Cheers, ; S",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:160,Testability,log,log,160,"Hi, . Thanks for the great tools included in scanpy. . I was searching to see what the logfoldchange numbers in the `rank_genes_groups` are. i.e. is it natural log/log base 10 or log2? The code in https://github.com/theislab/scanpy/blob/master/scanpy/tools/_rank_genes_groups.py shows that it is log2foldchange. . Perhaps this should be made clear in the documentation? Or `logfoldchange` should be chnaged to `log2foldchange`. . Cheers, ; S",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:164,Testability,log,log,164,"Hi, . Thanks for the great tools included in scanpy. . I was searching to see what the logfoldchange numbers in the `rank_genes_groups` are. i.e. is it natural log/log base 10 or log2? The code in https://github.com/theislab/scanpy/blob/master/scanpy/tools/_rank_genes_groups.py shows that it is log2foldchange. . Perhaps this should be made clear in the documentation? Or `logfoldchange` should be chnaged to `log2foldchange`. . Cheers, ; S",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:374,Testability,log,logfoldchange,374,"Hi, . Thanks for the great tools included in scanpy. . I was searching to see what the logfoldchange numbers in the `rank_genes_groups` are. i.e. is it natural log/log base 10 or log2? The code in https://github.com/theislab/scanpy/blob/master/scanpy/tools/_rank_genes_groups.py shows that it is log2foldchange. . Perhaps this should be made clear in the documentation? Or `logfoldchange` should be chnaged to `log2foldchange`. . Cheers, ; S",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/446:342,Usability,clear,clear,342,"Hi, . Thanks for the great tools included in scanpy. . I was searching to see what the logfoldchange numbers in the `rank_genes_groups` are. i.e. is it natural log/log base 10 or log2? The code in https://github.com/theislab/scanpy/blob/master/scanpy/tools/_rank_genes_groups.py shows that it is log2foldchange. . Perhaps this should be made clear in the documentation? Or `logfoldchange` should be chnaged to `log2foldchange`. . Cheers, ; S",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446
https://github.com/scverse/scanpy/issues/449:80,Modifiability,variab,variable,80,"Hi, the documentation describes running MNN batch correction on a set of highly variable genes - how many genes are recommended? . * I tried running with the top 10,000 variable genes. Is this a good number. * Also, does selecting for these highly variable genes effectively permanently filter the dataset for only these genes?. * I noticed that the minimum gene expression level for some genes was less than zero after batch correction, is this expected?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:169,Modifiability,variab,variable,169,"Hi, the documentation describes running MNN batch correction on a set of highly variable genes - how many genes are recommended? . * I tried running with the top 10,000 variable genes. Is this a good number. * Also, does selecting for these highly variable genes effectively permanently filter the dataset for only these genes?. * I noticed that the minimum gene expression level for some genes was less than zero after batch correction, is this expected?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/449:248,Modifiability,variab,variable,248,"Hi, the documentation describes running MNN batch correction on a set of highly variable genes - how many genes are recommended? . * I tried running with the top 10,000 variable genes. Is this a good number. * Also, does selecting for these highly variable genes effectively permanently filter the dataset for only these genes?. * I noticed that the minimum gene expression level for some genes was less than zero after batch correction, is this expected?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449
https://github.com/scverse/scanpy/issues/450:1821,Availability,down,downcast,1821,"# a normalized disperion of 1; 116 one_gene_per_bin = disp_std_bin.isnull(); --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(); 118 if len(gen_indices) > 0:; 119 logg.msg(. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 Please use .at[] or .iat[] accessors.; 910 ; --> 911 Parameters; 912 ----------; 913 index : label. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 -------; 952 series : Series; --> 953 If label is contained, will be reference to calling Series,; 954 otherwise a new object; 955 """""". ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4344 ; 4345 elif not is_list_like(value):; -> 4346 new_data = self._data.fillna(value=value, limit=limit,; 4347 inplace=inplace,; 4348 downcast=downcast). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4357 return self._constructor(new_data).__finalize__(self); 4358 ; -> 4359 def ffill(self, axis=None, inplace=False, limit=None, downcast=None):; 4360 """"""; 4361 Synonym for :meth:`DataFrame.fillna(method='ffill') <DataFrame.fillna>`. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance); 501 # in which case we are going to conform to the passed Categorical; 502 new_target = np.asarray(new_target); --> 503 if is_categorical_dtype(target):; 504 new_target = target._shallow_copy(new_target, name=self.name); 505 else:. ValueError: cannot reindex with a non-unique indexer. **; ```; The error is gone with pandas 0.23.4. There was a change in the API of reindex in pandas: http://pandas.pydata.org/pand",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1830,Availability,down,downcast,1830,"# a normalized disperion of 1; 116 one_gene_per_bin = disp_std_bin.isnull(); --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(); 118 if len(gen_indices) > 0:; 119 logg.msg(. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 Please use .at[] or .iat[] accessors.; 910 ; --> 911 Parameters; 912 ----------; 913 index : label. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 -------; 952 series : Series; --> 953 If label is contained, will be reference to calling Series,; 954 otherwise a new object; 955 """""". ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4344 ; 4345 elif not is_list_like(value):; -> 4346 new_data = self._data.fillna(value=value, limit=limit,; 4347 inplace=inplace,; 4348 downcast=downcast). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4357 return self._constructor(new_data).__finalize__(self); 4358 ; -> 4359 def ffill(self, axis=None, inplace=False, limit=None, downcast=None):; 4360 """"""; 4361 Synonym for :meth:`DataFrame.fillna(method='ffill') <DataFrame.fillna>`. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance); 501 # in which case we are going to conform to the passed Categorical; 502 new_target = np.asarray(new_target); --> 503 if is_categorical_dtype(target):; 504 new_target = target._shallow_copy(new_target, name=self.name); 505 else:. ValueError: cannot reindex with a non-unique indexer. **; ```; The error is gone with pandas 0.23.4. There was a change in the API of reindex in pandas: http://pandas.pydata.org/pand",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1965,Availability,toler,tolerance,1965,"e_gene_per_bin = disp_std_bin.isnull(); --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(); 118 if len(gen_indices) > 0:; 119 logg.msg(. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 Please use .at[] or .iat[] accessors.; 910 ; --> 911 Parameters; 912 ----------; 913 index : label. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 -------; 952 series : Series; --> 953 If label is contained, will be reference to calling Series,; 954 otherwise a new object; 955 """""". ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4344 ; 4345 elif not is_list_like(value):; -> 4346 new_data = self._data.fillna(value=value, limit=limit,; 4347 inplace=inplace,; 4348 downcast=downcast). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4357 return self._constructor(new_data).__finalize__(self); 4358 ; -> 4359 def ffill(self, axis=None, inplace=False, limit=None, downcast=None):; 4360 """"""; 4361 Synonym for :meth:`DataFrame.fillna(method='ffill') <DataFrame.fillna>`. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance); 501 # in which case we are going to conform to the passed Categorical; 502 new_target = np.asarray(new_target); --> 503 if is_categorical_dtype(target):; 504 new_target = target._shallow_copy(new_target, name=self.name); 505 else:. ValueError: cannot reindex with a non-unique indexer. **; ```; The error is gone with pandas 0.23.4. There was a change in the API of reindex in pandas: http://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.24.0.html",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2132,Availability,down,downcast,2132,"e_gene_per_bin = disp_std_bin.isnull(); --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(); 118 if len(gen_indices) > 0:; 119 logg.msg(. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 Please use .at[] or .iat[] accessors.; 910 ; --> 911 Parameters; 912 ----------; 913 index : label. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 -------; 952 series : Series; --> 953 If label is contained, will be reference to calling Series,; 954 otherwise a new object; 955 """""". ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4344 ; 4345 elif not is_list_like(value):; -> 4346 new_data = self._data.fillna(value=value, limit=limit,; 4347 inplace=inplace,; 4348 downcast=downcast). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4357 return self._constructor(new_data).__finalize__(self); 4358 ; -> 4359 def ffill(self, axis=None, inplace=False, limit=None, downcast=None):; 4360 """"""; 4361 Synonym for :meth:`DataFrame.fillna(method='ffill') <DataFrame.fillna>`. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance); 501 # in which case we are going to conform to the passed Categorical; 502 new_target = np.asarray(new_target); --> 503 if is_categorical_dtype(target):; 504 new_target = target._shallow_copy(new_target, name=self.name); 505 else:. ValueError: cannot reindex with a non-unique indexer. **; ```; The error is gone with pandas 0.23.4. There was a change in the API of reindex in pandas: http://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.24.0.html",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2374,Availability,toler,tolerance,2374,"e_gene_per_bin = disp_std_bin.isnull(); --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(); 118 if len(gen_indices) > 0:; 119 logg.msg(. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 Please use .at[] or .iat[] accessors.; 910 ; --> 911 Parameters; 912 ----------; 913 index : label. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 -------; 952 series : Series; --> 953 If label is contained, will be reference to calling Series,; 954 otherwise a new object; 955 """""". ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4344 ; 4345 elif not is_list_like(value):; -> 4346 new_data = self._data.fillna(value=value, limit=limit,; 4347 inplace=inplace,; 4348 downcast=downcast). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4357 return self._constructor(new_data).__finalize__(self); 4358 ; -> 4359 def ffill(self, axis=None, inplace=False, limit=None, downcast=None):; 4360 """"""; 4361 Synonym for :meth:`DataFrame.fillna(method='ffill') <DataFrame.fillna>`. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance); 501 # in which case we are going to conform to the passed Categorical; 502 new_target = np.asarray(new_target); --> 503 if is_categorical_dtype(target):; 504 new_target = target._shallow_copy(new_target, name=self.name); 505 else:. ValueError: cannot reindex with a non-unique indexer. **; ```; The error is gone with pandas 0.23.4. There was a change in the API of reindex in pandas: http://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.24.0.html",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:2685,Availability,error,error,2685,"e_gene_per_bin = disp_std_bin.isnull(); --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(); 118 if len(gen_indices) > 0:; 119 logg.msg(. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 Please use .at[] or .iat[] accessors.; 910 ; --> 911 Parameters; 912 ----------; 913 index : label. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 -------; 952 series : Series; --> 953 If label is contained, will be reference to calling Series,; 954 otherwise a new object; 955 """""". ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4344 ; 4345 elif not is_list_like(value):; -> 4346 new_data = self._data.fillna(value=value, limit=limit,; 4347 inplace=inplace,; 4348 downcast=downcast). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4357 return self._constructor(new_data).__finalize__(self); 4358 ; -> 4359 def ffill(self, axis=None, inplace=False, limit=None, downcast=None):; 4360 """"""; 4361 Synonym for :meth:`DataFrame.fillna(method='ffill') <DataFrame.fillna>`. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance); 501 # in which case we are going to conform to the passed Categorical; 502 new_target = np.asarray(new_target); --> 503 if is_categorical_dtype(target):; 504 new_target = target._shallow_copy(new_target, name=self.name); 505 else:. ValueError: cannot reindex with a non-unique indexer. **; ```; The error is gone with pandas 0.23.4. There was a change in the API of reindex in pandas: http://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.24.0.html",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:1137,Security,access,accessors,1137,"(adata); ```; Output:; ```python; **scanpy==1.3.7 anndata==0.6.17 numpy==1.15.4 scipy==1.2.0 pandas==0.24.0 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-11-5d93fbf298b7> in <module>; 4 adata = sc.datasets.blobs(); 5 ; ----> 6 sc.pp.highly_variable_genes(adata). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 115 # a normalized disperion of 1; 116 one_gene_per_bin = disp_std_bin.isnull(); --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(); 118 if len(gen_indices) > 0:; 119 logg.msg(. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 Please use .at[] or .iat[] accessors.; 910 ; --> 911 Parameters; 912 ----------; 913 index : label. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 -------; 952 series : Series; --> 953 If label is contained, will be reference to calling Series,; 954 otherwise a new object; 955 """""". ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4344 ; 4345 elif not is_list_like(value):; -> 4346 new_data = self._data.fillna(value=value, limit=limit,; 4347 inplace=inplace,; 4348 downcast=downcast). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4357 return self._constructor(new_data).__finalize__(self); 4358 ; -> 4359 def ffill(self, axis=None, inplace=False, limit=None, downcas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:57,Testability,log,logging,57,"Minimal example:; ```python; import scanpy.api as sc; sc.logging.print_versions(). adata = sc.datasets.blobs(). sc.pp.highly_variable_genes(adata); ```; Output:; ```python; **scanpy==1.3.7 anndata==0.6.17 numpy==1.15.4 scipy==1.2.0 pandas==0.24.0 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-11-5d93fbf298b7> in <module>; 4 adata = sc.datasets.blobs(); 5 ; ----> 6 sc.pp.highly_variable_genes(adata). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 115 # a normalized disperion of 1; 116 one_gene_per_bin = disp_std_bin.isnull(); --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(); 118 if len(gen_indices) > 0:; 119 logg.msg(. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 Please use .at[] or .iat[] accessors.; 910 ; --> 911 Parameters; 912 ----------; 913 index : label. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 -------; 952 series : Series; --> 953 If label is contained, will be reference to calling Series,; 954 otherwise a new object; 955 """""". ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4344 ; 4345 elif not is_list_like(value):; -> 4346 new_data = self._data.fillna(value=value, limit=limit,; 4347 inplace=inplace,; 4348 downcast=downcast). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:988,Testability,log,logg,988,"Minimal example:; ```python; import scanpy.api as sc; sc.logging.print_versions(). adata = sc.datasets.blobs(). sc.pp.highly_variable_genes(adata); ```; Output:; ```python; **scanpy==1.3.7 anndata==0.6.17 numpy==1.15.4 scipy==1.2.0 pandas==0.24.0 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-11-5d93fbf298b7> in <module>; 4 adata = sc.datasets.blobs(); 5 ; ----> 6 sc.pp.highly_variable_genes(adata). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 115 # a normalized disperion of 1; 116 one_gene_per_bin = disp_std_bin.isnull(); --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(); 118 if len(gen_indices) > 0:; 119 logg.msg(. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 Please use .at[] or .iat[] accessors.; 910 ; --> 911 Parameters; 912 ----------; 913 index : label. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 -------; 952 series : Series; --> 953 If label is contained, will be reference to calling Series,; 954 otherwise a new object; 955 """""". ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4344 ; 4345 elif not is_list_like(value):; -> 4346 new_data = self._data.fillna(value=value, limit=limit,; 4347 inplace=inplace,; 4348 downcast=downcast). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/450:254,Usability,learn,learn,254,"Minimal example:; ```python; import scanpy.api as sc; sc.logging.print_versions(). adata = sc.datasets.blobs(). sc.pp.highly_variable_genes(adata); ```; Output:; ```python; **scanpy==1.3.7 anndata==0.6.17 numpy==1.15.4 scipy==1.2.0 pandas==0.24.0 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-11-5d93fbf298b7> in <module>; 4 adata = sc.datasets.blobs(); 5 ; ----> 6 sc.pp.highly_variable_genes(adata). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 115 # a normalized disperion of 1; 116 one_gene_per_bin = disp_std_bin.isnull(); --> 117 gen_indices = np.where(one_gene_per_bin[df['mean_bin']])[0].tolist(); 118 if len(gen_indices) > 0:; 119 logg.msg(. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 Please use .at[] or .iat[] accessors.; 910 ; --> 911 Parameters; 912 ----------; 913 index : label. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 -------; 952 series : Series; --> 953 If label is contained, will be reference to calling Series,; 954 otherwise a new object; 955 """""". ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4344 ; 4345 elif not is_list_like(value):; -> 4346 new_data = self._data.fillna(value=value, limit=limit,; 4347 inplace=inplace,; 4348 downcast=downcast). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450
https://github.com/scverse/scanpy/issues/453:386,Performance,cache,cache,386,"A list for keeping track of things that we might change when breaking backwards compat at some point:. - [ ] merge sparse pca https://github.com/theislab/scanpy/pull/1066; - [x] merge https://github.com/theislab/scanpy/pull/1111; - [ ] merge #572; - [ ] make `t-test` or `wilxocon` the default of `tl.rank_genes_groups`; - [ ] set the cachdir default to `user_cache_dir(…)`, `~/.scanpy/cache/` or `~/.cache/`; - [ ] stationary states in DPT: https://github.com/theislab/scanpy/blob/b11b4abe5e16053c010e57b2dd3a27396a4b0cf2/scanpy/neighbors/__init__.py#L853-L857 thanks to @Marius1311 for pointing it out!; - [ ] rename `log2fc` or similarly: #446; - [ ] add `inplace` functionality where easily possible, that's not a simple renaming; a function that has `inplace` in it, should only return the annotation if `inplace=False`; the `copy` functions return the whole `adata`, which we don't want...; - [ ] rename `n_comps` to `n_components` everywhere; - [ ] consider merging https://github.com/theislab/scanpy/pull/403; - [ ] replace default pca solver with 'arpack'; - [ ] change default solver in logreg solver in rank_genes_groups to lbfgs; - [x] merge #621; - [ ] make `pp.highly_variable_genes` return a df instead of a recarray...; - [ ] Transition away from positional APIs: #464 (actually backwards compatible through decorator!). anndata:; - [ ] merge https://github.com/theislab/anndata/pull/130 and fix Scanpy tests",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/453:401,Performance,cache,cache,401,"A list for keeping track of things that we might change when breaking backwards compat at some point:. - [ ] merge sparse pca https://github.com/theislab/scanpy/pull/1066; - [x] merge https://github.com/theislab/scanpy/pull/1111; - [ ] merge #572; - [ ] make `t-test` or `wilxocon` the default of `tl.rank_genes_groups`; - [ ] set the cachdir default to `user_cache_dir(…)`, `~/.scanpy/cache/` or `~/.cache/`; - [ ] stationary states in DPT: https://github.com/theislab/scanpy/blob/b11b4abe5e16053c010e57b2dd3a27396a4b0cf2/scanpy/neighbors/__init__.py#L853-L857 thanks to @Marius1311 for pointing it out!; - [ ] rename `log2fc` or similarly: #446; - [ ] add `inplace` functionality where easily possible, that's not a simple renaming; a function that has `inplace` in it, should only return the annotation if `inplace=False`; the `copy` functions return the whole `adata`, which we don't want...; - [ ] rename `n_comps` to `n_components` everywhere; - [ ] consider merging https://github.com/theislab/scanpy/pull/403; - [ ] replace default pca solver with 'arpack'; - [ ] change default solver in logreg solver in rank_genes_groups to lbfgs; - [x] merge #621; - [ ] make `pp.highly_variable_genes` return a df instead of a recarray...; - [ ] Transition away from positional APIs: #464 (actually backwards compatible through decorator!). anndata:; - [ ] merge https://github.com/theislab/anndata/pull/130 and fix Scanpy tests",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/453:262,Testability,test,test,262,"A list for keeping track of things that we might change when breaking backwards compat at some point:. - [ ] merge sparse pca https://github.com/theislab/scanpy/pull/1066; - [x] merge https://github.com/theislab/scanpy/pull/1111; - [ ] merge #572; - [ ] make `t-test` or `wilxocon` the default of `tl.rank_genes_groups`; - [ ] set the cachdir default to `user_cache_dir(…)`, `~/.scanpy/cache/` or `~/.cache/`; - [ ] stationary states in DPT: https://github.com/theislab/scanpy/blob/b11b4abe5e16053c010e57b2dd3a27396a4b0cf2/scanpy/neighbors/__init__.py#L853-L857 thanks to @Marius1311 for pointing it out!; - [ ] rename `log2fc` or similarly: #446; - [ ] add `inplace` functionality where easily possible, that's not a simple renaming; a function that has `inplace` in it, should only return the annotation if `inplace=False`; the `copy` functions return the whole `adata`, which we don't want...; - [ ] rename `n_comps` to `n_components` everywhere; - [ ] consider merging https://github.com/theislab/scanpy/pull/403; - [ ] replace default pca solver with 'arpack'; - [ ] change default solver in logreg solver in rank_genes_groups to lbfgs; - [x] merge #621; - [ ] make `pp.highly_variable_genes` return a df instead of a recarray...; - [ ] Transition away from positional APIs: #464 (actually backwards compatible through decorator!). anndata:; - [ ] merge https://github.com/theislab/anndata/pull/130 and fix Scanpy tests",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/453:1097,Testability,log,logreg,1097,"A list for keeping track of things that we might change when breaking backwards compat at some point:. - [ ] merge sparse pca https://github.com/theislab/scanpy/pull/1066; - [x] merge https://github.com/theislab/scanpy/pull/1111; - [ ] merge #572; - [ ] make `t-test` or `wilxocon` the default of `tl.rank_genes_groups`; - [ ] set the cachdir default to `user_cache_dir(…)`, `~/.scanpy/cache/` or `~/.cache/`; - [ ] stationary states in DPT: https://github.com/theislab/scanpy/blob/b11b4abe5e16053c010e57b2dd3a27396a4b0cf2/scanpy/neighbors/__init__.py#L853-L857 thanks to @Marius1311 for pointing it out!; - [ ] rename `log2fc` or similarly: #446; - [ ] add `inplace` functionality where easily possible, that's not a simple renaming; a function that has `inplace` in it, should only return the annotation if `inplace=False`; the `copy` functions return the whole `adata`, which we don't want...; - [ ] rename `n_comps` to `n_components` everywhere; - [ ] consider merging https://github.com/theislab/scanpy/pull/403; - [ ] replace default pca solver with 'arpack'; - [ ] change default solver in logreg solver in rank_genes_groups to lbfgs; - [x] merge #621; - [ ] make `pp.highly_variable_genes` return a df instead of a recarray...; - [ ] Transition away from positional APIs: #464 (actually backwards compatible through decorator!). anndata:; - [ ] merge https://github.com/theislab/anndata/pull/130 and fix Scanpy tests",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/453:1419,Testability,test,tests,1419,"A list for keeping track of things that we might change when breaking backwards compat at some point:. - [ ] merge sparse pca https://github.com/theislab/scanpy/pull/1066; - [x] merge https://github.com/theislab/scanpy/pull/1111; - [ ] merge #572; - [ ] make `t-test` or `wilxocon` the default of `tl.rank_genes_groups`; - [ ] set the cachdir default to `user_cache_dir(…)`, `~/.scanpy/cache/` or `~/.cache/`; - [ ] stationary states in DPT: https://github.com/theislab/scanpy/blob/b11b4abe5e16053c010e57b2dd3a27396a4b0cf2/scanpy/neighbors/__init__.py#L853-L857 thanks to @Marius1311 for pointing it out!; - [ ] rename `log2fc` or similarly: #446; - [ ] add `inplace` functionality where easily possible, that's not a simple renaming; a function that has `inplace` in it, should only return the annotation if `inplace=False`; the `copy` functions return the whole `adata`, which we don't want...; - [ ] rename `n_comps` to `n_components` everywhere; - [ ] consider merging https://github.com/theislab/scanpy/pull/403; - [ ] replace default pca solver with 'arpack'; - [ ] change default solver in logreg solver in rank_genes_groups to lbfgs; - [x] merge #621; - [ ] make `pp.highly_variable_genes` return a df instead of a recarray...; - [ ] Transition away from positional APIs: #464 (actually backwards compatible through decorator!). anndata:; - [ ] merge https://github.com/theislab/anndata/pull/130 and fix Scanpy tests",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/453:718,Usability,simpl,simple,718,"A list for keeping track of things that we might change when breaking backwards compat at some point:. - [ ] merge sparse pca https://github.com/theislab/scanpy/pull/1066; - [x] merge https://github.com/theislab/scanpy/pull/1111; - [ ] merge #572; - [ ] make `t-test` or `wilxocon` the default of `tl.rank_genes_groups`; - [ ] set the cachdir default to `user_cache_dir(…)`, `~/.scanpy/cache/` or `~/.cache/`; - [ ] stationary states in DPT: https://github.com/theislab/scanpy/blob/b11b4abe5e16053c010e57b2dd3a27396a4b0cf2/scanpy/neighbors/__init__.py#L853-L857 thanks to @Marius1311 for pointing it out!; - [ ] rename `log2fc` or similarly: #446; - [ ] add `inplace` functionality where easily possible, that's not a simple renaming; a function that has `inplace` in it, should only return the annotation if `inplace=False`; the `copy` functions return the whole `adata`, which we don't want...; - [ ] rename `n_comps` to `n_components` everywhere; - [ ] consider merging https://github.com/theislab/scanpy/pull/403; - [ ] replace default pca solver with 'arpack'; - [ ] change default solver in logreg solver in rank_genes_groups to lbfgs; - [x] merge #621; - [ ] make `pp.highly_variable_genes` return a df instead of a recarray...; - [ ] Transition away from positional APIs: #464 (actually backwards compatible through decorator!). anndata:; - [ ] merge https://github.com/theislab/anndata/pull/130 and fix Scanpy tests",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453
https://github.com/scverse/scanpy/issues/454:275,Availability,error,error,275,"Hi!; As someone else posted on [stackoverflow](https://stackoverflow.com/questions/54366505/importerror-dll-load-failed-while-file-is-in-working-directory/54441575#54441575), there seem to be problems with the tables dependencies for windows users resulting in the following error when importing scanpy:. ```pytb; >>> import scanpy; ...; File ""C:\Miniconda3\envs\py36\lib\site-packages\scanpy\readwrite.py"", line 9, in; import tables; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables__init__.py"", line 131, in; from .file import File, open_file, copy_file; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables\file.py"", line 35, in; from . import hdf5extension; ImportError: DLL load failed: The specified procedure could not be found.; ```. I've also posted an answer suggestion there. Maybe you could require h5py to have a fixed older version like 2.8 to avoid this problem for other windows users. Downgrading to that version worked for me.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:910,Availability,Down,Downgrading,910,"Hi!; As someone else posted on [stackoverflow](https://stackoverflow.com/questions/54366505/importerror-dll-load-failed-while-file-is-in-working-directory/54441575#54441575), there seem to be problems with the tables dependencies for windows users resulting in the following error when importing scanpy:. ```pytb; >>> import scanpy; ...; File ""C:\Miniconda3\envs\py36\lib\site-packages\scanpy\readwrite.py"", line 9, in; import tables; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables__init__.py"", line 131, in; from .file import File, open_file, copy_file; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables\file.py"", line 35, in; from . import hdf5extension; ImportError: DLL load failed: The specified procedure could not be found.; ```. I've also posted an answer suggestion there. Maybe you could require h5py to have a fixed older version like 2.8 to avoid this problem for other windows users. Downgrading to that version worked for me.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:217,Integrability,depend,dependencies,217,"Hi!; As someone else posted on [stackoverflow](https://stackoverflow.com/questions/54366505/importerror-dll-load-failed-while-file-is-in-working-directory/54441575#54441575), there seem to be problems with the tables dependencies for windows users resulting in the following error when importing scanpy:. ```pytb; >>> import scanpy; ...; File ""C:\Miniconda3\envs\py36\lib\site-packages\scanpy\readwrite.py"", line 9, in; import tables; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables__init__.py"", line 131, in; from .file import File, open_file, copy_file; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables\file.py"", line 35, in; from . import hdf5extension; ImportError: DLL load failed: The specified procedure could not be found.; ```. I've also posted an answer suggestion there. Maybe you could require h5py to have a fixed older version like 2.8 to avoid this problem for other windows users. Downgrading to that version worked for me.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:108,Performance,load,load-failed-while-file-is-in-working-directory,108,"Hi!; As someone else posted on [stackoverflow](https://stackoverflow.com/questions/54366505/importerror-dll-load-failed-while-file-is-in-working-directory/54441575#54441575), there seem to be problems with the tables dependencies for windows users resulting in the following error when importing scanpy:. ```pytb; >>> import scanpy; ...; File ""C:\Miniconda3\envs\py36\lib\site-packages\scanpy\readwrite.py"", line 9, in; import tables; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables__init__.py"", line 131, in; from .file import File, open_file, copy_file; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables\file.py"", line 35, in; from . import hdf5extension; ImportError: DLL load failed: The specified procedure could not be found.; ```. I've also posted an answer suggestion there. Maybe you could require h5py to have a fixed older version like 2.8 to avoid this problem for other windows users. Downgrading to that version worked for me.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:687,Performance,load,load,687,"Hi!; As someone else posted on [stackoverflow](https://stackoverflow.com/questions/54366505/importerror-dll-load-failed-while-file-is-in-working-directory/54441575#54441575), there seem to be problems with the tables dependencies for windows users resulting in the following error when importing scanpy:. ```pytb; >>> import scanpy; ...; File ""C:\Miniconda3\envs\py36\lib\site-packages\scanpy\readwrite.py"", line 9, in; import tables; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables__init__.py"", line 131, in; from .file import File, open_file, copy_file; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables\file.py"", line 35, in; from . import hdf5extension; ImportError: DLL load failed: The specified procedure could not be found.; ```. I've also posted an answer suggestion there. Maybe you could require h5py to have a fixed older version like 2.8 to avoid this problem for other windows users. Downgrading to that version worked for me.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/454:866,Safety,avoid,avoid,866,"Hi!; As someone else posted on [stackoverflow](https://stackoverflow.com/questions/54366505/importerror-dll-load-failed-while-file-is-in-working-directory/54441575#54441575), there seem to be problems with the tables dependencies for windows users resulting in the following error when importing scanpy:. ```pytb; >>> import scanpy; ...; File ""C:\Miniconda3\envs\py36\lib\site-packages\scanpy\readwrite.py"", line 9, in; import tables; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables__init__.py"", line 131, in; from .file import File, open_file, copy_file; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables\file.py"", line 35, in; from . import hdf5extension; ImportError: DLL load failed: The specified procedure could not be found.; ```. I've also posted an answer suggestion there. Maybe you could require h5py to have a fixed older version like 2.8 to avoid this problem for other windows users. Downgrading to that version worked for me.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454
https://github.com/scverse/scanpy/issues/455:379,Availability,error,error,379,"I would like to color the umap representation using gene expression values. For ease of use I'd like to display the Gene name instead of gene_id which are the adata.var_names in my case. Setting `gene_symbols = 'Symbol'` doesn't seem to work for me or I am using it the wrong way. When running `sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2'])`. I get the follwoing error message:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-116-e09d49f2528c> in <module>; ----> 1 sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2']). /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 275 color_vector, categorical = _get_color_values(adata, value_to_plot,; 276 groups=groups, palette=palette,; --> 277 use_raw=use_raw); 278 ; 279 # check if higher value points should be plot on top. /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw); 665 raise ValueError(""The passed `color` {} is not a valid observation annotation ""; 666 ""or variable name. Valid observation annotation keys are: {}""; --> 667 .format(value_to_plot, adata.obs.columns)); 668 ; 669 return color_vector, categorical. ValueError: The passed `color` Tnnt2 is not a valid observation annotation or variable name. Valid observation annotat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:385,Integrability,message,message,385,"I would like to color the umap representation using gene expression values. For ease of use I'd like to display the Gene name instead of gene_id which are the adata.var_names in my case. Setting `gene_symbols = 'Symbol'` doesn't seem to work for me or I am using it the wrong way. When running `sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2'])`. I get the follwoing error message:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-116-e09d49f2528c> in <module>; ----> 1 sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2']). /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 275 color_vector, categorical = _get_color_values(adata, value_to_plot,; 276 groups=groups, palette=palette,; --> 277 use_raw=use_raw); 278 ; 279 # check if higher value points should be plot on top. /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw); 665 raise ValueError(""The passed `color` {} is not a valid observation annotation ""; 666 ""or variable name. Valid observation annotation keys are: {}""; --> 667 .format(value_to_plot, adata.obs.columns)); 668 ; 669 return color_vector, categorical. ValueError: The passed `color` Tnnt2 is not a valid observation annotation or variable name. Valid observation annotat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1728,Modifiability,variab,variable,1728,"odule>; ----> 1 sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2']). /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 275 color_vector, categorical = _get_color_values(adata, value_to_plot,; 276 groups=groups, palette=palette,; --> 277 use_raw=use_raw); 278 ; 279 # check if higher value points should be plot on top. /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw); 665 raise ValueError(""The passed `color` {} is not a valid observation annotation ""; 666 ""or variable name. Valid observation annotation keys are: {}""; --> 667 .format(value_to_plot, adata.obs.columns)); 668 ; 669 return color_vector, categorical. ValueError: The passed `color` Tnnt2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['Sample', 'n_counts', 'n_genes', 'percent_mito', 'log_counts',; 'louvain'],; dtype='object'). ```. adata.var contains the column ""Symbol"" and ""Tnnt2"" is present:. `adata.var[adata.var['Symbol'] == 'Tnnt2']`. . Symbol | type | highly_variable | means | dispersions | dispersions_norm; -- | -- | -- | -- | -- | --; Tnnt2 | protein_coding | True | 0.923869 | 4.090601 | 11.370244. run with:; `scanpy==1.3.7 anndata==0.6.17 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:1961,Modifiability,variab,variable,1961,"odule>; ----> 1 sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2']). /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 275 color_vector, categorical = _get_color_values(adata, value_to_plot,; 276 groups=groups, palette=palette,; --> 277 use_raw=use_raw); 278 ; 279 # check if higher value points should be plot on top. /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw); 665 raise ValueError(""The passed `color` {} is not a valid observation annotation ""; 666 ""or variable name. Valid observation annotation keys are: {}""; --> 667 .format(value_to_plot, adata.obs.columns)); 668 ; 669 return color_vector, categorical. ValueError: The passed `color` Tnnt2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['Sample', 'n_counts', 'n_genes', 'percent_mito', 'log_counts',; 'louvain'],; dtype='object'). ```. adata.var contains the column ""Symbol"" and ""Tnnt2"" is present:. `adata.var[adata.var['Symbol'] == 'Tnnt2']`. . Symbol | type | highly_variable | means | dispersions | dispersions_norm; -- | -- | -- | -- | -- | --; Tnnt2 | protein_coding | True | 0.923869 | 4.090601 | 11.370244. run with:; `scanpy==1.3.7 anndata==0.6.17 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/455:2491,Usability,learn,learn,2491,"odule>; ----> 1 sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2']). /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 275 color_vector, categorical = _get_color_values(adata, value_to_plot,; 276 groups=groups, palette=palette,; --> 277 use_raw=use_raw); 278 ; 279 # check if higher value points should be plot on top. /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw); 665 raise ValueError(""The passed `color` {} is not a valid observation annotation ""; 666 ""or variable name. Valid observation annotation keys are: {}""; --> 667 .format(value_to_plot, adata.obs.columns)); 668 ; 669 return color_vector, categorical. ValueError: The passed `color` Tnnt2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['Sample', 'n_counts', 'n_genes', 'percent_mito', 'log_counts',; 'louvain'],; dtype='object'). ```. adata.var contains the column ""Symbol"" and ""Tnnt2"" is present:. `adata.var[adata.var['Symbol'] == 'Tnnt2']`. . Symbol | type | highly_variable | means | dispersions | dispersions_norm; -- | -- | -- | -- | -- | --; Tnnt2 | protein_coding | True | 0.923869 | 4.090601 | 11.370244. run with:; `scanpy==1.3.7 anndata==0.6.17 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455
https://github.com/scverse/scanpy/issues/456:386,Availability,error,error,386,"I am following workflow of '_Best-practices in single-cell RNA-seq: a tutorial_' to analyze my single-cell sequencing data sets.; I have calculated the size factor using the scran package and did not perform the batch correction step as I have only one sample. Then, I intended to extract highly variable genes by using the function sc.pp.highly_variable_genes. Unfortunately, I got an error:. > LinAlgError: Last 2 dimensions of the array must be square. <details><summary>Traceback</summary>. ```pytb; LinAlgError Traceback (most recent call last); in ; ----> 1 sc.pp.highly_variable_genes(adata). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 94 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X; 95; ---> 96 mean, var = materialize_as_ndarray(_get_mean_var(X)); 97 # now actually compute the dispersion; 98 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:2206,Availability,down,downgraded,2206,"canpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arrays):; ```. </details>. Versions of my modules:; scanpy==1.3.7 anndata==0.6.17 numpy==1.15.4 scipy==1.2.0 pandas==0.24.0 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. I have downgraded pandas to 0.23.4, however, it not works. But I figured out where the problem lies in. ```py; adata.X /= adata.obs['size_factors'].values[:,None]; ```. This step transform the adata.X to a structure of matrix.; Before the adata.X is. ```; <6242x15065 sparse matrix of type '<class 'numpy.float32'>'; with 19234986 stored elements in Compressed Sparse Row format>; ```. But after performing this step, the adata.X is; This is my adata.X looks like right now:. ```py; matrix([[0. , 0. , 0. , ..., 0. , 0. , 0. ],; [0. , 0. , 1.203, ..., 0. , 0. , 0. ],; [0. , 1.096, 0. , ..., 0. , 0. , 0. ],; ...,; [0. , 0. , 2.042, ..., 0. , 0. , 0. ],; [0. , 0. , 0. , ..., 0.926, 0. , 0. ],; [0. , 0. , 2.951, ..., 0. , 0. , 0. ]]),; ```. And this format of adata.X caused error of sc.pp.highly_variable_genes. But I don't know how to fix it. Looking forward your response!; Thank you !",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:2975,Availability,error,error,2975,"canpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arrays):; ```. </details>. Versions of my modules:; scanpy==1.3.7 anndata==0.6.17 numpy==1.15.4 scipy==1.2.0 pandas==0.24.0 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. I have downgraded pandas to 0.23.4, however, it not works. But I figured out where the problem lies in. ```py; adata.X /= adata.obs['size_factors'].values[:,None]; ```. This step transform the adata.X to a structure of matrix.; Before the adata.X is. ```; <6242x15065 sparse matrix of type '<class 'numpy.float32'>'; with 19234986 stored elements in Compressed Sparse Row format>; ```. But after performing this step, the adata.X is; This is my adata.X looks like right now:. ```py; matrix([[0. , 0. , 0. , ..., 0. , 0. , 0. ],; [0. , 0. , 1.203, ..., 0. , 0. , 0. ],; [0. , 1.096, 0. , ..., 0. , 0. , 0. ],; ...,; [0. , 0. , 2.042, ..., 0. , 0. , 0. ],; [0. , 0. , 0. , ..., 0.926, 0. , 0. ],; [0. , 0. , 2.951, ..., 0. , 0. , 0. ]]),; ```. And this format of adata.X caused error of sc.pp.highly_variable_genes. But I don't know how to fix it. Looking forward your response!; Thank you !",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:296,Modifiability,variab,variable,296,"I am following workflow of '_Best-practices in single-cell RNA-seq: a tutorial_' to analyze my single-cell sequencing data sets.; I have calculated the size factor using the scran package and did not perform the batch correction step as I have only one sample. Then, I intended to extract highly variable genes by using the function sc.pp.highly_variable_genes. Unfortunately, I got an error:. > LinAlgError: Last 2 dimensions of the array must be square. <details><summary>Traceback</summary>. ```pytb; LinAlgError Traceback (most recent call last); in ; ----> 1 sc.pp.highly_variable_genes(adata). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 94 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X; 95; ---> 96 mean, var = materialize_as_ndarray(_get_mean_var(X)); 97 # now actually compute the dispersion; 98 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:200,Performance,perform,perform,200,"I am following workflow of '_Best-practices in single-cell RNA-seq: a tutorial_' to analyze my single-cell sequencing data sets.; I have calculated the size factor using the scran package and did not perform the batch correction step as I have only one sample. Then, I intended to extract highly variable genes by using the function sc.pp.highly_variable_genes. Unfortunately, I got an error:. > LinAlgError: Last 2 dimensions of the array must be square. <details><summary>Traceback</summary>. ```pytb; LinAlgError Traceback (most recent call last); in ; ----> 1 sc.pp.highly_variable_genes(adata). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 94 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X; 95; ---> 96 mean, var = materialize_as_ndarray(_get_mean_var(X)); 97 # now actually compute the dispersion; 98 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:2595,Performance,perform,performing,2595,"canpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arrays):; ```. </details>. Versions of my modules:; scanpy==1.3.7 anndata==0.6.17 numpy==1.15.4 scipy==1.2.0 pandas==0.24.0 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. I have downgraded pandas to 0.23.4, however, it not works. But I figured out where the problem lies in. ```py; adata.X /= adata.obs['size_factors'].values[:,None]; ```. This step transform the adata.X to a structure of matrix.; Before the adata.X is. ```; <6242x15065 sparse matrix of type '<class 'numpy.float32'>'; with 19234986 stored elements in Compressed Sparse Row format>; ```. But after performing this step, the adata.X is; This is my adata.X looks like right now:. ```py; matrix([[0. , 0. , 0. , ..., 0. , 0. , 0. ],; [0. , 0. , 1.203, ..., 0. , 0. , 0. ],; [0. , 1.096, 0. , ..., 0. , 0. , 0. ],; ...,; [0. , 0. , 2.042, ..., 0. , 0. , 0. ],; [0. , 0. , 0. , ..., 0.926, 0. , 0. ],; [0. , 0. , 2.951, ..., 0. , 0. , 0. ]]),; ```. And this format of adata.X caused error of sc.pp.highly_variable_genes. But I don't know how to fix it. Looking forward your response!; Thank you !",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/456:2129,Usability,learn,learn,2129,"canpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arrays):; ```. </details>. Versions of my modules:; scanpy==1.3.7 anndata==0.6.17 numpy==1.15.4 scipy==1.2.0 pandas==0.24.0 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. I have downgraded pandas to 0.23.4, however, it not works. But I figured out where the problem lies in. ```py; adata.X /= adata.obs['size_factors'].values[:,None]; ```. This step transform the adata.X to a structure of matrix.; Before the adata.X is. ```; <6242x15065 sparse matrix of type '<class 'numpy.float32'>'; with 19234986 stored elements in Compressed Sparse Row format>; ```. But after performing this step, the adata.X is; This is my adata.X looks like right now:. ```py; matrix([[0. , 0. , 0. , ..., 0. , 0. , 0. ],; [0. , 0. , 1.203, ..., 0. , 0. , 0. ],; [0. , 1.096, 0. , ..., 0. , 0. , 0. ],; ...,; [0. , 0. , 2.042, ..., 0. , 0. , 0. ],; [0. , 0. , 0. , ..., 0.926, 0. , 0. ],; [0. , 0. , 2.951, ..., 0. , 0. , 0. ]]),; ```. And this format of adata.X caused error of sc.pp.highly_variable_genes. But I don't know how to fix it. Looking forward your response!; Thank you !",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456
https://github.com/scverse/scanpy/issues/460:863,Safety,detect,detected,863,"Dear all,. I am writing to ask you some other functionalities.; I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function?. 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods?. 3. I think that two parameters in the `rank_genes_groups` function should be added.; - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene.; - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance.; Best,; Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:610,Testability,test,test,610,"Dear all,. I am writing to ask you some other functionalities.; I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function?. 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods?. 3. I think that two parameters in the `rank_genes_groups` function should be added.; - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene.; - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance.; Best,; Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:834,Testability,test,test,834,"Dear all,. I am writing to ask you some other functionalities.; I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function?. 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods?. 3. I think that two parameters in the `rank_genes_groups` function should be added.; - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene.; - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance.; Best,; Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/pull/462:16,Deployability,update,update,16,"This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`; * I've removed `numba` from functions currently only used for testing; * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:271,Energy Efficiency,reduce,reduces,271,"This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`; * I've removed `numba` from functions currently only used for testing; * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:619,Performance,cache,cached,619,"This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`; * I've removed `numba` from functions currently only used for testing; * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:75,Testability,test,test,75,"This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`; * I've removed `numba` from functions currently only used for testing; * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:248,Testability,test,testing,248,"This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`; * I've removed `numba` from functions currently only used for testing; * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:279,Testability,test,test,279,"This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`; * I've removed `numba` from functions currently only used for testing; * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:412,Testability,Test,Test,412,"This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`; * I've removed `numba` from functions currently only used for testing; * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/issues/464:153,Integrability,wrap,wrap,153,We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:202,Integrability,wrap,wrap,202,We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/pull/466:7,Testability,test,test,7,Adding test for pca.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/466
https://github.com/scverse/scanpy/pull/467:234,Availability,mainten,maintenance,234,"Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:158,Integrability,wrap,wrapper,158,"Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:219,Testability,test,tested,219,"Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:437,Testability,test,tests,437,"Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/issues/468:518,Availability,failure,failure,518,"Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:; 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py; #utils.py; def annotate_doc_types(mod: ModuleType, root: str):; for c_or_f in descend_classes_and_funcs(mod, root):; print(c_or_f) #added line to track descend_classes_and_funcs() function--TR; c_or_f.getdoc = partial(getdoc, c_or_f); ```. 2. open ipython. ```py; import scanpy as sc; # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib; importlib.reload(sc); # endless loop of function names from the descend_classes_and_funcs() function; # due to recursive yield statement; ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py; #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):; for obj in vars(mod).values():; if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):; continue; if isinstance(obj, Callable):; yield obj; if isinstance(obj, type):; yield from (m for m in vars(obj).values() if isinstance(m, Callable)); elif isinstance(obj, ModuleType):; yield from descend_classes_and_funcs(obj, root); ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py; import sys; sys.modules.pop('scanpy'); ```. and then import scanpy from scr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:171,Modifiability,variab,variables,171,"Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:; 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py; #utils.py; def annotate_doc_types(mod: ModuleType, root: str):; for c_or_f in descend_classes_and_funcs(mod, root):; print(c_or_f) #added line to track descend_classes_and_funcs() function--TR; c_or_f.getdoc = partial(getdoc, c_or_f); ```. 2. open ipython. ```py; import scanpy as sc; # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib; importlib.reload(sc); # endless loop of function names from the descend_classes_and_funcs() function; # due to recursive yield statement; ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py; #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):; for obj in vars(mod).values():; if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):; continue; if isinstance(obj, Callable):; yield obj; if isinstance(obj, type):; yield from (m for m in vars(obj).values() if isinstance(m, Callable)); elif isinstance(obj, ModuleType):; yield from descend_classes_and_funcs(obj, root); ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py; import sys; sys.modules.pop('scanpy'); ```. and then import scanpy from scr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:2079,Modifiability,enhance,enhance,2079,"he environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:; 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py; #utils.py; def annotate_doc_types(mod: ModuleType, root: str):; for c_or_f in descend_classes_and_funcs(mod, root):; print(c_or_f) #added line to track descend_classes_and_funcs() function--TR; c_or_f.getdoc = partial(getdoc, c_or_f); ```. 2. open ipython. ```py; import scanpy as sc; # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib; importlib.reload(sc); # endless loop of function names from the descend_classes_and_funcs() function; # due to recursive yield statement; ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py; #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):; for obj in vars(mod).values():; if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):; continue; if isinstance(obj, Callable):; yield obj; if isinstance(obj, type):; yield from (m for m in vars(obj).values() if isinstance(m, Callable)); elif isinstance(obj, ModuleType):; yield from descend_classes_and_funcs(obj, root); ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py; import sys; sys.modules.pop('scanpy'); ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package?. Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:194,Testability,test,testing,194,"Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:; 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py; #utils.py; def annotate_doc_types(mod: ModuleType, root: str):; for c_or_f in descend_classes_and_funcs(mod, root):; print(c_or_f) #added line to track descend_classes_and_funcs() function--TR; c_or_f.getdoc = partial(getdoc, c_or_f); ```. 2. open ipython. ```py; import scanpy as sc; # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib; importlib.reload(sc); # endless loop of function names from the descend_classes_and_funcs() function; # due to recursive yield statement; ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py; #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):; for obj in vars(mod).values():; if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):; continue; if isinstance(obj, Callable):; yield obj; if isinstance(obj, type):; yield from (m for m in vars(obj).values() if isinstance(m, Callable)); elif isinstance(obj, ModuleType):; yield from descend_classes_and_funcs(obj, root); ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py; import sys; sys.modules.pop('scanpy'); ```. and then import scanpy from scr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:319,Testability,test,test,319,"Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:; 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py; #utils.py; def annotate_doc_types(mod: ModuleType, root: str):; for c_or_f in descend_classes_and_funcs(mod, root):; print(c_or_f) #added line to track descend_classes_and_funcs() function--TR; c_or_f.getdoc = partial(getdoc, c_or_f); ```. 2. open ipython. ```py; import scanpy as sc; # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib; importlib.reload(sc); # endless loop of function names from the descend_classes_and_funcs() function; # due to recursive yield statement; ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py; #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):; for obj in vars(mod).values():; if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):; continue; if isinstance(obj, Callable):; yield obj; if isinstance(obj, type):; yield from (m for m in vars(obj).values() if isinstance(m, Callable)); elif isinstance(obj, ModuleType):; yield from descend_classes_and_funcs(obj, root); ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py; import sys; sys.modules.pop('scanpy'); ```. and then import scanpy from scr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:412,Testability,test,test,412,"Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:; 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py; #utils.py; def annotate_doc_types(mod: ModuleType, root: str):; for c_or_f in descend_classes_and_funcs(mod, root):; print(c_or_f) #added line to track descend_classes_and_funcs() function--TR; c_or_f.getdoc = partial(getdoc, c_or_f); ```. 2. open ipython. ```py; import scanpy as sc; # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib; importlib.reload(sc); # endless loop of function names from the descend_classes_and_funcs() function; # due to recursive yield statement; ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py; #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):; for obj in vars(mod).values():; if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):; continue; if isinstance(obj, Callable):; yield obj; if isinstance(obj, type):; yield from (m for m in vars(obj).values() if isinstance(m, Callable)); elif isinstance(obj, ModuleType):; yield from descend_classes_and_funcs(obj, root); ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py; import sys; sys.modules.pop('scanpy'); ```. and then import scanpy from scr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/469:93,Safety,avoid,avoid,93,"Using `method=wilcoxon` in the `rank_genes_groups` function chunks the genes into groups (to avoid using too much memory). The scores for the genes on the endpoints of these chunks are never computed, however. This can easily be observed by looking at a dataset containing 5000 genes (so that the chunk size is 2000) and setting `rankby_abs=True`. The genes with indices 2000, 4000, 6000, .... will all be selected as marker genes with p-values of 0. This is due to the fact that the rank-sums for these genes are left at 0 (they are never computed), which is then judged to be EXTREMELY significant. . I will submit a PR with a fix for this in a minute :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/469
https://github.com/scverse/scanpy/issues/470:50,Testability,log,logreg,50,The `rankby_abs` parameter is ignored for `method=logreg` in the `rank_genes_groups` function.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/470
https://github.com/scverse/scanpy/pull/471:71,Availability,error,error,71,"This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call ; ``` ; np.where(adata.obs[key].cat.categories.values == name)[0][0]; ```; which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at ; ``` ; np.where(adata.obs[key].cat.categories.values == float(name))[0][0]; ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:884,Availability,error,error,884,"This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call ; ``` ; np.where(adata.obs[key].cat.categories.values == name)[0][0]; ```; which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at ; ``` ; np.where(adata.obs[key].cat.categories.values == float(name))[0][0]; ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1226,Availability,error,error,1226,"This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call ; ``` ; np.where(adata.obs[key].cat.categories.values == name)[0][0]; ```; which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at ; ``` ; np.where(adata.obs[key].cat.categories.values == float(name))[0][0]; ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:246,Deployability,update,updates,246,"This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call ; ``` ; np.where(adata.obs[key].cat.categories.values == name)[0][0]; ```; which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at ; ``` ; np.where(adata.obs[key].cat.categories.values == float(name))[0][0]; ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:688,Modifiability,variab,variable,688,"This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call ; ``` ; np.where(adata.obs[key].cat.categories.values == name)[0][0]; ```; which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at ; ``` ; np.where(adata.obs[key].cat.categories.values == float(name))[0][0]; ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:132,Testability,log,logreg,132,"This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call ; ``` ; np.where(adata.obs[key].cat.categories.values == name)[0][0]; ```; which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at ; ``` ; np.where(adata.obs[key].cat.categories.values == float(name))[0][0]; ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/issues/472:273,Performance,perform,performed,273,"When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace?; Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:63,Safety,avoid,avoid,63,"When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace?; Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/473:179,Deployability,Continuous,Continuous,179,"Hi,. Even when we set `sc.settings.set_figure_params(transparent=False)` the scatter plot for **CATEGORYCAL** data is transparented.; (ex. `sc.pl.umap(adata, color='louvain')` ); Continuous data goes well, so I think it is a bug. When you switch Jupyterlab's theme into dark, it will be easy to check. Best,; Yoshiaki",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/pull/474:39,Availability,down,downsampling,39,"Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:246,Availability,down,down,246,"Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:0,Deployability,Update,Update,0,"Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:557,Deployability,integrat,integration,557,"Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:557,Integrability,integrat,integration,557,"Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:254,Testability,test,test,254,"Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:673,Testability,test,test,673,"Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/477:104,Deployability,continuous,continuous,104,"Documenting `color_map` argument for scatter plots. This should reduce confusion about how to provide a continuous palette (#476). I'd also be up for having the arguments have names like `cont_palette` and `cat_palette`, which could be more clear.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:64,Energy Efficiency,reduce,reduce,64,"Documenting `color_map` argument for scatter plots. This should reduce confusion about how to provide a continuous palette (#476). I'd also be up for having the arguments have names like `cont_palette` and `cat_palette`, which could be more clear.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:241,Usability,clear,clear,241,"Documenting `color_map` argument for scatter plots. This should reduce confusion about how to provide a continuous palette (#476). I'd also be up for having the arguments have names like `cont_palette` and `cat_palette`, which could be more clear.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/issues/478:474,Modifiability,variab,variable,474,"I'm trying to use an array for the size argument to my umap/scatterplot with the following code; ```; import scanpy.api as sc; import numpy as np; sc.settings.figdir = ""testdir""; sc.settings.file_format_figs = ""png""; sc.logging.print_versions(); ```; With these libraries; `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size.; ```; somedata = sc.datasets.paul15(); sc.pp.pca(somedata); sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); sc.tl.leiden(somedata, resolution=0.5, random_state=42); z = np.abs(somedata.obsm['X_pca'][:,0])**1; sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); ```; I get the following two figure as output; ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png); ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:169,Testability,test,testdir,169,"I'm trying to use an array for the size argument to my umap/scatterplot with the following code; ```; import scanpy.api as sc; import numpy as np; sc.settings.figdir = ""testdir""; sc.settings.file_format_figs = ""png""; sc.logging.print_versions(); ```; With these libraries; `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size.; ```; somedata = sc.datasets.paul15(); sc.pp.pca(somedata); sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); sc.tl.leiden(somedata, resolution=0.5, random_state=42); z = np.abs(somedata.obsm['X_pca'][:,0])**1; sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); ```; I get the following two figure as output; ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png); ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:220,Testability,log,logging,220,"I'm trying to use an array for the size argument to my umap/scatterplot with the following code; ```; import scanpy.api as sc; import numpy as np; sc.settings.figdir = ""testdir""; sc.settings.file_format_figs = ""png""; sc.logging.print_versions(); ```; With these libraries; `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size.; ```; somedata = sc.datasets.paul15(); sc.pp.pca(somedata); sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); sc.tl.leiden(somedata, resolution=0.5, random_state=42); z = np.abs(somedata.obsm['X_pca'][:,0])**1; sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); ```; I get the following two figure as output; ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png); ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:353,Usability,learn,learn,353,"I'm trying to use an array for the size argument to my umap/scatterplot with the following code; ```; import scanpy.api as sc; import numpy as np; sc.settings.figdir = ""testdir""; sc.settings.file_format_figs = ""png""; sc.logging.print_versions(); ```; With these libraries; `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size.; ```; somedata = sc.datasets.paul15(); sc.pp.pca(somedata); sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); sc.tl.leiden(somedata, resolution=0.5, random_state=42); z = np.abs(somedata.obsm['X_pca'][:,0])**1; sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); ```; I get the following two figure as output; ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png); ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/482:61,Availability,error,error,61,"Tried to install via `$ pip3 install -e .` but returned this error:; ```; Obtaining file://path/to/scanpy_1.4/scanpy; Complete output from command python setup.py egg_info:; /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""path/to/scanpy/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>; check_versions(); File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions; .format(__version__, anndata.__version__)); NameError: name '__version__' is not defined. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/; ```; The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:994,Availability,error,error,994,"Tried to install via `$ pip3 install -e .` but returned this error:; ```; Obtaining file://path/to/scanpy_1.4/scanpy; Complete output from command python setup.py egg_info:; /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""path/to/scanpy/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>; check_versions(); File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions; .format(__version__, anndata.__version__)); NameError: name '__version__' is not defined. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/; ```; The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:9,Deployability,install,install,9,"Tried to install via `$ pip3 install -e .` but returned this error:; ```; Obtaining file://path/to/scanpy_1.4/scanpy; Complete output from command python setup.py egg_info:; /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""path/to/scanpy/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>; check_versions(); File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions; .format(__version__, anndata.__version__)); NameError: name '__version__' is not defined. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/; ```; The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:29,Deployability,install,install,29,"Tried to install via `$ pip3 install -e .` but returned this error:; ```; Obtaining file://path/to/scanpy_1.4/scanpy; Complete output from command python setup.py egg_info:; /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""path/to/scanpy/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>; check_versions(); File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions; .format(__version__, anndata.__version__)); NameError: name '__version__' is not defined. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/; ```; The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:1036,Modifiability,variab,variable,1036,"Tried to install via `$ pip3 install -e .` but returned this error:; ```; Obtaining file://path/to/scanpy_1.4/scanpy; Complete output from command python setup.py egg_info:; /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""path/to/scanpy/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>; check_versions(); File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions; .format(__version__, anndata.__version__)); NameError: name '__version__' is not defined. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/; ```; The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/483:1674,Modifiability,variab,variable,1674,"I noticed that `sc.pl.paga` function has this piece of code. ```py; # compute positions; if pos is None:; adj_tree = None; if layout in {'rt', 'rt_circular', 'eq_tree'}:; adj_tree = adata.uns['paga']['connectivities_tree']; pos = _compute_pos(; adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root); ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb; >>> sc.pl.paga(data, color='leiden'); ---------------------------------------------------------------------------; UnboundLocalError Traceback (most recent call last); <ipython-input-138-ed5614508f4e> in <module>(); ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax); 443 adj_tree = adata.uns['paga']['connectivities_tree']; 444 pos = _compute_pos(; --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root); 446 ; 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment; ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/pull/486:93,Security,expose,exposed,93,"This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:104,Testability,test,tests,104,"This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:124,Testability,test,tests,124,"This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:246,Testability,test,tests,246,"This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:307,Testability,test,test,307,"This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:371,Testability,test,test,371,"This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:276,Usability,simpl,simple,276,"This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/issues/487:47,Availability,error,error,47,"Running `sc.pl.paga(adata)` in v1.4 returns an error:; ```; Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>; sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga; adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment; ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:; ```; sc.pl.paga(adata, layout='rt'); sc.pl.paga(adata, layout='rt_circular'); sc.pl.paga(adata, layout='eq_tree'); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:410,Modifiability,variab,variable,410,"Running `sc.pl.paga(adata)` in v1.4 returns an error:; ```; Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>; sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga; adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment; ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:; ```; sc.pl.paga(adata, layout='rt'); sc.pl.paga(adata, layout='rt_circular'); sc.pl.paga(adata, layout='eq_tree'); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/489:65,Security,access,accession,65,"I've written up a little script for making an `AnnData` given an accession for the EBI single cell expression atlas ([gist](https://gist.github.com/ivirshup/e7f0c435474d9b06b622c63d4221afe6)), and was wondering if it'd be useful to add under `sc.datasets`. It could be nice to make it easier to try out methods/ have examples which weren't for pbmcs.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/489
https://github.com/scverse/scanpy/pull/493:26,Integrability,wrap,wrapper,26,@falexwolf ; I'm adding a wrapper for Palantir by [Setty et al. (2018)](https://doi.org/10.1101/385328); Please let me know if you have any comments,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493
https://github.com/scverse/scanpy/issues/495:252,Performance,load,loaded,252,"How were the bulk labels generated then assigned to cells in the pbmc68k dataset? I'm trying to do the same on my data.; Ideally I would like to use my own list to label cells. For example a cell has Gene X and Gene Y, then the 'bulk_label' in .obs is loaded with a string 'Cell Z'. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/495
https://github.com/scverse/scanpy/issues/496:67,Availability,error,error,67,"@flying-sheep: After the recent changes I am getting the following error:. ```bash; /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v); 36 def _settings_verbosity_greater_or_equal_than(v):; 37 if isinstance(settings.verbosity, str):; ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]; 39 else:; 40 settings_v = settings.verbosity; KeyError: 'warning'; ```. The problem is solved by setting the verbosity level. E.g. ```; sc.settings.verbosity = 3; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/496:104,Testability,log,logging,104,"@flying-sheep: After the recent changes I am getting the following error:. ```bash; /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v); 36 def _settings_verbosity_greater_or_equal_than(v):; 37 if isinstance(settings.verbosity, str):; ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]; 39 else:; 40 settings_v = settings.verbosity; KeyError: 'warning'; ```. The problem is solved by setting the verbosity level. E.g. ```; sc.settings.verbosity = 3; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/497:251,Availability,avail,available,251,"Hi,; More of a request than an issue. I am trying to replicate FindVariableFeatures with option selection.method = ""vst"" in seurat by using highly_variable_genes function in scanpy,i went through the documentation but could not find this option,is it available and am i missing something or is it not implemented yet. It would be nice to have this option. Thank you; Sasi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/497
https://github.com/scverse/scanpy/issues/499:398,Availability,error,error,398,"Dear,; When I Calculate qc metrics for visualization according to the example in https://scanpy.readthedocs.io/en/latest/api/scanpy.pp.calculate_qc_metrics.html#scanpy.pp.calculate_qc_metrics:. ```py; >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.calculate_qc_metrics(adata, inplace=True); >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""); ```. The following error occurred:; AttributeError: 'str' object has no attribute 'get'; It seems that sns.jointplot are not compatible well with adata.obs, anybody who can help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/499
https://github.com/scverse/scanpy/issues/501:16,Deployability,update,updated,16,"Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2); -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)); ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:568,Safety,detect,detected,568,"Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2); -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)); ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:674,Usability,feedback,feedback,674,"Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2); -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)); ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/502:20,Availability,error,error,20,"I get the following error when I tun dotplot:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-54-afab88c299fa> in <module>(); ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds); 409 ; 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,; --> 411 groupby=groupby, key=key, show=show, save=save, **kwds); 412 ; 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds); 291 ; 292 # sum(list, []) is used to flatten the gene list; --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []); 294 ; 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0); 291 ; 292 # sum(list, []) is used to flatten the gene list; --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []); 294 ; 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2; ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/pull/503:25,Integrability,wrap,wrapper,25,@falexwolf; I'm adding a wrapper for Harmony by [Setty et al. (2018)](https://doi.org/10.1101/471078); Please let me know if you have any comments,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/503
https://github.com/scverse/scanpy/issues/504:297,Availability,error,error,297,"I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1; I'm trying to do a pca on a annData object; `sc.tl.pca(adata, svd_solver='arpack')`; and get the following error even after restarting the jupyter notebook:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-26-eb775d53dbfd> in <module>; ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 504 ; 505 if data_is_AnnData:; --> 506 adata.obsm['X_pca'] = X_pca; 507 if use_highly_variable:; 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:87,Usability,learn,learn,87,"I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1; I'm trying to do a pca on a annData object; `sc.tl.pca(adata, svd_solver='arpack')`; and get the following error even after restarting the jupyter notebook:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-26-eb775d53dbfd> in <module>; ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 504 ; 505 if data_is_AnnData:; --> 506 adata.obsm['X_pca'] = X_pca; 507 if use_highly_variable:; 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/506:274,Availability,error,error,274,"Hi Guys, . this it perhaps rather a question than issue, ; Is there a way to export raw data in csv format? If I do this ; `adata.write_csvs(""filename"", skip_data=False)` . it works perfectly fine . but with ; `adata.raw.write_csvs(""filename"", skip_data=False)`. I get this error; `AttributeError: 'Raw' object has no attribute 'write_csvs'`. Thanks,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/506
https://github.com/scverse/scanpy/issues/507:184,Availability,error,error,184,"Hey,. I just stumbled into an issue with scanpy.; If I have an gene or cell annotation that is not 1-dimensional (I store ERCCs as a M x 92 float matrix in col_attrs), it generates an error:; ""Exception: Data must be 1-dimensional"". However, it is allowed by the loompy nomenclature to have multiple dimensions. ; And actually loompy package accepts it. Could it be corrected, so it can be accepted as a valid Loom file/annotation?. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/507
https://github.com/scverse/scanpy/pull/508:16,Testability,test,test,16,"Using `is` in a test, tests for identity and not equality, this leads to; inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:22,Testability,test,tests,22,"Using `is` in a test, tests for identity and not equality, this leads to; inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:103,Testability,test,testing,103,"Using `is` in a test, tests for identity and not equality, this leads to; inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/issues/510:1383,Availability,down,downstream,1383,", which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cut",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:171,Deployability,integrat,integrated,171,"I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:310,Deployability,integrat,integration,310,"I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:323,Deployability,Install,Installing,323,"I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1267,Deployability,integrat,integrated,1267,"o do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram inte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1366,Deployability,Integrat,Integration,1366,", which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cut",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1723,Deployability,integrat,integrate,1723,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2267,Deployability,integrat,integration,2267,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1114,Energy Efficiency,reduce,reduce,1114,"PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogen",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:171,Integrability,integrat,integrated,171,"I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:310,Integrability,integrat,integration,310,"I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1267,Integrability,integrat,integrated,1267,"o do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram inte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1366,Integrability,Integrat,Integration,1366,", which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cut",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1723,Integrability,integrat,integrate,1723,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2267,Integrability,integrat,integration,2267,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1951,Security,access,access,1951,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:236,Usability,simpl,simple,236,"I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:472,Usability,learn,learned,472,"I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:553,Usability,simpl,simple,553,"I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:564,Usability,intuit,intuitive,564,"I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/511:415,Availability,error,error,415,"Hi,; I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website).; I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells); I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling); Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/pull/512:508,Testability,log,logarithmized,508,"Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:172,Usability,simpl,simply,172,"Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/issues/514:85,Availability,error,error,85,"Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python; sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,); ```. gives the error. ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-a9dd619ada2e> in <module>; 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15); ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15); 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 params = locals(); 83 kwargs = params.pop('kwargs'); ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 215 batch_list = adata.obs[batch_key].values; 216 #call BBKNN proper; --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs); 218 #optionally save knn_indices; 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:398,Availability,error,error,398,"Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python; sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,); ```. gives the error. ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-a9dd619ada2e> in <module>; 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15); ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15); 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 params = locals(); 83 kwargs = params.pop('kwargs'); ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 215 batch_list = adata.obs[batch_key].values; 216 #call BBKNN proper; --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs); 218 #optionally save knn_indices; 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:200,Integrability,wrap,wrapper,200,"Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python; sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,); ```. gives the error. ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-a9dd619ada2e> in <module>; 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15); ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15); 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 params = locals(); 83 kwargs = params.pop('kwargs'); ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 215 batch_list = adata.obs[batch_key].values; 216 #call BBKNN proper; --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs); 218 #optionally save knn_indices; 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/515:28,Availability,error,error,28,"I encountered the following error when trying to save data to h5ad file:. ```; ... storing 'run' as categorical; ... storing 'batch' as categorical; ... storing 'dis_stat' as categorical; ... storing 'org_day' as categorical; ... storing 'louvain' as categorical; ... storing 'louvain_1' as categorical; ... storing 'louvain_2' as categorical; ... storing 'split_cell_type' as categorical; ... storing 'split_major_cell_type' as categorical; ... storing 'phase' as categorical; ... storing 'split_major_cell_type2' as categorical; ... storing 'feature_types-190111-3' as categorical; ... storing 'feature_types-190111-4' as categorical; ... storing 'feature_types-190111-5' as categorical; ... storing 'feature_types-190111-6' as categorical; ... storing 'feature_types-190111-7' as categorical; ... storing 'feature_types-190111-8' as categorical; ... storing 'feature_types-180418-4' as categorical; ... storing 'feature_types-180418-5' as categorical; ... storing 'feature_types-180418-6' as categorical; ... storing 'feature_types-180418-7' as categorical; ... storing 'feature_types-180905-3' as categorical; ... storing 'feature_types-180905-4' as categorical; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-72-19c7ca58c3a2> in <module>; ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 1951 ; 1952 _write_h5ad(filename, self, compression=compression,; -> 1953 compression_opts=compression_opts, force_dense=force_dense); 1954 ; 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs); 217 if not dirname.is_dir():; 218 dirname.mkdir(parents=True, exist_ok=True); --> 219 d = adata._to_dict_fixed_wid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:5010,Availability,error,error,5010,"7 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._reduce(name, skipna=skipna, **kwds); 3616 elif is_datetime64_dtype(delegate):; 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs); 2179 msg = 'Categorical cannot perform the operation {op}'; 2180 raise TypeError(msg.format(op=name)); -> 2181 return func(**kwargs); 2182 ; 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs); 2222 max : the maximum of this `Categorical`; 2223 """"""; -> 2224 self.check_for_ordered('max'); 2225 if numeric_only:; 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op); 1517 raise TypeError(""Categorical is not ordered for operation {op}\n""; 1518 ""you can use .as_ordered() to change the ""; -> 1519 ""Categorical to an ordered one\n"".format(op=op)); 1520 ; 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one; ```. I was confused for two reasons:; 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;; 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3351,Integrability,interface,interface,3351,"records_fixed_width(self._obs); 2186 var_rec, uns_var = df_to_records_fixed_width(self._var); 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df); 212 names.append(k); 213 if is_string_dtype(df[k]):; --> 214 max_len_index = df[k].map(len).max(); 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 numeric_only=numeric_only); 10957 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._reduce(name, skipna=skipna, **kwds); 3616 elif is_datetime64_dtype(delegate):; 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs); 2179 msg = 'Categorical cannot perform the operation {op}'; 2180 raise TypeError(msg.format(op=name)); -> 2181 return func(**kwargs); 2182 ; 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs); 2222 max : the maximum of this `Categorical`; 2223 """"""; -> 2224 self.check_for_ordered('max'); 2225 if numeric_only:; 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorica",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2434,Modifiability,layers,layers,2434,"conda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 1951 ; 1952 _write_h5ad(filename, self, compression=compression,; -> 1953 compression_opts=compression_opts, force_dense=force_dense); 1954 ; 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs); 217 if not dirname.is_dir():; 218 dirname.mkdir(parents=True, exist_ok=True); --> 219 d = adata._to_dict_fixed_width_arrays(); 220 # we're writing to a different location than the backing file; 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self); 2183 """"""; 2184 self.strings_to_categoricals(); -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs); 2186 var_rec, uns_var = df_to_records_fixed_width(self._var); 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df); 212 names.append(k); 213 if is_string_dtype(df[k]):; --> 214 max_len_index = df[k].map(len).max(); 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 numeric_only=numeric_only); 10957 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2448,Modifiability,layers,layers,2448,"n3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 1951 ; 1952 _write_h5ad(filename, self, compression=compression,; -> 1953 compression_opts=compression_opts, force_dense=force_dense); 1954 ; 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs); 217 if not dirname.is_dir():; 218 dirname.mkdir(parents=True, exist_ok=True); --> 219 d = adata._to_dict_fixed_width_arrays(); 220 # we're writing to a different location than the backing file; 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self); 2183 """"""; 2184 self.strings_to_categoricals(); -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs); 2186 var_rec, uns_var = df_to_records_fixed_width(self._var); 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df); 212 names.append(k); 213 if is_string_dtype(df[k]):; --> 214 max_len_index = df[k].map(len).max(); 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 numeric_only=numeric_only); 10957 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._reduce(name, skipn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2089,Performance,load,load,2089,"gorical; ... storing 'feature_types-180905-3' as categorical; ... storing 'feature_types-180905-4' as categorical; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-72-19c7ca58c3a2> in <module>; ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 1951 ; 1952 _write_h5ad(filename, self, compression=compression,; -> 1953 compression_opts=compression_opts, force_dense=force_dense); 1954 ; 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs); 217 if not dirname.is_dir():; 218 dirname.mkdir(parents=True, exist_ok=True); --> 219 d = adata._to_dict_fixed_width_arrays(); 220 # we're writing to a different location than the backing file; 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self); 2183 """"""; 2184 self.strings_to_categoricals(); -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs); 2186 var_rec, uns_var = df_to_records_fixed_width(self._var); 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df); 212 names.append(k); 213 if is_string_dtype(df[k]):; --> 214 max_len_index = df[k].map(len).max(); 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3771,Performance,perform,perform,3771,")); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 numeric_only=numeric_only); 10957 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._reduce(name, skipna=skipna, **kwds); 3616 elif is_datetime64_dtype(delegate):; 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs); 2179 msg = 'Categorical cannot perform the operation {op}'; 2180 raise TypeError(msg.format(op=name)); -> 2181 return func(**kwargs); 2182 ; 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs); 2222 max : the maximum of this `Categorical`; 2223 """"""; -> 2224 self.check_for_ordered('max'); 2225 if numeric_only:; 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op); 1517 raise TypeError(""Categorical is not ordered for operation {op}\n""; 1518 ""you can use .as_ordered() to change the ""; -> 1519 ""Categorical to an ordered one\n"".format(op=op)); 1520 ; 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one; ```. I was confused for two reason",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/516:269,Performance,perform,performed,269,"Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed).; I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"".; Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516
https://github.com/scverse/scanpy/issues/517:739,Deployability,pipeline,pipelines,739,"Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts?. For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:802,Deployability,pipeline,pipelines,802,"Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts?. For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:268,Testability,log,log,268,"Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts?. For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:628,Testability,test,testing,628,"Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts?. For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:664,Testability,test,tests,664,"Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts?. For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:768,Testability,test,tests,768,"Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts?. For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:965,Testability,log,log,965,"Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts?. For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1133,Testability,log,log-transformed,1133,"Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts?. For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:472,Usability,intuit,intuitive,472,"Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts?. For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/518:73,Deployability,update,update,73,"After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:166,Deployability,update,updates,166,"After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:235,Deployability,install,installed,235,"After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:130,Integrability,depend,dependancies,130,"After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:148,Integrability,depend,dependent,148,"After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/pull/519:135,Testability,log,log-transformed,135,"This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:222,Testability,log,log-transomed,222,"This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:472,Testability,test,tests,472,"This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:481,Testability,log,log-space,481,"This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:603,Testability,test,test,603,"This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/issues/522:1068,Availability,avail,available,1068," a year ago for various reasons (see below). Meanwhile, the following two functions can maybe direcly imported from UMAP, if not, we could make a PR there or use pynndescent?; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1362,Deployability,install,installation,1362,"anpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen ver",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2076,Deployability,integrat,integrated,2076,"7251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2076,Integrability,integrat,integrated,2076,"7251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1429,Modifiability,rewrite,rewrite,1429,"7251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1315,Performance,perform,performance,1315,"anpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen ver",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1223,Usability,clear,clear,1223,"anpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen ver",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/pull/524:127,Deployability,update,updates,127,Allow greater customizability and visibility when using dotplot by allowing the user to set the smallest dot size. This change updates the dots along with the dot size legend.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/524
https://github.com/scverse/scanpy/issues/526:2507,Availability,down,downstream,2507,"cell cycle**; The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):; ```; adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]; sc.pp.regress_out(adata, ['cell_cycle_diff']); ```; Like that, the differences between dividing and non-dividing cells should be preserved. ; Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot (below) it seems that the cell-cycle labels correlate with the cell type (i.e. cancer cells and myeloid cells got the G1 label assigned more likely than T cells). . **What is 'best practice'?**; I quickly discussed this offline with @flying-sheep, and he encouraged me to create this issue. . * Is it just a problem with visualizing the first PC's and `regress_out` should be applied regardless; * Should `regress_out` be skipped and only applied in a more downstream step when focusing on a single cell type? ; * Are there any other situations where `regress_out` could do more harm than good? . **PCA plots before and after `regress_out`**; ![regress_out](https://user-images.githubusercontent.com/7051479/54083302-f088f500-4321-11e9-877a-1cbef6f4f489.png). **UMAP-plots**; The cell cycle label correlates with the cell type (other dataset, but to show what I mean): ; ![2019-03-10_11:20:31_384x234](https://user-images.githubusercontent.com/7051479/54083671-29779880-4327-11e9-94d6-9be34383b909.png); ![2019-03-10_11:25:30_428x231](https://user-images.githubusercontent.com/7051479/54083675-3a280e80-4327-11e9-954f-34ef1404961b.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:851,Energy Efficiency,reduce,reduced,851,"According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**; I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: ; * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA; * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**; The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):; ```; adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]; sc.pp.regress_out(adata, ['cell_cycle_diff']); ```; Like that, the differences between dividing and non-dividing cells should be preserved. ; Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no cle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:347,Performance,load,loaded,347,"According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**; I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: ; * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA; * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**; The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):; ```; adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]; sc.pp.regress_out(adata, ['cell_cycle_diff']); ```; Like that, the differences between dividing and non-dividing cells should be preserved. ; Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no cle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:165,Safety,detect,detected,165,"According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**; I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: ; * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA; * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**; The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):; ```; adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]; sc.pp.regress_out(adata, ['cell_cycle_diff']); ```; Like that, the differences between dividing and non-dividing cells should be preserved. ; Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no cle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:327,Safety,detect,detected,327,"According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**; I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: ; * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA; * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**; The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):; ```; adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]; sc.pp.regress_out(adata, ['cell_cycle_diff']); ```; Like that, the differences between dividing and non-dividing cells should be preserved. ; Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no cle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:817,Safety,detect,detected,817,"According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**; I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: ; * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA; * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**; The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):; ```; adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]; sc.pp.regress_out(adata, ['cell_cycle_diff']); ```; Like that, the differences between dividing and non-dividing cells should be preserved. ; Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no cle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:1045,Safety,detect,detected,1045,"C 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**; I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: ; * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA; * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**; The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):; ```; adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]; sc.pp.regress_out(adata, ['cell_cycle_diff']); ```; Like that, the differences between dividing and non-dividing cells should be preserved. ; Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Havin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:1998,Usability,clear,clear,1998," the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**; The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):; ```; adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]; sc.pp.regress_out(adata, ['cell_cycle_diff']); ```; Like that, the differences between dividing and non-dividing cells should be preserved. ; Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot (below) it seems that the cell-cycle labels correlate with the cell type (i.e. cancer cells and myeloid cells got the G1 label assigned more likely than T cells). . **What is 'best practice'?**; I quickly discussed this offline with @flying-sheep, and he encouraged me to create this issue. . * Is it just a problem with visualizing the first PC's and `regress_out` should be applied regardless; * Should `regress_out` be skipped and only applied in a more downstream step when focusing on a single cell type? ; * Are there any other situations where `regress_out` could do more harm than good? . **PCA plots before and after `regress_out`**; ![regress_out](https://user-images.githubusercontent.com/7051479/54083302-f088f500-4321-11e9-877a-1cbef6f4f489.png). **UMAP-plots**; The cell cycle label correlates with the cell type (other dataset, but to show what I mean): ; ![2019-03-10_11:20:31_384x234](https://user-images.githubusercontent.co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/pull/527:101,Availability,error,error,101,Looks like there was a typo at the bottom of `scanpy/preprocessing/_dca.py` that was causing a parse error. This should fix it.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/527
https://github.com/scverse/scanpy/pull/528:195,Deployability,integrat,integrated,195,"This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/pull/528:195,Integrability,integrat,integrated,195,"This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/pull/528:8,Modifiability,extend,extends,8,"This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/issues/530:10,Availability,error,error,10,"I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash; C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars; (n_active * m_active * (n_active + m_active + 1) / 12)); ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-160-dd19114ff660> in <module>; 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]; ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 296 ; 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(; --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)); 299 scores[np.isnan(scores)] = 0; 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error; ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1261,Availability,error,error,1261,"I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash; C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars; (n_active * m_active * (n_active + m_active + 1) / 12)); ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-160-dd19114ff660> in <module>; 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]; ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 296 ; 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(; --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)); 299 scores[np.isnan(scores)] = 0; 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error; ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1509,Availability,error,error,1509,"I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash; C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars; (n_active * m_active * (n_active + m_active + 1) / 12)); ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-160-dd19114ff660> in <module>; 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]; ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 296 ; 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(; --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)); 299 scores[np.isnan(scores)] = 0; 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error; ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:768,Testability,test,test,768,"I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash; C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars; (n_active * m_active * (n_active + m_active + 1) / 12)); ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-160-dd19114ff660> in <module>; 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]; ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 296 ; 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(; --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)); 299 scores[np.isnan(scores)] = 0; 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error; ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1568,Testability,test,test,1568,"I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash; C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars; (n_active * m_active * (n_active + m_active + 1) / 12)); ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-160-dd19114ff660> in <module>; 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]; ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 296 ; 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(; --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)); 299 scores[np.isnan(scores)] = 0; 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error; ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/534:298,Availability,error,error,298,"Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing?. ```python; user_input = input(""Enter the path of your file: ""); def convert_h5_to_adata(filename):; filename = str(filename); if os.access(filename, os.R_OK):; sc.read_10x_h5(filename); return; convert_h5_to_adata(user_input); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:318,Modifiability,Variab,Variable,318,"Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing?. ```python; user_input = input(""Enter the path of your file: ""); def convert_h5_to_adata(filename):; filename = str(filename); if os.access(filename, os.R_OK):; sc.read_10x_h5(filename); return; convert_h5_to_adata(user_input); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:664,Security,access,access,664,"Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing?. ```python; user_input = input(""Enter the path of your file: ""); def convert_h5_to_adata(filename):; filename = str(filename); if os.access(filename, os.R_OK):; sc.read_10x_h5(filename); return; convert_h5_to_adata(user_input); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/535:318,Integrability,wrap,wraps,318,"It'd be nice if some level of tab completion of argument names could be maintained for functions like `sc.pl.umap`. This came up recently in #455, and I find myself frequently misspelling (mostly mis-pluralizing) argument names like `color/ colors` and `gene_name/ gene_names`. I've given this a shot using `functools.wraps`, but no luck yet. Any ideas on if we could do this @flying-sheep?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535
https://github.com/scverse/scanpy/issues/536:62,Availability,avail,available,62,"I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:; ```python; import random; import scanpy as sc. adata = sc.datasets.blobs(); adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata); sc.pl.pca(adata, color='property', size=50); ```; While this should plot 10 cells it only shows one cell:; ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png); I can get the plot I want by filtering cells first:; ```python; sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50); ```; ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png); Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/537:101,Deployability,update,update,101,I see that some functions have been deprecated and replaced by new ones. What is the simplest way to update Scanpy using conda?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537
https://github.com/scverse/scanpy/issues/537:85,Usability,simpl,simplest,85,I see that some functions have been deprecated and replaced by new ones. What is the simplest way to update Scanpy using conda?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537
https://github.com/scverse/scanpy/issues/542:262,Energy Efficiency,power,powerusers,262,Many bio data analysis packages hosted on GitHub will have a Gitter.im chat room with a direct link in the readme. This is very helpful for newcomers with questions that may not qualify as an issue but can be quickly answered by the package contributors or just powerusers. I recommend scanpy start a gitter. It may help slow the posting of new issues and allow faster triaging and help.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542
https://github.com/scverse/scanpy/pull/543:223,Energy Efficiency,adapt,adapted,223,This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543
https://github.com/scverse/scanpy/pull/543:223,Modifiability,adapt,adapted,223,This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543
https://github.com/scverse/scanpy/issues/544:190,Availability,error,error,190,"Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last); <ipython-input-5-dfc1e4d9ed06> in <module>(); ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:; scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ?. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:196,Integrability,message,message,196,"Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last); <ipython-input-5-dfc1e4d9ed06> in <module>(); ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:; scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ?. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:574,Usability,learn,learn,574,"Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last); <ipython-input-5-dfc1e4d9ed06> in <module>(); ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:; scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ?. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/547:2698,Integrability,wrap,wrapper,2698,"url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds); 373 convert_float=convert_float,; 374 mangle_dupe_cols=mangle_dupe_cols,; --> 375 **kwds); 376 ; 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2826,Integrability,wrap,wrapper,2826,"url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds); 373 convert_float=convert_float,; 374 mangle_dupe_cols=mangle_dupe_cols,; --> 375 **kwds); 376 ; 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2952,Integrability,wrap,wrapper,2952,"ache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds); 373 convert_float=convert_float,; 374 mangle_dupe_cols=mangle_dupe_cols,; --> 375 **kwds); 376 ; 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds); 716 convert_float=convert_float,; 717 mangle_dupe_cols=mangle_dupe_cols,; --> 718 **kwds); 719 ; 720 @property. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:3080,Integrability,wrap,wrapper,3080,"ache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds); 373 convert_float=convert_float,; 374 mangle_dupe_cols=mangle_dupe_cols,; --> 375 **kwds); 376 ; 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds); 716 convert_float=convert_float,; 717 mangle_dupe_cols=mangle_dupe_cols,; --> 718 **kwds); 719 ; 720 @property. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:1270,Performance,cache,cache,1270,"ues, cast_type, column); 1807 values = astype_nansafe(values, cast_type,; -> 1808 copy=True, skipna=True); 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna); 701 # Explicit copy, or required since NumPy can't view from / to object.; --> 702 return arr.astype(dtype, copy=True); 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last); <ipython-input-3-bf986d1f9b8c> in <module>; 1 import scanpy as sc; ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(); 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'; 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url); 107 # filter out 4 genes as in Haghverdi et al. (2016); 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:1605,Performance,cache,cache,1605," ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last); <ipython-input-3-bf986d1f9b8c> in <module>; 1 import scanpy as sc; ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(); 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'; 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url); 107 # filter out 4 genes as in Haghverdi et al. (2016); 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:1782,Performance,cache,cache,1782," ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last); <ipython-input-3-bf986d1f9b8c> in <module>; 1 import scanpy as sc; ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(); 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'; 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url); 107 # filter out 4 genes as in Haghverdi et al. (2016); 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:1788,Performance,cache,cache,1788," ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last); <ipython-input-3-bf986d1f9b8c> in <module>; 1 import scanpy as sc; ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(); 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'; 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url); 107 # filter out 4 genes as in Haghverdi et al. (2016); 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2030,Performance,cache,cache,2030,"canpy/datasets/__init__.py in moignard15(); 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'; 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url); 107 # filter out 4 genes as in Haghverdi et al. (2016); 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; -",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:6003,Testability,log,logging,6003,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds); 601 **kwds); 602 ; --> 603 output[asheetname] = parser.read(nrows=nrows); 604 ; 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows); 1137 def read(self, nrows=None):; 1138 nrows = _validate_integer('nrows', nrows); -> 1139 ret = self._engine.read(nrows); 1140 ; 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows); 2419 columns, data = self._do_date_conversions(columns, data); 2420 ; -> 2421 data = self._convert_data(data); 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow); 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data); 2485 return self._convert_to_ndarrays(data, clean_na_values,; 2486 clean_na_fvalues, self.verbose,; -> 2487 clean_conv, clean_dtypes); 2488 ; 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes); 1703 # invalid input to is_bool_dtype; 1704 pass; -> 1705 cvals = self._cast_types(cvals, cast_type, c); 1706 ; 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column); 1809 except ValueError:; 1810 raise ValueError(""Unable to convert column %s to ""; -> 1811 ""type %s"" % (column, cast_type)); 1812 return values; 1813 . ValueError: Unable to convert column Cell to type float32; ```; ```python; sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:6106,Usability,learn,learn,6106,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds); 601 **kwds); 602 ; --> 603 output[asheetname] = parser.read(nrows=nrows); 604 ; 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows); 1137 def read(self, nrows=None):; 1138 nrows = _validate_integer('nrows', nrows); -> 1139 ret = self._engine.read(nrows); 1140 ; 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows); 2419 columns, data = self._do_date_conversions(columns, data); 2420 ; -> 2421 data = self._convert_data(data); 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow); 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data); 2485 return self._convert_to_ndarrays(data, clean_na_values,; 2486 clean_na_fvalues, self.verbose,; -> 2487 clean_conv, clean_dtypes); 2488 ; 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes); 1703 # invalid input to is_bool_dtype; 1704 pass; -> 1705 cvals = self._cast_types(cvals, cast_type, c); 1706 ; 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column); 1809 except ValueError:; 1810 raise ValueError(""Unable to convert column %s to ""; -> 1811 ""type %s"" % (column, cast_type)); 1812 return values; 1813 . ValueError: Unable to convert column Cell to type float32; ```; ```python; sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/pull/549:422,Availability,avail,available,422,"Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:; - write a test; - finish documentation; - allow p-value thresholding when available; - allow using top X marker genes rather than all calculated markers; - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/pull/549:358,Testability,test,test,358,"Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:; - write a test; - finish documentation; - allow p-value thresholding when available; - allow using top X marker genes rather than all calculated markers; - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/pull/549:504,Testability,test,test,504,"Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:; - write a test; - finish documentation; - allow p-value thresholding when available; - allow using top X marker genes rather than all calculated markers; - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/pull/551:93,Usability,simpl,simplified,93,Addressing #435 . I've opened this with a not-pretty version. I think this function could be simplified a lot by not requiring it to work on arrays. I'm willing to leave it ugly for now. Left to resolve: return type of `downsample_counts`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/551
https://github.com/scverse/scanpy/pull/556:15,Availability,redundant,redundant,15,Every piece of redundant code we delete is one we don’t have to maintain.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/556
https://github.com/scverse/scanpy/pull/556:15,Safety,redund,redundant,15,Every piece of redundant code we delete is one we don’t have to maintain.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/556
https://github.com/scverse/scanpy/issues/558:33,Availability,down,downloader,33,"I'm adding that expression atlas downloader now (#489), and wondering where the files should go. `pbmc68k_reduced` and `toggleswitch` put the datasets relative to where scanpy is installed (via `__file__`). All other functions place the data relative to where the python process was started. While I like not storing the same files all over a filesystem, I'm not sure in the `scanpy` installation directory is the right place to be storing data. Thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:120,Deployability,toggle,toggleswitch,120,"I'm adding that expression atlas downloader now (#489), and wondering where the files should go. `pbmc68k_reduced` and `toggleswitch` put the datasets relative to where scanpy is installed (via `__file__`). All other functions place the data relative to where the python process was started. While I like not storing the same files all over a filesystem, I'm not sure in the `scanpy` installation directory is the right place to be storing data. Thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:179,Deployability,install,installed,179,"I'm adding that expression atlas downloader now (#489), and wondering where the files should go. `pbmc68k_reduced` and `toggleswitch` put the datasets relative to where scanpy is installed (via `__file__`). All other functions place the data relative to where the python process was started. While I like not storing the same files all over a filesystem, I'm not sure in the `scanpy` installation directory is the right place to be storing data. Thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:384,Deployability,install,installation,384,"I'm adding that expression atlas downloader now (#489), and wondering where the files should go. `pbmc68k_reduced` and `toggleswitch` put the datasets relative to where scanpy is installed (via `__file__`). All other functions place the data relative to where the python process was started. While I like not storing the same files all over a filesystem, I'm not sure in the `scanpy` installation directory is the right place to be storing data. Thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/559:410,Availability,error,error,410,"Hi @fidelram ,. When I try to use . ```; plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'); ```; I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```; plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'); ```. I get the following error. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-24-4ac38158d4d0> in <module>; ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds); 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks); 1684 ; -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds); 1686 ; 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),; 1804 RuntimeWarning, stacklevel=2); -> 1805 return func(ax, *args, **kwargs); 1806 ; 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs); 5762 kwargs.setdefault('snap', False); 5763 ; -> 5764 collection = mcoll.PolyCollection(verts, **kwargs); 5765 ; 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs); 931 %(Collection)s; 932 """"""; --> 933 Collection.__init__(self, **kwargs); 934 self.set_sizes(sizes); 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2261,Deployability,update,update,2261,"bel_namer, func.__name__),; 1804 RuntimeWarning, stacklevel=2); -> 1805 return func(ax, *args, **kwargs); 1806 ; 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs); 5762 kwargs.setdefault('snap', False); 5763 ; -> 5764 collection = mcoll.PolyCollection(verts, **kwargs); 5765 ; 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs); 931 %(Collection)s; 932 """"""; --> 933 Collection.__init__(self, **kwargs); 934 self.set_sizes(sizes); 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs); 164 ; 165 self._path_effects = None; --> 166 self.update(kwargs); 167 self._paths = None; 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v); 910 func = getattr(self, 'set_' + k, None); 911 if not callable(func):; --> 912 raise AttributeError('Unknown property %s' % k); 913 return func(v); 914 . AttributeError: Unknown property standard_scale; ```; Any idea of why I'm getting this? . Package info:. ```; scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Th",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2365,Deployability,update,update,2365," ; 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs); 5762 kwargs.setdefault('snap', False); 5763 ; -> 5764 collection = mcoll.PolyCollection(verts, **kwargs); 5765 ; 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs); 931 %(Collection)s; 932 """"""; --> 933 Collection.__init__(self, **kwargs); 934 self.set_sizes(sizes); 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs); 164 ; 165 self._path_effects = None; --> 166 self.update(kwargs); 167 self._paths = None; 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v); 910 func = getattr(self, 'set_' + k, None); 911 if not callable(func):; --> 912 raise AttributeError('Unknown property %s' % k); 913 return func(v); 914 . AttributeError: Unknown property standard_scale; ```; Any idea of why I'm getting this? . Package info:. ```; scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Thank you!. PS: this happens also when I just use the example data as in [here](https://scanpy-tutorials.readth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:800,Testability,log,log,800,"Hi @fidelram ,. When I try to use . ```; plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'); ```; I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```; plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'); ```. I get the following error. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-24-4ac38158d4d0> in <module>; ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds); 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks); 1684 ; -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds); 1686 ; 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),; 1804 RuntimeWarning, stacklevel=2); -> 1805 return func(ax, *args, **kwargs); 1806 ; 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs); 5762 kwargs.setdefault('snap', False); 5763 ; -> 5764 collection = mcoll.PolyCollection(verts, **kwargs); 5765 ; 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs); 931 %(Collection)s; 932 """"""; --> 933 Collection.__init__(self, **kwargs); 934 self.set_sizes(sizes); 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:3196,Usability,learn,learn,3196,"_,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs); 5762 kwargs.setdefault('snap', False); 5763 ; -> 5764 collection = mcoll.PolyCollection(verts, **kwargs); 5765 ; 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs); 931 %(Collection)s; 932 """"""; --> 933 Collection.__init__(self, **kwargs); 934 self.set_sizes(sizes); 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs); 164 ; 165 self._path_effects = None; --> 166 self.update(kwargs); 167 self._paths = None; 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v); 910 func = getattr(self, 'set_' + k, None); 911 if not callable(func):; --> 912 raise AttributeError('Unknown property %s' % k); 913 return func(v); 914 . AttributeError: Unknown property standard_scale; ```; Any idea of why I'm getting this? . Package info:. ```; scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Thank you!. PS: this happens also when I just use the example data as in [here](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/560:31,Availability,down,down,31,"Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-5-49e0357ed731> in <module>; ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds); 1644 var_names=var_names,; 1645 var_group_labels=var_group_labels,; -> 1646 var_group_positions=var_group_positions); 1647 ; 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions); 2332 """"""; 2333 ; -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby); 2335 ; 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby); 2406 ; 2407 if dendrogram_key not in adata.uns:; -> 2408 from ..tools._dendrogram import dendrogram; 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.den",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:597,Availability,error,error,597,"Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-5-49e0357ed731> in <module>; ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds); 1644 var_names=var_names,; 1645 var_group_labels=var_group_labels,; -> 1646 var_group_positions=var_group_positions); 1647 ; 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions); 2332 """"""; 2333 ; -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby); 2335 ; 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby); 2406 ; 2407 if dendrogram_key not in adata.uns:; -> 2408 from ..tools._dendrogram import dendrogram; 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.den",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:372,Deployability,install,install,372,"Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-5-49e0357ed731> in <module>; ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds); 1644 var_names=var_names,; 1645 var_group_labels=var_group_labels,; -> 1646 var_group_positions=var_group_positions); 1647 ; 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions); 2332 """"""; 2333 ; -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby); 2335 ; 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby); 2406 ; 2407 if dendrogram_key not in adata.uns:; -> 2408 from ..tools._dendrogram import dendrogram; 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.den",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:910,Testability,test,test,910,"Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-5-49e0357ed731> in <module>; ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds); 1644 var_names=var_names,; 1645 var_group_labels=var_group_labels,; -> 1646 var_group_positions=var_group_positions); 1647 ; 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions); 2332 """"""; 2333 ; -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby); 2335 ; 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby); 2406 ; 2407 if dendrogram_key not in adata.uns:; -> 2408 from ..tools._dendrogram import dendrogram; 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.den",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1021,Testability,log,log,1021,"anpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-5-49e0357ed731> in <module>; ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds); 1644 var_names=var_names,; 1645 var_group_labels=var_group_labels,; -> 1646 var_group_positions=var_group_positions); 1647 ; 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions); 2332 """"""; 2333 ; -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby); 2335 ; 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby); 2406 ; 2407 if dendrogram_key not in adata.uns:; -> 2408 from ..tools._dendrogram import dendrogram; 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` ""; 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1389,Testability,test,test,1389," ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-5-49e0357ed731> in <module>; ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds); 1644 var_names=var_names,; 1645 var_group_labels=var_group_labels,; -> 1646 var_group_positions=var_group_positions); 1647 ; 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions); 2332 """"""; 2333 ; -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby); 2335 ; 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby); 2406 ; 2407 if dendrogram_key not in adata.uns:; -> 2408 from ..tools._dendrogram import dendrogram; 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` ""; 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'; ```; And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something?. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1707,Testability,test,test,1707," ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-5-49e0357ed731> in <module>; ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds); 1644 var_names=var_names,; 1645 var_group_labels=var_group_labels,; -> 1646 var_group_positions=var_group_positions); 1647 ; 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions); 2332 """"""; 2333 ; -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby); 2335 ; 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby); 2406 ; 2407 if dendrogram_key not in adata.uns:; -> 2408 from ..tools._dendrogram import dendrogram; 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` ""; 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'; ```; And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something?. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1930,Testability,log,logg,1930," ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-5-49e0357ed731> in <module>; ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds); 1644 var_names=var_names,; 1645 var_group_labels=var_group_labels,; -> 1646 var_group_positions=var_group_positions); 1647 ; 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions); 2332 """"""; 2333 ; -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby); 2335 ; 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby); 2406 ; 2407 if dendrogram_key not in adata.uns:; -> 2408 from ..tools._dendrogram import dendrogram; 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` ""; 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'; ```; And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something?. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/561:122,Deployability,install,installed,122,"Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb; import scanpy as sc; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-111-0074c9bc0b31> in <module>(); ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(); 29 ; 30 # the actual API; ---> 31 from . import tools as tl; 32 from . import preprocessing as pp; 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(); 8 from ._rank_genes_groups import rank_genes_groups; 9 from ._dpt import dpt; ---> 10 from ._leiden import leiden; 11 from ._louvain import louvain; 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(); 29 n_iterations: int = -1,; 30 partition_type: Optional[Type[MutableVertexPartition]] = None,; ---> 31 copy: bool = False,; 32 **partition_kwargs; 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg); 647 def __getitem__(self, arg):; 648 arg = _type_check(arg, ""Optional[t] requires a single type.""); --> 649 return Union[arg, type(None)]; 650 ; 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters); 550 parameters = (parameters,); 551 return self.__class__(self.__name__, self.__bases__,; --> 552 dict(self.__dict__), parameters, _root=True); 553 ; 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root); 510 continue; 511 if any(isinstance(t2, type) and issubclass(t1, t2); --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):; 513 all_params.remove(t1); 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0); 510 continue; 511 if any(isinstance(t2, type) and issubclass(t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:160,Deployability,update,update,160,"Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb; import scanpy as sc; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-111-0074c9bc0b31> in <module>(); ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(); 29 ; 30 # the actual API; ---> 31 from . import tools as tl; 32 from . import preprocessing as pp; 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(); 8 from ._rank_genes_groups import rank_genes_groups; 9 from ._dpt import dpt; ---> 10 from ._leiden import leiden; 11 from ._louvain import louvain; 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(); 29 n_iterations: int = -1,; 30 partition_type: Optional[Type[MutableVertexPartition]] = None,; ---> 31 copy: bool = False,; 32 **partition_kwargs; 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg); 647 def __getitem__(self, arg):; 648 arg = _type_check(arg, ""Optional[t] requires a single type.""); --> 649 return Union[arg, type(None)]; 650 ; 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters); 550 parameters = (parameters,); 551 return self.__class__(self.__name__, self.__bases__,; --> 552 dict(self.__dict__), parameters, _root=True); 553 ; 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root); 510 continue; 511 if any(isinstance(t2, type) and issubclass(t1, t2); --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):; 513 all_params.remove(t1); 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0); 510 continue; 511 if any(isinstance(t2, type) and issubclass(t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/562:159,Usability,simpl,simple,159,"@fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`?. Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/563:505,Modifiability,variab,variable,505,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:521,Modifiability,variab,variables-axis,521,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:107,Performance,cache,cache,107,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:150,Performance,cache,cache,150,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:552,Performance,cache,cache,552,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:574,Performance,cache,cache,574,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:740,Performance,cache,cache,740,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:859,Performance,cache,cache,859,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:865,Performance,cache,cache,865,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1041,Performance,cache,cache,1041," when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1180,Performance,cache,cache,1180,"t time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1186,Performance,cache,cache,1186,"t time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1500,Performance,cache,cache,1500,"aster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1677,Performance,cache,cache,1677,"aster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1683,Performance,cache,cache,1683,"aster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1925,Performance,cache,cache,1925,"\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1972,Performance,cache,cache,1972,"\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:2449,Performance,race condition,race condition,2449," read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:2729,Performance,race condition,race condition,2729,"79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:3009,Performance,race condition,race condition,3009,"e to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:3289,Performance,race condition,race condition,3289,"\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:3569,Performance,race condition,race condition,3569,"\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 218 return; 219 try:; --> 220 mkdir(name, mode); 221 except OSError:; 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'; ```. Looks like the directory",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:3849,Performance,race condition,race condition,3849,"cal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 218 return; 219 try:; --> 220 mkdir(name, mode); 221 except OSError:; 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'; ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea?. Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:4129,Performance,race condition,race condition,4129,"cal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 218 return; 219 try:; --> 220 mkdir(name, mode); 221 except OSError:; 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'; ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea?. Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:4501,Performance,cache,cache,4501,"cal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 218 return; 219 try:; --> 220 mkdir(name, mode); 221 except OSError:; 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'; ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea?. Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:4583,Performance,cache,cache,4583,"cal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 218 return; 219 try:; --> 220 mkdir(name, mode); 221 except OSError:; 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'; ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea?. Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:4665,Performance,cache,cache,4665,"cal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 218 return; 219 try:; --> 220 mkdir(name, mode); 221 except OSError:; 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'; ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea?. Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/pull/564:10,Availability,error,error,10,It’s less error prone and a nicer API. Fixes #563,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/564
https://github.com/scverse/scanpy/issues/565:190,Availability,error,error,190,"Hi, . I have an issue with the standard_scale ='var' function.; Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-432-bef389f3fd99> in <module>; ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds); 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks); 1684 ; -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds); 1686 ; 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs); 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),; 1809 RuntimeWarning, stacklevel=2); -> 1810 return func(ax, *args, **kwargs); 1811 ; 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs); 5773 kwargs.setdefault('snap', False); 5774 ; -> 5775 collection = mcoll.PolyCollection(verts, **kwargs); 5776 ; 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs); 931 %(Collection)s; 932 """"""; --> 933 Collection.__init__(self, **kwargs); 934 self.set_sizes(sizes); 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, ca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2179,Deployability,update,update,2179,"Matplotlib list!)"" % (label_namer, func.__name__),; 1809 RuntimeWarning, stacklevel=2); -> 1810 return func(ax, *args, **kwargs); 1811 ; 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs); 5773 kwargs.setdefault('snap', False); 5774 ; -> 5775 collection = mcoll.PolyCollection(verts, **kwargs); 5776 ; 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs); 931 %(Collection)s; 932 """"""; --> 933 Collection.__init__(self, **kwargs); 934 self.set_sizes(sizes); 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs); 164 ; 165 self._path_effects = None; --> 166 self.update(kwargs); 167 self._paths = None; 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v); 910 func = getattr(self, 'set_' + k, None); 911 if not callable(func):; --> 912 raise AttributeError('Unknown property %s' % k); 913 return func(v); 914 . AttributeError: Unknown property standard_scale; ```; Any idea of what I'm missing here?. Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2307,Deployability,update,update,2307,"Matplotlib list!)"" % (label_namer, func.__name__),; 1809 RuntimeWarning, stacklevel=2); -> 1810 return func(ax, *args, **kwargs); 1811 ; 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs); 5773 kwargs.setdefault('snap', False); 5774 ; -> 5775 collection = mcoll.PolyCollection(verts, **kwargs); 5776 ; 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs); 931 %(Collection)s; 932 """"""; --> 933 Collection.__init__(self, **kwargs); 934 self.set_sizes(sizes); 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs); 164 ; 165 self._path_effects = None; --> 166 self.update(kwargs); 167 self._paths = None; 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v); 910 func = getattr(self, 'set_' + k, None); 911 if not callable(func):; --> 912 raise AttributeError('Unknown property %s' % k); 913 return func(v); 914 . AttributeError: Unknown property standard_scale; ```; Any idea of what I'm missing here?. Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:622,Testability,log,log,622,"Hi, . I have an issue with the standard_scale ='var' function.; Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-432-bef389f3fd99> in <module>; ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds); 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks); 1684 ; -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds); 1686 ; 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs); 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),; 1809 RuntimeWarning, stacklevel=2); -> 1810 return func(ax, *args, **kwargs); 1811 ; 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs); 5773 kwargs.setdefault('snap', False); 5774 ; -> 5775 collection = mcoll.PolyCollection(verts, **kwargs); 5776 ; 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs); 931 %(Collection)s; 932 """"""; --> 933 Collection.__init__(self, **kwargs); 934 self.set_sizes(sizes); 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, ca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/566:8,Availability,error,error,8,"Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```; ranking genes. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-385-c2fa7bb8ea8d> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 352 ; 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(; --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)); 355 scores[np.isnan(scores)] = 0; 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine.; I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1086,Availability,error,error,1086,"Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```; ranking genes. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-385-c2fa7bb8ea8d> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 352 ; 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(; --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)); 355 scores[np.isnan(scores)] = 0; 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine.; I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:90,Testability,test,testing,90,"Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```; ranking genes. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-385-c2fa7bb8ea8d> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 352 ; 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(; --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)); 355 scores[np.isnan(scores)] = 0; 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine.; I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1102,Testability,log,logistic,1102,"Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```; ranking genes. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-385-c2fa7bb8ea8d> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 352 ; 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(; --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)); 355 scores[np.isnan(scores)] = 0; 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine.; I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1128,Testability,test,test,1128,"Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```; ranking genes. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-385-c2fa7bb8ea8d> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds); 352 ; 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(; --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)); 355 scores[np.isnan(scores)] = 0; 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine.; I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/567:3384,Safety,Abort,Abort,3384,on + 404; 	17 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528; 	18 python 0x000000010a42d078 function_code_fastcall + 120; 	19 python 0x000000010a56dafe call_function + 174; 	20 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284; 	21 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418; 	22 python 0x000000010a42c577 _PyFunction_FastCallDict + 231; 	23 python 0x000000010a4304a2 method_call + 130; 	24 python 0x000000010a42def2 PyObject_Call + 130; 	25 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871; 	26 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418; 	27 python 0x000000010a42c577 _PyFunction_FastCallDict + 231; 	28 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871; 	29 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418; 	30 python 0x000000010a42c577 _PyFunction_FastCallDict + 231; 	31 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871; 	32 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418; 	33 python 0x000000010a42c577 _PyFunction_FastCallDict + 231; 	34 python 0x000000010a4b00b1 slot_tp_init + 193; 	35 python 0x000000010a4ba091 type_call + 241; 	36 python 0x000000010a42d283 _PyObject_FastCallKeywords + 179; 	37 python 0x000000010a56dbe4 call_function + 404; 	38 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528; 	39 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418; 	40 python 0x000000010a42d783 _PyFunction_FastCallKeywords + 195; 	41 python 0x000000010a56dafe call_function + 174; 	42 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528; 	43 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418; 	44 python 0x000000010a5c3c40 PyRun_FileExFlags + 256; 	45 python 0x000000010a5c30b7 PyRun_SimpleFileExFlags + 391; 	46 python 0x000000010a5ef7ec pymain_main + 9564; 	47 python 0x000000010a3ffa9d main + 125; 	48 libdyld.dylib 0x00007fff72169ed9 start + 1; 	49 ??? 0x0000000000000002 0x0 + 2; ); libc++abi.dylib: terminating with uncaught exception of type NSException; Abort trap: 6,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/pull/568:68,Usability,guid,guidelines,68,Saw a couple duplicate issues and figured updating the contributing guidelines could help with that. Anything I forgot to mention?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/issues/570:37,Availability,redundant,redundant,37,"`louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:37,Safety,redund,redundant,37,"`louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/570:75,Usability,learn,learned,75,"`louvain` and `leiden` have a lot of redundant documentation. After having learned in #557, I could file a PR to deduplicate this. Would it be valid to shuffle the arguments in such a way that the shared documentation is grouped together? Otherwise, one would have to introduce many short strings and puzzle them together.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570
https://github.com/scverse/scanpy/issues/571:169,Testability,log,logo,169,"Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console; $ git checkout 1.4 -b stable; $ git cherry-pick 4b1504c c78de5b # the logo commits; ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/issues/571:258,Testability,log,logo,258,"Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console; $ git checkout 1.4 -b stable; $ git cherry-pick 4b1504c c78de5b # the logo commits; ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/issues/571:308,Usability,simpl,simply,308,"Since we don’t have a chatroom yet, I’ll announce this with an issue. I created the branch stable to have the 1.4 docs without development features, but also the scanpy logo:. ```console; $ git checkout 1.4 -b stable; $ git cherry-pick 4b1504c c78de5b # the logo commits; ```. Once 1.4.1 comes along, we can simply delete it and readthedocs/stable will point to the 1.4.1 tag again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/571
https://github.com/scverse/scanpy/pull/572:354,Availability,error,error,354,"This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:; ```R; else if (binning.method==""equal_frequency"") {; data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))); }; ```; The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:858,Availability,error,error,858,"This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:; ```R; else if (binning.method==""equal_frequency"") {; data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))); }; ```; The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:41,Modifiability,variab,variable,41,"This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:; ```R; else if (binning.method==""equal_frequency"") {; data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))); }; ```; The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/572:246,Testability,test,test,246,"This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:; ```R; else if (binning.method==""equal_frequency"") {; data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))); }; ```; The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572
https://github.com/scverse/scanpy/pull/573:27,Availability,down,downloader,27,"Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:119,Availability,down,downloaded,119,"Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:224,Availability,down,downloaded,224,"Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/573:150,Modifiability,variab,variable,150,"Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573
https://github.com/scverse/scanpy/pull/574:93,Availability,error,error,93,"While testing my changes to dataset code, I saw that `sc.datasets.burczynski06()` raised the error:. ```python; ValueError: `X` needs to be of one of ndarray, MaskedArray, spmatrix, ZarrArray, ZappyArray, not <class 'dict'>.; ```. But it had a pretty easy fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/574
https://github.com/scverse/scanpy/pull/574:159,Availability,Mask,MaskedArray,159,"While testing my changes to dataset code, I saw that `sc.datasets.burczynski06()` raised the error:. ```python; ValueError: `X` needs to be of one of ndarray, MaskedArray, spmatrix, ZarrArray, ZappyArray, not <class 'dict'>.; ```. But it had a pretty easy fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/574
https://github.com/scverse/scanpy/pull/574:6,Testability,test,testing,6,"While testing my changes to dataset code, I saw that `sc.datasets.burczynski06()` raised the error:. ```python; ValueError: `X` needs to be of one of ndarray, MaskedArray, spmatrix, ZarrArray, ZappyArray, not <class 'dict'>.; ```. But it had a pretty easy fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/574
https://github.com/scverse/scanpy/issues/575:190,Modifiability,extend,extend,190,"Hi @LuckyMD, I'm trying out embedding_density() using the latest scanpy version. . First of all, this is a wonderful feature - thank you!. Second, I was wondering if it would be possible to extend this and create a 'differential density' to visualize differences between two conditions? (possibly on a lower resolution grid?). btw. there is a typo on https://icb-scanpy.readthedocs-hosted.com/en/latest/index.html under Master: the name is switched to 'density_embedding()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575
https://github.com/scverse/scanpy/pull/576:34,Testability,test,test,34,"Still need to figure out why paga test fails. Also `simplicial_set_embedding` from umap requires data and metrics. Data is `adata.X` and i set `metrics='euclidean'`, but this is not clear. Fixes #522",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/576:182,Usability,clear,clear,182,"Still need to figure out why paga test fails. Also `simplicial_set_embedding` from umap requires data and metrics. Data is `adata.X` and i set `metrics='euclidean'`, but this is not clear. Fixes #522",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576
https://github.com/scverse/scanpy/pull/578:96,Security,access,access,96,"I think this should fix it – it fixes it on my machine and the tests pass. There's no reason to access raw if `use_raw` isn't passed, right?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/578
https://github.com/scverse/scanpy/pull/578:63,Testability,test,tests,63,"I think this should fix it – it fixes it on my machine and the tests pass. There's no reason to access raw if `use_raw` isn't passed, right?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/578
https://github.com/scverse/scanpy/pull/579:15,Testability,log,logic,15,"Seems like the logic was broken. ```; # we test for raw, but check in adata.var_names; elif use_raw and value_to_plot in adata.var_names:; color_vector = adata.raw[:, value_to_plot].X; # use_raw might be false but we still check adata.raw.var_names; elif value_to_plot in adata.raw.var_names:; color_vector = adata[:, value_to_plot].X; ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:43,Testability,test,test,43,"Seems like the logic was broken. ```; # we test for raw, but check in adata.var_names; elif use_raw and value_to_plot in adata.var_names:; color_vector = adata.raw[:, value_to_plot].X; # use_raw might be false but we still check adata.raw.var_names; elif value_to_plot in adata.raw.var_names:; color_vector = adata[:, value_to_plot].X; ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/pull/579:371,Usability,simpl,simplify,371,"Seems like the logic was broken. ```; # we test for raw, but check in adata.var_names; elif use_raw and value_to_plot in adata.var_names:; color_vector = adata.raw[:, value_to_plot].X; # use_raw might be false but we still check adata.raw.var_names; elif value_to_plot in adata.raw.var_names:; color_vector = adata[:, value_to_plot].X; ```. Apart from fixing that I also simplify the code above. Fixes #577",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/579
https://github.com/scverse/scanpy/issues/580:240,Availability,error,error,240,"The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```; _____________________________ test_marker_overlap ______________________________; def test_marker_overlap():; # Test all overlap calculations on artificial data; test_data = sc.AnnData(X = np.ones((9,10))); test_data.uns['rank_genes_groups'] = dict(); test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(; [['a', 'b','c','d','e'], ['a','f','g','h','i']]); test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(; [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]); ; marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}; ; t1 = sc.tl.marker_gene_overlap(test_data, marker_genes); t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'); t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'); t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'); t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2); t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01); ; > assert t1.iloc[1,1] == 3.0; E assert 1.0 == 3.0; scanpy/tests/test_marker_gene_overlap.py:22: AssertionError; ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:4,Testability,test,test,4,"The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```; _____________________________ test_marker_overlap ______________________________; def test_marker_overlap():; # Test all overlap calculations on artificial data; test_data = sc.AnnData(X = np.ones((9,10))); test_data.uns['rank_genes_groups'] = dict(); test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(; [['a', 'b','c','d','e'], ['a','f','g','h','i']]); test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(; [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]); ; marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}; ; t1 = sc.tl.marker_gene_overlap(test_data, marker_genes); t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'); t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'); t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'); t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2); t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01); ; > assert t1.iloc[1,1] == 3.0; E assert 1.0 == 3.0; scanpy/tests/test_marker_gene_overlap.py:22: AssertionError; ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:246,Testability,log,log,246,"The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```; _____________________________ test_marker_overlap ______________________________; def test_marker_overlap():; # Test all overlap calculations on artificial data; test_data = sc.AnnData(X = np.ones((9,10))); test_data.uns['rank_genes_groups'] = dict(); test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(; [['a', 'b','c','d','e'], ['a','f','g','h','i']]); test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(; [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]); ; marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}; ; t1 = sc.tl.marker_gene_overlap(test_data, marker_genes); t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'); t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'); t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'); t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2); t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01); ; > assert t1.iloc[1,1] == 3.0; E assert 1.0 == 3.0; scanpy/tests/test_marker_gene_overlap.py:22: AssertionError; ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:450,Testability,Test,Test,450,"The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```; _____________________________ test_marker_overlap ______________________________; def test_marker_overlap():; # Test all overlap calculations on artificial data; test_data = sc.AnnData(X = np.ones((9,10))); test_data.uns['rank_genes_groups'] = dict(); test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(; [['a', 'b','c','d','e'], ['a','f','g','h','i']]); test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(; [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]); ; marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}; ; t1 = sc.tl.marker_gene_overlap(test_data, marker_genes); t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'); t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'); t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'); t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2); t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01); ; > assert t1.iloc[1,1] == 3.0; E assert 1.0 == 3.0; scanpy/tests/test_marker_gene_overlap.py:22: AssertionError; ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:1364,Testability,assert,assert,1364,"The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```; _____________________________ test_marker_overlap ______________________________; def test_marker_overlap():; # Test all overlap calculations on artificial data; test_data = sc.AnnData(X = np.ones((9,10))); test_data.uns['rank_genes_groups'] = dict(); test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(; [['a', 'b','c','d','e'], ['a','f','g','h','i']]); test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(; [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]); ; marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}; ; t1 = sc.tl.marker_gene_overlap(test_data, marker_genes); t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'); t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'); t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'); t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2); t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01); ; > assert t1.iloc[1,1] == 3.0; E assert 1.0 == 3.0; scanpy/tests/test_marker_gene_overlap.py:22: AssertionError; ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:1394,Testability,assert,assert,1394,"The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```; _____________________________ test_marker_overlap ______________________________; def test_marker_overlap():; # Test all overlap calculations on artificial data; test_data = sc.AnnData(X = np.ones((9,10))); test_data.uns['rank_genes_groups'] = dict(); test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(; [['a', 'b','c','d','e'], ['a','f','g','h','i']]); test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(; [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]); ; marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}; ; t1 = sc.tl.marker_gene_overlap(test_data, marker_genes); t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'); t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'); t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'); t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2); t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01); ; > assert t1.iloc[1,1] == 3.0; E assert 1.0 == 3.0; scanpy/tests/test_marker_gene_overlap.py:22: AssertionError; ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:1420,Testability,test,tests,1420,"The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```; _____________________________ test_marker_overlap ______________________________; def test_marker_overlap():; # Test all overlap calculations on artificial data; test_data = sc.AnnData(X = np.ones((9,10))); test_data.uns['rank_genes_groups'] = dict(); test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(; [['a', 'b','c','d','e'], ['a','f','g','h','i']]); test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(; [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]); ; marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}; ; t1 = sc.tl.marker_gene_overlap(test_data, marker_genes); t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'); t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'); t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'); t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2); t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01); ; > assert t1.iloc[1,1] == 3.0; E assert 1.0 == 3.0; scanpy/tests/test_marker_gene_overlap.py:22: AssertionError; ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/580:1458,Testability,Assert,AssertionError,1458,"The test `test_marker_overlap` keeps failing on the travis build for python 3.5. This seems to happen on the first build from a PR, but if the build is restarted it passes. Given that my `n=3` for this, it could also be random. Grabbed the error log from #579 (build [1735.1](https://travis-ci.org/theislab/scanpy/jobs/514097606)):. ```; _____________________________ test_marker_overlap ______________________________; def test_marker_overlap():; # Test all overlap calculations on artificial data; test_data = sc.AnnData(X = np.ones((9,10))); test_data.uns['rank_genes_groups'] = dict(); test_data.uns['rank_genes_groups']['names'] = np.rec.fromarrays(; [['a', 'b','c','d','e'], ['a','f','g','h','i']]); test_data.uns['rank_genes_groups']['pvals_adj'] = np.rec.fromarrays(; [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]]); ; marker_genes = {'type 1':{'a','b','c'}, 'type 2':{'a','f','g'}}; ; t1 = sc.tl.marker_gene_overlap(test_data, marker_genes); t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize='reference'); t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='overlap_coef'); t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method='jaccard'); t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2); t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01); ; > assert t1.iloc[1,1] == 3.0; E assert 1.0 == 3.0; scanpy/tests/test_marker_gene_overlap.py:22: AssertionError; ```. Here's a [gist](https://gist.github.com/ivirshup/6965ebe2530c4eac67aebf41c3961959) of the full output. Any idea what's up @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580
https://github.com/scverse/scanpy/issues/581:640,Testability,log,log-ed,640,"```python; import scanpy as sc; print(sc.__version__) # 1.4+222.gdd052ed; a = sc.datasets.pbmc3k(); b = sc.pp.scale(a, copy=True); # /Users/isaac/github/scanpy/scanpy/preprocessing/_simple.py:1105: RuntimeWarning: invalid value encountered in true_divide; # X /= scale; a.X.data; # array([1., 1., 2., ..., 1., 1., 3.], dtype=float32); b.X; # array([[nan, nan, nan, ..., nan, nan, nan],; # [nan, nan, nan, ..., nan, nan, nan],; # [nan, nan, nan, ..., nan, nan, nan],; # ...,; # [nan, nan, nan, ..., nan, nan, nan],; # [nan, nan, nan, ..., nan, nan, nan],; # [nan, nan, nan, ..., nan, nan, nan]], dtype=float32); ```. Seems off. Even if I've log-ed it or do it inplace the same thing happens.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581
https://github.com/scverse/scanpy/issues/582:34,Integrability,synchroniz,synchronize,34,"Hi, ; I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. ; `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). ; I'd be happy to have that. . Best,; M",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582
https://github.com/scverse/scanpy/pull/583:82,Modifiability,flexible,flexible,82,"Noticed that I did not normalized as intended, and made the input dictionary more flexible. Now:; 1. Normalization is not just performed so that rows/columns sum to 1, but instead over the number of marker genes in the reference/the number of marker genes used from the data.; 2. Reference marker dictionaries now accept `Union[Dict[str, set], Dict[str,list]]`. Dictionaries of lists are easier to use in other applications, like scoring based on gene sets. Still no idea why Travis is failing though :/.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/583:127,Performance,perform,performed,127,"Noticed that I did not normalized as intended, and made the input dictionary more flexible. Now:; 1. Normalization is not just performed so that rows/columns sum to 1, but instead over the number of marker genes in the reference/the number of marker genes used from the data.; 2. Reference marker dictionaries now accept `Union[Dict[str, set], Dict[str,list]]`. Dictionaries of lists are easier to use in other applications, like scoring based on gene sets. Still no idea why Travis is failing though :/.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583
https://github.com/scverse/scanpy/pull/585:86,Availability,error,error,86,"I am not sure if it has been already addressed.; This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); in ; ----> 1 import scanpy as sc; 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures; 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in ; 31 from . import preprocessing as pp; 32 from . import plotting as pl; ---> 33 from . import datasets, logging, queries, settings, external; 34 ; 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in ; ----> 1 from . import tl; 2 from . import pl; 3 from . import pp; 4 ; 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in ; 2 from ..tools._phate import phate; 3 from ..tools._phenograph import phenograph; ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:470,Availability,error,errors,470,"I am not sure if it has been already addressed.; This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); in ; ----> 1 import scanpy as sc; 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures; 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in ; 31 from . import preprocessing as pp; 32 from . import plotting as pl; ---> 33 from . import datasets, logging, queries, settings, external; 34 ; 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in ; ----> 1 from . import tl; 2 from . import pl; 3 from . import pp; 4 ; 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in ; 2 from ..tools._phate import phate; 3 from ..tools._phenograph import phenograph; ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:200,Integrability,wrap,wrap,200,"I am not sure if it has been already addressed.; This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); in ; ----> 1 import scanpy as sc; 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures; 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in ; 31 from . import preprocessing as pp; 32 from . import plotting as pl; ---> 33 from . import datasets, logging, queries, settings, external; 34 ; 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in ; ----> 1 from . import tl; 2 from . import pl; 3 from . import pp; 4 ; 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in ; 2 from ..tools._phate import phate; 3 from ..tools._phenograph import phenograph; ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/585:862,Testability,log,logging,862,"I am not sure if it has been already addressed.; This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); in ; ----> 1 import scanpy as sc; 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures; 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in ; 31 from . import preprocessing as pp; 32 from . import plotting as pl; ---> 33 from . import datasets, logging, queries, settings, external; 34 ; 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in ; ----> 1 from . import tl; 2 from . import pl; 3 from . import pp; 4 ; 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in ; 2 from ..tools._phate import phate; 3 from ..tools._phenograph import phenograph; ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585
https://github.com/scverse/scanpy/pull/586:74,Testability,Test,Tests,74,"Added restrict_to parameter to leiden by using louvain code as template.; Tests are not yet provided. A simple example of execution and checks:. ```python; # First split on cluster 4; sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,; key_added='leiden_res0.4_4_sub'). # Additional split; sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),; resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together; sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',; 'leiden_res0.4_4_add_sub']). # Partition size check; ## Original size of clusters; adata.obs['leiden_res0.4'].value_counts(); 0 932; 1 853; 3 676; 2 676; 4 338; 5 57; Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits); ## Assignment of samples in original clusters to subsplit clusters; adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),; 'leiden_res0.4_4_sub'].value_counts(); 4,0 103; 4,1 68; 4,2 66; 4,3 57; 4,4 44; 5 0; 3 0; 2 0; 1 0; 0 0; Name: leiden_res0.4_4_sub, dtype: int64; ## Assignment of samples not in original clusters to subsplit clusters; adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),; 'leiden_res0.4_4_sub'].value_counts(); 0 932; 1 853; 3 676; 2 676; 5 57; 4,4 0; 4,3 0; 4,2 0; 4,1 0; 4,0 0; Name: leiden_res0.4_4_sub, dtype: int64. ...; ```; ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/pull/586:104,Usability,simpl,simple,104,"Added restrict_to parameter to leiden by using louvain code as template.; Tests are not yet provided. A simple example of execution and checks:. ```python; # First split on cluster 4; sc.tl.leiden(adata, restrict_to=('leiden_res0.4', ['4']), resolution=0.6,; key_added='leiden_res0.4_4_sub'). # Additional split; sc.tl.leiden(adata, restrict_to=('leiden_res0.4_4_sub', ['1', '2', '3', '4,4']),; resolution=0.6, key_added='leiden_res0.4_4_add_sub'). # All partitions together; sc.pl.tsne(adata, color=['leiden_res0.4', 'leiden_res0.4_4_sub',; 'leiden_res0.4_4_add_sub']). # Partition size check; ## Original size of clusters; adata.obs['leiden_res0.4'].value_counts(); 0 932; 1 853; 3 676; 2 676; 4 338; 5 57; Name: leiden_res0.4, dtype: int64. # Check if first split is correct (can be iterated for subsequent splits); ## Assignment of samples in original clusters to subsplit clusters; adata.obs.loc[(adata.obs['leiden_res0.4'].isin(['4'])),; 'leiden_res0.4_4_sub'].value_counts(); 4,0 103; 4,1 68; 4,2 66; 4,3 57; 4,4 44; 5 0; 3 0; 2 0; 1 0; 0 0; Name: leiden_res0.4_4_sub, dtype: int64; ## Assignment of samples not in original clusters to subsplit clusters; adata.obs.loc[~(adata.obs['leiden_res0.4'].isin(['4'])),; 'leiden_res0.4_4_sub'].value_counts(); 0 932; 1 853; 3 676; 2 676; 5 57; 4,4 0; 4,3 0; 4,2 0; 4,1 0; 4,0 0; Name: leiden_res0.4_4_sub, dtype: int64. ...; ```; ![Image](https://user-images.githubusercontent.com/697622/55434369-7553e100-5565-11e9-91ee-0d0396ee6138.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586
https://github.com/scverse/scanpy/issues/587:1330,Modifiability,layers,layers,1330,"n <module>(); ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(); 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(); 16 ; 17 from . import settings; ---> 18 from . import logging as logg; 19 ; 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(); 4 import time as time_module; 5 import datetime; ----> 6 from anndata import logging; 7 from . import settings; 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .base import AnnData; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(); 40 return 'mock zappy.base.ZappyArray'; 41 ; ---> 42 from . import h5py; 43 from .layers import AnnDataLayers; 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 5 ; 6 import six; ----> 7 import h5py; 8 import numpy as np; 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _register_converters(); 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:2160,Performance,load,load,2160,"n <module>(); ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(); 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(); 16 ; 17 from . import settings; ---> 18 from . import logging as logg; 19 ; 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(); 4 import time as time_module; 5 import datetime; ----> 6 from anndata import logging; 7 from . import settings; 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .base import AnnData; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(); 40 return 'mock zappy.base.ZappyArray'; 41 ; ---> 42 from . import h5py; 43 from .layers import AnnDataLayers; 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 5 ; 6 import six; ----> 7 import h5py; 8 import numpy as np; 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _register_converters(); 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:670,Testability,log,logging,670,"this is what i get after running `import scanpy as sc`. ```pytb; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>(); ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(); 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(); 16 ; 17 from . import settings; ---> 18 from . import logging as logg; 19 ; 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(); 4 import time as time_module; 5 import datetime; ----> 6 from anndata import logging; 7 from . import settings; 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .base import AnnData; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(); 40 return 'mock zappy.base.ZappyArray'; 41 ; ---> 42 from . import h5py; 43 from .layers import AnnDataLayers; 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 5 ; 6 import six; ----> 7 import h5py; 8 import numpy as np; 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:681,Testability,log,logg,681,"this is what i get after running `import scanpy as sc`. ```pytb; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>(); ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(); 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(); 16 ; 17 from . import settings; ---> 18 from . import logging as logg; 19 ; 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(); 4 import time as time_module; 5 import datetime; ----> 6 from anndata import logging; 7 from . import settings; 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .base import AnnData; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(); 40 return 'mock zappy.base.ZappyArray'; 41 ; ---> 42 from . import h5py; 43 from .layers import AnnDataLayers; 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 5 ; 6 import six; ----> 7 import h5py; 8 import numpy as np; 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:769,Testability,log,logging,769,"this is what i get after running `import scanpy as sc`. ```pytb; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>(); ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(); 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(); 16 ; 17 from . import settings; ---> 18 from . import logging as logg; 19 ; 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(); 4 import time as time_module; 5 import datetime; ----> 6 from anndata import logging; 7 from . import settings; 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .base import AnnData; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(); 40 return 'mock zappy.base.ZappyArray'; 41 ; ---> 42 from . import h5py; 43 from .layers import AnnDataLayers; 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 5 ; 6 import six; ----> 7 import h5py; 8 import numpy as np; 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:872,Testability,log,logging,872,"this is what i get after running `import scanpy as sc`. ```pytb; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>(); ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(); 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(); 16 ; 17 from . import settings; ---> 18 from . import logging as logg; 19 ; 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(); 4 import time as time_module; 5 import datetime; ----> 6 from anndata import logging; 7 from . import settings; 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .base import AnnData; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(); 40 return 'mock zappy.base.ZappyArray'; 41 ; ---> 42 from . import h5py; 43 from .layers import AnnDataLayers; 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 5 ; 6 import six; ----> 7 import h5py; 8 import numpy as np; 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/issues/587:1259,Testability,mock,mock,1259,"n <module>(); ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(); 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(); 16 ; 17 from . import settings; ---> 18 from . import logging as logg; 19 ; 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(); 4 import time as time_module; 5 import datetime; ----> 6 from anndata import logging; 7 from . import settings; 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .base import AnnData; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(); 40 return 'mock zappy.base.ZappyArray'; 41 ; ---> 42 from . import h5py; 43 from .layers import AnnDataLayers; 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 5 ; 6 import six; ----> 7 import h5py; 8 import numpy as np; 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _register_converters(); 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587
https://github.com/scverse/scanpy/pull/592:391,Availability,error,error,391,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:112,Deployability,integrat,integration,112,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:112,Integrability,integrat,integration,112,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:414,Performance,cache,cache-ing,414,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:625,Performance,cache,cache,625,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:267,Testability,test,tests,267,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:460,Testability,test,tests,460,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:638,Testability,test,testing,638,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/pull/592:90,Usability,learn,learn,90,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592
https://github.com/scverse/scanpy/issues/593:37,Availability,error,error,37,"Hi guys,. I am getting the following error after running this:. ```py; sc.pl.dotplot(adata, marker_genes1, groupby='louvain'); ```; ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-138-e642551f77de> in <module>(); ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1350 if isinstance(var_names, str):; 1351 var_names = [var_names]; -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 1353 ; 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 495 ; 496 def __getitem__(self, index):; --> 497 oidx, vidx = self._normalize_indices(index); 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index); 523 obs, var = super()._unpack_index(packed_index); 524 obs = _normalize_index(obs, self._adata.obs_names); --> 525 var = _normalize_index(var, self.var_names); 526 return obs, var; 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names);",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2302,Availability,error,error,2302,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1350 if isinstance(var_names, str):; 1351 var_names = [var_names]; -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 1353 ; 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 495 ; 496 def __getitem__(self, index):; --> 497 oidx, vidx = self._normalize_indices(index); 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index); 523 obs, var = super()._unpack_index(packed_index); 524 obs = _normalize_index(obs, self._adata.obs_names); --> 525 var = _normalize_index(var, self.var_names); 526 return obs, var; 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names); 268 raise KeyError(; 269 'Indices ""{}"" contain invalid observation/variables names/indices.'; --> 270 .format(index)); 271 return positions.values; 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'; ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1137,Modifiability,layers,layers,1137,"--------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-138-e642551f77de> in <module>(); ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1350 if isinstance(var_names, str):; 1351 var_names = [var_names]; -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 1353 ; 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 495 ; 496 def __getitem__(self, index):; --> 497 oidx, vidx = self._normalize_indices(index); 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index); 523 obs, var = super()._unpack_index(packed_index); 524 obs = _normalize_index(obs, self._adata.obs_names); --> 525 var = _normalize_index(var, self.var_names); 526 return obs, var; 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names); 268 raise KeyError(; 269 'Indices ""{}"" contain invalid observation/variables names/indices.'; --> 270 .format(index)); 271 return positions.values; 272 else:. KeyEr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2069,Modifiability,variab,variables,2069,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1350 if isinstance(var_names, str):; 1351 var_names = [var_names]; -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 1353 ; 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 495 ; 496 def __getitem__(self, index):; --> 497 oidx, vidx = self._normalize_indices(index); 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index); 523 obs, var = super()._unpack_index(packed_index); 524 obs = _normalize_index(obs, self._adata.obs_names); --> 525 var = _normalize_index(var, self.var_names); 526 return obs, var; 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names); 268 raise KeyError(; 269 'Indices ""{}"" contain invalid observation/variables names/indices.'; --> 270 .format(index)); 271 return positions.values; 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'; ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:2253,Modifiability,variab,variables,2253,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1350 if isinstance(var_names, str):; 1351 var_names = [var_names]; -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 1353 ; 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 495 ; 496 def __getitem__(self, index):; --> 497 oidx, vidx = self._normalize_indices(index); 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index); 523 obs, var = super()._unpack_index(packed_index); 524 obs = _normalize_index(obs, self._adata.obs_names); --> 525 var = _normalize_index(var, self.var_names); 526 return obs, var; 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names); 268 raise KeyError(; 269 'Indices ""{}"" contain invalid observation/variables names/indices.'; --> 270 .format(index)); 271 return positions.values; 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'; ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:503,Testability,log,log,503,"Hi guys,. I am getting the following error after running this:. ```py; sc.pl.dotplot(adata, marker_genes1, groupby='louvain'); ```; ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-138-e642551f77de> in <module>(); ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1350 if isinstance(var_names, str):; 1351 var_names = [var_names]; -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 1353 ; 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 495 ; 496 def __getitem__(self, index):; --> 497 oidx, vidx = self._normalize_indices(index); 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index); 523 obs, var = super()._unpack_index(packed_index); 524 obs = _normalize_index(obs, self._adata.obs_names); --> 525 var = _normalize_index(var, self.var_names); 526 return obs, var; 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names);",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:814,Testability,log,log,814,"Hi guys,. I am getting the following error after running this:. ```py; sc.pl.dotplot(adata, marker_genes1, groupby='louvain'); ```; ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-138-e642551f77de> in <module>(); ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1350 if isinstance(var_names, str):; 1351 var_names = [var_names]; -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 1353 ; 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 495 ; 496 def __getitem__(self, index):; --> 497 oidx, vidx = self._normalize_indices(index); 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index); 523 obs, var = super()._unpack_index(packed_index); 524 obs = _normalize_index(obs, self._adata.obs_names); --> 525 var = _normalize_index(var, self.var_names); 526 return obs, var; 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names);",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/593:1074,Testability,log,log,1074,"l.dotplot(adata, marker_genes1, groupby='louvain'); ```; ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-138-e642551f77de> in <module>(); ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1350 if isinstance(var_names, str):; 1351 var_names = [var_names]; -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 1353 ; 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 495 ; 496 def __getitem__(self, index):; --> 497 oidx, vidx = self._normalize_indices(index); 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index); 523 obs, var = super()._unpack_index(packed_index); 524 obs = _normalize_index(obs, self._adata.obs_names); --> 525 var = _normalize_index(var, self.var_names); 526 return obs, var; 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names); 268 raise KeyError(; 269 'Indices ""{}"" contain invalid observation/variab",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593
https://github.com/scverse/scanpy/issues/596:445,Energy Efficiency,green,green,445,"Could we change colors for groups from being defined by an array of colors to being a dict mapping from the relevant key/ category to the color? I would find this more intuitive, and much easier to modify. Plus tools like `seaborn` can accept `dict`s directly as a palette:. ```python; import seaborn as sns. iris = sns.load_dataset(""iris""); g = sns.FacetGrid(; iris, ; row=""species"", ; hue=""species"", ; palette={""setosa"": ""red"", ""versicolor"": ""green"", ""virginica"": ""blue""}; ); g.map(sns.kdeplot, ""sepal_width"").show(); ```. ![example](https://user-images.githubusercontent.com/8238804/55700859-48477880-5a14-11e9-921c-612c3387b7cb.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/596:168,Usability,intuit,intuitive,168,"Could we change colors for groups from being defined by an array of colors to being a dict mapping from the relevant key/ category to the color? I would find this more intuitive, and much easier to modify. Plus tools like `seaborn` can accept `dict`s directly as a palette:. ```python; import seaborn as sns. iris = sns.load_dataset(""iris""); g = sns.FacetGrid(; iris, ; row=""species"", ; hue=""species"", ; palette={""setosa"": ""red"", ""versicolor"": ""green"", ""virginica"": ""blue""}; ); g.map(sns.kdeplot, ""sepal_width"").show(); ```. ![example](https://user-images.githubusercontent.com/8238804/55700859-48477880-5a14-11e9-921c-612c3387b7cb.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596
https://github.com/scverse/scanpy/issues/598:204,Availability,error,error,204,"I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:623,Modifiability,layers,layers,623,"I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:666,Modifiability,layers,layers,666,"I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:718,Modifiability,layers,layers,718,"I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:750,Modifiability,layers,layers,750,"I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:976,Modifiability,layers,layers,976,"I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1040,Modifiability,layers,layers,1040,"by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:; 56 for key in self.keys():; ---> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/598:1151,Modifiability,layers,layers,1151,"_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:; 56 for key in self.keys():; ---> 57 yield (key, self[key]); 58 ; 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_man",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598
https://github.com/scverse/scanpy/issues/599:140,Deployability,pipeline,pipeline,140,"Hi guys,. I would like to filter cells by an arbitrary threshold set on the expression of a specific gene (elav) at the very begging of the pipeline. I am new to scanpy and relatively new to python. . this is what I do at the begining but I am having trouble getting it to work (see third command) with setting the corresponding threshold:. #filtering by n_genes and percent mito; adata = adata[adata.obs['n_genes'] < 3500, :]; adata = adata[adata.obs['percent_mito'] < 0.5, :]. #filtering by elav; adata = adata[adata.var['elav'] > 0.5, :]. This last part doesn't seem to work. All help appreciated!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/601:125,Availability,error,error,125,"Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):; File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>; import scanpy.external as sce; File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>; from . import datasets, logging, queries, external; File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>; from . import tl; File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>; from ._tools._palantir import palantir; ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? ; Kind regards,; Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:390,Testability,log,logging,390,"Dear developers, . in an attempt to instal the latest version of scanpy from GitHub (Master branch), I receive the following error:. Traceback (most recent call last):; File ""/home/vladie/PycharmProjects/PY3/RPE_MYCN_10X.py"", line 4, in <module>; import scanpy.external as sce; File ""/usr/local/lib/python3.6/dist-packages/scanpy/__init__.py"", line 33, in <module>; from . import datasets, logging, queries, external; File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/__init__.py"", line 1, in <module>; from . import tl; File ""/usr/local/lib/python3.6/dist-packages/scanpy/external/tl.py"", line 4, in <module>; from ._tools._palantir import palantir; ModuleNotFoundError: No module named 'scanpy.external._tools'. I would like to run palentir through Scanpy, is this already possible ? ; Kind regards,; Vladie0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/pull/602:66,Availability,down,downsample,66,Just a minor improvement. Now you can specify the total counts to downsample to on a per-cell basis by passing an array for `counts_per_cell`. This is so I don't have to split and merge an AnnData object when I want multiple distributions of counts per cell.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/602
https://github.com/scverse/scanpy/pull/605:6,Testability,test,tests,6,"Added tests for normalize_total, changed quantile argument name to fraction.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/605
https://github.com/scverse/scanpy/issues/606:16,Deployability,upgrade,upgraded,16,"Hi!. I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:; sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated!. Thanks!!!. Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; matplotlib == 2.2.3. INSTALLED VERSIONS; ------------------; commit: None; python: 3.7.0.final.0; python-bits: 64; OS: Windows; OS-release: 8.1; machine: AMD64; processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel; byteorder: little; LC_ALL: None; LANG: None; LOCALE: None.None. pandas: 0.23.4; pytest: 3.8.0; pip: 19.0.3; setuptools: 40.2.0; Cython: 0.28.5; numpy: 1.15.4; scipy: 1.1.0; pyarrow: None; xarray: None; IPython: 6.5.0; sphinx: 1.7.9; patsy: 0.5.0; dateutil: 2.7.3; pytz: 2018.5; blosc: None; bottleneck: 1.2.1; tables: 3.4.4; numexpr: 2.6.8; feather: None; matplotlib: 2.2.3; openpyxl: 2.5.6; xlrd: 1.1.0; xlwt: 1.3.0; xlsxwriter: 1.1.0; lxml: 4.2.5; bs4: 4.6.3; html5lib: 1.0.1; sqlalchemy: 1.2.11; pymysql: None; psycopg2: None; jinja2: 2.10; s3fs: None; fastparquet: None; pandas_gbq: None; pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:860,Deployability,INSTALL,INSTALLED,860,"Hi!. I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:; sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated!. Thanks!!!. Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; matplotlib == 2.2.3. INSTALLED VERSIONS; ------------------; commit: None; python: 3.7.0.final.0; python-bits: 64; OS: Windows; OS-release: 8.1; machine: AMD64; processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel; byteorder: little; LC_ALL: None; LANG: None; LOCALE: None.None. pandas: 0.23.4; pytest: 3.8.0; pip: 19.0.3; setuptools: 40.2.0; Cython: 0.28.5; numpy: 1.15.4; scipy: 1.1.0; pyarrow: None; xarray: None; IPython: 6.5.0; sphinx: 1.7.9; patsy: 0.5.0; dateutil: 2.7.3; pytz: 2018.5; blosc: None; bottleneck: 1.2.1; tables: 3.4.4; numexpr: 2.6.8; feather: None; matplotlib: 2.2.3; openpyxl: 2.5.6; xlrd: 1.1.0; xlwt: 1.3.0; xlsxwriter: 1.1.0; lxml: 4.2.5; bs4: 4.6.3; html5lib: 1.0.1; sqlalchemy: 1.2.11; pymysql: None; psycopg2: None; jinja2: 2.10; s3fs: None; fastparquet: None; pandas_gbq: None; pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:970,Deployability,release,release,970,"Hi!. I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:; sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated!. Thanks!!!. Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; matplotlib == 2.2.3. INSTALLED VERSIONS; ------------------; commit: None; python: 3.7.0.final.0; python-bits: 64; OS: Windows; OS-release: 8.1; machine: AMD64; processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel; byteorder: little; LC_ALL: None; LANG: None; LOCALE: None.None. pandas: 0.23.4; pytest: 3.8.0; pip: 19.0.3; setuptools: 40.2.0; Cython: 0.28.5; numpy: 1.15.4; scipy: 1.1.0; pyarrow: None; xarray: None; IPython: 6.5.0; sphinx: 1.7.9; patsy: 0.5.0; dateutil: 2.7.3; pytz: 2018.5; blosc: None; bottleneck: 1.2.1; tables: 3.4.4; numexpr: 2.6.8; feather: None; matplotlib: 2.2.3; openpyxl: 2.5.6; xlrd: 1.1.0; xlwt: 1.3.0; xlsxwriter: 1.1.0; lxml: 4.2.5; bs4: 4.6.3; html5lib: 1.0.1; sqlalchemy: 1.2.11; pymysql: None; psycopg2: None; jinja2: 2.10; s3fs: None; fastparquet: None; pandas_gbq: None; pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:1354,Performance,bottleneck,bottleneck,1354,"Hi!. I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:; sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated!. Thanks!!!. Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; matplotlib == 2.2.3. INSTALLED VERSIONS; ------------------; commit: None; python: 3.7.0.final.0; python-bits: 64; OS: Windows; OS-release: 8.1; machine: AMD64; processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel; byteorder: little; LC_ALL: None; LANG: None; LOCALE: None.None. pandas: 0.23.4; pytest: 3.8.0; pip: 19.0.3; setuptools: 40.2.0; Cython: 0.28.5; numpy: 1.15.4; scipy: 1.1.0; pyarrow: None; xarray: None; IPython: 6.5.0; sphinx: 1.7.9; patsy: 0.5.0; dateutil: 2.7.3; pytz: 2018.5; blosc: None; bottleneck: 1.2.1; tables: 3.4.4; numexpr: 2.6.8; feather: None; matplotlib: 2.2.3; openpyxl: 2.5.6; xlrd: 1.1.0; xlwt: 1.3.0; xlsxwriter: 1.1.0; lxml: 4.2.5; bs4: 4.6.3; html5lib: 1.0.1; sqlalchemy: 1.2.11; pymysql: None; psycopg2: None; jinja2: 2.10; s3fs: None; fastparquet: None; pandas_gbq: None; pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:768,Usability,learn,learn,768,"Hi!. I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:; sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated!. Thanks!!!. Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; matplotlib == 2.2.3. INSTALLED VERSIONS; ------------------; commit: None; python: 3.7.0.final.0; python-bits: 64; OS: Windows; OS-release: 8.1; machine: AMD64; processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel; byteorder: little; LC_ALL: None; LANG: None; LOCALE: None.None. pandas: 0.23.4; pytest: 3.8.0; pip: 19.0.3; setuptools: 40.2.0; Cython: 0.28.5; numpy: 1.15.4; scipy: 1.1.0; pyarrow: None; xarray: None; IPython: 6.5.0; sphinx: 1.7.9; patsy: 0.5.0; dateutil: 2.7.3; pytz: 2018.5; blosc: None; bottleneck: 1.2.1; tables: 3.4.4; numexpr: 2.6.8; feather: None; matplotlib: 2.2.3; openpyxl: 2.5.6; xlrd: 1.1.0; xlwt: 1.3.0; xlsxwriter: 1.1.0; lxml: 4.2.5; bs4: 4.6.3; html5lib: 1.0.1; sqlalchemy: 1.2.11; pymysql: None; psycopg2: None; jinja2: 2.10; s3fs: None; fastparquet: None; pandas_gbq: None; pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/607:68,Deployability,release,release,68,"Dear,; Scanpy are winning more and more popularity since it's first release in single cell sequencing field. Now single molecule sequencing is growing fast, do you plan to develop a python package for single molecule sequencing analysis (both Nanopore and PacBio) ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607
https://github.com/scverse/scanpy/issues/612:708,Usability,feedback,feedback,708,"Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````; # get subset of cells that express GENE1; gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work; adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works!; adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1; ````; Would you know what the recommended way of doing this is and why is my first solution not working?. I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:729,Usability,learn,learning,729,"Hi,. I was wondering why subsetting of the `anndata` object does not work, but subsetting of the `obs` dataframe works when updating the `obs` dataframe. I got a little bit confused while trying to assign a custom cluster with values:. ````; # get subset of cells that express GENE1; gene1_cells = list(adata[(adata.raw[:, ['GENE1']].X > 0),].obs.index). # First solution: assign subset to cluster 1 -- does not work; adata[gene1_obs,:].obs[""my_cluster""] = 1. # Second solution: assign subset to cluster 1 -- works!; adata.obs.loc[adata.obs.index.isin(gene1_cells), ""my_cluster""] = 1; ````; Would you know what the recommended way of doing this is and why is my first solution not working?. I appreciate any feedback as I'm into learning the ropes of scanpy now. PAGA got me :). Thanks for your great work on scanpy btw!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/pull/614:153,Energy Efficiency,reduce,reduce,153,"I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~; - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list.; - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python; sc.pp.highly_variable_genes(ad); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:277,Energy Efficiency,reduce,reduces,277,"I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~; - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list.; - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python; sc.pp.highly_variable_genes(ad); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1457,Energy Efficiency,adapt,adapted,1457,"irst concatenated into a single dataframe.; - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~; - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list.; - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python; sc.pp.highly_variable_genes(ad); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python; sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1457,Modifiability,adapt,adapted,1457,"irst concatenated into a single dataframe.; - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~; - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list.; - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python; sc.pp.highly_variable_genes(ad); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python; sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:181,Safety,avoid,avoiding,181,"I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~; - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list.; - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python; sc.pp.highly_variable_genes(ad); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:700,Safety,detect,detected,700,"I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~; - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list.; - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python; sc.pp.highly_variable_genes(ad); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:1007,Safety,detect,detected,1007,"I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~; - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list.; - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python; sc.pp.highly_variable_genes(ad); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:2757,Testability,Test,Tests,2757,"ter than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~; - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list.; - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python; sc.pp.highly_variable_genes(ad); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python; sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python; sc.pp.highly_variable_genes(ad, n_top_genes=1000); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462824-a6227b80-6397-11e9-8a35-f6aa4b745ad1.png). Plase review thoroughly :) Tests are missing, I'll add them soon.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/615:818,Availability,avail,available,818,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:26,Deployability,update,updates,26,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:1234,Deployability,update,updates,1234,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:1334,Deployability,update,updates,1334,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:688,Modifiability,variab,variable,688,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:1289,Modifiability,layers,layers,1289,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:297,Performance,cache,cached,297,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:530,Performance,perform,performing,530,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:583,Performance,cache,cached,583,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:64,Testability,Test,Tests,64,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:158,Testability,test,tests,158,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:343,Testability,test,tests,343,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/616:333,Testability,test,test,333,"Three minor fixes:. 1. Remove noisy print statement from doc builds. Looked like it might have been left over from debugging, and doesn't seem to break anything. @flying-sheep, does that sound right?; 2. When setting `sc.settings.datasetdir`, I'd meant to use `Path.resolve` instead of `Path.absolute`.; 3. Sped up embedding density test by using a dataset where umap was precomputed. Is this fine @LuckyMD?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/616
https://github.com/scverse/scanpy/pull/618:47,Modifiability,variab,variables,47,It's convenient to be able to specify multiple variables in combat in the R package. So I added the support for extra covariates (categorical or numeric) and converted some methods to private. There are tests for the new covariate option and also the private _design_matrix function now.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/618:203,Testability,test,tests,203,It's convenient to be able to specify multiple variables in combat in the R package. So I added the support for extra covariates (categorical or numeric) and converted some methods to private. There are tests for the new covariate option and also the private _design_matrix function now.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/619:972,Modifiability,layers,layers,972,"This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:96,Security,access,access,96,"This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:321,Security,access,access,321,"This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:397,Security,access,access,397,"This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:511,Security,access,access,511,"This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:741,Security,access,accessing,741,"This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/issues/620:1409,Availability,error,error,1409,",; obs=pd.DataFrame(; {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},; index=[f""cell{i}"" for i in range(a.shape[0])]; ),; var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),; ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious; print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])); # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right; ```. `""logreg""` on the other hand, throws an error:. ```python; sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""); <ipython-input-7-29e46f287a31> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds); 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(; 398 [n for n in rankings_gene_scores],; --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]); 400 adata.uns[key_added]['names'] = np.rec.fromarrays(; 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder); 615 # Determine shape from data-type.; 616 if len(descr) != len(arrayList):; --> 617 raise ValueError(""mismatch between the number of fields ""; 618 ""and the number of arrays""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:19,Testability,test,test,19,"So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python; import scanpy as sc. import numpy as np; import pandas as pd; from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)); a[:10, 0] = 5; adata = sc.AnnData(; a,; obs=pd.DataFrame(; {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},; index=[f""cell{i}"" for i in range(a.shape[0])]; ),; var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),; ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious; print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])); # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right; ```. `""logreg""` on the other hand, throws an error:. ```python; sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""); <ipython-input-7-29e46f287a31> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds); 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(; 398 [n for n in rankings_gene_scores],; --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]); 400 adata.uns[key_added",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:637,Testability,test,test,637,"So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python; import scanpy as sc. import numpy as np; import pandas as pd; from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)); a[:10, 0] = 5; adata = sc.AnnData(; a,; obs=pd.DataFrame(; {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},; index=[f""cell{i}"" for i in range(a.shape[0])]; ),; var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),; ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious; print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])); # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right; ```. `""logreg""` on the other hand, throws an error:. ```python; sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""); <ipython-input-7-29e46f287a31> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds); 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(; 398 [n for n in rankings_gene_scores],; --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]); 400 adata.uns[key_added",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:852,Testability,test,test,852,"So I was writing a test for `rank_genes_groups_df` from #619 when I got some results that seem pretty wrong. Here's an example:. ```python; import scanpy as sc. import numpy as np; import pandas as pd; from scipy import stats. from itertools import repeat, chain. # Making data where ""gene0"" is definitely differentially expressed. a = np.zeros((20, 3)); a[:10, 0] = 5; adata = sc.AnnData(; a,; obs=pd.DataFrame(; {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},; index=[f""cell{i}"" for i in range(a.shape[0])]; ),; var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),; ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious; print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])); # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right; ```. `""logreg""` on the other hand, throws an error:. ```python; sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""); <ipython-input-7-29e46f287a31> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds); 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(; 398 [n for n in rankings_gene_scores],; --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]); 400 adata.uns[key_added",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1371,Testability,log,logreg,1371,",; obs=pd.DataFrame(; {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},; index=[f""cell{i}"" for i in range(a.shape[0])]; ),; var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),; ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious; print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])); # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right; ```. `""logreg""` on the other hand, throws an error:. ```python; sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""); <ipython-input-7-29e46f287a31> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds); 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(; 398 [n for n in rankings_gene_scores],; --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]); 400 adata.uns[key_added]['names'] = np.rec.fromarrays(; 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder); 615 # Determine shape from data-type.; 616 if len(descr) != len(arrayList):; --> 617 raise ValueError(""mismatch between the number of fields ""; 618 ""and the number of arrays""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1487,Testability,log,logreg,1487,"taFrame(; {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},; index=[f""cell{i}"" for i in range(a.shape[0])]; ),; var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),; ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious; print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])); # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right; ```. `""logreg""` on the other hand, throws an error:. ```python; sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""); <ipython-input-7-29e46f287a31> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds); 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(; 398 [n for n in rankings_gene_scores],; --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]); 400 adata.uns[key_added]['names'] = np.rec.fromarrays(; 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder); 615 # Determine shape from data-type.; 616 if len(descr) != len(arrayList):; --> 617 raise ValueError(""mismatch between the number of fields ""; 618 ""and the number of arrays""); 619 ; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:1608,Testability,log,logreg,1608,"taFrame(; {""celltype"": list(chain(repeat(""a"", 10), repeat(""b"", 10)))},; index=[f""cell{i}"" for i in range(a.shape[0])]; ),; var=pd.DataFrame(index=[f""gene{i}"" for i in range(a.shape[1])]),; ). # Running differential expression with t-test:. sc.tl.rank_genes_groups(adata, groupby=""celltype""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This seems wrong. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""t-test""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1., 1., 1.]) # This also seems wrong. # Checking to make sure I'm not forgetting something obvious; print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5])); # Ttest_indResult(statistic=-inf, pvalue=0.0) # This seems right. # Wilcoxon seems fine:. sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""wilcoxon""); print(adata.uns[""rank_genes_groups""][""pvals""][""a""]); # array([1.57052284e-04, 1.00000000e+00, 1.00000000e+00]) # This seems right; ```. `""logreg""` on the other hand, throws an error:. ```python; sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""); <ipython-input-7-29e46f287a31> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby=""celltype"", method=""logreg""). ~/github/scanpy/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds); 397 adata.uns[key_added]['scores'] = np.rec.fromarrays(; 398 [n for n in rankings_gene_scores],; --> 399 dtype=[(rn, 'float32') for rn in groups_order_save]); 400 adata.uns[key_added]['names'] = np.rec.fromarrays(; 401 [n for n in rankings_gene_names],. /usr/local/lib/python3.7/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder); 615 # Determine shape from data-type.; 616 if len(descr) != len(arrayList):; --> 617 raise ValueError(""mismatch between the number of fields ""; 618 ""and the number of arrays""); 619 ; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/pull/622:204,Energy Efficiency,reduce,reduce,204,"(sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:328,Energy Efficiency,reduce,reduces,328,"(sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:1344,Energy Efficiency,adapt,adapted,1344,"(sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:1344,Modifiability,adapt,adapted,1344,"(sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:232,Safety,avoid,avoiding,232,"(sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:740,Safety,detect,detected,740,"(sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:895,Safety,detect,detected,895,"(sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/issues/625:484,Availability,avail,available,484,"Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers).; Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks!; Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:201,Testability,test,test,201,"Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers).; Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks!; Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:286,Testability,test,test,286,"Hi, . is there a possibility to calculate _downregulated_ genes between two clusters? In the function `tl.rank_genes_groups()` I did not find such option though it should be possible with the Wilcoxon test. Afaik in `Seurat.FindMarkers()` there is an option `only.pos` for the Wilcoxon test (https://www.rdocumentation.org/packages/Seurat/versions/3.0.0/topics/FindMarkers).; Following another discussion here about DEG I tried to switch to MAST to get around that but it seems to be available only through R (https://github.com/RGLab/MAST/issues/102). Also Wilcoxon did reasonable well in a recent paper (https://www.nature.com/articles/nmeth.4612).. . Thanks!; Tilo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/626:2839,Integrability,wrap,wrapper,2839,":; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds); 139 userblock_size=userblock_size,; 140 swmr=swmr,; --> 141 **kwds,; 142 ); 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds); 267 with phil:; 268 fapl = make_fapl(driver, libver, **kwds); --> 269 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr); 270 ; 271 if swmr_support:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr); 97 if swmr and swmr_support:; 98 flags |= h5f.ACC_SWMR_READ; ---> 99 fid = h5f.open(name, flags, fapl=fapl); 100 elif mode == 'r+':; 101 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5f.pyx in h5py.h5f.open(). OSError: Unable to open file (truncated file: eof = 1241513984, sblock->base_addr = 0, stored_eof = 14011376022); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:2895,Integrability,wrap,wrapper,2895,":; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds); 139 userblock_size=userblock_size,; 140 swmr=swmr,; --> 141 **kwds,; 142 ); 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds); 267 with phil:; 268 fapl = make_fapl(driver, libver, **kwds); --> 269 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr); 270 ; 271 if swmr_support:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr); 97 if swmr and swmr_support:; 98 flags |= h5f.ACC_SWMR_READ; ---> 99 fid = h5f.open(name, flags, fapl=fapl); 100 elif mode == 'r+':; 101 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5f.pyx in h5py.h5f.open(). OSError: Unable to open file (truncated file: eof = 1241513984, sblock->base_addr = 0, stored_eof = 14011376022); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:543,Performance,cache,cache,543,"```; adata=sc.read('./CONFIDENTIAL_04022019.h5ad'); ```; ---------------------------------------------------------------------------; ```; OSErrorTraceback (most recent call last); <ipython-input-11-759ccdc7c8be> in <module>(); ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'); 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 433 if ext in {'h5', 'h5ad'}:; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:720,Performance,cache,cache,720,"```; adata=sc.read('./CONFIDENTIAL_04022019.h5ad'); ```; ---------------------------------------------------------------------------; ```; OSErrorTraceback (most recent call last); <ipython-input-11-759ccdc7c8be> in <module>(); ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'); 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 433 if ext in {'h5', 'h5ad'}:; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:726,Performance,cache,cache,726,"```; adata=sc.read('./CONFIDENTIAL_04022019.h5ad'); ```; ---------------------------------------------------------------------------; ```; OSErrorTraceback (most recent call last); <ipython-input-11-759ccdc7c8be> in <module>(); ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'); 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 433 if ext in {'h5', 'h5ad'}:; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:986,Performance,cache,cache,986,"DENTIAL_04022019.h5ad'); ```; ---------------------------------------------------------------------------; ```; OSErrorTraceback (most recent call last); <ipython-input-11-759ccdc7c8be> in <module>(); ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'); 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 433 if ext in {'h5', 'h5ad'}:; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds); 139 userblock_size=us",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1373,Performance,load,load,1373,"nvs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 433 if ext in {'h5', 'h5ad'}:; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds); 139 userblock_size=userblock_size,; 140 swmr=swmr,; --> 141 **kwds,; 142 ); 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds); 267 with phil:; 268 fapl = make_fapl(driver, libver, **kwds); --> 269 fid = make_fid(name, mode, userblock_s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:1148,Testability,log,logg,1148,"DENTIAL_04022019.h5ad'); ```; ---------------------------------------------------------------------------; ```; OSErrorTraceback (most recent call last); <ipython-input-11-759ccdc7c8be> in <module>(); ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'); 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 433 if ext in {'h5', 'h5ad'}:; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds); 139 userblock_size=us",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/627:532,Deployability,update,updated,532,"Hi, ; I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks; [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:550,Testability,test,tested,550,"Hi, ; I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks; [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:580,Testability,test,test,580,"Hi, ; I am using weighted sampling data as input of . > scanpy , but i didn't find any help when i have a distinct weight for each observation. So I modified few of the . > scanpy. files. . `use_weights` is a boolean parameter either your data is weighted or not, if weighted then it will calculated weighted mean and weighted variance, in other case it will be same as previous. `Default is False`. `weights` is a 1D weight vector, where each row is weight of observation in original matrix. `Default is None`. I have attached the updated files and tested with well. But you can test again for adding into your tool. . Thanks; [Weighted_Sampling.zip](https://github.com/theislab/scanpy/files/3134213/Weighted_Sampling.zip)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/629:27,Testability,test,test,27,"After we switch to scipy t-test, I started getting a scipy warning (`invalid value encountered`) when I run the following the code:. ```python; adata = sc.datasets.paul15(); adata.X[:, 10] = 0.0; sc.tl.rank_genes_groups(adata, 'paul15_clusters'); ```. Output:. ![image](https://user-images.githubusercontent.com/1140359/57115885-88ea9700-6d1f-11e9-8752-a865cda602b7.png). @ivirshup is having a look now.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/pull/630:264,Deployability,update,updated,264,"I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:401,Deployability,update,update,401,"I have added source code for weighted sampled data. I have already preprocessed data and found the top few PC's and then input to `scanpy `to find `louvain `communities , `marker genes` and later variety of plots like `dotPlot`, `violinPlot `and `heatmap`. I have updated `scanpy `for `weighted `sampled data where each row has its weight, but this support for clustering and plotting. We can further update` sparse PCA` as well to support weighted data points.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/issues/632:85,Availability,error,error,85,"Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```; scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1; pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Cmds:. ```py; import scanpy.external as sce; sce.pp.bbknn(adata, batch_key='sample', copy=False); ```. Error info:. ```pytb; sce.pp.bbknn(adata, batch_key='sample', copy=False); computing batch balanced neighbors; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-34-5b7ebd13c9e6> in <module>; 1 # Correct; 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'); ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 except ImportError:; 83 raise ImportError('Please install bbknn: `pip install bbknn`.'); ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 215 batch_list = adata.obs[batch_key].values; 216 #call BBKNN proper; --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs); 218 #optionally save knn_indices; 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn); 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], ; 273 knn_indices.shape[1], bandwidth=bandwidth,; --> 274 											 local_connectivity=local_connectivity); 275 #optional trimming; 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwidth'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:441,Availability,Error,Error,441,"Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```; scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1; pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Cmds:. ```py; import scanpy.external as sce; sce.pp.bbknn(adata, batch_key='sample', copy=False); ```. Error info:. ```pytb; sce.pp.bbknn(adata, batch_key='sample', copy=False); computing batch balanced neighbors; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-34-5b7ebd13c9e6> in <module>; 1 # Correct; 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'); ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 except ImportError:; 83 raise ImportError('Please install bbknn: `pip install bbknn`.'); ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 215 batch_list = adata.obs[batch_key].values; 216 #call BBKNN proper; --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs); 218 #optionally save knn_indices; 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn); 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], ; 273 knn_indices.shape[1], bandwidth=bandwidth,; --> 274 											 local_connectivity=local_connectivity); 275 #optional trimming; 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwidth'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:1036,Deployability,install,install,1036,"Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```; scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1; pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Cmds:. ```py; import scanpy.external as sce; sce.pp.bbknn(adata, batch_key='sample', copy=False); ```. Error info:. ```pytb; sce.pp.bbknn(adata, batch_key='sample', copy=False); computing batch balanced neighbors; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-34-5b7ebd13c9e6> in <module>; 1 # Correct; 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'); ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 except ImportError:; 83 raise ImportError('Please install bbknn: `pip install bbknn`.'); ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 215 batch_list = adata.obs[batch_key].values; 216 #call BBKNN proper; --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs); 218 #optionally save knn_indices; 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn); 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], ; 273 knn_indices.shape[1], bandwidth=bandwidth,; --> 274 											 local_connectivity=local_connectivity); 275 #optional trimming; 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwidth'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:1056,Deployability,install,install,1056,"Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```; scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1; pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Cmds:. ```py; import scanpy.external as sce; sce.pp.bbknn(adata, batch_key='sample', copy=False); ```. Error info:. ```pytb; sce.pp.bbknn(adata, batch_key='sample', copy=False); computing batch balanced neighbors; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-34-5b7ebd13c9e6> in <module>; 1 # Correct; 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'); ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 except ImportError:; 83 raise ImportError('Please install bbknn: `pip install bbknn`.'); ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 215 batch_list = adata.obs[batch_key].values; 216 #call BBKNN proper; --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs); 218 #optionally save knn_indices; 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn); 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], ; 273 knn_indices.shape[1], bandwidth=bandwidth,; --> 274 											 local_connectivity=local_connectivity); 275 #optional trimming; 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwidth'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:262,Usability,learn,learn,262,"Hi all:. it seems there is a problem on the batch correction with bbknn. It gives an error at the compute_connectivities_umap() step of bbknn. Version of packages:. ```; scanpy==1.4.2 anndata==0.6.19 umap==0.3.8 numpy==1.15.4 scipy==1.2.1; pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Cmds:. ```py; import scanpy.external as sce; sce.pp.bbknn(adata, batch_key='sample', copy=False); ```. Error info:. ```pytb; sce.pp.bbknn(adata, batch_key='sample', copy=False); computing batch balanced neighbors; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-34-5b7ebd13c9e6> in <module>; 1 # Correct; 2 #sc.pp.pca(adata, n_comps=50, svd_solver='arpack'); ----> 3 sce.pp.bbknn(adata, batch_key='sample', copy=False, n_pcs=15). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 except ImportError:; 83 raise ImportError('Please install bbknn: `pip install bbknn`.'); ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 215 batch_list = adata.obs[batch_key].values; 216 #call BBKNN proper; --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs); 218 #optionally save knn_indices; 219 if save_knn:. ~/miniconda3/lib/python3.6/site-packages/bbknn/__init__.py in bbknn_pca_matrix(pca, batch_list, neighbors_within_batch, n_pcs, trim, approx, n_trees, use_faiss, metric, bandwidth, local_connectivity, save_knn); 272 	dist, cnts = compute_connectivities_umap(knn_indices, knn_distances, knn_indices.shape[0], ; 273 knn_indices.shape[1], bandwidth=bandwidth,; --> 274 											 local_connectivity=local_connectivity); 275 #optional trimming; 276 if trim:. TypeError: compute_connectivities_umap() got an unexpected keyword argument 'bandwidth'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/633:319,Integrability,message,message,319,"I was trying to plot a heatmap using this command:; `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',; use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,; var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:; WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`; /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: ; The maximal number of iterations maxit (set to 20 by the program); allowed for finding a smoothing spline with fp=s has been reached: s; too small.; There is an approximation returned but the corresponding weighted sum; of squared residuals does not satisfy the condition abs(fp-s)/s < tol.; warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:848,Integrability,message,message,848,"I was trying to plot a heatmap using this command:; `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',; use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,; var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:; WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`; /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: ; The maximal number of iterations maxit (set to 20 by the program); allowed for finding a smoothing spline with fp=s has been reached: s; too small.; There is an approximation returned but the corresponding weighted sum; of squared residuals does not satisfy the condition abs(fp-s)/s < tol.; warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/635:53,Integrability,wrap,wrapper,53,"Some of the arguments for bbknn have changed, so the wrapper is broken at the moment. https://github.com/Teichlab/bbknn/issues/10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635
https://github.com/scverse/scanpy/pull/636:172,Deployability,update,update,172,"I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636
https://github.com/scverse/scanpy/pull/636:115,Integrability,wrap,wrapper,115,"I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636
https://github.com/scverse/scanpy/issues/638:473,Energy Efficiency,efficient,efficiently,473,"Dear all,; I am very interested to set my own set of markers and see the expression of those markers in my umap. So I basically want to see expression of multiple signature genes in one plot. since I am new to python and scanpy I am not sure how can it be done and if there is already a function for that.; In R I had a similar situation in which I just took the avg expression of those cells and treated them as on feature and plotted them but here I am not how can it be efficiently done!. Any help would be very appreciated thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638
https://github.com/scverse/scanpy/issues/639:111,Availability,error,error,111,"I am trying to follow this tutorial ; https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,; but some error just happened in the initial QC stage.; After running:; mito_genes = adata.var_names.str.startswith('MT-'); adata.obs['percent_mito'] = np.sum(; adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1; and plot the violinplot,; I just get this graph; ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:; __________________________________________________________________________________________________; [In] np.sum(adata.obs); [Out] n_genes 14918559.0; **percent_mito 0.0**; dtype: float64. __________________________________________________________________________________________________; It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:967,Availability,error,error,967,"I am trying to follow this tutorial ; https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html,; but some error just happened in the initial QC stage.; After running:; mito_genes = adata.var_names.str.startswith('MT-'); adata.obs['percent_mito'] = np.sum(; adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1; and plot the violinplot,; I just get this graph; ![percent_mito](https://user-images.githubusercontent.com/41959955/57546005-6ba47100-738e-11e9-8cb6-c6a18b89f8dd.png). I felt somthing wrong and checked this:; __________________________________________________________________________________________________; [In] np.sum(adata.obs); [Out] n_genes 14918559.0; **percent_mito 0.0**; dtype: float64. __________________________________________________________________________________________________; It seems that scanpy just didn't recognized the mitogene or removed it, but there's nothing wrong when I used Seurat. I have no idea where the error is located and need your help. Thanks in advance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/641:447,Availability,error,error,447,"Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`.; After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```; adata.X /= adata.obs['size_factors'].values[:,None]; sc.pp.log1p(adata); ```; When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```; LinAlgError Traceback (most recent call last); <ipython-input-97-96c692867dde> in <module>; ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X; 102 ; --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)); 104 # now actually compute the dispersion; 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other); 226 ; 227 def __pow__(self, other):; --> 228 return matrix_power(self, other); 229 ; 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603 ;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:458,Availability,error,error,458,"Hej. I have been looking at the single-cell-tutorial repository and tried the `scran` normalization with `R`.; After calculating the size factors in `scran`, I use them to normalize cell-wise my data matrix following the tutorial commands:. ```; adata.X /= adata.obs['size_factors'].values[:,None]; sc.pp.log1p(adata); ```; When I look for highly expressed genes with. `sc.preprocessing.highly_variable_genes(adata, n_top_genes=5000)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```; LinAlgError Traceback (most recent call last); <ipython-input-97-96c692867dde> in <module>; ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X; 102 ; --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)); 104 # now actually compute the dispersion; 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other); 226 ; 227 def __pow__(self, other):; --> 228 return matrix_power(self, other); 229 ; 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603 ;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:2388,Availability,error,error,2388,"00)`. I get this error (the error does not show up when I use the normalization command from `scanpy` instead of the normalization with size factors from `scran`):. ```; LinAlgError Traceback (most recent call last); <ipython-input-97-96c692867dde> in <module>; ----> 1 sc.preprocessing.highly_variable_genes(adultAll, n_top_genes=10000, flavor='cellranger'). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 101 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X; 102 ; --> 103 mean, var = materialize_as_ndarray(_get_mean_var(X)); 104 # now actually compute the dispersion; 105 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in __pow__(self, other); 226 ; 227 def __pow__(self, other):; --> 228 return matrix_power(self, other); 229 ; 230 def __ipow__(self, other):. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603 ; 604 try:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216 ; 217 def _assertFinite(*arrays):. LinAlgError: Last 2 dimensions of the array must be square; ```. Do you have any hints? I am trying to find the error but so far I have been unsuccessful.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/643:63,Availability,error,error,63,"Hi,. When I ran; ```; import scanpy.api as sc; ```; I met this error:; ```; 19 from scipy import sparse; 20 from scipy.sparse import issparse; ---> 21 from scipy.sparse.sputils import IndexMixin; 22 from natsort import natsorted; 23 . ImportError: cannot import name 'IndexMixin'; ```; Is there any requirements for the version of scipy?. Thanks in advance,; BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/pull/644:332,Testability,test,test,332,"I have modified the following files in `scanpy` version `1.4`. 1. _anndata.py (added support for weighted sampling data , where each row has its non-zero weight, changes made for dotPlot, violinPlot and heatmap). 2. _rank_genes_groups ( To find marker genes for data where each row has different non-zero weight, I have modified 't-test' and 'wilcoxon'). Suggestion : . For weighted sampling data one can modify the PCA as well, I used matlab pca for weighted data. Thanks; Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/issues/645:25,Availability,error,error,25,"Hi,; I'm encountering an error when trying to write result file, after perform cell cycle score.; After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]; s_genes=[g for g in cc_genes[:43] if g in adata.var_names]; g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes); `. The field 'phase' of the obs. matrix is of type object:; `adata.obs.phase.dtypes; dtype('O')`. When I write the annData object, I got the error:; `adata.write(results_file); ... storing 'phase' as categorical; TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:; `adata.obs.phase.dtypes; CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again.; Following my version packages:; `sc.logging.print_versions(); scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot!; Raffaella",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:572,Availability,error,error,572,"Hi,; I'm encountering an error when trying to write result file, after perform cell cycle score.; After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]; s_genes=[g for g in cc_genes[:43] if g in adata.var_names]; g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes); `. The field 'phase' of the obs. matrix is of type object:; `adata.obs.phase.dtypes; dtype('O')`. When I write the annData object, I got the error:; `adata.write(results_file); ... storing 'phase' as categorical; TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:; `adata.obs.phase.dtypes; CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again.; Following my version packages:; `sc.logging.print_versions(); scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot!; Raffaella",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1254,Modifiability,variab,variables,1254,"Hi,; I'm encountering an error when trying to write result file, after perform cell cycle score.; After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]; s_genes=[g for g in cc_genes[:43] if g in adata.var_names]; g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes); `. The field 'phase' of the obs. matrix is of type object:; `adata.obs.phase.dtypes; dtype('O')`. When I write the annData object, I got the error:; `adata.write(results_file); ... storing 'phase' as categorical; TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:; `adata.obs.phase.dtypes; CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again.; Following my version packages:; `sc.logging.print_versions(); scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot!; Raffaella",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:71,Performance,perform,perform,71,"Hi,; I'm encountering an error when trying to write result file, after perform cell cycle score.; After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]; s_genes=[g for g in cc_genes[:43] if g in adata.var_names]; g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes); `. The field 'phase' of the obs. matrix is of type object:; `adata.obs.phase.dtypes; dtype('O')`. When I write the annData object, I got the error:; `adata.write(results_file); ... storing 'phase' as categorical; TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:; `adata.obs.phase.dtypes; CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again.; Following my version packages:; `sc.logging.print_versions(); scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot!; Raffaella",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:146,Performance,perform,perform,146,"Hi,; I'm encountering an error when trying to write result file, after perform cell cycle score.; After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]; s_genes=[g for g in cc_genes[:43] if g in adata.var_names]; g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes); `. The field 'phase' of the obs. matrix is of type object:; `adata.obs.phase.dtypes; dtype('O')`. When I write the annData object, I got the error:; `adata.write(results_file); ... storing 'phase' as categorical; TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:; `adata.obs.phase.dtypes; CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again.; Following my version packages:; `sc.logging.print_versions(); scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot!; Raffaella",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1034,Testability,log,logging,1034,"Hi,; I'm encountering an error when trying to write result file, after perform cell cycle score.; After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]; s_genes=[g for g in cc_genes[:43] if g in adata.var_names]; g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes); `. The field 'phase' of the obs. matrix is of type object:; `adata.obs.phase.dtypes; dtype('O')`. When I write the annData object, I got the error:; `adata.write(results_file); ... storing 'phase' as categorical; TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:; `adata.obs.phase.dtypes; CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again.; Following my version packages:; `sc.logging.print_versions(); scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot!; Raffaella",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:1151,Usability,learn,learn,1151,"Hi,; I'm encountering an error when trying to write result file, after perform cell cycle score.; After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]; s_genes=[g for g in cc_genes[:43] if g in adata.var_names]; g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes); `. The field 'phase' of the obs. matrix is of type object:; `adata.obs.phase.dtypes; dtype('O')`. When I write the annData object, I got the error:; `adata.write(results_file); ... storing 'phase' as categorical; TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:; `adata.obs.phase.dtypes; CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again.; Following my version packages:; `sc.logging.print_versions(); scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot!; Raffaella",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/646:844,Availability,error,error,844,"We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:382,Usability,simpl,simple,382,"We typically have some marker information in the form of an Excel sheet, pandas DataFrame, and eventually a Python dictionary. Using these as gene annotations in various plotting functions (not pl.rank_genes_groups_* family but the others) is a very common task and it looks awesome thanks to @fidelram's `var_group_*` parameters. It would be even more fantastic to be able to pass simple dict (e.g. the ones we already use in [Malte's marker_gene_overlap](https://scanpy.readthedocs.io/en/latest/api/scanpy.tl.marker_gene_overlap.html#scanpy.tl.marker_gene_overlap)) to plotting functions where `var_group_positions` and `var_group_labels` are populated automatically. . One caveat is that there might be genes covered by multiple keys, but this is similar to supplying overlapping `var_group_position`s in current api, which can exit with an error. I already have a function for that but it's absolutely super ugly. I can send a PR after tidying it up, but if anyone else wants to do it, it's perfectly fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/649:51,Availability,error,error,51,"Hi, when importing a loom file I get the following error. ```; >>> import scanpy.api as sc; >>> adata = sc.read_loom('/path/to/loom_file.loom'); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py"", line 186, in read_loom; dtype=dtype); File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 672, in __init__; filename=filename, filemode=filemode); File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 850, in _init_as_actual; ['obs_names', 'row_names', 'smp_names']); File ""/opt/conda/lib/python3.7/site-packages/anndata/base.py"", line 287, in _gen_dataframe; columns=[k for k in anno.keys() if k != index_name]); File ""/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py"", line 392, in __init__; mgr = init_dict(data, index, columns, dtype=dtype); File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 212, in init_dict; return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype); File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 56, in arrays_to_mgr; arrays = _homogenize(arrays, index, dtype); File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 277, in _homogenize; raise_cast_failure=False); File ""/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 658, in sanitize_array; raise Exception('Data must be 1-dimensional'); Exception: Data must be 1-dimensional; ```. Does anybody knows the reason why?; Thank you in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/649
https://github.com/scverse/scanpy/issues/650:243,Availability,error,error,243,"If I read a file with read_h5ad() and then process with . `sc.pp.filter_genes(adata, min_cells=int(foo)); `; Things work as intended. But if I change that read line to be read_h5ad(h5_path, backed='r') then when I attempt to filter I get this error instead:. ```; File ""/opt/Python-3.7.3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 228, in filter_genes:; else X > 0, axis=0):; TypeError: '>' not supported between instances of 'SparseDataset' and 'int'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/pull/651:499,Deployability,update,updated,499,"As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations; > ; > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly; > ; > adata_small.obsm['X_model'] = model(adata_small.X); > ; > ingest.neighbors(adata_small) # adata_small with just 1000 observations; > ; > now, we have the updated neighbors graph with 1,001,000 observations; > we want to do the same things as always; > ; > by leveraging the neighbors of the new data within the old data, ; > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors; > ; > ingest.umap(adata_small); > ; > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster ; > ; > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:878,Deployability,update,update,878,"As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations; > ; > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly; > ; > adata_small.obsm['X_model'] = model(adata_small.X); > ; > ingest.neighbors(adata_small) # adata_small with just 1000 observations; > ; > now, we have the updated neighbors graph with 1,001,000 observations; > we want to do the same things as always; > ; > by leveraging the neighbors of the new data within the old data, ; > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors; > ; > ingest.umap(adata_small); > ; > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster ; > ; > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:266,Security,expose,exposes,266,"As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations; > ; > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly; > ; > adata_small.obsm['X_model'] = model(adata_small.X); > ; > ingest.neighbors(adata_small) # adata_small with just 1000 observations; > ; > now, we have the updated neighbors graph with 1,001,000 observations; > we want to do the same things as always; > ; > by leveraging the neighbors of the new data within the old data, ; > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors; > ; > ingest.umap(adata_small); > ; > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster ; > ; > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:231,Usability,learn,learned,231,"As discussed with @falexwolf this PR introduces a new Ingest class to process new small pieces of data. > sc.pp.neighbors(adata) # adata is huge with 1M observations; > ; > ingest = sc.Ingest(adata) # represents the existing data, learned annotations, structure and exposes it to functionality that allows to ingest new data very quickly; > ; > adata_small.obsm['X_model'] = model(adata_small.X); > ; > ingest.neighbors(adata_small) # adata_small with just 1000 observations; > ; > now, we have the updated neighbors graph with 1,001,000 observations; > we want to do the same things as always; > ; > by leveraging the neighbors of the new data within the old data, ; > map the new data into the embedding (umap), by just computing a correction to the existing embedding: a new data point gets the mean position of its k nearest neighbors; > ; > ingest.umap(adata_small); > ; > update the clustering (mapping the 1000 observations into the existing clusters): a new data point maps into a cluster if the majority of its neighbors is a member of the cluster ; > ; > ingest.louvain(adata_small)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/issues/654:379,Availability,down,downgrading,379,"Hey!. Scanpy does not seem to work correctly together with scikit-learn 0.21.1.; When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference.; ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine.; The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:66,Usability,learn,learn,66,"Hey!. Scanpy does not seem to work correctly together with scikit-learn 0.21.1.; When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference.; ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine.; The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:398,Usability,learn,learn,398,"Hey!. Scanpy does not seem to work correctly together with scikit-learn 0.21.1.; When running the PBMC clustering tutorial (https://github.com/theislab/scanpy-tutorials/blob/master/pbmc3k.ipynb), the produced UMAP plots look very different to the reference.; ![wrong_umap](https://user-images.githubusercontent.com/50872326/58096076-92577880-7bd4-11e9-9383-dda48c4efeac.png). By downgrading scikit-learn to 0.20.0, everything works fine.; The problem seems to arise already at the computation of the neighborhood graph, as the clustering is also different.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/655:111,Testability,log,logging,111,"Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):; ```python; import scanpy as sc; sc.logging.print_versions(); from pypairs import __version__; print('pypairs==', __version__). adata = sc.datasets.blobs(); marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names); ```. ```python; scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; pypairs== v3.1.0. ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-20-33f7b7cad989> in <module>; 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}; 8 ; ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs); 132 ; 133 from pypairs.pairs import cyclone; --> 134 from . import settings; 135 from pypairs import settings as pp_settings; 136 . ImportError: cannot import name 'settings'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/655:499,Usability,learn,learn,499,"Minimal example (scanpy commit: 7266e67fe15a6320bc6d5e1642479f53e44a6d6b):; ```python; import scanpy as sc; sc.logging.print_versions(); from pypairs import __version__; print('pypairs==', __version__). adata = sc.datasets.blobs(); marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}. sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names); ```. ```python; scanpy==0+unknown anndata==0.6.19 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; pypairs== v3.1.0. ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-20-33f7b7cad989> in <module>; 7 marker_pairs = {'G1': [('1', '2')], 'S': [('3', '4')], 'G2M': [('5', '6')]}; 8 ; ----> 9 sc.external.tl.cyclone(adata, marker_pairs, adata.var_names, adata.obs_names). ~/software/scanpy/scanpy/tools/_pypairs.py in cyclone(adata, marker_pairs, gene_names, sample_names, iterations, min_iter, min_pairs); 132 ; 133 from pypairs.pairs import cyclone; --> 134 from . import settings; 135 from pypairs import settings as pp_settings; 136 . ImportError: cannot import name 'settings'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/655
https://github.com/scverse/scanpy/issues/658:168,Availability,redundant,redundant,168,"Hey!. I have recently gotten a quite deeply clinically phenotyped dataset and have been pondering how the metadata should best be stored in an anndata object. It feels redundant to duplicate a label for every cell from the same patient. Instead, one could save patient-level data in `adata.uns` and then have a function that links categories in an obs column to e.g., keys in a dict in `adata.uns`. This would save quite a lot of space in anndata objects if you have a lot of clinical metadata. I'm thinking of this as a hidden function that plotting functions could use instead of just looking for `.obs` columns to plot data. This may be somewhat linked to #619.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:168,Safety,redund,redundant,168,"Hey!. I have recently gotten a quite deeply clinically phenotyped dataset and have been pondering how the metadata should best be stored in an anndata object. It feels redundant to duplicate a label for every cell from the same patient. Instead, one could save patient-level data in `adata.uns` and then have a function that links categories in an obs column to e.g., keys in a dict in `adata.uns`. This would save quite a lot of space in anndata objects if you have a lot of clinical metadata. I'm thinking of this as a hidden function that plotting functions could use instead of just looking for `.obs` columns to plot data. This may be somewhat linked to #619.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/pull/659:22,Deployability,release,released,22,"Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=16):; sc.pp.neighbors(adata); ```. Running on the 130K dataset on a 16 core machine before the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:01:31.54); ```; and with the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:00:32.02); ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:182,Integrability,wrap,wrap,182,"Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=16):; sc.pp.neighbors(adata); ```. Running on the 130K dataset on a 16 core machine before the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:01:31.54); ```; and with the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:00:32.02); ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:58,Performance,multi-thread,multi-threading,58,"Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=16):; sc.pp.neighbors(adata); ```. Running on the 130K dataset on a 16 core machine before the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:01:31.54); ```; and with the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:00:32.02); ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:122,Performance,multi-thread,multi-threading,122,"Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=16):; sc.pp.neighbors(adata); ```. Running on the 130K dataset on a 16 core machine before the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:01:31.54); ```; and with the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:00:32.02); ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/661:1201,Deployability,Update,Update,1201,"This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON; marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], ; 'T-cell': 'CD3D',; 'T-cell CD8+': ['CD8A', 'CD8B'],; 'NK': ['GNLY', 'NKG7'],; 'Myeloid': ['CST3', 'LYZ'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; # use marker genes as dict to group them; ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'); ```; ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1221,Integrability,depend,dependencies,1221,"This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON; marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], ; 'T-cell': 'CD3D',; 'T-cell CD8+': ['CD8A', 'CD8B'],; 'NK': ['GNLY', 'NKG7'],; 'Myeloid': ['CST3', 'LYZ'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; # use marker genes as dict to group them; ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'); ```; ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1160,Modifiability,extend,extend,1160,"This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON; marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], ; 'T-cell': 'CD3D',; 'T-cell CD8+': ['CD8A', 'CD8B'],; 'NK': ['GNLY', 'NKG7'],; 'Myeloid': ['CST3', 'LYZ'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; # use marker genes as dict to group them; ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'); ```; ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1135,Safety,avoid,avoid,1135,"This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON; marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], ; 'T-cell': 'CD3D',; 'T-cell CD8+': ['CD8A', 'CD8B'],; 'NK': ['GNLY', 'NKG7'],; 'Myeloid': ['CST3', 'LYZ'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; # use marker genes as dict to group them; ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'); ```; ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1291,Testability,test,tests,1291,"This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON; marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], ; 'T-cell': 'CD3D',; 'T-cell CD8+': ['CD8A', 'CD8B'],; 'NK': ['GNLY', 'NKG7'],; 'Myeloid': ['CST3', 'LYZ'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; # use marker genes as dict to group them; ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'); ```; ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/issues/662:354,Availability,error,error,354,"Hi, . Im using scanpy 1.4.2 to analyze my data, using the following command:. `sc.pp.highly_variable_genes(heart_cmc, flavor = 'cell_ranger', n_top_genes = 1000)`. However, instead of getting 1000 HVG, it reports 1488 HVG. Similar thing happens with higher numbers of HVG (e.g. `n_top_genes = 2000` returns 1999). The scaling then fails with a following error:; _ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported._. Any suggestions on how to fix it? When I dont specify n_top_genes, the thing runs without problems.; Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/663:595,Availability,error,error,595,"Dear all; I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error!. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems?. Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:269,Deployability,update,update,269,"Dear all; I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error!. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems?. Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:442,Usability,learn,learn,442,"Dear all; I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error!. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems?. Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:696,Usability,learn,learn,696,"Dear all; I would like to project my umap from scanpy in 3d but I have faced the following problem:. > ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (816,4). It's very strange because before I update some of my packages, I could run it it with no problem with the following packages:. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1 . but after updating some of my packages it was not possible due to that error!. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1. Should I roll back to the previous version of annadata or scanpy? has anyone ran this feature with my package version with no problems?. Thanks a lot. Here are the packages I use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/666:25,Availability,error,error,25,"Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb; sc.tl.umap(adata, init_pos='paga'). computing UMAP; using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------; TypingError Traceback (most recent call last); <ipython-input-35-924452b37e5b> in <module>; ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy); 137 neigh_params.get('metric', 'euclidean'),; 138 neigh_params.get('metric_kwds', {}),; --> 139 verbose=max(0, verbosity-3)); 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates; 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose); 984 initial_alpha,; 985 negative_sample_rate,; --> 986 verbose=verbose,; 987 ); 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 348 e.patch_message(msg); 349 ; --> 350 error_rewrite(e, 'typing'); 351 except errors.UnsupportedError as e:; 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise v",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:111,Availability,error,error,111,"Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb; sc.tl.umap(adata, init_pos='paga'). computing UMAP; using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------; TypingError Traceback (most recent call last); <ipython-input-35-924452b37e5b> in <module>; ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy); 137 neigh_params.get('metric', 'euclidean'),; 138 neigh_params.get('metric_kwds', {}),; --> 139 verbose=max(0, verbosity-3)); 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates; 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose); 984 initial_alpha,; 985 negative_sample_rate,; --> 986 verbose=verbose,; 987 ); 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 348 e.patch_message(msg); 349 ; --> 350 error_rewrite(e, 'typing'); 351 except errors.UnsupportedError as e:; 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise v",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:185,Availability,error,error,185,"Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb; sc.tl.umap(adata, init_pos='paga'). computing UMAP; using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------; TypingError Traceback (most recent call last); <ipython-input-35-924452b37e5b> in <module>; ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy); 137 neigh_params.get('metric', 'euclidean'),; 138 neigh_params.get('metric_kwds', {}),; --> 139 verbose=max(0, verbosity-3)); 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates; 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose); 984 initial_alpha,; 985 negative_sample_rate,; --> 986 verbose=verbose,; 987 ); 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 348 e.patch_message(msg); 349 ; --> 350 error_rewrite(e, 'typing'); 351 except errors.UnsupportedError as e:; 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise v",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:275,Availability,error,error,275,"Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb; sc.tl.umap(adata, init_pos='paga'). computing UMAP; using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------; TypingError Traceback (most recent call last); <ipython-input-35-924452b37e5b> in <module>; ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy); 137 neigh_params.get('metric', 'euclidean'),; 138 neigh_params.get('metric_kwds', {}),; --> 139 verbose=max(0, verbosity-3)); 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates; 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose); 984 initial_alpha,; 985 negative_sample_rate,; --> 986 verbose=verbose,; 987 ); 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 348 e.patch_message(msg); 349 ; --> 350 error_rewrite(e, 'typing'); 351 except errors.UnsupportedError as e:; 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise v",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:1571,Availability,error,errors,1571,"ule>; ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy); 137 neigh_params.get('metric', 'euclidean'),; 138 neigh_params.get('metric_kwds', {}),; --> 139 verbose=max(0, verbosity-3)); 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates; 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose); 984 initial_alpha,; 985 negative_sample_rate,; --> 986 verbose=verbose,; 987 ); 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 348 e.patch_message(msg); 349 ; --> 350 error_rewrite(e, 'typing'); 351 except errors.UnsupportedError as e:; 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). Fi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3089,Availability,error,errors,3089,"thon3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new; ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization; 2. PCA, bbknn, louvain; 3. combat, HVG, PCA, UMAP (works well); 4. Paga (with louvain from 2., works well); 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed?; Best,; Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3280,Availability,error,error,3280,"thon3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new; ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization; 2. PCA, bbknn, louvain; 3. combat, HVG, PCA, UMAP (works well); 4. Paga (with louvain from 2., works well); 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed?; Best,; Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2086,Deployability,pipeline,pipeline,2086," metric, metric_kwds, verbose); 984 initial_alpha,; 985 negative_sample_rate,; --> 986 verbose=verbose,; 987 ); 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 348 e.patch_message(msg); 349 ; --> 350 error_rewrite(e, 'typing'); 351 except errors.UnsupportedError as e:; 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please repor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2890,Deployability,release,release,2890,"thon3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new; ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization; 2. PCA, bbknn, louvain; 3. combat, HVG, PCA, UMAP (works well); 4. Paga (with louvain from 2., works well); 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed?; Best,; Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3286,Integrability,message,message,3286,"thon3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new; ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization; 2. PCA, bbknn, louvain; 3. combat, HVG, PCA, UMAP (works well); 4. Paga (with louvain from 2., works well); 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed?; Best,; Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:2339,Modifiability,parameteriz,parameterized,2339," metric, metric_kwds, verbose); 984 initial_alpha,; 985 negative_sample_rate,; --> 986 verbose=verbose,; 987 ); 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 348 e.patch_message(msg); 349 ; --> 350 error_rewrite(e, 'typing'); 351 except errors.UnsupportedError as e:; 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please repor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:984,Testability,log,logg,984,"Hi Alex,. UMAP throws an error if I use `scanpy.tl.ump` with initial positions from `sc.tl.paga`. Based on the error (see below) I thought it was a problem of UMAP itself. However, the error is not thrown when called without initial positions from paga. Here is the output / error:. ```pytb; sc.tl.umap(adata, init_pos='paga'). computing UMAP; using 'X_pca' with n_pcs = 50. ---------------------------------------------------------------------------; TypingError Traceback (most recent call last); <ipython-input-35-924452b37e5b> in <module>; ----> 1 sc.tl.umap(adata, init_pos='paga'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy); 137 neigh_params.get('metric', 'euclidean'),; 138 neigh_params.get('metric_kwds', {}),; --> 139 verbose=max(0, verbosity-3)); 140 adata.obsm['X_umap'] = X_umap # annotate samples with UMAP coordinates; 141 logg.info(' finished', time=True, end=' ' if _settings_verbosity_greater_or_equal_than(3) else '\n'). /opt/conda/lib/python3.7/site-packages/umap/umap_.py in simplicial_set_embedding(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, verbose); 984 initial_alpha,; 985 negative_sample_rate,; --> 986 verbose=verbose,; 987 ); 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 348 e.patch_message(msg); 349 ; --> 350 error_rewrite(e, 'typing'); 351 except errors.UnsupportedError as e:; 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise v",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:3469,Testability,log,logarithmization,3469,"thon3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new; ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization; 2. PCA, bbknn, louvain; 3. combat, HVG, PCA, UMAP (works well); 4. Paga (with louvain from 2., works well); 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed?; Best,; Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/667:405,Modifiability,variab,variable,405,"One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```; sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/668:400,Availability,error,error,400,"Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`; `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help!. Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/668:320,Performance,perform,perform,320,"Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`; `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help!. Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668
https://github.com/scverse/scanpy/issues/669:1239,Integrability,wrap,wrapper,1239,"Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets?. After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:; `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`; I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper.; Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect?. Do you think this is the right way to do this and could you point me in the right direction to solve this?; I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:862,Testability,log,logreg,862,"Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets?. After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:; `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`; I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper.; Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect?. Do you think this is the right way to do this and could you point me in the right direction to solve this?; I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:944,Testability,log,logistic,944,"Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets?. After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:; `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`; I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper.; Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect?. Do you think this is the right way to do this and could you point me in the right direction to solve this?; I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/670:111,Performance,optimiz,optimize,111,"Hi,. I have few queries regarding scanpy. 1. As scanpy is using Louvain Leiden algorithms for clustering which optimize modularity 'Q', so how we can access and print modularity funciton?. 2. Resolution parameter gave us different number of clusters when we iterated between the best suggested 0.6-1.2. So how we know best number of cluster and how we can choose the optimum value of parameter 'resolution'?. Thanks,; Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:150,Security,access,access,150,"Hi,. I have few queries regarding scanpy. 1. As scanpy is using Louvain Leiden algorithms for clustering which optimize modularity 'Q', so how we can access and print modularity funciton?. 2. Resolution parameter gave us different number of clusters when we iterated between the best suggested 0.6-1.2. So how we know best number of cluster and how we can choose the optimum value of parameter 'resolution'?. Thanks,; Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/671:88,Testability,stub,stub,88,"Hi,; the parameter `log_transformed` looks as it's not used inside the method.; Is it a stub or is it used in other ways?. https://github.com/theislab/scanpy/blob/9b71dab77768fe0eb8b86aed473bee76b3aefd8e/scanpy/tools/_rank_genes_groups.py#L28",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/671
https://github.com/scverse/scanpy/issues/674:48,Integrability,message,messages,48,"Hello everyone,; I have faced the below warning messages during running `sc.tl.rank_genes_groups` with parameters t-test and t-test overestimated. > RuntimeWarning: invalid value encountered in greater; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less_equal; cond2 = cond0 & (x <= self.a). I found exactly the same warning message in a forum but there the warning message was coming from auto-sklearn package and since there is no auto-sklearn used in scanpy and instead we got scikit-learn I am wondering is it a problem with the version that I am using or is it a bug from scikit or scanpy it self. my versions are:. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/674
https://github.com/scverse/scanpy/issues/674:451,Integrability,message,message,451,"Hello everyone,; I have faced the below warning messages during running `sc.tl.rank_genes_groups` with parameters t-test and t-test overestimated. > RuntimeWarning: invalid value encountered in greater; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less_equal; cond2 = cond0 & (x <= self.a). I found exactly the same warning message in a forum but there the warning message was coming from auto-sklearn package and since there is no auto-sklearn used in scanpy and instead we got scikit-learn I am wondering is it a problem with the version that I am using or is it a bug from scikit or scanpy it self. my versions are:. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/674
https://github.com/scverse/scanpy/issues/674:492,Integrability,message,message,492,"Hello everyone,; I have faced the below warning messages during running `sc.tl.rank_genes_groups` with parameters t-test and t-test overestimated. > RuntimeWarning: invalid value encountered in greater; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less_equal; cond2 = cond0 & (x <= self.a). I found exactly the same warning message in a forum but there the warning message was coming from auto-sklearn package and since there is no auto-sklearn used in scanpy and instead we got scikit-learn I am wondering is it a problem with the version that I am using or is it a bug from scikit or scanpy it self. my versions are:. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/674
https://github.com/scverse/scanpy/issues/674:116,Testability,test,test,116,"Hello everyone,; I have faced the below warning messages during running `sc.tl.rank_genes_groups` with parameters t-test and t-test overestimated. > RuntimeWarning: invalid value encountered in greater; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less_equal; cond2 = cond0 & (x <= self.a). I found exactly the same warning message in a forum but there the warning message was coming from auto-sklearn package and since there is no auto-sklearn used in scanpy and instead we got scikit-learn I am wondering is it a problem with the version that I am using or is it a bug from scikit or scanpy it self. my versions are:. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/674
https://github.com/scverse/scanpy/issues/674:127,Testability,test,test,127,"Hello everyone,; I have faced the below warning messages during running `sc.tl.rank_genes_groups` with parameters t-test and t-test overestimated. > RuntimeWarning: invalid value encountered in greater; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less_equal; cond2 = cond0 & (x <= self.a). I found exactly the same warning message in a forum but there the warning message was coming from auto-sklearn package and since there is no auto-sklearn used in scanpy and instead we got scikit-learn I am wondering is it a problem with the version that I am using or is it a bug from scikit or scanpy it self. my versions are:. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/674
https://github.com/scverse/scanpy/issues/674:613,Usability,learn,learn,613,"Hello everyone,; I have faced the below warning messages during running `sc.tl.rank_genes_groups` with parameters t-test and t-test overestimated. > RuntimeWarning: invalid value encountered in greater; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less_equal; cond2 = cond0 & (x <= self.a). I found exactly the same warning message in a forum but there the warning message was coming from auto-sklearn package and since there is no auto-sklearn used in scanpy and instead we got scikit-learn I am wondering is it a problem with the version that I am using or is it a bug from scikit or scanpy it self. my versions are:. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/674
https://github.com/scverse/scanpy/issues/674:840,Usability,learn,learn,840,"Hello everyone,; I have faced the below warning messages during running `sc.tl.rank_genes_groups` with parameters t-test and t-test overestimated. > RuntimeWarning: invalid value encountered in greater; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less_equal; cond2 = cond0 & (x <= self.a). I found exactly the same warning message in a forum but there the warning message was coming from auto-sklearn package and since there is no auto-sklearn used in scanpy and instead we got scikit-learn I am wondering is it a problem with the version that I am using or is it a bug from scikit or scanpy it self. my versions are:. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/674
https://github.com/scverse/scanpy/pull/676:503,Integrability,message,message,503,"Here we go. ~~It’s not done yet~~:. - [x] The timing stuff isn’t yet implemented. I think we just do timing stuff when doing “info” level logging, correct?. If yes, we probably just need to override `RootLogger.info` so it sets `self.handlers[0].formatter.passed_time = kwargs.get('t', False)`. - [x] All the places hackily using `_settings_verbosity_greater_or_equal_than(2|3)` have to work differently. I propose that we just add a kwarg `deepinfo: str` or so which only adds the passed string to the message if the active verbosity is higher than the function’s (i.e. calling `logging.warn('foo', deepinfo='bar')` and the loglevel/verbosity is 'INFO' or noisier adds 'bar'). - [x] We now use vanilla log function syntax, so no more using it like `print`. We switched to Python 3.6+ though, so I propose f-strings everywhere! That looks better and works. Fixes #256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:138,Testability,log,logging,138,"Here we go. ~~It’s not done yet~~:. - [x] The timing stuff isn’t yet implemented. I think we just do timing stuff when doing “info” level logging, correct?. If yes, we probably just need to override `RootLogger.info` so it sets `self.handlers[0].formatter.passed_time = kwargs.get('t', False)`. - [x] All the places hackily using `_settings_verbosity_greater_or_equal_than(2|3)` have to work differently. I propose that we just add a kwarg `deepinfo: str` or so which only adds the passed string to the message if the active verbosity is higher than the function’s (i.e. calling `logging.warn('foo', deepinfo='bar')` and the loglevel/verbosity is 'INFO' or noisier adds 'bar'). - [x] We now use vanilla log function syntax, so no more using it like `print`. We switched to Python 3.6+ though, so I propose f-strings everywhere! That looks better and works. Fixes #256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:580,Testability,log,logging,580,"Here we go. ~~It’s not done yet~~:. - [x] The timing stuff isn’t yet implemented. I think we just do timing stuff when doing “info” level logging, correct?. If yes, we probably just need to override `RootLogger.info` so it sets `self.handlers[0].formatter.passed_time = kwargs.get('t', False)`. - [x] All the places hackily using `_settings_verbosity_greater_or_equal_than(2|3)` have to work differently. I propose that we just add a kwarg `deepinfo: str` or so which only adds the passed string to the message if the active verbosity is higher than the function’s (i.e. calling `logging.warn('foo', deepinfo='bar')` and the loglevel/verbosity is 'INFO' or noisier adds 'bar'). - [x] We now use vanilla log function syntax, so no more using it like `print`. We switched to Python 3.6+ though, so I propose f-strings everywhere! That looks better and works. Fixes #256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:625,Testability,log,loglevel,625,"Here we go. ~~It’s not done yet~~:. - [x] The timing stuff isn’t yet implemented. I think we just do timing stuff when doing “info” level logging, correct?. If yes, we probably just need to override `RootLogger.info` so it sets `self.handlers[0].formatter.passed_time = kwargs.get('t', False)`. - [x] All the places hackily using `_settings_verbosity_greater_or_equal_than(2|3)` have to work differently. I propose that we just add a kwarg `deepinfo: str` or so which only adds the passed string to the message if the active verbosity is higher than the function’s (i.e. calling `logging.warn('foo', deepinfo='bar')` and the loglevel/verbosity is 'INFO' or noisier adds 'bar'). - [x] We now use vanilla log function syntax, so no more using it like `print`. We switched to Python 3.6+ though, so I propose f-strings everywhere! That looks better and works. Fixes #256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:703,Testability,log,log,703,"Here we go. ~~It’s not done yet~~:. - [x] The timing stuff isn’t yet implemented. I think we just do timing stuff when doing “info” level logging, correct?. If yes, we probably just need to override `RootLogger.info` so it sets `self.handlers[0].formatter.passed_time = kwargs.get('t', False)`. - [x] All the places hackily using `_settings_verbosity_greater_or_equal_than(2|3)` have to work differently. I propose that we just add a kwarg `deepinfo: str` or so which only adds the passed string to the message if the active verbosity is higher than the function’s (i.e. calling `logging.warn('foo', deepinfo='bar')` and the loglevel/verbosity is 'INFO' or noisier adds 'bar'). - [x] We now use vanilla log function syntax, so no more using it like `print`. We switched to Python 3.6+ though, so I propose f-strings everywhere! That looks better and works. Fixes #256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/issues/677:125,Availability,error,error,125,"Hi,; I am using umap in 3d. I want to plot all 3 components, so I use the `components='all'` parameter.; However, I get this error:. ```python. > sc.pl.umap(adata_g, color=[key_added], cmap='viridis', projection='3d', components='all'). ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); in ; 1 #%%; ----> 2 sc.pl.umap(adata_g, color=[key_added], cmap='viridis', projection='3d', components='all'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs); 282 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 283 """"""; --> 284 return plot_scatter(adata, 'umap', **kwargs); 285 ; 286 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 191 if projection == '3d':; 192 cax = ax.scatter(; --> 193 _data_points[:, 0], _data_points[:, 1], _data_points[:, 2],; 194 marker=""."", c=color_vector, rasterized=settings._vector_friendly,; 195 **kwargs,. IndexError: index 2 is out of bounds for axis 1 with size 2; ```. I was able to plot in 3d by changing it to the following method signature:; ```python; > sc.pl.umap(adata_g, color=[key_added], cmap='viridis', projection='3d', components=['1,2,3']); ```. Probably the documentation should be updated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/677
https://github.com/scverse/scanpy/issues/677:1701,Deployability,update,updated,1701,"Hi,; I am using umap in 3d. I want to plot all 3 components, so I use the `components='all'` parameter.; However, I get this error:. ```python. > sc.pl.umap(adata_g, color=[key_added], cmap='viridis', projection='3d', components='all'). ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); in ; 1 #%%; ----> 2 sc.pl.umap(adata_g, color=[key_added], cmap='viridis', projection='3d', components='all'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs); 282 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 283 """"""; --> 284 return plot_scatter(adata, 'umap', **kwargs); 285 ; 286 . ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 191 if projection == '3d':; 192 cax = ax.scatter(; --> 193 _data_points[:, 0], _data_points[:, 1], _data_points[:, 2],; 194 marker=""."", c=color_vector, rasterized=settings._vector_friendly,; 195 **kwargs,. IndexError: index 2 is out of bounds for axis 1 with size 2; ```. I was able to plot in 3d by changing it to the following method signature:; ```python; > sc.pl.umap(adata_g, color=[key_added], cmap='viridis', projection='3d', components=['1,2,3']); ```. Probably the documentation should be updated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/677
https://github.com/scverse/scanpy/issues/680:264,Deployability,integrat,integrating,264,"Hello, . I have managed to get my Seurat object converted into Loom and then read into Scanpy. Now my main objective is to use the clusters identified using Seurat in order to create a PAGA trajectory map. I was able to do a similar thing for Seurat -> Monocle by integrating the Seurat clusters and allow Monocle to perform a trajectory analysis on them. . I have the following Scanpy object:; ![scanpy_adata](https://user-images.githubusercontent.com/11708268/58907732-7a075380-86d4-11e9-9f2a-4c539ea58c80.png). All the cluster information along with cell ids are present in the obs part of the Scanpy object. Is there anyway to use that information in order to perform a PAGA trajectory analysis?. Thank you,; Behram",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:264,Integrability,integrat,integrating,264,"Hello, . I have managed to get my Seurat object converted into Loom and then read into Scanpy. Now my main objective is to use the clusters identified using Seurat in order to create a PAGA trajectory map. I was able to do a similar thing for Seurat -> Monocle by integrating the Seurat clusters and allow Monocle to perform a trajectory analysis on them. . I have the following Scanpy object:; ![scanpy_adata](https://user-images.githubusercontent.com/11708268/58907732-7a075380-86d4-11e9-9f2a-4c539ea58c80.png). All the cluster information along with cell ids are present in the obs part of the Scanpy object. Is there anyway to use that information in order to perform a PAGA trajectory analysis?. Thank you,; Behram",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:317,Performance,perform,perform,317,"Hello, . I have managed to get my Seurat object converted into Loom and then read into Scanpy. Now my main objective is to use the clusters identified using Seurat in order to create a PAGA trajectory map. I was able to do a similar thing for Seurat -> Monocle by integrating the Seurat clusters and allow Monocle to perform a trajectory analysis on them. . I have the following Scanpy object:; ![scanpy_adata](https://user-images.githubusercontent.com/11708268/58907732-7a075380-86d4-11e9-9f2a-4c539ea58c80.png). All the cluster information along with cell ids are present in the obs part of the Scanpy object. Is there anyway to use that information in order to perform a PAGA trajectory analysis?. Thank you,; Behram",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:664,Performance,perform,perform,664,"Hello, . I have managed to get my Seurat object converted into Loom and then read into Scanpy. Now my main objective is to use the clusters identified using Seurat in order to create a PAGA trajectory map. I was able to do a similar thing for Seurat -> Monocle by integrating the Seurat clusters and allow Monocle to perform a trajectory analysis on them. . I have the following Scanpy object:; ![scanpy_adata](https://user-images.githubusercontent.com/11708268/58907732-7a075380-86d4-11e9-9f2a-4c539ea58c80.png). All the cluster information along with cell ids are present in the obs part of the Scanpy object. Is there anyway to use that information in order to perform a PAGA trajectory analysis?. Thank you,; Behram",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/681:142,Integrability,depend,dependencies,142,"Hi. I have a problem with reproducing my analysis using scanpy 1.4.3. I have used two different machines using the same software version (all dependencies as well), and I keep having different results using the same commands and seed numbers. In one machine I get this:. <img width=""340"" alt=""Screenshot 2019-06-05 at 15 44 31"" src=""https://user-images.githubusercontent.com/3297906/58965800-13a03500-87a9-11e9-8a7c-bec104f4718e.png"">. and in the other I get this:. <img width=""334"" alt=""Screenshot 2019-06-05 at 15 44 14"" src=""https://user-images.githubusercontent.com/3297906/58965827-1e5aca00-87a9-11e9-95e5-90c09303a1c2.png"">. Can you recommend which aspects to check? or how to deal with this issue?. Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/pull/683:50,Testability,test,test,50,Filtered warning for when all zero genes have a t-test run on them. There may be a more elegant solution for this.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/683
https://github.com/scverse/scanpy/issues/684:13,Testability,log,logging,13,"The rehauled logging module tends to clutter output, see, for instance; ```; import scanpy as sc; sc.settings.verbosity = 2; adata_sc = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(adata_sc); ```; which produces; ![image](https://user-images.githubusercontent.com/16916678/59159145-a56bb300-8ac5-11e9-8371-170995e43dfa.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/687:211,Availability,error,error,211,"Hi there. Love the package. I was working in a google colab notebook, and installed scanpy as 'pip install scanpy'. Then when using the method: sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). I got this error:; ----------------------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-54-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). 5 frames; /usr/local/lib/python3.6/dist-packages/statsmodels/distributions/edgeworth.py in <module>(); 5 import numpy as np; 6 from numpy.polynomial.hermite_e import HermiteE; ----> 7 from scipy.misc import factorial; 8 from scipy.stats import rv_continuous; 9 import scipy.special as special. ImportError: cannot import name 'factorial'; ----------------------------------------------------------------------------------------. Seems that scipy.misc.factorial is depreciated and is now under scipy.special.factorial. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:74,Deployability,install,installed,74,"Hi there. Love the package. I was working in a google colab notebook, and installed scanpy as 'pip install scanpy'. Then when using the method: sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). I got this error:; ----------------------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-54-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). 5 frames; /usr/local/lib/python3.6/dist-packages/statsmodels/distributions/edgeworth.py in <module>(); 5 import numpy as np; 6 from numpy.polynomial.hermite_e import HermiteE; ----> 7 from scipy.misc import factorial; 8 from scipy.stats import rv_continuous; 9 import scipy.special as special. ImportError: cannot import name 'factorial'; ----------------------------------------------------------------------------------------. Seems that scipy.misc.factorial is depreciated and is now under scipy.special.factorial. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:99,Deployability,install,install,99,"Hi there. Love the package. I was working in a google colab notebook, and installed scanpy as 'pip install scanpy'. Then when using the method: sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). I got this error:; ----------------------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-54-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). 5 frames; /usr/local/lib/python3.6/dist-packages/statsmodels/distributions/edgeworth.py in <module>(); 5 import numpy as np; 6 from numpy.polynomial.hermite_e import HermiteE; ----> 7 from scipy.misc import factorial; 8 from scipy.stats import rv_continuous; 9 import scipy.special as special. ImportError: cannot import name 'factorial'; ----------------------------------------------------------------------------------------. Seems that scipy.misc.factorial is depreciated and is now under scipy.special.factorial. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/688:2481,Availability,error,errors,2481,"ocker pull dynverse/ti_paga_issue. # enter the container ; docker run --entrypoint bash -it dynverse/ti_paga_issue. # create an example dataset and save it at /input.h5; /code/example.sh /input.h5. # enter python; python; ```; Inside python; ```python; import dynclipy; task = dynclipy.main([""--dataset"", ""/input.h5"", ""--output"", ""/output.h5""]). import scanpy.api as sc; import anndata. counts = task[""counts""]. adata = anndata.AnnData(counts); sc.pp.recipe_zheng17(adata, n_top_genes=101); sc.tl.pca(adata, n_comps=50); sc.pp.neighbors(adata, n_neighbors=15); ```; Which generates the following warning:; ```; /usr/local/lib/python3.7/site-packages/umap/umap_.py:349: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""usr/local/lib/python3.7/site-packages/umap/umap_.py"", line 467:; def fuzzy_simplicial_set(; <source elided>; if knn_indices is None or knn_dists is None:; knn_indices, knn_dists, _ = nearest_neighbors(; ^. @numba.jit(); /usr/local/lib/python3.7/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""usr/local/lib/python3.7/site-packages/umap/umap_.py"", line 350:; @numba.jit(); def fuzzy_simplicial_set(; ^. self.func_ir.loc)); /usr/local/lib/python3.7/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""usr/local/lib/python3.7/site-packages/umap/umap_.py"", line 350:; @numba.jit(); def fuzzy_simplicial_set(; ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:2154,Safety,detect,detected,2154,"ocker pull dynverse/ti_paga_issue. # enter the container ; docker run --entrypoint bash -it dynverse/ti_paga_issue. # create an example dataset and save it at /input.h5; /code/example.sh /input.h5. # enter python; python; ```; Inside python; ```python; import dynclipy; task = dynclipy.main([""--dataset"", ""/input.h5"", ""--output"", ""/output.h5""]). import scanpy.api as sc; import anndata. counts = task[""counts""]. adata = anndata.AnnData(counts); sc.pp.recipe_zheng17(adata, n_top_genes=101); sc.tl.pca(adata, n_comps=50); sc.pp.neighbors(adata, n_neighbors=15); ```; Which generates the following warning:; ```; /usr/local/lib/python3.7/site-packages/umap/umap_.py:349: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""usr/local/lib/python3.7/site-packages/umap/umap_.py"", line 467:; def fuzzy_simplicial_set(; <source elided>; if knn_indices is None or knn_dists is None:; knn_indices, knn_dists, _ = nearest_neighbors(; ^. @numba.jit(); /usr/local/lib/python3.7/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""usr/local/lib/python3.7/site-packages/umap/umap_.py"", line 350:; @numba.jit(); def fuzzy_simplicial_set(; ^. self.func_ir.loc)); /usr/local/lib/python3.7/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""usr/local/lib/python3.7/site-packages/umap/umap_.py"", line 350:; @numba.jit(); def fuzzy_simplicial_set(; ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:316,Usability,simpl,simplicity,316,"With the most recent version of scanpy, I'm getting the above mentioned warning, which causes the [`dynverse/ti_paga`](https://travis-ci.org/dynverse/ti_paga/builds/544731805) builds to fail. . The problem can be replicated by building the `dynverse/ti_paga` container manually and trying to run it. For the sake of simplicity, I have built the container for you and pushed it to `dynverse/ti_paga_issue`. ### Minimum reproducible example; Inside terminal:; ```bash; # fetch newest dynverse/ti_paga container in which this problem occurs; docker pull dynverse/ti_paga_issue. # enter the container ; docker run --entrypoint bash -it dynverse/ti_paga_issue. # create an example dataset and save it at /input.h5; /code/example.sh /input.h5. # enter python; python; ```; Inside python; ```python; import dynclipy; task = dynclipy.main([""--dataset"", ""/input.h5"", ""--output"", ""/output.h5""]). import scanpy.api as sc; import anndata. counts = task[""counts""]. adata = anndata.AnnData(counts); sc.pp.recipe_zheng17(adata, n_top_genes=101); sc.tl.pca(adata, n_comps=50); sc.pp.neighbors(adata, n_neighbors=15); ```; Which generates the following warning:; ```; /usr/local/lib/python3.7/site-packages/umap/umap_.py:349: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""usr/local/lib/python3.7/site-packages/umap/umap_.py"", line 467:; def fuzzy_simplicial_set(; <source elided>; if knn_indices is None or knn_dists is None:; knn_indices, knn_dists, _ = nearest_neighbors(; ^. @numba.jit(); /usr/local/lib/python3.7/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""usr/local/lib/python3.7/site-packages/umap/umap_.py"", line 350:; @numba.jit(); def fuzzy_simplicial_set(; ^. self.func_ir.loc)); /usr/local/lib/python3.7/s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/689:100,Performance,perform,perform,100,"Hi,. I found there is 'use_rep' for tool.tsne(), but not for tool.umap().; Is there any solution to perform umap on a selected anndata's obsm?. Thanks in advance,; BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/689
https://github.com/scverse/scanpy/issues/690:256,Modifiability,layers,layers,256,"Hi I got this warning when I used sc.pl.scatter:; ```; .../scanpy/plotting/_anndata.py:311: DeprecationWarning: Use obs_vector instead of _get_obs_array, _get_obs_array will be removed in the future.; x_arr = adata._get_obs_array(x, use_raw=use_raw, layer=layers[0]); .../anndata/base.py:1618: FutureWarning: In a future version of AnnData, access to `.X` by passing `layer='X'` will be removed. Instead pass `layer=None`.; FutureWarning; ```. I would like to know if the plots generated with this warning are correct.; Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/690
https://github.com/scverse/scanpy/issues/690:341,Security,access,access,341,"Hi I got this warning when I used sc.pl.scatter:; ```; .../scanpy/plotting/_anndata.py:311: DeprecationWarning: Use obs_vector instead of _get_obs_array, _get_obs_array will be removed in the future.; x_arr = adata._get_obs_array(x, use_raw=use_raw, layer=layers[0]); .../anndata/base.py:1618: FutureWarning: In a future version of AnnData, access to `.X` by passing `layer='X'` will be removed. Instead pass `layer=None`.; FutureWarning; ```. I would like to know if the plots generated with this warning are correct.; Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/690
https://github.com/scverse/scanpy/issues/691:440,Availability,error,error,440,"Hi,. In scran's findMarkers(), users can set batch as the 'block' in the model, so marker identification will not be influenced by batch effect (https://rdrr.io/bioc/scran/man/findMarkers.html, I found this really useful).; Can I do similar things in scanpy? Thanks!. I tried using [anndata2ri](https://github.com/theislab/anndata2ri) to convert anndata to SingleCellExperiment, so as to still use scran's findMarker(), but always got this error:; ```; ValueError: Converting pandas ""Category"" series to R factor is only possible when categories are strings.; ```; I have checked several columns in my adata.obs, but cannot find the cause. Thanks in advance,; BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/695:408,Availability,error,error,408,"After studying the PAGA tutorial, I tried to apply it to my data (40,000 cells, 8,000 highly variable genes). I ran the following commands:; ```python; sc.pp.neighbors(adata_hvg); sc.tl.louvain(adata_hvg); sc.tl.draw_graph(adata_hvg); ```; Till here, everything works nicely, but then I try to get the PAGA representation:. ```python; sc.tl.paga(adata_hvg, groups=""louvain""); ```. This returns the following error:. ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-249-7cc787ba28f9> in <module>; ----> 1 sc.tl.paga(adata_hvg, groups=""louvain""). ~/miniconda3/envs/wot/lib/python3.6/site-packages/scanpy/tools/_paga.py in paga(adata, groups, use_rna_velocity, model, copy); 93 adata.uns['paga'] = {}; 94 if not use_rna_velocity:; ---> 95 paga.compute_connectivities(); 96 adata.uns['paga']['connectivities'] = paga.connectivities; 97 adata.uns['paga']['connectivities_tree'] = paga.connectivities_tree. ~/miniconda3/envs/wot/lib/python3.6/site-packages/scanpy/tools/_paga.py in compute_connectivities(self); 127 def compute_connectivities(self):; 128 if self._model == 'v1.2':; --> 129 return self._compute_connectivities_v1_2(); 130 elif self._model == 'v1.0':; 131 return self._compute_connectivities_v1_0(). ~/miniconda3/envs/wot/lib/python3.6/site-packages/scanpy/tools/_paga.py in _compute_connectivities_v1_2(self); 161 if scaled_value > 1:; 162 scaled_value = 1; --> 163 connectivities[i, j] = scaled_value; 164 expected_n_edges[i, j] = expected_random_null; 165 # set attributes. ~/miniconda3/envs/wot/lib/python3.6/site-packages/scipy/sparse/_index.py in __setitem__(self, key, x); 67 if x.size != 1:; 68 raise ValueError('Trying to assign a sequence to an item'); ---> 69 self._set_intXint(row, col, x.flat[0]); 70 return; 71 . ~/miniconda3/envs/wot/lib/python3.6/site-packages/scipy/sparse/compressed.py in _set_intXint(self, row, col, x); 795 def _set_intXint(self, row, col, x):; 796 i, j = self._",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/695
https://github.com/scverse/scanpy/issues/695:93,Modifiability,variab,variable,93,"After studying the PAGA tutorial, I tried to apply it to my data (40,000 cells, 8,000 highly variable genes). I ran the following commands:; ```python; sc.pp.neighbors(adata_hvg); sc.tl.louvain(adata_hvg); sc.tl.draw_graph(adata_hvg); ```; Till here, everything works nicely, but then I try to get the PAGA representation:. ```python; sc.tl.paga(adata_hvg, groups=""louvain""); ```. This returns the following error:. ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-249-7cc787ba28f9> in <module>; ----> 1 sc.tl.paga(adata_hvg, groups=""louvain""). ~/miniconda3/envs/wot/lib/python3.6/site-packages/scanpy/tools/_paga.py in paga(adata, groups, use_rna_velocity, model, copy); 93 adata.uns['paga'] = {}; 94 if not use_rna_velocity:; ---> 95 paga.compute_connectivities(); 96 adata.uns['paga']['connectivities'] = paga.connectivities; 97 adata.uns['paga']['connectivities_tree'] = paga.connectivities_tree. ~/miniconda3/envs/wot/lib/python3.6/site-packages/scanpy/tools/_paga.py in compute_connectivities(self); 127 def compute_connectivities(self):; 128 if self._model == 'v1.2':; --> 129 return self._compute_connectivities_v1_2(); 130 elif self._model == 'v1.0':; 131 return self._compute_connectivities_v1_0(). ~/miniconda3/envs/wot/lib/python3.6/site-packages/scanpy/tools/_paga.py in _compute_connectivities_v1_2(self); 161 if scaled_value > 1:; 162 scaled_value = 1; --> 163 connectivities[i, j] = scaled_value; 164 expected_n_edges[i, j] = expected_random_null; 165 # set attributes. ~/miniconda3/envs/wot/lib/python3.6/site-packages/scipy/sparse/_index.py in __setitem__(self, key, x); 67 if x.size != 1:; 68 raise ValueError('Trying to assign a sequence to an item'); ---> 69 self._set_intXint(row, col, x.flat[0]); 70 return; 71 . ~/miniconda3/envs/wot/lib/python3.6/site-packages/scipy/sparse/compressed.py in _set_intXint(self, row, col, x); 795 def _set_intXint(self, row, col, x):; 796 i, j = self._",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/695
https://github.com/scverse/scanpy/issues/698:83,Testability,test,test,83,"Hi,; Thanks a lot for the library. We're having some issues with Wilcoxon-Rank-Sum test in `rank_genes_groups`. And I noticed a suspicious code in the [implementation](https://github.com/theislab/scanpy/blob/9b71dab77768fe0eb8b86aed473bee76b3aefd8e/scanpy/tools/_rank_genes_groups.py#L311):; ```; scores[left:right] = np.sum(ranks.loc[0:n_active, :]); ```. Shouldn't it be `.iloc`?. Additionally, it seems there is no tie correction in the code. I think for sparse data correction this could be an issue. ![image](https://user-images.githubusercontent.com/671660/59773834-9ac3d180-92ae-11e9-9530-b8d271bd58e5.png). There is an [implementation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.tiecorrect.html?highlight=tiecorrect#scipy.stats.tiecorrect) of `tiecorrect` in scipy. Thanks,; Iakov",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/699:46,Testability,log,logging,46,"When I run this. ```; import scanpy as sc. sc.logging.print_versions(). pbmc = sc.datasets.pbmc68k_reduced(); pbmc = pbmc[pbmc.obs['louvain'] == '0', :]; sc.pp.scale(pbmc); ```. I get this. ```; scanpy==1.4.3 anndata==0.6.21 umap==0.3.9 numpy==1.16.2 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0; /stor/home/ericb123/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:1120: RuntimeWarning: invalid value encountered in sqrt; scale = np.sqrt(var); Trying to set attribute `.X` of view, making a copy.; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/stor/home/ericb123/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 860, in scale; scale(adata.X, zero_center=zero_center, max_value=max_value, copy=False); File ""/stor/home/ericb123/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 876, in scale; _scale(X, zero_center); File ""/stor/home/ericb123/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 1126, in _scale; scale[scale == 0] = 1e-12; File ""/stor/home/ericb123/miniconda3/lib/python3.7/site-packages/anndata/base.py"", line 392, in __setitem__; getattr(adata_view, attr_name)[idx] = value; IndexError: boolean index did not match indexed array along dimension 0; dimension is 130 but corresponding boolean dimension is 765; ```. Any idea what's going on?. Thanks,. -Eric",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699
https://github.com/scverse/scanpy/issues/699:286,Usability,learn,learn,286,"When I run this. ```; import scanpy as sc. sc.logging.print_versions(). pbmc = sc.datasets.pbmc68k_reduced(); pbmc = pbmc[pbmc.obs['louvain'] == '0', :]; sc.pp.scale(pbmc); ```. I get this. ```; scanpy==1.4.3 anndata==0.6.21 umap==0.3.9 numpy==1.16.2 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0; /stor/home/ericb123/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py:1120: RuntimeWarning: invalid value encountered in sqrt; scale = np.sqrt(var); Trying to set attribute `.X` of view, making a copy.; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/stor/home/ericb123/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 860, in scale; scale(adata.X, zero_center=zero_center, max_value=max_value, copy=False); File ""/stor/home/ericb123/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 876, in scale; _scale(X, zero_center); File ""/stor/home/ericb123/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 1126, in _scale; scale[scale == 0] = 1e-12; File ""/stor/home/ericb123/miniconda3/lib/python3.7/site-packages/anndata/base.py"", line 392, in __setitem__; getattr(adata_view, attr_name)[idx] = value; IndexError: boolean index did not match indexed array along dimension 0; dimension is 130 but corresponding boolean dimension is 765; ```. Any idea what's going on?. Thanks,. -Eric",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699
https://github.com/scverse/scanpy/issues/700:94,Availability,error,error,94,"If you try to color a scatter plot by gene expression in a layer, but the layer is sparse, an error is thrown. This occurs on both current release and master. Code to reproduce:. ```python; import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(); pbmc.layers[""sparse""] = pbmc.raw.X; sc.pl.pca(pbmc, color=[""HES4""], layer=""sparse"") ; ```. <details>; <summary> Traceback: </summary>. ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-89244dc07987> in <module>; 3 pbmc = sc.datasets.pbmc68k_reduced(); 4 pbmc.layers[""sparse""] = pbmc.raw.X; ----> 5 sc.pl.pca(pbmc, color=[""HES4""], layer=""sparse""). ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 410 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 411 """"""; --> 412 return plot_scatter(adata, 'pca', **kwargs); 413 ; 414 . ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 199 _data_points[:, 0], _data_points[:, 1],; 200 marker=""."", c=color_vector, rasterized=settings._vector_friendly,; --> 201 **kwargs,; 202 ); 203 . ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1587 def inner(ax, *args, data=None, **kwargs):; 1588 if data is None:; -> 1589 return func(ax, *map(sanitize_sequence, args), **kwargs); 1590 ; 1591 bound = new_sig.bind(ax, *args, **kwargs). ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/700
https://github.com/scverse/scanpy/issues/700:139,Deployability,release,release,139,"If you try to color a scatter plot by gene expression in a layer, but the layer is sparse, an error is thrown. This occurs on both current release and master. Code to reproduce:. ```python; import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(); pbmc.layers[""sparse""] = pbmc.raw.X; sc.pl.pca(pbmc, color=[""HES4""], layer=""sparse"") ; ```. <details>; <summary> Traceback: </summary>. ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-89244dc07987> in <module>; 3 pbmc = sc.datasets.pbmc68k_reduced(); 4 pbmc.layers[""sparse""] = pbmc.raw.X; ----> 5 sc.pl.pca(pbmc, color=[""HES4""], layer=""sparse""). ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 410 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 411 """"""; --> 412 return plot_scatter(adata, 'pca', **kwargs); 413 ; 414 . ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 199 _data_points[:, 0], _data_points[:, 1],; 200 marker=""."", c=color_vector, rasterized=settings._vector_friendly,; --> 201 **kwargs,; 202 ); 203 . ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1587 def inner(ax, *args, data=None, **kwargs):; 1588 if data is None:; -> 1589 return func(ax, *map(sanitize_sequence, args), **kwargs); 1590 ; 1591 bound = new_sig.bind(ax, *args, **kwargs). ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/700
https://github.com/scverse/scanpy/issues/700:254,Modifiability,layers,layers,254,"If you try to color a scatter plot by gene expression in a layer, but the layer is sparse, an error is thrown. This occurs on both current release and master. Code to reproduce:. ```python; import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(); pbmc.layers[""sparse""] = pbmc.raw.X; sc.pl.pca(pbmc, color=[""HES4""], layer=""sparse"") ; ```. <details>; <summary> Traceback: </summary>. ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-89244dc07987> in <module>; 3 pbmc = sc.datasets.pbmc68k_reduced(); 4 pbmc.layers[""sparse""] = pbmc.raw.X; ----> 5 sc.pl.pca(pbmc, color=[""HES4""], layer=""sparse""). ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 410 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 411 """"""; --> 412 return plot_scatter(adata, 'pca', **kwargs); 413 ; 414 . ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 199 _data_points[:, 0], _data_points[:, 1],; 200 marker=""."", c=color_vector, rasterized=settings._vector_friendly,; --> 201 **kwargs,; 202 ); 203 . ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1587 def inner(ax, *args, data=None, **kwargs):; 1588 if data is None:; -> 1589 return func(ax, *map(sanitize_sequence, args), **kwargs); 1590 ; 1591 bound = new_sig.bind(ax, *args, **kwargs). ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/700
https://github.com/scverse/scanpy/issues/700:608,Modifiability,layers,layers,608,"If you try to color a scatter plot by gene expression in a layer, but the layer is sparse, an error is thrown. This occurs on both current release and master. Code to reproduce:. ```python; import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(); pbmc.layers[""sparse""] = pbmc.raw.X; sc.pl.pca(pbmc, color=[""HES4""], layer=""sparse"") ; ```. <details>; <summary> Traceback: </summary>. ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-89244dc07987> in <module>; 3 pbmc = sc.datasets.pbmc68k_reduced(); 4 pbmc.layers[""sparse""] = pbmc.raw.X; ----> 5 sc.pl.pca(pbmc, color=[""HES4""], layer=""sparse""). ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 410 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 411 """"""; --> 412 return plot_scatter(adata, 'pca', **kwargs); 413 ; 414 . ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 199 _data_points[:, 0], _data_points[:, 1],; 200 marker=""."", c=color_vector, rasterized=settings._vector_friendly,; --> 201 **kwargs,; 202 ); 203 . ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1587 def inner(ax, *args, data=None, **kwargs):; 1588 if data is None:; -> 1589 return func(ax, *map(sanitize_sequence, args), **kwargs); 1590 ; 1591 bound = new_sig.bind(ax, *args, **kwargs). ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/700
https://github.com/scverse/scanpy/issues/701:348,Availability,down,down,348,"After running `rank_genes_groups` with 100 genes and 30 clusters, the `adata.uns['rank_genes_groups']['pvals_adj']` results in a `100x30` array of p-values. Each column is a cluster, so the first row has the top-scoring genes for each cluster. But if you look at the p-values, some of them are 1. And the p-values do not seem to increase as you go down the rows, but the scores do decrease as you go down the rows. . So if we want to just isolate marker genes that are statistically significant at some threshold, how do we do that?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:400,Availability,down,down,400,"After running `rank_genes_groups` with 100 genes and 30 clusters, the `adata.uns['rank_genes_groups']['pvals_adj']` results in a `100x30` array of p-values. Each column is a cluster, so the first row has the top-scoring genes for each cluster. But if you look at the p-values, some of them are 1. And the p-values do not seem to increase as you go down the rows, but the scores do decrease as you go down the rows. . So if we want to just isolate marker genes that are statistically significant at some threshold, how do we do that?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/pull/703:512,Availability,down,down,512,"Cuts around 1 second off import of scanpy. ```sh; isaac@Mimir ~/github/scanpy$ time python3 -c ""import scanpy"" ✭defer-seaborn ; python3 -c ""import scanpy"" 3.36s user 0.61s system 104% cpu 3.801 total; isaac@Mimir ~/github/scanpy$ git checkout master ✭defer-seaborn ; Switched to branch 'master'; Your branch is up to date with 'upstream/master'.; isaac@Mimir ~/github/scanpy$ time python3 -c ""import scanpy"" ✭master ; python3 -c ""import scanpy"" 4.23s user 0.48s system 108% cpu 4.324 total; ```. I'd like to cut down import time even more, but I figure this is a good place to start.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/703
https://github.com/scverse/scanpy/pull/704:49,Availability,down,down,49,"Follow up to #703, this should take import times down another second. It does add a dependency, but that dependency will be in the stdlib from 3.8+. After this it looks like the next biggest contributors to import times are caused by `numba` (which would be hard to factor out) and `sklearn.metrics`, but these are on a smaller scale. Evidence of speedup:. ```sh; isaac@Mimir ~/github/scanpy$ time python3 -c ""import scanpy as sc"" ✹ ✭master ; python3 -c ""import scanpy as sc"" 3.10s user 0.41s system 116% cpu 3.010 total; isaac@Mimir ~/github/scanpy$ git checkout defer-umap ✹ ✭master ; Switched to branch 'defer-umap'; Your branch is up to date with 'origin/defer-umap'.; isaac@Mimir ~/github/scanpy$ time python3 -c ""import scanpy as sc"" ✹ ✭defer-umap ; python3 -c ""import scanpy as sc"" 2.18s user 0.42s system 131% cpu 1.981 total; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:84,Integrability,depend,dependency,84,"Follow up to #703, this should take import times down another second. It does add a dependency, but that dependency will be in the stdlib from 3.8+. After this it looks like the next biggest contributors to import times are caused by `numba` (which would be hard to factor out) and `sklearn.metrics`, but these are on a smaller scale. Evidence of speedup:. ```sh; isaac@Mimir ~/github/scanpy$ time python3 -c ""import scanpy as sc"" ✹ ✭master ; python3 -c ""import scanpy as sc"" 3.10s user 0.41s system 116% cpu 3.010 total; isaac@Mimir ~/github/scanpy$ git checkout defer-umap ✹ ✭master ; Switched to branch 'defer-umap'; Your branch is up to date with 'origin/defer-umap'.; isaac@Mimir ~/github/scanpy$ time python3 -c ""import scanpy as sc"" ✹ ✭defer-umap ; python3 -c ""import scanpy as sc"" 2.18s user 0.42s system 131% cpu 1.981 total; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:105,Integrability,depend,dependency,105,"Follow up to #703, this should take import times down another second. It does add a dependency, but that dependency will be in the stdlib from 3.8+. After this it looks like the next biggest contributors to import times are caused by `numba` (which would be hard to factor out) and `sklearn.metrics`, but these are on a smaller scale. Evidence of speedup:. ```sh; isaac@Mimir ~/github/scanpy$ time python3 -c ""import scanpy as sc"" ✹ ✭master ; python3 -c ""import scanpy as sc"" 3.10s user 0.41s system 116% cpu 3.010 total; isaac@Mimir ~/github/scanpy$ git checkout defer-umap ✹ ✭master ; Switched to branch 'defer-umap'; Your branch is up to date with 'origin/defer-umap'.; isaac@Mimir ~/github/scanpy$ time python3 -c ""import scanpy as sc"" ✹ ✭defer-umap ; python3 -c ""import scanpy as sc"" 2.18s user 0.42s system 131% cpu 1.981 total; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/issues/706:307,Availability,error,error,307,"Hi guys,. Sorry I am new to github, I just posted this a comment on a closed thread (#530) and wasn't sure if that would be seen or not so I am just posting it up in as a new issue, just in case. I am having trouble running the rank_gene_groups test using wilcoxon, every time I try it throws a math domain error. I think this might be a duplicate of #566. As with the OP in #566 I have no trouble running the rank_genes_groups using t-test or logreg, the problem only arises when using method='wilcoxon'. For the example provided below `adata` is using one of our real data sets and I am using custom clusters but I have the same problem when replicating the tutorial workflow. My colleague has had the same experience when trying to work through the tutorial on his system, again using the most recent release of Scanpy. I've made sure 'log_transformed' was being applied to include the #519 fix provided by **a-munoz-rojas** in the hope that this might help but no such luck, I get the same error either way. Any ideas on how to solve this?. Here is what I ran:. ```; sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); ```. And here is the resulting error:. ```; ValueError Traceback (most recent call last); <ipython-input-117-a5ba74ea872c> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transfo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/706:994,Availability,error,error,994,"Hi guys,. Sorry I am new to github, I just posted this a comment on a closed thread (#530) and wasn't sure if that would be seen or not so I am just posting it up in as a new issue, just in case. I am having trouble running the rank_gene_groups test using wilcoxon, every time I try it throws a math domain error. I think this might be a duplicate of #566. As with the OP in #566 I have no trouble running the rank_genes_groups using t-test or logreg, the problem only arises when using method='wilcoxon'. For the example provided below `adata` is using one of our real data sets and I am using custom clusters but I have the same problem when replicating the tutorial workflow. My colleague has had the same experience when trying to work through the tutorial on his system, again using the most recent release of Scanpy. I've made sure 'log_transformed' was being applied to include the #519 fix provided by **a-munoz-rojas** in the hope that this might help but no such luck, I get the same error either way. Any ideas on how to solve this?. Here is what I ran:. ```; sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); ```. And here is the resulting error:. ```; ValueError Traceback (most recent call last); <ipython-input-117-a5ba74ea872c> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transfo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/706:1394,Availability,error,error,1394,"ve no trouble running the rank_genes_groups using t-test or logreg, the problem only arises when using method='wilcoxon'. For the example provided below `adata` is using one of our real data sets and I am using custom clusters but I have the same problem when replicating the tutorial workflow. My colleague has had the same experience when trying to work through the tutorial on his system, again using the most recent release of Scanpy. I've made sure 'log_transformed' was being applied to include the #519 fix provided by **a-munoz-rojas** in the hope that this might help but no such luck, I get the same error either way. Any ideas on how to solve this?. Here is what I ran:. ```; sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); ```. And here is the resulting error:. ```; ValueError Traceback (most recent call last); <ipython-input-117-a5ba74ea872c> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds); 367 ; 368 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(; --> 369 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)); 370 scores[np.isnan(scores)] = 0; 371 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error; ```. P.S I just want to say thank you for all the work on Scanpy, loving it",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/706:2302,Availability,error,error,2302,"coxon'. For the example provided below `adata` is using one of our real data sets and I am using custom clusters but I have the same problem when replicating the tutorial workflow. My colleague has had the same experience when trying to work through the tutorial on his system, again using the most recent release of Scanpy. I've made sure 'log_transformed' was being applied to include the #519 fix provided by **a-munoz-rojas** in the hope that this might help but no such luck, I get the same error either way. Any ideas on how to solve this?. Here is what I ran:. ```; sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); ```. And here is the resulting error:. ```; ValueError Traceback (most recent call last); <ipython-input-117-a5ba74ea872c> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds); 367 ; 368 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(; --> 369 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)); 370 scores[np.isnan(scores)] = 0; 371 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error; ```. P.S I just want to say thank you for all the work on Scanpy, loving it so far!. _Originally posted by @HKanenew in https://github.com/theislab/scanpy/issues/530#issuecomment-505305611_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/706:804,Deployability,release,release,804,"Hi guys,. Sorry I am new to github, I just posted this a comment on a closed thread (#530) and wasn't sure if that would be seen or not so I am just posting it up in as a new issue, just in case. I am having trouble running the rank_gene_groups test using wilcoxon, every time I try it throws a math domain error. I think this might be a duplicate of #566. As with the OP in #566 I have no trouble running the rank_genes_groups using t-test or logreg, the problem only arises when using method='wilcoxon'. For the example provided below `adata` is using one of our real data sets and I am using custom clusters but I have the same problem when replicating the tutorial workflow. My colleague has had the same experience when trying to work through the tutorial on his system, again using the most recent release of Scanpy. I've made sure 'log_transformed' was being applied to include the #519 fix provided by **a-munoz-rojas** in the hope that this might help but no such luck, I get the same error either way. Any ideas on how to solve this?. Here is what I ran:. ```; sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); ```. And here is the resulting error:. ```; ValueError Traceback (most recent call last); <ipython-input-117-a5ba74ea872c> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transfo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/706:245,Testability,test,test,245,"Hi guys,. Sorry I am new to github, I just posted this a comment on a closed thread (#530) and wasn't sure if that would be seen or not so I am just posting it up in as a new issue, just in case. I am having trouble running the rank_gene_groups test using wilcoxon, every time I try it throws a math domain error. I think this might be a duplicate of #566. As with the OP in #566 I have no trouble running the rank_genes_groups using t-test or logreg, the problem only arises when using method='wilcoxon'. For the example provided below `adata` is using one of our real data sets and I am using custom clusters but I have the same problem when replicating the tutorial workflow. My colleague has had the same experience when trying to work through the tutorial on his system, again using the most recent release of Scanpy. I've made sure 'log_transformed' was being applied to include the #519 fix provided by **a-munoz-rojas** in the hope that this might help but no such luck, I get the same error either way. Any ideas on how to solve this?. Here is what I ran:. ```; sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); ```. And here is the resulting error:. ```; ValueError Traceback (most recent call last); <ipython-input-117-a5ba74ea872c> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transfo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/706:436,Testability,test,test,436,"Hi guys,. Sorry I am new to github, I just posted this a comment on a closed thread (#530) and wasn't sure if that would be seen or not so I am just posting it up in as a new issue, just in case. I am having trouble running the rank_gene_groups test using wilcoxon, every time I try it throws a math domain error. I think this might be a duplicate of #566. As with the OP in #566 I have no trouble running the rank_genes_groups using t-test or logreg, the problem only arises when using method='wilcoxon'. For the example provided below `adata` is using one of our real data sets and I am using custom clusters but I have the same problem when replicating the tutorial workflow. My colleague has had the same experience when trying to work through the tutorial on his system, again using the most recent release of Scanpy. I've made sure 'log_transformed' was being applied to include the #519 fix provided by **a-munoz-rojas** in the hope that this might help but no such luck, I get the same error either way. Any ideas on how to solve this?. Here is what I ran:. ```; sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); ```. And here is the resulting error:. ```; ValueError Traceback (most recent call last); <ipython-input-117-a5ba74ea872c> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transfo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/706:444,Testability,log,logreg,444,"Hi guys,. Sorry I am new to github, I just posted this a comment on a closed thread (#530) and wasn't sure if that would be seen or not so I am just posting it up in as a new issue, just in case. I am having trouble running the rank_gene_groups test using wilcoxon, every time I try it throws a math domain error. I think this might be a duplicate of #566. As with the OP in #566 I have no trouble running the rank_genes_groups using t-test or logreg, the problem only arises when using method='wilcoxon'. For the example provided below `adata` is using one of our real data sets and I am using custom clusters but I have the same problem when replicating the tutorial workflow. My colleague has had the same experience when trying to work through the tutorial on his system, again using the most recent release of Scanpy. I've made sure 'log_transformed' was being applied to include the #519 fix provided by **a-munoz-rojas** in the hope that this might help but no such luck, I get the same error either way. Any ideas on how to solve this?. Here is what I ran:. ```; sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); ```. And here is the resulting error:. ```; ValueError Traceback (most recent call last); <ipython-input-117-a5ba74ea872c> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transfo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/710:45,Availability,error,error,45,"Great work! . Python (64bit) throws a memory error when projecting ~1 million cells into 3D UMAP with 40 features (PCs). I suppose that matrix products are super big, but I'm performing it on 256G RAM.; Is there a way to decrease memory usage?; I would appreciate your advice!; Thanks ahead!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:175,Performance,perform,performing,175,"Great work! . Python (64bit) throws a memory error when projecting ~1 million cells into 3D UMAP with 40 features (PCs). I suppose that matrix products are super big, but I'm performing it on 256G RAM.; Is there a way to decrease memory usage?; I would appreciate your advice!; Thanks ahead!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/pull/716:194,Usability,simpl,simple,194,"The phate too makes a call to a function named `_settings_verbosity_greater_or_equal_than()`, which appears to no longer exist, preventing phate from running at all. I exchanged that call for a simple comparison that yields what I believe the old function returned. In any case, the change results in phate being able to run.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/716
https://github.com/scverse/scanpy/pull/717:0,Performance,Cache,Cache,0,"Cache datasets so notebook tests can run without requiring an external server, since they cover realistic use cases and a good amount of the API.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/717
https://github.com/scverse/scanpy/pull/717:27,Testability,test,tests,27,"Cache datasets so notebook tests can run without requiring an external server, since they cover realistic use cases and a good amount of the API.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/717
https://github.com/scverse/scanpy/issues/723:81,Modifiability,refactor,refactoring,81,"Hi, @falexwolf ; Do you have any specific things in mind for `rank_genes_groups` refactoring? What should be done?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/pull/724:57,Availability,error,error,57,* Now actually allows passing an array (previously threw error from `if init_pos in adata.obsm.keys()`); * Additionally allows providing an array (even through key of obsm or via paga) of dtype other than float32; * Code converting arrays to float32 can be removed if https://github.com/lmcinnes/umap/pull/262 gets merged and released; * Should solve #666 related issues,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:326,Deployability,release,released,326,* Now actually allows passing an array (previously threw error from `if init_pos in adata.obsm.keys()`); * Additionally allows providing an array (even through key of obsm or via paga) of dtype other than float32; * Code converting arrays to float32 can be removed if https://github.com/lmcinnes/umap/pull/262 gets merged and released; * Should solve #666 related issues,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/issues/725:73,Deployability,pipeline,pipelines,73,"I was just wondering if you guys have any plans for scATAC data analysis pipelines and especially integration with scRNA-seq? . cheers,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/725
https://github.com/scverse/scanpy/issues/725:98,Deployability,integrat,integration,98,"I was just wondering if you guys have any plans for scATAC data analysis pipelines and especially integration with scRNA-seq? . cheers,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/725
https://github.com/scverse/scanpy/issues/725:98,Integrability,integrat,integration,98,"I was just wondering if you guys have any plans for scATAC data analysis pipelines and especially integration with scRNA-seq? . cheers,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/725
https://github.com/scverse/scanpy/issues/728:58,Deployability,upgrade,upgrade,58,"suddenly I have this problem, maybe related to an anndata upgrade. pip says all requirements are satisfied:. scanpy==1.4.3 anndata==0.6.22rc1 umap==0.3.9 numpy==1.16.4 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1 ; ```; ; adata ; AnnData object with n_obs × n_vars = 466 × 28685 ; obs: 'GEO_Sample_age', 'age', 'age_unit', 'biosample_source_life_stage', 'biosample_source_gender', 'sample_category', 'biosample_cell_type', 'n_genes', 'n_counts', 'percent_mito'; var: 'n_cells'. fig1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', save=""_gene_count""). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _get_obs_array(self, k, use_raw, layer); 1527 obs.keys and then var.index.""""""; 1528 if use_raw:; -> 1529 return self.raw.obs_vector(k); 1530 else:; 1531 return self.obs_vector(k=k, layer=layer). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in obs_vector(self, k); 408 as `.obs_names`.; 409 """"""; --> 410 a = self[:, k].X; 411 if issparse(a):; 412 a = a.toarray(). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in __getitem__(self, index); 331 ; 332 def __getitem__(self, index):; --> 333 oidx, vidx = self._normalize_indices(index); 334 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 335 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_indices(self, packed_index); 360 obs, var = unpack_index(packed_index); 361 obs = _normalize_index(obs, self._adata.obs_names); --> 362 var = _normalize_index(var, self.var_names); 363 return obs, var; 364 . ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_index(index, names); 153 return slice(start, stop, step); 154 elif isinstance(index, (np.integer, int, str)):; --> 155 return name_idx(index); 156 elif isinstance(index, (Sequence, np.ndarray, pd.Ind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2232,Modifiability,variab,variable,2232,"pl.scatter(adata, x='n_counts', y='n_genes', save=""_gene_count""). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _get_obs_array(self, k, use_raw, layer); 1527 obs.keys and then var.index.""""""; 1528 if use_raw:; -> 1529 return self.raw.obs_vector(k); 1530 else:; 1531 return self.obs_vector(k=k, layer=layer). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in obs_vector(self, k); 408 as `.obs_names`.; 409 """"""; --> 410 a = self[:, k].X; 411 if issparse(a):; 412 a = a.toarray(). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in __getitem__(self, index); 331 ; 332 def __getitem__(self, index):; --> 333 oidx, vidx = self._normalize_indices(index); 334 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 335 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_indices(self, packed_index); 360 obs, var = unpack_index(packed_index); 361 obs = _normalize_index(obs, self._adata.obs_names); --> 362 var = _normalize_index(var, self.var_names); 363 return obs, var; 364 . ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_index(index, names); 153 return slice(start, stop, step); 154 elif isinstance(index, (np.integer, int, str)):; --> 155 return name_idx(index); 156 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 157 # here, we replaced the implementation based on name_idx with this. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in name_idx(i); 140 raise IndexError(; 141 'Key ""{}"" is not valid observation/variable name/index.'; --> 142 .format(i)); 143 i = i_found[0]; 144 return i; ```. I don't understand why anndata thinks that . IndexError: Key ""n_counts"" is not valid observation/variable name/index. even though it's clearly in adata.obs... any suggestions what to do? add print statements to the various functions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2412,Modifiability,variab,variable,2412,"pl.scatter(adata, x='n_counts', y='n_genes', save=""_gene_count""). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _get_obs_array(self, k, use_raw, layer); 1527 obs.keys and then var.index.""""""; 1528 if use_raw:; -> 1529 return self.raw.obs_vector(k); 1530 else:; 1531 return self.obs_vector(k=k, layer=layer). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in obs_vector(self, k); 408 as `.obs_names`.; 409 """"""; --> 410 a = self[:, k].X; 411 if issparse(a):; 412 a = a.toarray(). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in __getitem__(self, index); 331 ; 332 def __getitem__(self, index):; --> 333 oidx, vidx = self._normalize_indices(index); 334 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 335 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_indices(self, packed_index); 360 obs, var = unpack_index(packed_index); 361 obs = _normalize_index(obs, self._adata.obs_names); --> 362 var = _normalize_index(var, self.var_names); 363 return obs, var; 364 . ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_index(index, names); 153 return slice(start, stop, step); 154 elif isinstance(index, (np.integer, int, str)):; --> 155 return name_idx(index); 156 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 157 # here, we replaced the implementation based on name_idx with this. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in name_idx(i); 140 raise IndexError(; 141 'Key ""{}"" is not valid observation/variable name/index.'; --> 142 .format(i)); 143 i = i_found[0]; 144 return i; ```. I don't understand why anndata thinks that . IndexError: Key ""n_counts"" is not valid observation/variable name/index. even though it's clearly in adata.obs... any suggestions what to do? add print statements to the various functions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:203,Usability,learn,learn,203,"suddenly I have this problem, maybe related to an anndata upgrade. pip says all requirements are satisfied:. scanpy==1.4.3 anndata==0.6.22rc1 umap==0.3.9 numpy==1.16.4 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1 ; ```; ; adata ; AnnData object with n_obs × n_vars = 466 × 28685 ; obs: 'GEO_Sample_age', 'age', 'age_unit', 'biosample_source_life_stage', 'biosample_source_gender', 'sample_category', 'biosample_cell_type', 'n_genes', 'n_counts', 'percent_mito'; var: 'n_cells'. fig1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', save=""_gene_count""). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _get_obs_array(self, k, use_raw, layer); 1527 obs.keys and then var.index.""""""; 1528 if use_raw:; -> 1529 return self.raw.obs_vector(k); 1530 else:; 1531 return self.obs_vector(k=k, layer=layer). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in obs_vector(self, k); 408 as `.obs_names`.; 409 """"""; --> 410 a = self[:, k].X; 411 if issparse(a):; 412 a = a.toarray(). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in __getitem__(self, index); 331 ; 332 def __getitem__(self, index):; --> 333 oidx, vidx = self._normalize_indices(index); 334 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 335 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_indices(self, packed_index); 360 obs, var = unpack_index(packed_index); 361 obs = _normalize_index(obs, self._adata.obs_names); --> 362 var = _normalize_index(var, self.var_names); 363 return obs, var; 364 . ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_index(index, names); 153 return slice(start, stop, step); 154 elif isinstance(index, (np.integer, int, str)):; --> 155 return name_idx(index); 156 elif isinstance(index, (Sequence, np.ndarray, pd.Ind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2450,Usability,clear,clearly,2450,"pl.scatter(adata, x='n_counts', y='n_genes', save=""_gene_count""). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _get_obs_array(self, k, use_raw, layer); 1527 obs.keys and then var.index.""""""; 1528 if use_raw:; -> 1529 return self.raw.obs_vector(k); 1530 else:; 1531 return self.obs_vector(k=k, layer=layer). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in obs_vector(self, k); 408 as `.obs_names`.; 409 """"""; --> 410 a = self[:, k].X; 411 if issparse(a):; 412 a = a.toarray(). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in __getitem__(self, index); 331 ; 332 def __getitem__(self, index):; --> 333 oidx, vidx = self._normalize_indices(index); 334 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 335 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_indices(self, packed_index); 360 obs, var = unpack_index(packed_index); 361 obs = _normalize_index(obs, self._adata.obs_names); --> 362 var = _normalize_index(var, self.var_names); 363 return obs, var; 364 . ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_index(index, names); 153 return slice(start, stop, step); 154 elif isinstance(index, (np.integer, int, str)):; --> 155 return name_idx(index); 156 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 157 # here, we replaced the implementation based on name_idx with this. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in name_idx(i); 140 raise IndexError(; 141 'Key ""{}"" is not valid observation/variable name/index.'; --> 142 .format(i)); 143 i = i_found[0]; 144 return i; ```. I don't understand why anndata thinks that . IndexError: Key ""n_counts"" is not valid observation/variable name/index. even though it's clearly in adata.obs... any suggestions what to do? add print statements to the various functions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/pull/729:34,Testability,log,logic,34,Fixes #699. Corrects inconsistent logic about when a copy of a view would be made by preprocessing functions. Discussed here: https://github.com/theislab/anndata/issues/171#issuecomment-508619952,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/729
https://github.com/scverse/scanpy/pull/730:605,Modifiability,layers,layers,605,"Fixes for deprecation warnings from anndata`v0.6.22`. This is mostly by replacing access to `.X` with access via `{obs,var}_vector` or `sc.get.obs_df`. Supercedes #713, fixes #700 and #690. Functions changed:. * `sc.pl.violin`; * Dataframe for plotting now constructed with `obs_df`, access to `adata.X` no longer used.; * `sc.pl.scatter`; * Changed default `layer` from `""X""` to `""None""`. `""X""` is still supported, but should throw a deprecation warning if it's explicitly used.; * Replace usage of `._get_obs_array` with `.obs_vector`; * `sc.pl._tools.scatterplots.plot_scatter`; * Normalized access to layers, now sparse and dense should similarly.; * `sc.get.obs_df`; * Added support for `use_raw`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:82,Security,access,access,82,"Fixes for deprecation warnings from anndata`v0.6.22`. This is mostly by replacing access to `.X` with access via `{obs,var}_vector` or `sc.get.obs_df`. Supercedes #713, fixes #700 and #690. Functions changed:. * `sc.pl.violin`; * Dataframe for plotting now constructed with `obs_df`, access to `adata.X` no longer used.; * `sc.pl.scatter`; * Changed default `layer` from `""X""` to `""None""`. `""X""` is still supported, but should throw a deprecation warning if it's explicitly used.; * Replace usage of `._get_obs_array` with `.obs_vector`; * `sc.pl._tools.scatterplots.plot_scatter`; * Normalized access to layers, now sparse and dense should similarly.; * `sc.get.obs_df`; * Added support for `use_raw`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:102,Security,access,access,102,"Fixes for deprecation warnings from anndata`v0.6.22`. This is mostly by replacing access to `.X` with access via `{obs,var}_vector` or `sc.get.obs_df`. Supercedes #713, fixes #700 and #690. Functions changed:. * `sc.pl.violin`; * Dataframe for plotting now constructed with `obs_df`, access to `adata.X` no longer used.; * `sc.pl.scatter`; * Changed default `layer` from `""X""` to `""None""`. `""X""` is still supported, but should throw a deprecation warning if it's explicitly used.; * Replace usage of `._get_obs_array` with `.obs_vector`; * `sc.pl._tools.scatterplots.plot_scatter`; * Normalized access to layers, now sparse and dense should similarly.; * `sc.get.obs_df`; * Added support for `use_raw`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:284,Security,access,access,284,"Fixes for deprecation warnings from anndata`v0.6.22`. This is mostly by replacing access to `.X` with access via `{obs,var}_vector` or `sc.get.obs_df`. Supercedes #713, fixes #700 and #690. Functions changed:. * `sc.pl.violin`; * Dataframe for plotting now constructed with `obs_df`, access to `adata.X` no longer used.; * `sc.pl.scatter`; * Changed default `layer` from `""X""` to `""None""`. `""X""` is still supported, but should throw a deprecation warning if it's explicitly used.; * Replace usage of `._get_obs_array` with `.obs_vector`; * `sc.pl._tools.scatterplots.plot_scatter`; * Normalized access to layers, now sparse and dense should similarly.; * `sc.get.obs_df`; * Added support for `use_raw`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:595,Security,access,access,595,"Fixes for deprecation warnings from anndata`v0.6.22`. This is mostly by replacing access to `.X` with access via `{obs,var}_vector` or `sc.get.obs_df`. Supercedes #713, fixes #700 and #690. Functions changed:. * `sc.pl.violin`; * Dataframe for plotting now constructed with `obs_df`, access to `adata.X` no longer used.; * `sc.pl.scatter`; * Changed default `layer` from `""X""` to `""None""`. `""X""` is still supported, but should throw a deprecation warning if it's explicitly used.; * Replace usage of `._get_obs_array` with `.obs_vector`; * `sc.pl._tools.scatterplots.plot_scatter`; * Normalized access to layers, now sparse and dense should similarly.; * `sc.get.obs_df`; * Added support for `use_raw`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/issues/731:28,Availability,error,error,28,"sc.pp.scale is giving me an error when I run it for the first time, then running fine on the second. This occurs when I run on an object generated using .copy() . ![Screen Shot 2019-07-08 at 10 55 34 AM](https://user-images.githubusercontent.com/26631928/60820560-a8fb6400-a16f-11e9-9915-7d808561af69.png). In terms of the numbers at the end--I have 2176 cells and 1600 highly variable genes. . If I run on an object not generated using copy, I get ""Trying to set attribute `.obs` of view, making a copy."" but it finishes on first run. . If I try to regress out counts first, I get. ![Screen Shot 2019-07-08 at 11 12 57 AM](https://user-images.githubusercontent.com/26631928/60821330-58850600-a171-11e9-9a50-666694bf2c1c.png). One additional oddity--if I run sc.pphighly_variable_genes with flavor = 'seurat' instead of flavor = 'cell_ranger' and call sc.pp.regress_out(Bcell, 'n_counts') prior to running sc.pp.scale(Bcell, max_value = 10) I don't get any error. If I don't run regress_counts but have the 'seurat' flavor I get ; ![Screen Shot 2019-07-08 at 11 22 04 AM](https://user-images.githubusercontent.com/26631928/60822004-9f273000-a172-11e9-814d-dfc83155b488.png). Really not sure what's happening but figured I should let you know. Thanks!. sc.settings.verbosity = 3. scanpy==1.4.3 anndata==0.6.21 umap==0.3.9 numpy==1.16.4 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:957,Availability,error,error,957,"sc.pp.scale is giving me an error when I run it for the first time, then running fine on the second. This occurs when I run on an object generated using .copy() . ![Screen Shot 2019-07-08 at 10 55 34 AM](https://user-images.githubusercontent.com/26631928/60820560-a8fb6400-a16f-11e9-9915-7d808561af69.png). In terms of the numbers at the end--I have 2176 cells and 1600 highly variable genes. . If I run on an object not generated using copy, I get ""Trying to set attribute `.obs` of view, making a copy."" but it finishes on first run. . If I try to regress out counts first, I get. ![Screen Shot 2019-07-08 at 11 12 57 AM](https://user-images.githubusercontent.com/26631928/60821330-58850600-a171-11e9-9a50-666694bf2c1c.png). One additional oddity--if I run sc.pphighly_variable_genes with flavor = 'seurat' instead of flavor = 'cell_ranger' and call sc.pp.regress_out(Bcell, 'n_counts') prior to running sc.pp.scale(Bcell, max_value = 10) I don't get any error. If I don't run regress_counts but have the 'seurat' flavor I get ; ![Screen Shot 2019-07-08 at 11 22 04 AM](https://user-images.githubusercontent.com/26631928/60822004-9f273000-a172-11e9-814d-dfc83155b488.png). Really not sure what's happening but figured I should let you know. Thanks!. sc.settings.verbosity = 3. scanpy==1.4.3 anndata==0.6.21 umap==0.3.9 numpy==1.16.4 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:377,Modifiability,variab,variable,377,"sc.pp.scale is giving me an error when I run it for the first time, then running fine on the second. This occurs when I run on an object generated using .copy() . ![Screen Shot 2019-07-08 at 10 55 34 AM](https://user-images.githubusercontent.com/26631928/60820560-a8fb6400-a16f-11e9-9915-7d808561af69.png). In terms of the numbers at the end--I have 2176 cells and 1600 highly variable genes. . If I run on an object not generated using copy, I get ""Trying to set attribute `.obs` of view, making a copy."" but it finishes on first run. . If I try to regress out counts first, I get. ![Screen Shot 2019-07-08 at 11 12 57 AM](https://user-images.githubusercontent.com/26631928/60821330-58850600-a171-11e9-9a50-666694bf2c1c.png). One additional oddity--if I run sc.pphighly_variable_genes with flavor = 'seurat' instead of flavor = 'cell_ranger' and call sc.pp.regress_out(Bcell, 'n_counts') prior to running sc.pp.scale(Bcell, max_value = 10) I don't get any error. If I don't run regress_counts but have the 'seurat' flavor I get ; ![Screen Shot 2019-07-08 at 11 22 04 AM](https://user-images.githubusercontent.com/26631928/60822004-9f273000-a172-11e9-814d-dfc83155b488.png). Really not sure what's happening but figured I should let you know. Thanks!. sc.settings.verbosity = 3. scanpy==1.4.3 anndata==0.6.21 umap==0.3.9 numpy==1.16.4 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:1370,Usability,learn,learn,1370,"sc.pp.scale is giving me an error when I run it for the first time, then running fine on the second. This occurs when I run on an object generated using .copy() . ![Screen Shot 2019-07-08 at 10 55 34 AM](https://user-images.githubusercontent.com/26631928/60820560-a8fb6400-a16f-11e9-9915-7d808561af69.png). In terms of the numbers at the end--I have 2176 cells and 1600 highly variable genes. . If I run on an object not generated using copy, I get ""Trying to set attribute `.obs` of view, making a copy."" but it finishes on first run. . If I try to regress out counts first, I get. ![Screen Shot 2019-07-08 at 11 12 57 AM](https://user-images.githubusercontent.com/26631928/60821330-58850600-a171-11e9-9a50-666694bf2c1c.png). One additional oddity--if I run sc.pphighly_variable_genes with flavor = 'seurat' instead of flavor = 'cell_ranger' and call sc.pp.regress_out(Bcell, 'n_counts') prior to running sc.pp.scale(Bcell, max_value = 10) I don't get any error. If I don't run regress_counts but have the 'seurat' flavor I get ; ![Screen Shot 2019-07-08 at 11 22 04 AM](https://user-images.githubusercontent.com/26631928/60822004-9f273000-a172-11e9-814d-dfc83155b488.png). Really not sure what's happening but figured I should let you know. Thanks!. sc.settings.verbosity = 3. scanpy==1.4.3 anndata==0.6.21 umap==0.3.9 numpy==1.16.4 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/734:321,Availability,error,errors,321,"Heya,. I have been trying to get scanpy loaded and a simple example up and running. . I tried following the "" Clustering 3K PBMCs Following a Seurat Tutorial"" by trying to execute the following code:. ```py; import numpy as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(inde",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:40,Performance,load,loaded,40,"Heya,. I have been trying to get scanpy loaded and a simple example up and running. . I tried following the "" Clustering 3K PBMCs Following a Seurat Tutorial"" by trying to execute the following code:. ```py; import numpy as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(inde",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:607,Performance,cache,cache,607,"Heya,. I have been trying to get scanpy loaded and a simple example up and running. . I tried following the "" Clustering 3K PBMCs Following a Seurat Tutorial"" by trying to execute the following code:. ```py; import numpy as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(inde",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:1201,Performance,cache,cache,1201,"as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(index); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1234, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/Use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:1212,Performance,cache,cache,1212,"as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(index); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1234, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/Use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:371,Testability,log,logging,371,"Heya,. I have been trying to get scanpy loaded and a simple example up and running. . I tried following the "" Clustering 3K PBMCs Following a Seurat Tutorial"" by trying to execute the following code:. ```py; import numpy as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(inde",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:1373,Testability,test,test,1373,"), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(index); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1234, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 561, in __init__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:53,Usability,simpl,simple,53,"Heya,. I have been trying to get scanpy loaded and a simple example up and running. . I tried following the "" Clustering 3K PBMCs Following a Seurat Tutorial"" by trying to execute the following code:. ```py; import numpy as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(inde",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:1148,Usability,learn,learn,1148," Tutorial"" by trying to execute the following code:. ```py; import numpy as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(index); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1234, in _getitem",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/738:191,Availability,error,error,191,"hello, recently I use the scanpy package to preprocess the single cell rna-seq data, the following is my process step. But when I go to the last step, namely I want to scale the dataset, the error occurs. I have 4271 cells and 1024 genes after running ""adata = adata[:, adata.var[""highly_variable""]]"". But the error says the 4271 is not equal to 1024 in the dimension 0. I do not know the reason, so can you give me an answer?Thanks very much.; sc.pp.filter_genes(adata, min_counts = filter_min_counts); sc.pp.filter_cells(adata, min_counts = filter_min_counts); sc.pp.normalize_per_cell(adata); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True); adata = adata[:, adata.var[""highly_variable""]]; sc.pp.scale(adata)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:310,Availability,error,error,310,"hello, recently I use the scanpy package to preprocess the single cell rna-seq data, the following is my process step. But when I go to the last step, namely I want to scale the dataset, the error occurs. I have 4271 cells and 1024 genes after running ""adata = adata[:, adata.var[""highly_variable""]]"". But the error says the 4271 is not equal to 1024 in the dimension 0. I do not know the reason, so can you give me an answer?Thanks very much.; sc.pp.filter_genes(adata, min_counts = filter_min_counts); sc.pp.filter_cells(adata, min_counts = filter_min_counts); sc.pp.normalize_per_cell(adata); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True); adata = adata[:, adata.var[""highly_variable""]]; sc.pp.scale(adata)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/739:113,Availability,error,error,113,"Hey!; I just updated to latest master branch and I can no longer load scanpy. `import scanpy as sc` gives me the error:; ``` ---------------------------------------------------------------------------; PackageNotFoundError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/new_scanpy/scanpy/scanpy/__init__.py in <module>; 25 __version__ = get_versions()['version']; 26 ; ---> 27 check_versions(); 28 del get_versions, check_versions; 29 . ~/new_scanpy/scanpy/scanpy/utils.py in check_versions(); 38 ; 39 anndata_version = version(""anndata""); ---> 40 umap_version = version(""umap-learn""); 41 ; 42 if anndata_version < LooseVersion('0.6.10'):. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in version(package); 103 ""Version"" metadata key.; 104 """"""; --> 105 return distribution(package).version; 106 ; 107 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in distribution(package); 84 :return: A ``Distribution`` instance (or subclass thereof).; 85 """"""; ---> 86 return Distribution.from_name(package); 87 ; 88 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in from_name(cls, name); 50 return resolved; 51 else:; ---> 52 raise PackageNotFoundError(name); 53 ; 54 @staticmethod. PackageNotFoundError: umap-learn ; ```. I have `umap-learn` 0.3.9 installed. . Scanpy version: 1.4.3+115.g1aecabf; Anndata version: 0.6.22rc1. It seems to work with umap-learn 0.3.8, scanpy 1.4.3+105.gc748b35. and anndata 0.6.22.post1+1.g8dcc3cd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:13,Deployability,update,updated,13,"Hey!; I just updated to latest master branch and I can no longer load scanpy. `import scanpy as sc` gives me the error:; ``` ---------------------------------------------------------------------------; PackageNotFoundError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/new_scanpy/scanpy/scanpy/__init__.py in <module>; 25 __version__ = get_versions()['version']; 26 ; ---> 27 check_versions(); 28 del get_versions, check_versions; 29 . ~/new_scanpy/scanpy/scanpy/utils.py in check_versions(); 38 ; 39 anndata_version = version(""anndata""); ---> 40 umap_version = version(""umap-learn""); 41 ; 42 if anndata_version < LooseVersion('0.6.10'):. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in version(package); 103 ""Version"" metadata key.; 104 """"""; --> 105 return distribution(package).version; 106 ; 107 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in distribution(package); 84 :return: A ``Distribution`` instance (or subclass thereof).; 85 """"""; ---> 86 return Distribution.from_name(package); 87 ; 88 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in from_name(cls, name); 50 return resolved; 51 else:; ---> 52 raise PackageNotFoundError(name); 53 ; 54 @staticmethod. PackageNotFoundError: umap-learn ; ```. I have `umap-learn` 0.3.9 installed. . Scanpy version: 1.4.3+115.g1aecabf; Anndata version: 0.6.22rc1. It seems to work with umap-learn 0.3.8, scanpy 1.4.3+105.gc748b35. and anndata 0.6.22.post1+1.g8dcc3cd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:1411,Deployability,install,installed,1411,"Hey!; I just updated to latest master branch and I can no longer load scanpy. `import scanpy as sc` gives me the error:; ``` ---------------------------------------------------------------------------; PackageNotFoundError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/new_scanpy/scanpy/scanpy/__init__.py in <module>; 25 __version__ = get_versions()['version']; 26 ; ---> 27 check_versions(); 28 del get_versions, check_versions; 29 . ~/new_scanpy/scanpy/scanpy/utils.py in check_versions(); 38 ; 39 anndata_version = version(""anndata""); ---> 40 umap_version = version(""umap-learn""); 41 ; 42 if anndata_version < LooseVersion('0.6.10'):. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in version(package); 103 ""Version"" metadata key.; 104 """"""; --> 105 return distribution(package).version; 106 ; 107 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in distribution(package); 84 :return: A ``Distribution`` instance (or subclass thereof).; 85 """"""; ---> 86 return Distribution.from_name(package); 87 ; 88 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in from_name(cls, name); 50 return resolved; 51 else:; ---> 52 raise PackageNotFoundError(name); 53 ; 54 @staticmethod. PackageNotFoundError: umap-learn ; ```. I have `umap-learn` 0.3.9 installed. . Scanpy version: 1.4.3+115.g1aecabf; Anndata version: 0.6.22rc1. It seems to work with umap-learn 0.3.8, scanpy 1.4.3+105.gc748b35. and anndata 0.6.22.post1+1.g8dcc3cd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:65,Performance,load,load,65,"Hey!; I just updated to latest master branch and I can no longer load scanpy. `import scanpy as sc` gives me the error:; ``` ---------------------------------------------------------------------------; PackageNotFoundError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/new_scanpy/scanpy/scanpy/__init__.py in <module>; 25 __version__ = get_versions()['version']; 26 ; ---> 27 check_versions(); 28 del get_versions, check_versions; 29 . ~/new_scanpy/scanpy/scanpy/utils.py in check_versions(); 38 ; 39 anndata_version = version(""anndata""); ---> 40 umap_version = version(""umap-learn""); 41 ; 42 if anndata_version < LooseVersion('0.6.10'):. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in version(package); 103 ""Version"" metadata key.; 104 """"""; --> 105 return distribution(package).version; 106 ; 107 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in distribution(package); 84 :return: A ``Distribution`` instance (or subclass thereof).; 85 """"""; ---> 86 return Distribution.from_name(package); 87 ; 88 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in from_name(cls, name); 50 return resolved; 51 else:; ---> 52 raise PackageNotFoundError(name); 53 ; 54 @staticmethod. PackageNotFoundError: umap-learn ; ```. I have `umap-learn` 0.3.9 installed. . Scanpy version: 1.4.3+115.g1aecabf; Anndata version: 0.6.22rc1. It seems to work with umap-learn 0.3.8, scanpy 1.4.3+105.gc748b35. and anndata 0.6.22.post1+1.g8dcc3cd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:640,Usability,learn,learn,640,"Hey!; I just updated to latest master branch and I can no longer load scanpy. `import scanpy as sc` gives me the error:; ``` ---------------------------------------------------------------------------; PackageNotFoundError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/new_scanpy/scanpy/scanpy/__init__.py in <module>; 25 __version__ = get_versions()['version']; 26 ; ---> 27 check_versions(); 28 del get_versions, check_versions; 29 . ~/new_scanpy/scanpy/scanpy/utils.py in check_versions(); 38 ; 39 anndata_version = version(""anndata""); ---> 40 umap_version = version(""umap-learn""); 41 ; 42 if anndata_version < LooseVersion('0.6.10'):. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in version(package); 103 ""Version"" metadata key.; 104 """"""; --> 105 return distribution(package).version; 106 ; 107 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in distribution(package); 84 :return: A ``Distribution`` instance (or subclass thereof).; 85 """"""; ---> 86 return Distribution.from_name(package); 87 ; 88 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in from_name(cls, name); 50 return resolved; 51 else:; ---> 52 raise PackageNotFoundError(name); 53 ; 54 @staticmethod. PackageNotFoundError: umap-learn ; ```. I have `umap-learn` 0.3.9 installed. . Scanpy version: 1.4.3+115.g1aecabf; Anndata version: 0.6.22rc1. It seems to work with umap-learn 0.3.8, scanpy 1.4.3+105.gc748b35. and anndata 0.6.22.post1+1.g8dcc3cd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:1372,Usability,learn,learn,1372,"Hey!; I just updated to latest master branch and I can no longer load scanpy. `import scanpy as sc` gives me the error:; ``` ---------------------------------------------------------------------------; PackageNotFoundError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/new_scanpy/scanpy/scanpy/__init__.py in <module>; 25 __version__ = get_versions()['version']; 26 ; ---> 27 check_versions(); 28 del get_versions, check_versions; 29 . ~/new_scanpy/scanpy/scanpy/utils.py in check_versions(); 38 ; 39 anndata_version = version(""anndata""); ---> 40 umap_version = version(""umap-learn""); 41 ; 42 if anndata_version < LooseVersion('0.6.10'):. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in version(package); 103 ""Version"" metadata key.; 104 """"""; --> 105 return distribution(package).version; 106 ; 107 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in distribution(package); 84 :return: A ``Distribution`` instance (or subclass thereof).; 85 """"""; ---> 86 return Distribution.from_name(package); 87 ; 88 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in from_name(cls, name); 50 return resolved; 51 else:; ---> 52 raise PackageNotFoundError(name); 53 ; 54 @staticmethod. PackageNotFoundError: umap-learn ; ```. I have `umap-learn` 0.3.9 installed. . Scanpy version: 1.4.3+115.g1aecabf; Anndata version: 0.6.22rc1. It seems to work with umap-learn 0.3.8, scanpy 1.4.3+105.gc748b35. and anndata 0.6.22.post1+1.g8dcc3cd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:1398,Usability,learn,learn,1398,"Hey!; I just updated to latest master branch and I can no longer load scanpy. `import scanpy as sc` gives me the error:; ``` ---------------------------------------------------------------------------; PackageNotFoundError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/new_scanpy/scanpy/scanpy/__init__.py in <module>; 25 __version__ = get_versions()['version']; 26 ; ---> 27 check_versions(); 28 del get_versions, check_versions; 29 . ~/new_scanpy/scanpy/scanpy/utils.py in check_versions(); 38 ; 39 anndata_version = version(""anndata""); ---> 40 umap_version = version(""umap-learn""); 41 ; 42 if anndata_version < LooseVersion('0.6.10'):. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in version(package); 103 ""Version"" metadata key.; 104 """"""; --> 105 return distribution(package).version; 106 ; 107 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in distribution(package); 84 :return: A ``Distribution`` instance (or subclass thereof).; 85 """"""; ---> 86 return Distribution.from_name(package); 87 ; 88 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in from_name(cls, name); 50 return resolved; 51 else:; ---> 52 raise PackageNotFoundError(name); 53 ; 54 @staticmethod. PackageNotFoundError: umap-learn ; ```. I have `umap-learn` 0.3.9 installed. . Scanpy version: 1.4.3+115.g1aecabf; Anndata version: 0.6.22rc1. It seems to work with umap-learn 0.3.8, scanpy 1.4.3+105.gc748b35. and anndata 0.6.22.post1+1.g8dcc3cd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:1515,Usability,learn,learn,1515,"Hey!; I just updated to latest master branch and I can no longer load scanpy. `import scanpy as sc` gives me the error:; ``` ---------------------------------------------------------------------------; PackageNotFoundError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/new_scanpy/scanpy/scanpy/__init__.py in <module>; 25 __version__ = get_versions()['version']; 26 ; ---> 27 check_versions(); 28 del get_versions, check_versions; 29 . ~/new_scanpy/scanpy/scanpy/utils.py in check_versions(); 38 ; 39 anndata_version = version(""anndata""); ---> 40 umap_version = version(""umap-learn""); 41 ; 42 if anndata_version < LooseVersion('0.6.10'):. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in version(package); 103 ""Version"" metadata key.; 104 """"""; --> 105 return distribution(package).version; 106 ; 107 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in distribution(package); 84 :return: A ``Distribution`` instance (or subclass thereof).; 85 """"""; ---> 86 return Distribution.from_name(package); 87 ; 88 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in from_name(cls, name); 50 return resolved; 51 else:; ---> 52 raise PackageNotFoundError(name); 53 ; 54 @staticmethod. PackageNotFoundError: umap-learn ; ```. I have `umap-learn` 0.3.9 installed. . Scanpy version: 1.4.3+115.g1aecabf; Anndata version: 0.6.22rc1. It seems to work with umap-learn 0.3.8, scanpy 1.4.3+105.gc748b35. and anndata 0.6.22.post1+1.g8dcc3cd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/pull/741:38,Deployability,release,release,38,"With the new ebi single cell database release one of the files is a little different (col names/ obs_names). That part of the reader has been fixed, and I've updated the test for the reprocessed data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/741
https://github.com/scverse/scanpy/pull/741:158,Deployability,update,updated,158,"With the new ebi single cell database release one of the files is a little different (col names/ obs_names). That part of the reader has been fixed, and I've updated the test for the reprocessed data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/741
https://github.com/scverse/scanpy/pull/741:170,Testability,test,test,170,"With the new ebi single cell database release one of the files is a little different (col names/ obs_names). That part of the reader has been fixed, and I've updated the test for the reprocessed data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/741
https://github.com/scverse/scanpy/issues/744:7,Performance,perform,perform,7,"how to perform analysis to get bulk_labels, S_score, G2M_score, phase?. After following the tutorial ""[Clustering 3K PBMCs](https://icb-scanpy-tutorials.readthedocs-hosted.com/en/latest/pbmc3k.html)"", I got the object ""adata"" with following description:; _>>> adata; AnnData object with n_obs × n_vars = 691 × 4549; **obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain'**; var: 'gene_ids', 'feature_types', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'_ . however, the tutorial ""[Visualizing marker genes](https://icb-scanpy-tutorials.readthedocs-hosted.com/en/latest/visualizing-marker-genes.html?highlight=%27bulk_labels%27)"" use the object ""pbmc"" with following description:; _>>> adata; AnnData object with n_obs × n_vars = 691 × 4549; **obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain'**; var: 'gene_ids', 'feature_types', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'_ . i want to process my data according those two tutorials and got the figures. but without those information (obs: 'bulk_labels', 'S_score', 'G2M_score', 'phase'), i cannot continue the tutorial from ""Clustering 3K PBMCs"" to ""Visualizing marker genes""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/744
https://github.com/scverse/scanpy/issues/745:7,Performance,perform,perform,7,"how to perform analysis to get bulk_labels, S_score, G2M_score, phase?. After following the tutorial ""[Clustering 3K PBMCs](https://icb-scanpy-tutorials.readthedocs-hosted.com/en/latest/pbmc3k.html)"", I got the object ""adata"" with following description:; _>>> adata; AnnData object with n_obs × n_vars = 691 × 4549; **obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain'**; var: 'gene_ids', 'feature_types', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'_ . however, the tutorial ""[Visualizing marker genes](https://icb-scanpy-tutorials.readthedocs-hosted.com/en/latest/visualizing-marker-genes.html?highlight=%27bulk_labels%27)"" use the object ""pbmc"" with following description:; _>>> pbmc; AnnData object with n_obs × n_vars = 700 × 765; **obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score', 'G2M_score', 'phase', 'louvain'**; var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'; uns: 'bulk_labels_colors', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'_. i want to process my data according those two tutorials and got the figures. but without those information (obs: 'bulk_labels', 'S_score', 'G2M_score', 'phase'), i cannot continue the tutorial from ""Clustering 3K PBMCs"" to ""Visualizing marker genes""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/745
https://github.com/scverse/scanpy/issues/746:46,Testability,log,logging,46,"Hello,. I tried to swap BBKNN over to the new logging, and while the timing aspect of it is functional, the `deep` refuses to cooperate. I checked with `sc.pp.neighbors()` just to make sure I'm not screwing things up massively, and it turns out that its deep also doesn't work. <img width=""940"" alt=""Screen Shot 2019-07-23 at 09 37 10"" src=""https://user-images.githubusercontent.com/14993986/61696691-b434bf00-ad2d-11e9-83db-1092dcc61fea.png"">. Any idea what's going on here? I'm on python 3.6.7 on Bionic, jupyter 1.0.0, and all this:. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:634,Usability,learn,learn,634,"Hello,. I tried to swap BBKNN over to the new logging, and while the timing aspect of it is functional, the `deep` refuses to cooperate. I checked with `sc.pp.neighbors()` just to make sure I'm not screwing things up massively, and it turns out that its deep also doesn't work. <img width=""940"" alt=""Screen Shot 2019-07-23 at 09 37 10"" src=""https://user-images.githubusercontent.com/14993986/61696691-b434bf00-ad2d-11e9-83db-1092dcc61fea.png"">. Any idea what's going on here? I'm on python 3.6.7 on Bionic, jupyter 1.0.0, and all this:. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/747:259,Availability,error,error,259,"Hi, I am using anndata 0.6.21 and scanpy 1.4.3; I executed this code:; ```; sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]; ```. and I got this error:; `AssertionError: Don’t call _normalize_index with non-categorical/string names; `; Can you help me?. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:268,Testability,Assert,AssertionError,268,"Hi, I am using anndata 0.6.21 and scanpy 1.4.3; I executed this code:; ```; sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]; ```. and I got this error:; `AssertionError: Don’t call _normalize_index with non-categorical/string names; `; Can you help me?. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/748:242,Testability,test,test,242,"After clustering cells with a restricted gene set, I would like to see the contribution of ""specified genes"" in subgrouping the cells. ""sc.tl.rank_genes_groups"" uses all the genes in the background for the statistical calculations. I want to test it for all the Louvain groups against the rest of the data (so, groups='all', reference='rest'). Is there a way, we can specify the gene list? (I tried using the 'use_raw' of sc.tl.rank_genes_groups to subset). I don't find any other options to restrict gene lists here. . `subset_genes = ldata[:, ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']]. sc.tl.rank_genes_groups(ldata, 'louvain', method='wilcoxon', use_raw= 'subset_genes', n_genes = 100). sc.pl.rank_genes_groups(ldata, n_genes=15, sharey=False)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/751:689,Availability,error,error,689,"Good day!. I have been trying to run the single cell tutorial but have had some issues concatenating several datasets. I am able to read successfully the first data set. However, once I want to load the other datasets, there is a problem concatenating the files. . <img width=""749"" alt=""Screenshot 2019-07-27 at 12 49 49"" src=""https://user-images.githubusercontent.com/37718031/61993507-06971800-b06d-11e9-815e-acf667f818a5.png"">; <img width=""730"" alt=""Screenshot 2019-07-27 at 12 41 58"" src=""https://user-images.githubusercontent.com/37718031/61993416-edda3280-b06b-11e9-9a4f-7d4a1259cd47.png"">. This happens in the first loop to load all the datasets. If I run only one dataset the same error `(unsupported operand type(s) for +: 'int' and 'str')` showed up when I plot some data quality summary plots:. For instance:; `p1 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac') p2 = sc.pl.scatter(adata[adata.obs['n_counts']<10000], 'n_counts', 'n_genes', color='mt_frac')`; `adata = adata[adata.obs['mt_frac'] < 0.2]; print('Number of cells after MT filter: {:d}'.format(adata.n_obs))`; `sc.pp.filter_cells(adata, min_genes = 700); print('Number of cells after gene filter: {:d}'.format(adata.n_obs))`. I am using data generated by 10x V3 and CellRanger v3.0.2. I really do not know where the problem is. . I really appreciate any advice/help to solve this issue. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/751
https://github.com/scverse/scanpy/issues/751:194,Performance,load,load,194,"Good day!. I have been trying to run the single cell tutorial but have had some issues concatenating several datasets. I am able to read successfully the first data set. However, once I want to load the other datasets, there is a problem concatenating the files. . <img width=""749"" alt=""Screenshot 2019-07-27 at 12 49 49"" src=""https://user-images.githubusercontent.com/37718031/61993507-06971800-b06d-11e9-815e-acf667f818a5.png"">; <img width=""730"" alt=""Screenshot 2019-07-27 at 12 41 58"" src=""https://user-images.githubusercontent.com/37718031/61993416-edda3280-b06b-11e9-9a4f-7d4a1259cd47.png"">. This happens in the first loop to load all the datasets. If I run only one dataset the same error `(unsupported operand type(s) for +: 'int' and 'str')` showed up when I plot some data quality summary plots:. For instance:; `p1 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac') p2 = sc.pl.scatter(adata[adata.obs['n_counts']<10000], 'n_counts', 'n_genes', color='mt_frac')`; `adata = adata[adata.obs['mt_frac'] < 0.2]; print('Number of cells after MT filter: {:d}'.format(adata.n_obs))`; `sc.pp.filter_cells(adata, min_genes = 700); print('Number of cells after gene filter: {:d}'.format(adata.n_obs))`. I am using data generated by 10x V3 and CellRanger v3.0.2. I really do not know where the problem is. . I really appreciate any advice/help to solve this issue. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/751
https://github.com/scverse/scanpy/issues/751:631,Performance,load,load,631,"Good day!. I have been trying to run the single cell tutorial but have had some issues concatenating several datasets. I am able to read successfully the first data set. However, once I want to load the other datasets, there is a problem concatenating the files. . <img width=""749"" alt=""Screenshot 2019-07-27 at 12 49 49"" src=""https://user-images.githubusercontent.com/37718031/61993507-06971800-b06d-11e9-815e-acf667f818a5.png"">; <img width=""730"" alt=""Screenshot 2019-07-27 at 12 41 58"" src=""https://user-images.githubusercontent.com/37718031/61993416-edda3280-b06b-11e9-9a4f-7d4a1259cd47.png"">. This happens in the first loop to load all the datasets. If I run only one dataset the same error `(unsupported operand type(s) for +: 'int' and 'str')` showed up when I plot some data quality summary plots:. For instance:; `p1 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac') p2 = sc.pl.scatter(adata[adata.obs['n_counts']<10000], 'n_counts', 'n_genes', color='mt_frac')`; `adata = adata[adata.obs['mt_frac'] < 0.2]; print('Number of cells after MT filter: {:d}'.format(adata.n_obs))`; `sc.pp.filter_cells(adata, min_genes = 700); print('Number of cells after gene filter: {:d}'.format(adata.n_obs))`. I am using data generated by 10x V3 and CellRanger v3.0.2. I really do not know where the problem is. . I really appreciate any advice/help to solve this issue. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/751
https://github.com/scverse/scanpy/issues/752:114,Testability,log,logs,114,"Bioconda builds are failing for `v1.4.4`, https://github.com/bioconda/bioconda-recipes/pull/16473. From the build logs, it looks like this is related to `importlib_metadata` 😕.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/752
https://github.com/scverse/scanpy/issues/753:120,Availability,down,down,120,"When using long gene names the rank_genes_groups function does not work properly anymore as long gene names are trimmed down to 50 characters, which makes it difficult to look them up again in the adata object. I think the problem is caused on this line:; https://github.com/theislab/scanpy/blob/c5c32f2277ad3f9c5388fc5d0a602151f3bab42b/scanpy/tools/_rank_genes_groups.py#L401 where the gene names are casted to an unicode array with 50 elements. ```; np.array(['LOOONG','SHORT'],dtype='U5'); > array(['LOOON', 'SHORT'], dtype='<U5'); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/753
https://github.com/scverse/scanpy/issues/754:43,Modifiability,variab,variable,43,"Hi,; I have two levels in my ```groupby``` variable and was trying to find the differential expression genes between the two levels using the wilcoxon rank sum test. The following two commands will give different pvals and slightly different logfoldchanges; ```; sc.tl.rank_genes_groups(adata, groupby='status', groups=['ALS'], reference='ctrl', n_genes=100000, method='wilcoxon', use_raw=False); sc.tl.rank_genes_groups(adata, groupby='status', groups=['ctrl'], reference='ALS', n_genes=100000, method='wilcoxon', use_raw=False); ```; ```; 	gene	logfoldchanges_ALS_ctrl	pvals_ALS_ctrl	logfoldchanges_ctrl_ALS	pvals_ctrl_ALS; 0	SLC11A1	2.9489155	5.91E-75	-2.9489155	2.08E-73; 1	NEAT1	1.1250153	5.11E-66	-1.1250151	6.82E-64; 2	FKBP5	2.7334108	8.94E-47	-2.7334108	1.78E-45; 3	SPP1	2.1242297	2.27E-42	-2.1242297	2.69E-41; 4	FCGR3A	2.6661332	6.95E-40	-2.6661332	5.37E-39; 5	HAMP	5.394592	1.27E-37	-5.394592	2.27E-36; 6	CD163	3.0886266	9.11E-36	-3.0886264	1.71E-34; 7	RASSF4	2.3211384	2.83E-34	-2.3211384	3.74E-33; 8	DSE	2.8529236	7.43E-33	-2.8529236	7.86E-32; 9	MAFB	2.7013724	3.67E-32	-2.7013724	6.43E-31; 10	DENND3	1.4753485	5.13E-29	-1.4753484	1.19E-27; 11	APOE	1.4111803	1.12E-28	-1.4111804	9.04E-28; 12	C1QB	1.5169998	3.53E-27	-1.5169998	1.68E-25; 13	C3	1.3675922	1.05E-25	-1.3675922	2.62E-25; ```; Am I not doing it right, or because of the tie issue mentioned here?; #698 ; Thanks for your help!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/754
https://github.com/scverse/scanpy/issues/754:160,Testability,test,test,160,"Hi,; I have two levels in my ```groupby``` variable and was trying to find the differential expression genes between the two levels using the wilcoxon rank sum test. The following two commands will give different pvals and slightly different logfoldchanges; ```; sc.tl.rank_genes_groups(adata, groupby='status', groups=['ALS'], reference='ctrl', n_genes=100000, method='wilcoxon', use_raw=False); sc.tl.rank_genes_groups(adata, groupby='status', groups=['ctrl'], reference='ALS', n_genes=100000, method='wilcoxon', use_raw=False); ```; ```; 	gene	logfoldchanges_ALS_ctrl	pvals_ALS_ctrl	logfoldchanges_ctrl_ALS	pvals_ctrl_ALS; 0	SLC11A1	2.9489155	5.91E-75	-2.9489155	2.08E-73; 1	NEAT1	1.1250153	5.11E-66	-1.1250151	6.82E-64; 2	FKBP5	2.7334108	8.94E-47	-2.7334108	1.78E-45; 3	SPP1	2.1242297	2.27E-42	-2.1242297	2.69E-41; 4	FCGR3A	2.6661332	6.95E-40	-2.6661332	5.37E-39; 5	HAMP	5.394592	1.27E-37	-5.394592	2.27E-36; 6	CD163	3.0886266	9.11E-36	-3.0886264	1.71E-34; 7	RASSF4	2.3211384	2.83E-34	-2.3211384	3.74E-33; 8	DSE	2.8529236	7.43E-33	-2.8529236	7.86E-32; 9	MAFB	2.7013724	3.67E-32	-2.7013724	6.43E-31; 10	DENND3	1.4753485	5.13E-29	-1.4753484	1.19E-27; 11	APOE	1.4111803	1.12E-28	-1.4111804	9.04E-28; 12	C1QB	1.5169998	3.53E-27	-1.5169998	1.68E-25; 13	C3	1.3675922	1.05E-25	-1.3675922	2.62E-25; ```; Am I not doing it right, or because of the tie issue mentioned here?; #698 ; Thanks for your help!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/754
https://github.com/scverse/scanpy/issues/754:242,Testability,log,logfoldchanges,242,"Hi,; I have two levels in my ```groupby``` variable and was trying to find the differential expression genes between the two levels using the wilcoxon rank sum test. The following two commands will give different pvals and slightly different logfoldchanges; ```; sc.tl.rank_genes_groups(adata, groupby='status', groups=['ALS'], reference='ctrl', n_genes=100000, method='wilcoxon', use_raw=False); sc.tl.rank_genes_groups(adata, groupby='status', groups=['ctrl'], reference='ALS', n_genes=100000, method='wilcoxon', use_raw=False); ```; ```; 	gene	logfoldchanges_ALS_ctrl	pvals_ALS_ctrl	logfoldchanges_ctrl_ALS	pvals_ctrl_ALS; 0	SLC11A1	2.9489155	5.91E-75	-2.9489155	2.08E-73; 1	NEAT1	1.1250153	5.11E-66	-1.1250151	6.82E-64; 2	FKBP5	2.7334108	8.94E-47	-2.7334108	1.78E-45; 3	SPP1	2.1242297	2.27E-42	-2.1242297	2.69E-41; 4	FCGR3A	2.6661332	6.95E-40	-2.6661332	5.37E-39; 5	HAMP	5.394592	1.27E-37	-5.394592	2.27E-36; 6	CD163	3.0886266	9.11E-36	-3.0886264	1.71E-34; 7	RASSF4	2.3211384	2.83E-34	-2.3211384	3.74E-33; 8	DSE	2.8529236	7.43E-33	-2.8529236	7.86E-32; 9	MAFB	2.7013724	3.67E-32	-2.7013724	6.43E-31; 10	DENND3	1.4753485	5.13E-29	-1.4753484	1.19E-27; 11	APOE	1.4111803	1.12E-28	-1.4111804	9.04E-28; 12	C1QB	1.5169998	3.53E-27	-1.5169998	1.68E-25; 13	C3	1.3675922	1.05E-25	-1.3675922	2.62E-25; ```; Am I not doing it right, or because of the tie issue mentioned here?; #698 ; Thanks for your help!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/754
https://github.com/scverse/scanpy/issues/755:31,Availability,error,error,31,"Dear all,. I had the following error when going through the PBMC3K tutorial. Everything was fine until I got to this step (Embedding the neighbourhood graph):. ```py; sc.tl.paga(adata); ```. which raises the following:. <details><summary>KeyError: 'louvain', Traceback:</summary>. ```pytb; running PAGA; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); C:\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 2656 try:; -> 2657 return self._engine.get_loc(key); 2658 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'louvain'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-31-5aa170e493c3> in <module>; ----> 1 sc.tl.paga(adata). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in paga(adata, groups, use_rna_velocity, model, copy); 92 adata.uns['paga'] = {}; 93 if not use_rna_velocity:; ---> 94 paga.compute_connectivities(); 95 adata.uns['paga']['connectivities'] = paga.connectivities; 96 adata.uns['paga']['connectivities_tree'] = paga.connectivities_tree. C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in compute_connectivities(self); 126 def compute_connectivities(self):; 127 if self._model == 'v1.2':; --> 128 return self._compute_connectivities_v1_2(); 129 elif self._model == 'v1.0':; 130 return self._compute_connectivities_v1_0(). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in _compute_connectivities_v1_2(self); 141 g = utils.get_igraph_from_adjacency(ones, directed=True); 142 vc = igraph.VertexClustering(; --> 143 g, membership=self._adata.obs[self._groups_key",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/755
https://github.com/scverse/scanpy/issues/755:513,Availability,toler,tolerance,513,"Dear all,. I had the following error when going through the PBMC3K tutorial. Everything was fine until I got to this step (Embedding the neighbourhood graph):. ```py; sc.tl.paga(adata); ```. which raises the following:. <details><summary>KeyError: 'louvain', Traceback:</summary>. ```pytb; running PAGA; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); C:\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 2656 try:; -> 2657 return self._engine.get_loc(key); 2658 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'louvain'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-31-5aa170e493c3> in <module>; ----> 1 sc.tl.paga(adata). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in paga(adata, groups, use_rna_velocity, model, copy); 92 adata.uns['paga'] = {}; 93 if not use_rna_velocity:; ---> 94 paga.compute_connectivities(); 95 adata.uns['paga']['connectivities'] = paga.connectivities; 96 adata.uns['paga']['connectivities_tree'] = paga.connectivities_tree. C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in compute_connectivities(self); 126 def compute_connectivities(self):; 127 if self._model == 'v1.2':; --> 128 return self._compute_connectivities_v1_2(); 129 elif self._model == 'v1.0':; 130 return self._compute_connectivities_v1_0(). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in _compute_connectivities_v1_2(self); 141 g = utils.get_igraph_from_adjacency(ones, directed=True); 142 vc = igraph.VertexClustering(; --> 143 g, membership=self._adata.obs[self._groups_key",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/755
https://github.com/scverse/scanpy/issues/755:2405,Availability,toler,tolerance,2405,"data.uns['paga']['connectivities_tree'] = paga.connectivities_tree. C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in compute_connectivities(self); 126 def compute_connectivities(self):; 127 if self._model == 'v1.2':; --> 128 return self._compute_connectivities_v1_2(); 129 elif self._model == 'v1.0':; 130 return self._compute_connectivities_v1_0(). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in _compute_connectivities_v1_2(self); 141 g = utils.get_igraph_from_adjacency(ones, directed=True); 142 vc = igraph.VertexClustering(; --> 143 g, membership=self._adata.obs[self._groups_key].cat.codes.values); 144 ns = vc.sizes(); 145 n = sum(ns). C:\Anaconda\lib\site-packages\pandas\core\frame.py in __getitem__(self, key); 2925 if self.columns.nlevels > 1:; 2926 return self._getitem_multilevel(key); -> 2927 indexer = self.columns.get_loc(key); 2928 if is_integer(indexer):; 2929 indexer = [indexer]. C:\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 2657 return self._engine.get_loc(key); 2658 except KeyError:; -> 2659 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2660 indexer = self.get_indexer([key], method=method, tolerance=tolerance); 2661 if indexer.ndim > 1 or indexer.size > 1:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'louvain'; ```. </details>. I am a beginner in Python, so I fully realise that the background information may not be complete. Below is my system info:; ```; {'commit_hash': 'd774f565b',; 'commit_source': 'installation',; 'default_encoding': 'cp1252',; 'ipython_path': 'C:\\Anaconda\\lib\\site-packages\\IPython',; 'ipython_version': '7.4.0',; 'os_name': 'nt',; 'platform': 'Windows-10-10.0.1836",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/755
https://github.com/scverse/scanpy/issues/755:2601,Availability,toler,tolerance,2601,"elf._model == 'v1.2':; --> 128 return self._compute_connectivities_v1_2(); 129 elif self._model == 'v1.0':; 130 return self._compute_connectivities_v1_0(). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in _compute_connectivities_v1_2(self); 141 g = utils.get_igraph_from_adjacency(ones, directed=True); 142 vc = igraph.VertexClustering(; --> 143 g, membership=self._adata.obs[self._groups_key].cat.codes.values); 144 ns = vc.sizes(); 145 n = sum(ns). C:\Anaconda\lib\site-packages\pandas\core\frame.py in __getitem__(self, key); 2925 if self.columns.nlevels > 1:; 2926 return self._getitem_multilevel(key); -> 2927 indexer = self.columns.get_loc(key); 2928 if is_integer(indexer):; 2929 indexer = [indexer]. C:\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 2657 return self._engine.get_loc(key); 2658 except KeyError:; -> 2659 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2660 indexer = self.get_indexer([key], method=method, tolerance=tolerance); 2661 if indexer.ndim > 1 or indexer.size > 1:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'louvain'; ```. </details>. I am a beginner in Python, so I fully realise that the background information may not be complete. Below is my system info:; ```; {'commit_hash': 'd774f565b',; 'commit_source': 'installation',; 'default_encoding': 'cp1252',; 'ipython_path': 'C:\\Anaconda\\lib\\site-packages\\IPython',; 'ipython_version': '7.4.0',; 'os_name': 'nt',; 'platform': 'Windows-10-10.0.18362-SP0',; 'sys_executable': 'C:\\Anaconda\\python.exe',; 'sys_platform': 'win32',; 'sys_version': '3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit '; '(AMD64)]'}; ```. Thank you very much for",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/755
https://github.com/scverse/scanpy/issues/755:2611,Availability,toler,tolerance,2611,"elf._model == 'v1.2':; --> 128 return self._compute_connectivities_v1_2(); 129 elif self._model == 'v1.0':; 130 return self._compute_connectivities_v1_0(). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in _compute_connectivities_v1_2(self); 141 g = utils.get_igraph_from_adjacency(ones, directed=True); 142 vc = igraph.VertexClustering(; --> 143 g, membership=self._adata.obs[self._groups_key].cat.codes.values); 144 ns = vc.sizes(); 145 n = sum(ns). C:\Anaconda\lib\site-packages\pandas\core\frame.py in __getitem__(self, key); 2925 if self.columns.nlevels > 1:; 2926 return self._getitem_multilevel(key); -> 2927 indexer = self.columns.get_loc(key); 2928 if is_integer(indexer):; 2929 indexer = [indexer]. C:\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 2657 return self._engine.get_loc(key); 2658 except KeyError:; -> 2659 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2660 indexer = self.get_indexer([key], method=method, tolerance=tolerance); 2661 if indexer.ndim > 1 or indexer.size > 1:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'louvain'; ```. </details>. I am a beginner in Python, so I fully realise that the background information may not be complete. Below is my system info:; ```; {'commit_hash': 'd774f565b',; 'commit_source': 'installation',; 'default_encoding': 'cp1252',; 'ipython_path': 'C:\\Anaconda\\lib\\site-packages\\IPython',; 'ipython_version': '7.4.0',; 'os_name': 'nt',; 'platform': 'Windows-10-10.0.18362-SP0',; 'sys_executable': 'C:\\Anaconda\\python.exe',; 'sys_platform': 'win32',; 'sys_version': '3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit '; '(AMD64)]'}; ```. Thank you very much for",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/755
https://github.com/scverse/scanpy/issues/755:3214,Deployability,install,installation,3214,"self._compute_connectivities_v1_2(); 129 elif self._model == 'v1.0':; 130 return self._compute_connectivities_v1_0(). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in _compute_connectivities_v1_2(self); 141 g = utils.get_igraph_from_adjacency(ones, directed=True); 142 vc = igraph.VertexClustering(; --> 143 g, membership=self._adata.obs[self._groups_key].cat.codes.values); 144 ns = vc.sizes(); 145 n = sum(ns). C:\Anaconda\lib\site-packages\pandas\core\frame.py in __getitem__(self, key); 2925 if self.columns.nlevels > 1:; 2926 return self._getitem_multilevel(key); -> 2927 indexer = self.columns.get_loc(key); 2928 if is_integer(indexer):; 2929 indexer = [indexer]. C:\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 2657 return self._engine.get_loc(key); 2658 except KeyError:; -> 2659 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2660 indexer = self.get_indexer([key], method=method, tolerance=tolerance); 2661 if indexer.ndim > 1 or indexer.size > 1:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'louvain'; ```. </details>. I am a beginner in Python, so I fully realise that the background information may not be complete. Below is my system info:; ```; {'commit_hash': 'd774f565b',; 'commit_source': 'installation',; 'default_encoding': 'cp1252',; 'ipython_path': 'C:\\Anaconda\\lib\\site-packages\\IPython',; 'ipython_version': '7.4.0',; 'os_name': 'nt',; 'platform': 'Windows-10-10.0.18362-SP0',; 'sys_executable': 'C:\\Anaconda\\python.exe',; 'sys_platform': 'win32',; 'sys_version': '3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit '; '(AMD64)]'}; ```. Thank you very much for your response. Best regards,; Mikhael",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/755
https://github.com/scverse/scanpy/issues/755:793,Security,hash,hashtable,793,"Dear all,. I had the following error when going through the PBMC3K tutorial. Everything was fine until I got to this step (Embedding the neighbourhood graph):. ```py; sc.tl.paga(adata); ```. which raises the following:. <details><summary>KeyError: 'louvain', Traceback:</summary>. ```pytb; running PAGA; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); C:\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 2656 try:; -> 2657 return self._engine.get_loc(key); 2658 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'louvain'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-31-5aa170e493c3> in <module>; ----> 1 sc.tl.paga(adata). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in paga(adata, groups, use_rna_velocity, model, copy); 92 adata.uns['paga'] = {}; 93 if not use_rna_velocity:; ---> 94 paga.compute_connectivities(); 95 adata.uns['paga']['connectivities'] = paga.connectivities; 96 adata.uns['paga']['connectivities_tree'] = paga.connectivities_tree. C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in compute_connectivities(self); 126 def compute_connectivities(self):; 127 if self._model == 'v1.2':; --> 128 return self._compute_connectivities_v1_2(); 129 elif self._model == 'v1.0':; 130 return self._compute_connectivities_v1_0(). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in _compute_connectivities_v1_2(self); 141 g = utils.get_igraph_from_adjacency(ones, directed=True); 142 vc = igraph.VertexClustering(; --> 143 g, membership=self._adata.obs[self._groups_key",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/755
https://github.com/scverse/scanpy/issues/755:889,Security,hash,hashtable,889,"Dear all,. I had the following error when going through the PBMC3K tutorial. Everything was fine until I got to this step (Embedding the neighbourhood graph):. ```py; sc.tl.paga(adata); ```. which raises the following:. <details><summary>KeyError: 'louvain', Traceback:</summary>. ```pytb; running PAGA; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); C:\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 2656 try:; -> 2657 return self._engine.get_loc(key); 2658 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'louvain'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-31-5aa170e493c3> in <module>; ----> 1 sc.tl.paga(adata). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in paga(adata, groups, use_rna_velocity, model, copy); 92 adata.uns['paga'] = {}; 93 if not use_rna_velocity:; ---> 94 paga.compute_connectivities(); 95 adata.uns['paga']['connectivities'] = paga.connectivities; 96 adata.uns['paga']['connectivities_tree'] = paga.connectivities_tree. C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in compute_connectivities(self); 126 def compute_connectivities(self):; 127 if self._model == 'v1.2':; --> 128 return self._compute_connectivities_v1_2(); 129 elif self._model == 'v1.0':; 130 return self._compute_connectivities_v1_0(). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in _compute_connectivities_v1_2(self); 141 g = utils.get_igraph_from_adjacency(ones, directed=True); 142 vc = igraph.VertexClustering(; --> 143 g, membership=self._adata.obs[self._groups_key",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/755
https://github.com/scverse/scanpy/issues/755:2862,Security,hash,hashtable,2862,"self._compute_connectivities_v1_2(); 129 elif self._model == 'v1.0':; 130 return self._compute_connectivities_v1_0(). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in _compute_connectivities_v1_2(self); 141 g = utils.get_igraph_from_adjacency(ones, directed=True); 142 vc = igraph.VertexClustering(; --> 143 g, membership=self._adata.obs[self._groups_key].cat.codes.values); 144 ns = vc.sizes(); 145 n = sum(ns). C:\Anaconda\lib\site-packages\pandas\core\frame.py in __getitem__(self, key); 2925 if self.columns.nlevels > 1:; 2926 return self._getitem_multilevel(key); -> 2927 indexer = self.columns.get_loc(key); 2928 if is_integer(indexer):; 2929 indexer = [indexer]. C:\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 2657 return self._engine.get_loc(key); 2658 except KeyError:; -> 2659 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2660 indexer = self.get_indexer([key], method=method, tolerance=tolerance); 2661 if indexer.ndim > 1 or indexer.size > 1:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'louvain'; ```. </details>. I am a beginner in Python, so I fully realise that the background information may not be complete. Below is my system info:; ```; {'commit_hash': 'd774f565b',; 'commit_source': 'installation',; 'default_encoding': 'cp1252',; 'ipython_path': 'C:\\Anaconda\\lib\\site-packages\\IPython',; 'ipython_version': '7.4.0',; 'os_name': 'nt',; 'platform': 'Windows-10-10.0.18362-SP0',; 'sys_executable': 'C:\\Anaconda\\python.exe',; 'sys_platform': 'win32',; 'sys_version': '3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit '; '(AMD64)]'}; ```. Thank you very much for your response. Best regards,; Mikhael",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/755
https://github.com/scverse/scanpy/issues/755:2958,Security,hash,hashtable,2958,"self._compute_connectivities_v1_2(); 129 elif self._model == 'v1.0':; 130 return self._compute_connectivities_v1_0(). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in _compute_connectivities_v1_2(self); 141 g = utils.get_igraph_from_adjacency(ones, directed=True); 142 vc = igraph.VertexClustering(; --> 143 g, membership=self._adata.obs[self._groups_key].cat.codes.values); 144 ns = vc.sizes(); 145 n = sum(ns). C:\Anaconda\lib\site-packages\pandas\core\frame.py in __getitem__(self, key); 2925 if self.columns.nlevels > 1:; 2926 return self._getitem_multilevel(key); -> 2927 indexer = self.columns.get_loc(key); 2928 if is_integer(indexer):; 2929 indexer = [indexer]. C:\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 2657 return self._engine.get_loc(key); 2658 except KeyError:; -> 2659 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2660 indexer = self.get_indexer([key], method=method, tolerance=tolerance); 2661 if indexer.ndim > 1 or indexer.size > 1:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'louvain'; ```. </details>. I am a beginner in Python, so I fully realise that the background information may not be complete. Below is my system info:; ```; {'commit_hash': 'd774f565b',; 'commit_source': 'installation',; 'default_encoding': 'cp1252',; 'ipython_path': 'C:\\Anaconda\\lib\\site-packages\\IPython',; 'ipython_version': '7.4.0',; 'os_name': 'nt',; 'platform': 'Windows-10-10.0.18362-SP0',; 'sys_executable': 'C:\\Anaconda\\python.exe',; 'sys_platform': 'win32',; 'sys_version': '3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit '; '(AMD64)]'}; ```. Thank you very much for your response. Best regards,; Mikhael",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/755
https://github.com/scverse/scanpy/issues/759:859,Modifiability,variab,variable,859,"Hello,. In some cases, I need to visualize clustering results (categorical) on UMAP for each batch. . I know `sc.pl.umap(adata[adata.obs['batch] == 'batch1], color = 'louvain')` is a solution. However, other cells are missing. I think the other cells colored by grey as background should be a better way. . I notice that `sc.pl.umap(adata, color = 'batch', groups = ['batch1'] )` can retain other cells as grey, though sometimes cells were submerged in the bottom layer (I used reoder_categories to bypass this issue). But, `color` and `groups` must be correspondence! . Is there any way to fulfill my needs in `Scanpy` if I missed something. ; Or, could the authors add a parameters, such as `restrict_to` in `sc.tl.louvain`, to implement this function: ① liberate strong associations between `color` and `groups`, and ② add support for ordering categorical variable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/759
https://github.com/scverse/scanpy/issues/760:113,Availability,error,error,113,"Hi; I am trying to concatenate two anndata objects(adata = adata1.concatenate(adata3, join='inner')), but get an error message informing me that concatenate is not found. Any idea what might cause this behavior? Here is what I have installed:scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. and here is the error:; AttributeError Traceback (most recent call last); <ipython-input-187-32c3eda3cdc8> in <module>; ----> 1 adata = adata1.concatenate(adata3, join='inner'). ~/anaconda3/envs/scenv/lib/python3.6/site-packages/scipy/sparse/base.py in __getattr__(self, attr); 687 return self.getnnz(); 688 else:; --> 689 raise AttributeError(attr + "" not found""); 690 ; 691 def transpose(self, axes=None, copy=False):. AttributeError: concatenate not found; Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/760
https://github.com/scverse/scanpy/issues/760:425,Availability,error,error,425,"Hi; I am trying to concatenate two anndata objects(adata = adata1.concatenate(adata3, join='inner')), but get an error message informing me that concatenate is not found. Any idea what might cause this behavior? Here is what I have installed:scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. and here is the error:; AttributeError Traceback (most recent call last); <ipython-input-187-32c3eda3cdc8> in <module>; ----> 1 adata = adata1.concatenate(adata3, join='inner'). ~/anaconda3/envs/scenv/lib/python3.6/site-packages/scipy/sparse/base.py in __getattr__(self, attr); 687 return self.getnnz(); 688 else:; --> 689 raise AttributeError(attr + "" not found""); 690 ; 691 def transpose(self, axes=None, copy=False):. AttributeError: concatenate not found; Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/760
https://github.com/scverse/scanpy/issues/760:232,Deployability,install,installed,232,"Hi; I am trying to concatenate two anndata objects(adata = adata1.concatenate(adata3, join='inner')), but get an error message informing me that concatenate is not found. Any idea what might cause this behavior? Here is what I have installed:scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. and here is the error:; AttributeError Traceback (most recent call last); <ipython-input-187-32c3eda3cdc8> in <module>; ----> 1 adata = adata1.concatenate(adata3, join='inner'). ~/anaconda3/envs/scenv/lib/python3.6/site-packages/scipy/sparse/base.py in __getattr__(self, attr); 687 return self.getnnz(); 688 else:; --> 689 raise AttributeError(attr + "" not found""); 690 ; 691 def transpose(self, axes=None, copy=False):. AttributeError: concatenate not found; Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/760
https://github.com/scverse/scanpy/issues/760:119,Integrability,message,message,119,"Hi; I am trying to concatenate two anndata objects(adata = adata1.concatenate(adata3, join='inner')), but get an error message informing me that concatenate is not found. Any idea what might cause this behavior? Here is what I have installed:scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. and here is the error:; AttributeError Traceback (most recent call last); <ipython-input-187-32c3eda3cdc8> in <module>; ----> 1 adata = adata1.concatenate(adata3, join='inner'). ~/anaconda3/envs/scenv/lib/python3.6/site-packages/scipy/sparse/base.py in __getattr__(self, attr); 687 return self.getnnz(); 688 else:; --> 689 raise AttributeError(attr + "" not found""); 690 ; 691 def transpose(self, axes=None, copy=False):. AttributeError: concatenate not found; Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/760
https://github.com/scverse/scanpy/issues/760:339,Usability,learn,learn,339,"Hi; I am trying to concatenate two anndata objects(adata = adata1.concatenate(adata3, join='inner')), but get an error message informing me that concatenate is not found. Any idea what might cause this behavior? Here is what I have installed:scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. and here is the error:; AttributeError Traceback (most recent call last); <ipython-input-187-32c3eda3cdc8> in <module>; ----> 1 adata = adata1.concatenate(adata3, join='inner'). ~/anaconda3/envs/scenv/lib/python3.6/site-packages/scipy/sparse/base.py in __getattr__(self, attr); 687 return self.getnnz(); 688 else:; --> 689 raise AttributeError(attr + "" not found""); 690 ; 691 def transpose(self, axes=None, copy=False):. AttributeError: concatenate not found; Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/760
https://github.com/scverse/scanpy/issues/762:20,Availability,error,error,20,"I get the following error when trying to use sc.pl.scatter to plot gene expression, with use_raw=False. I am using sc.pl.scatter instead of sc.pl.umap, .tsne, etc., because of the need to use custom basis names.; ```; File ""/opt/Python/3.6.5/lib/python3.6/site-packages/scanpy/plotting/_anndata.py"", line 118, in scatter; ax=ax); File ""/opt/Python/3.6.5/lib/python3.6/site-packages/scanpy/plotting/_anndata.py"", line 390, in _scatter_obs; c = adata.raw.obs_vector(key, layer=layers[2]); TypeError: obs_vector() got an unexpected keyword argument 'layer'; ```. The following snippet is copied from `_scatter_obs()` in /scanpy/plotting/_anndata.py; ``` python; # coloring according to gene expression; elif (use_raw; and adata.raw is not None; and key in adata.raw.var_names):; c = adata.raw.obs_vector(key); elif key in adata.var_names:; c = adata.raw.obs_vector(key, layer=layers[2]); ```; Should line 390 be c = adata.obs_vector(key, layer=layers[2]) since it is handling the case when use_raw==False and adata.raw.obs_vector does not take layer as argument.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762
https://github.com/scverse/scanpy/issues/762:475,Modifiability,layers,layers,475,"I get the following error when trying to use sc.pl.scatter to plot gene expression, with use_raw=False. I am using sc.pl.scatter instead of sc.pl.umap, .tsne, etc., because of the need to use custom basis names.; ```; File ""/opt/Python/3.6.5/lib/python3.6/site-packages/scanpy/plotting/_anndata.py"", line 118, in scatter; ax=ax); File ""/opt/Python/3.6.5/lib/python3.6/site-packages/scanpy/plotting/_anndata.py"", line 390, in _scatter_obs; c = adata.raw.obs_vector(key, layer=layers[2]); TypeError: obs_vector() got an unexpected keyword argument 'layer'; ```. The following snippet is copied from `_scatter_obs()` in /scanpy/plotting/_anndata.py; ``` python; # coloring according to gene expression; elif (use_raw; and adata.raw is not None; and key in adata.raw.var_names):; c = adata.raw.obs_vector(key); elif key in adata.var_names:; c = adata.raw.obs_vector(key, layer=layers[2]); ```; Should line 390 be c = adata.obs_vector(key, layer=layers[2]) since it is handling the case when use_raw==False and adata.raw.obs_vector does not take layer as argument.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762
https://github.com/scverse/scanpy/issues/762:873,Modifiability,layers,layers,873,"I get the following error when trying to use sc.pl.scatter to plot gene expression, with use_raw=False. I am using sc.pl.scatter instead of sc.pl.umap, .tsne, etc., because of the need to use custom basis names.; ```; File ""/opt/Python/3.6.5/lib/python3.6/site-packages/scanpy/plotting/_anndata.py"", line 118, in scatter; ax=ax); File ""/opt/Python/3.6.5/lib/python3.6/site-packages/scanpy/plotting/_anndata.py"", line 390, in _scatter_obs; c = adata.raw.obs_vector(key, layer=layers[2]); TypeError: obs_vector() got an unexpected keyword argument 'layer'; ```. The following snippet is copied from `_scatter_obs()` in /scanpy/plotting/_anndata.py; ``` python; # coloring according to gene expression; elif (use_raw; and adata.raw is not None; and key in adata.raw.var_names):; c = adata.raw.obs_vector(key); elif key in adata.var_names:; c = adata.raw.obs_vector(key, layer=layers[2]); ```; Should line 390 be c = adata.obs_vector(key, layer=layers[2]) since it is handling the case when use_raw==False and adata.raw.obs_vector does not take layer as argument.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762
https://github.com/scverse/scanpy/issues/762:941,Modifiability,layers,layers,941,"I get the following error when trying to use sc.pl.scatter to plot gene expression, with use_raw=False. I am using sc.pl.scatter instead of sc.pl.umap, .tsne, etc., because of the need to use custom basis names.; ```; File ""/opt/Python/3.6.5/lib/python3.6/site-packages/scanpy/plotting/_anndata.py"", line 118, in scatter; ax=ax); File ""/opt/Python/3.6.5/lib/python3.6/site-packages/scanpy/plotting/_anndata.py"", line 390, in _scatter_obs; c = adata.raw.obs_vector(key, layer=layers[2]); TypeError: obs_vector() got an unexpected keyword argument 'layer'; ```. The following snippet is copied from `_scatter_obs()` in /scanpy/plotting/_anndata.py; ``` python; # coloring according to gene expression; elif (use_raw; and adata.raw is not None; and key in adata.raw.var_names):; c = adata.raw.obs_vector(key); elif key in adata.var_names:; c = adata.raw.obs_vector(key, layer=layers[2]); ```; Should line 390 be c = adata.obs_vector(key, layer=layers[2]) since it is handling the case when use_raw==False and adata.raw.obs_vector does not take layer as argument.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762
https://github.com/scverse/scanpy/issues/763:242,Availability,error,error,242,"Env:; * Ubuntu 16.04; * python 3.7; * pandas 0.25.0; * scanpy 1.4.4.post1. I have an AnnData object called `adata`. The maximum value in the count matrix `adata.X` is 3701. When I do; `sc.pp.highly_variable_genes(adata)`. I get the following error; ```; /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scipy/sparse/data.py:132: RuntimeWarning: overflow encountered in expm1; result = op(self._deduped_data()); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scipy/sparse/data.py:132: RuntimeWarning: invalid value encountered in expm1; result = op(self._deduped_data()); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square; var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract; var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:86: RuntimeWarning: overflow encountered in log1p; mean = np.log1p(mean); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:86: RuntimeWarning: invalid value encountered in log1p; mean = np.log1p(mean); Traceback (most recent call last):; File ""../../scvi/scvi_adata.py"", line 75, in <module>; sc.pp.highly_variable_genes(adata); File ""/home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 257, in highly_variable_genes; flavor=flavor); File ""/home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 92, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins); File ""/home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763
https://github.com/scverse/scanpy/issues/763:2294,Modifiability,variab,variable,2294,"ed in expm1; result = op(self._deduped_data()); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scipy/sparse/data.py:132: RuntimeWarning: invalid value encountered in expm1; result = op(self._deduped_data()); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square; var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract; var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:86: RuntimeWarning: overflow encountered in log1p; mean = np.log1p(mean); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:86: RuntimeWarning: invalid value encountered in log1p; mean = np.log1p(mean); Traceback (most recent call last):; File ""../../scvi/scvi_adata.py"", line 75, in <module>; sc.pp.highly_variable_genes(adata); File ""/home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 257, in highly_variable_genes; flavor=flavor); File ""/home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 92, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins); File ""/home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 233, in cut; ""cannot specify integer `bins` when input data "" ""contains infinity""; ValueError: cannot specify integer `bins` when input data contains infinity; ```. Indeed, if I do `np.expm1(3701)` I get an overflow. I think it will be necessary to come up with a way to calculate highly variable genes without doing `expm1` on the raw counts, due to this overflow issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763
https://github.com/scverse/scanpy/pull/765:43,Integrability,depend,dependency,43,Dask no longer having fsspec as a required dependency means the dask tests fail. This should fix that.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/765
https://github.com/scverse/scanpy/pull/765:69,Testability,test,tests,69,Dask no longer having fsspec as a required dependency means the dask tests fail. This should fix that.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/765
https://github.com/scverse/scanpy/issues/768:95,Availability,error,error,95,"Hi, . I have been trying to apply filters to an object within `scanpy` and I got the following error (See attachment). . Do you have an idea of what's going on? . ![image](https://user-images.githubusercontent.com/3297906/62468966-2aeea500-b78f-11e9-809f-ab0c0c22b0b7.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/768
https://github.com/scverse/scanpy/issues/769:82,Availability,error,error,82,"Hello,. Trying to use sc.tl.umap with initial positions from sc.tl.paga. Seems an error with UMAP from the error log. But it is only called when calling paga positions, UMAP works otherwise. I do see https://github.com/theislab/scanpy/issues/666, and https://github.com/lmcinnes/umap/pull/262, but I am already running scanpy 1.4.4 and umap 0.3.9 so I don't understand how to solve the issue?. Attached error. Any suggestions? ; Thanks!. <img width=""575"" alt=""Screen Shot 2019-08-05 at 19 02 18"" src=""https://user-images.githubusercontent.com/20108378/62485139-c560df80-b7b3-11e9-8333-7e511c263a79.png"">",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769
https://github.com/scverse/scanpy/issues/769:107,Availability,error,error,107,"Hello,. Trying to use sc.tl.umap with initial positions from sc.tl.paga. Seems an error with UMAP from the error log. But it is only called when calling paga positions, UMAP works otherwise. I do see https://github.com/theislab/scanpy/issues/666, and https://github.com/lmcinnes/umap/pull/262, but I am already running scanpy 1.4.4 and umap 0.3.9 so I don't understand how to solve the issue?. Attached error. Any suggestions? ; Thanks!. <img width=""575"" alt=""Screen Shot 2019-08-05 at 19 02 18"" src=""https://user-images.githubusercontent.com/20108378/62485139-c560df80-b7b3-11e9-8333-7e511c263a79.png"">",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769
https://github.com/scverse/scanpy/issues/769:403,Availability,error,error,403,"Hello,. Trying to use sc.tl.umap with initial positions from sc.tl.paga. Seems an error with UMAP from the error log. But it is only called when calling paga positions, UMAP works otherwise. I do see https://github.com/theislab/scanpy/issues/666, and https://github.com/lmcinnes/umap/pull/262, but I am already running scanpy 1.4.4 and umap 0.3.9 so I don't understand how to solve the issue?. Attached error. Any suggestions? ; Thanks!. <img width=""575"" alt=""Screen Shot 2019-08-05 at 19 02 18"" src=""https://user-images.githubusercontent.com/20108378/62485139-c560df80-b7b3-11e9-8333-7e511c263a79.png"">",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769
https://github.com/scverse/scanpy/issues/769:113,Testability,log,log,113,"Hello,. Trying to use sc.tl.umap with initial positions from sc.tl.paga. Seems an error with UMAP from the error log. But it is only called when calling paga positions, UMAP works otherwise. I do see https://github.com/theislab/scanpy/issues/666, and https://github.com/lmcinnes/umap/pull/262, but I am already running scanpy 1.4.4 and umap 0.3.9 so I don't understand how to solve the issue?. Attached error. Any suggestions? ; Thanks!. <img width=""575"" alt=""Screen Shot 2019-08-05 at 19 02 18"" src=""https://user-images.githubusercontent.com/20108378/62485139-c560df80-b7b3-11e9-8333-7e511c263a79.png"">",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769
https://github.com/scverse/scanpy/issues/770:674,Deployability,install,install,674,"Reproducible example. ```py; import scanpy as sc; import scanpy.external as ice; from itertools import cycle. pbmc = sc.datasets.pbmc68k_reduced(); sc.pp.pca(adata); sce.pp.bbknn(adata, batch_key='batch', copy=False,trim=0, save_knn=False)`; ```. ```pytb; TypeError Traceback (most recent call last); <ipython-input-231-50baef9a10a9> in <module>; 5 pbmc = sc.datasets.pbmc68k_reduced(); 6 sc.pp.pca(adata); ----> 7 sce.pp.bbknn(adata, batch_key='batch', copy=False,trim=0, save_knn=False). ~/Library/Python/3.7/lib/python/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 except ImportError:; 83 raise ImportError('Please install bbknn: `pip install bbknn`.'); ---> 84 return bbknn(**params, **kwargs). ~/Bioinformatics/usr/local/lib/python3.7/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, approx, metric, copy, **kwargs); 281 ; 282 	logg.info('	finished', time=start,; --> 283 		deep=('added to `.uns[\'neighbors\']`\n'; 284 ' \'distances\', weighted adjacency matrix\n'; 285 		'	\'connectivities\', weighted adjacency matrix')). ~/Library/Python/3.7/lib/python/site-packages/scanpy/logging.py in info(*args, **kwargs); 17 ; 18 def info(*args, **kwargs):; ---> 19 return msg(*args, v='info', **kwargs); 20 ; 21 . TypeError: msg() got an unexpected keyword argument 'deep'`; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/770
https://github.com/scverse/scanpy/issues/770:694,Deployability,install,install,694,"Reproducible example. ```py; import scanpy as sc; import scanpy.external as ice; from itertools import cycle. pbmc = sc.datasets.pbmc68k_reduced(); sc.pp.pca(adata); sce.pp.bbknn(adata, batch_key='batch', copy=False,trim=0, save_knn=False)`; ```. ```pytb; TypeError Traceback (most recent call last); <ipython-input-231-50baef9a10a9> in <module>; 5 pbmc = sc.datasets.pbmc68k_reduced(); 6 sc.pp.pca(adata); ----> 7 sce.pp.bbknn(adata, batch_key='batch', copy=False,trim=0, save_knn=False). ~/Library/Python/3.7/lib/python/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 except ImportError:; 83 raise ImportError('Please install bbknn: `pip install bbknn`.'); ---> 84 return bbknn(**params, **kwargs). ~/Bioinformatics/usr/local/lib/python3.7/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, approx, metric, copy, **kwargs); 281 ; 282 	logg.info('	finished', time=start,; --> 283 		deep=('added to `.uns[\'neighbors\']`\n'; 284 ' \'distances\', weighted adjacency matrix\n'; 285 		'	\'connectivities\', weighted adjacency matrix')). ~/Library/Python/3.7/lib/python/site-packages/scanpy/logging.py in info(*args, **kwargs); 17 ; 18 def info(*args, **kwargs):; ---> 19 return msg(*args, v='info', **kwargs); 20 ; 21 . TypeError: msg() got an unexpected keyword argument 'deep'`; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/770
https://github.com/scverse/scanpy/issues/770:899,Testability,log,logg,899,"Reproducible example. ```py; import scanpy as sc; import scanpy.external as ice; from itertools import cycle. pbmc = sc.datasets.pbmc68k_reduced(); sc.pp.pca(adata); sce.pp.bbknn(adata, batch_key='batch', copy=False,trim=0, save_knn=False)`; ```. ```pytb; TypeError Traceback (most recent call last); <ipython-input-231-50baef9a10a9> in <module>; 5 pbmc = sc.datasets.pbmc68k_reduced(); 6 sc.pp.pca(adata); ----> 7 sce.pp.bbknn(adata, batch_key='batch', copy=False,trim=0, save_knn=False). ~/Library/Python/3.7/lib/python/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 except ImportError:; 83 raise ImportError('Please install bbknn: `pip install bbknn`.'); ---> 84 return bbknn(**params, **kwargs). ~/Bioinformatics/usr/local/lib/python3.7/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, approx, metric, copy, **kwargs); 281 ; 282 	logg.info('	finished', time=start,; --> 283 		deep=('added to `.uns[\'neighbors\']`\n'; 284 ' \'distances\', weighted adjacency matrix\n'; 285 		'	\'connectivities\', weighted adjacency matrix')). ~/Library/Python/3.7/lib/python/site-packages/scanpy/logging.py in info(*args, **kwargs); 17 ; 18 def info(*args, **kwargs):; ---> 19 return msg(*args, v='info', **kwargs); 20 ; 21 . TypeError: msg() got an unexpected keyword argument 'deep'`; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/770
https://github.com/scverse/scanpy/issues/770:1149,Testability,log,logging,1149,"Reproducible example. ```py; import scanpy as sc; import scanpy.external as ice; from itertools import cycle. pbmc = sc.datasets.pbmc68k_reduced(); sc.pp.pca(adata); sce.pp.bbknn(adata, batch_key='batch', copy=False,trim=0, save_knn=False)`; ```. ```pytb; TypeError Traceback (most recent call last); <ipython-input-231-50baef9a10a9> in <module>; 5 pbmc = sc.datasets.pbmc68k_reduced(); 6 sc.pp.pca(adata); ----> 7 sce.pp.bbknn(adata, batch_key='batch', copy=False,trim=0, save_knn=False). ~/Library/Python/3.7/lib/python/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 except ImportError:; 83 raise ImportError('Please install bbknn: `pip install bbknn`.'); ---> 84 return bbknn(**params, **kwargs). ~/Bioinformatics/usr/local/lib/python3.7/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, approx, metric, copy, **kwargs); 281 ; 282 	logg.info('	finished', time=start,; --> 283 		deep=('added to `.uns[\'neighbors\']`\n'; 284 ' \'distances\', weighted adjacency matrix\n'; 285 		'	\'connectivities\', weighted adjacency matrix')). ~/Library/Python/3.7/lib/python/site-packages/scanpy/logging.py in info(*args, **kwargs); 17 ; 18 def info(*args, **kwargs):; ---> 19 return msg(*args, v='info', **kwargs); 20 ; 21 . TypeError: msg() got an unexpected keyword argument 'deep'`; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/770
https://github.com/scverse/scanpy/pull/771:151,Deployability,release,release,151,Fixes #769. Looks like I only tested my use case in #724. Not sure if this will still be needed once https://github.com/lmcinnes/umap/pull/261 is in a release.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/771
https://github.com/scverse/scanpy/pull/771:30,Testability,test,tested,30,Fixes #769. Looks like I only tested my use case in #724. Not sure if this will still be needed once https://github.com/lmcinnes/umap/pull/261 is in a release.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/771
https://github.com/scverse/scanpy/pull/772:34,Testability,test,tests,34,Figured I would try and emphasize tests a bit more in the contributing guide.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/772
https://github.com/scverse/scanpy/pull/772:71,Usability,guid,guide,71,Figured I would try and emphasize tests a bit more in the contributing guide.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/772
https://github.com/scverse/scanpy/issues/775:234,Deployability,continuous,continuous,234,"Dear people,. In our plotting functions, vmin and vmax are great arguments, we all love them. First of all, let's all hold our hands, close our eyes and thank matplotlib developers for implementing them. But when we jointly plot some continuous variables that are not on the exact same scale (e.g. some genes), it's not possible to specify a single vmin/vmax value that fits all variables especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/627204",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775
https://github.com/scverse/scanpy/issues/775:1430,Deployability,continuous,continuous,1430,"bles especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I was wondering if it makes sense to add this to scanpy. What do you think?. Finally, here is the way I use it:. `plot_scatter(adata, basis='umap', color=['louvain', 'NKG7', 'GNLY', 'KIT'], cmap='Reds', vmax_quantile=0.999)`. PS: It needs a few more lines to be accessible from other functions like sc.pl.umap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775
https://github.com/scverse/scanpy/issues/775:245,Modifiability,variab,variables,245,"Dear people,. In our plotting functions, vmin and vmax are great arguments, we all love them. First of all, let's all hold our hands, close our eyes and thank matplotlib developers for implementing them. But when we jointly plot some continuous variables that are not on the exact same scale (e.g. some genes), it's not possible to specify a single vmin/vmax value that fits all variables especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/627204",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775
https://github.com/scverse/scanpy/issues/775:379,Modifiability,variab,variables,379,"Dear people,. In our plotting functions, vmin and vmax are great arguments, we all love them. First of all, let's all hold our hands, close our eyes and thank matplotlib developers for implementing them. But when we jointly plot some continuous variables that are not on the exact same scale (e.g. some genes), it's not possible to specify a single vmin/vmax value that fits all variables especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/627204",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775
https://github.com/scverse/scanpy/issues/775:1111,Modifiability,variab,variables,1111," we all love them. First of all, let's all hold our hands, close our eyes and thank matplotlib developers for implementing them. But when we jointly plot some continuous variables that are not on the exact same scale (e.g. some genes), it's not possible to specify a single vmin/vmax value that fits all variables especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775
https://github.com/scverse/scanpy/issues/775:1441,Modifiability,variab,variable,1441,"bles especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I was wondering if it makes sense to add this to scanpy. What do you think?. Finally, here is the way I use it:. `plot_scatter(adata, basis='umap', color=['louvain', 'NKG7', 'GNLY', 'KIT'], cmap='Reds', vmax_quantile=0.999)`. PS: It needs a few more lines to be accessible from other functions like sc.pl.umap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775
https://github.com/scverse/scanpy/issues/775:1559,Modifiability,variab,variable,1559,"bles especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I was wondering if it makes sense to add this to scanpy. What do you think?. Finally, here is the way I use it:. `plot_scatter(adata, basis='umap', color=['louvain', 'NKG7', 'GNLY', 'KIT'], cmap='Reds', vmax_quantile=0.999)`. PS: It needs a few more lines to be accessible from other functions like sc.pl.umap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775
https://github.com/scverse/scanpy/issues/775:1925,Modifiability,variab,variable,1925,"bles especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I was wondering if it makes sense to add this to scanpy. What do you think?. Finally, here is the way I use it:. `plot_scatter(adata, basis='umap', color=['louvain', 'NKG7', 'GNLY', 'KIT'], cmap='Reds', vmax_quantile=0.999)`. PS: It needs a few more lines to be accessible from other functions like sc.pl.umap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775
https://github.com/scverse/scanpy/issues/775:2336,Security,access,accessible,2336,"bles especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I was wondering if it makes sense to add this to scanpy. What do you think?. Finally, here is the way I use it:. `plot_scatter(adata, basis='umap', color=['louvain', 'NKG7', 'GNLY', 'KIT'], cmap='Reds', vmax_quantile=0.999)`. PS: It needs a few more lines to be accessible from other functions like sc.pl.umap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775
https://github.com/scverse/scanpy/issues/775:1013,Usability,simpl,simply,1013," we all love them. First of all, let's all hold our hands, close our eyes and thank matplotlib developers for implementing them. But when we jointly plot some continuous variables that are not on the exact same scale (e.g. some genes), it's not possible to specify a single vmin/vmax value that fits all variables especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775
https://github.com/scverse/scanpy/issues/775:1626,Usability,user-friendly,user-friendly,1626,"bles especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I was wondering if it makes sense to add this to scanpy. What do you think?. Finally, here is the way I use it:. `plot_scatter(adata, basis='umap', color=['louvain', 'NKG7', 'GNLY', 'KIT'], cmap='Reds', vmax_quantile=0.999)`. PS: It needs a few more lines to be accessible from other functions like sc.pl.umap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775
https://github.com/scverse/scanpy/issues/777:1317,Integrability,wrap,wrapper,1317,"g codes:. ```; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); sc.pp.log1p(adata); adata.raw = adata. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=4, min_disp=0.5); sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]; sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); sc.pp.scale(adata, max_value=10). sc.tl.pca(adata, svd_solver='arpack'); sc.pl.pca(adata, color='cell_types'). sc.pp.neighbors(adata, n_neighbors=20, n_pcs=20); sc.tl.louvain(adata). sc.tl.umap(adata); sc.tl.diffmap(adata, n_comps=15). adata.write(""Scanpy.h5ad""); ```. Then reload into another session and tring to plot:; ```; adata2 = anndata.read_h5ad(""Scanpy.h5ad"", backed = 'r'); sc.pl.umap(adata, color=['louvain', ""MMP3""]); ```. However the bug come out as:; ```; sc.pl.umap(adata, color=['louvain', ""MMP3""]); File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap; return plot_scatter(adata, basis='umap', **kwargs); File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 280, in plot_scatter; color_vector = color_vector[order]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/h5py/_hl/dataset.py"", line 476, in __getitem__; selection = sel.select(self.shape, args, dsid=self.id); File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/h5py/_hl/selections.py"", line 72, in select; sel[arg]; File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/h5py/_hl/selections.py"", line 212, in __getitem__; raise TypeError(""PointSelection __getitem__ only works with bool arrays""); TypeError: PointSelection __getitem__ only works with bool arrays; ```. The version of packages:; ```; 1.4 for scanpy; 0.6.22 for anndata; ```. Thanks for the help.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/777
https://github.com/scverse/scanpy/issues/777:1388,Integrability,wrap,wrapper,1388,"g codes:. ```; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); sc.pp.log1p(adata); adata.raw = adata. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=4, min_disp=0.5); sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]; sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); sc.pp.scale(adata, max_value=10). sc.tl.pca(adata, svd_solver='arpack'); sc.pl.pca(adata, color='cell_types'). sc.pp.neighbors(adata, n_neighbors=20, n_pcs=20); sc.tl.louvain(adata). sc.tl.umap(adata); sc.tl.diffmap(adata, n_comps=15). adata.write(""Scanpy.h5ad""); ```. Then reload into another session and tring to plot:; ```; adata2 = anndata.read_h5ad(""Scanpy.h5ad"", backed = 'r'); sc.pl.umap(adata, color=['louvain', ""MMP3""]); ```. However the bug come out as:; ```; sc.pl.umap(adata, color=['louvain', ""MMP3""]); File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap; return plot_scatter(adata, basis='umap', **kwargs); File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 280, in plot_scatter; color_vector = color_vector[order]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/h5py/_hl/dataset.py"", line 476, in __getitem__; selection = sel.select(self.shape, args, dsid=self.id); File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/h5py/_hl/selections.py"", line 72, in select; sel[arg]; File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/h5py/_hl/selections.py"", line 212, in __getitem__; raise TypeError(""PointSelection __getitem__ only works with bool arrays""); TypeError: PointSelection __getitem__ only works with bool arrays; ```. The version of packages:; ```; 1.4 for scanpy; 0.6.22 for anndata; ```. Thanks for the help.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/777
https://github.com/scverse/scanpy/issues/778:239,Availability,error,error,239,"I am trying to access tSNE coordinates from a sliced AnnData object. `adata.obsm.X_tsne` works fine. But if I were to filter the AnnData object,`filtered_adata = adata[:, some_filter]`, and then run `filtered_adata.obsm.X_tsne`, I get the error `AttributeError: 'ArrayView' object has no attribute 'X_tsne'`. I can, however, access the coords with `filtered_adata.obsm['X_tsne']`. Is this desired behavior? I feel the API should be the same between the two.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/778
https://github.com/scverse/scanpy/issues/778:15,Security,access,access,15,"I am trying to access tSNE coordinates from a sliced AnnData object. `adata.obsm.X_tsne` works fine. But if I were to filter the AnnData object,`filtered_adata = adata[:, some_filter]`, and then run `filtered_adata.obsm.X_tsne`, I get the error `AttributeError: 'ArrayView' object has no attribute 'X_tsne'`. I can, however, access the coords with `filtered_adata.obsm['X_tsne']`. Is this desired behavior? I feel the API should be the same between the two.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/778
https://github.com/scverse/scanpy/issues/778:325,Security,access,access,325,"I am trying to access tSNE coordinates from a sliced AnnData object. `adata.obsm.X_tsne` works fine. But if I were to filter the AnnData object,`filtered_adata = adata[:, some_filter]`, and then run `filtered_adata.obsm.X_tsne`, I get the error `AttributeError: 'ArrayView' object has no attribute 'X_tsne'`. I can, however, access the coords with `filtered_adata.obsm['X_tsne']`. Is this desired behavior? I feel the API should be the same between the two.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/778
https://github.com/scverse/scanpy/issues/779:21,Testability,test,testing,21,"Hi, . We might start testing scanpy with umap 0.4dev branch. Today I ran `sc.pp.neighbors` with umap 0.4dev branch accidentally and got the following exception:. ![image](https://user-images.githubusercontent.com/1140359/62968014-29009380-bdd8-11e9-995f-4131ebcb19f5.png). because `fuzzy_simplicial_set` function in UMAP returns a tuple, not a sparse matrix in 0.4: https://github.com/lmcinnes/umap/blob/0.4dev/umap/umap_.py#L528-L549. It'd be great to write some tests and check if it's enough to fix this exception.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779
https://github.com/scverse/scanpy/issues/779:464,Testability,test,tests,464,"Hi, . We might start testing scanpy with umap 0.4dev branch. Today I ran `sc.pp.neighbors` with umap 0.4dev branch accidentally and got the following exception:. ![image](https://user-images.githubusercontent.com/1140359/62968014-29009380-bdd8-11e9-995f-4131ebcb19f5.png). because `fuzzy_simplicial_set` function in UMAP returns a tuple, not a sparse matrix in 0.4: https://github.com/lmcinnes/umap/blob/0.4dev/umap/umap_.py#L528-L549. It'd be great to write some tests and check if it's enough to fix this exception.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779
https://github.com/scverse/scanpy/issues/780:103,Usability,guid,guidance,103,"Hi,. I wonder how one should use the `covariates` argument of combat. I cannot find an example or more guidance about it. Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/780
https://github.com/scverse/scanpy/issues/786:115,Availability,error,error,115,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:134,Availability,ERROR,ERROR,134,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:312,Availability,error,error,312,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:334,Availability,ERROR,ERROR,334,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:349,Availability,error,errored,349,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:951,Availability,error,error,951,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:1025,Availability,avail,available,1025,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:187,Deployability,install,install,187,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:235,Deployability,install,install,235,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:265,Deployability,install,install,265,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:285,Deployability,install,install,285,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:688,Deployability,install,install,688,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:713,Deployability,install,install-record,713,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:848,Deployability,install,install,848,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:903,Deployability,install,install,903,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:796,Testability,log,logs,796,"Hi all,; I'm having a trouble in running a code: ; sc.tl.louvain(adata); So, when I try to run the code, it has an error saying that; ERROR: Failed building wheel for louvain; I tried to install louvain in anaconda prompt, and I can't install it.; When I use:; pip install louvain; to install louvain, I have an error that . ```pytb; ERROR: Command errored out with exit status 1:; 'c:\users\prince and jacky\anaconda3\python.exe' \; -u \; -c '; import sys, setuptools, tokenize; sys.argv[0] = "".../louvain/setup.py""; __file__="".../louvain/setup.py""; f=getattr(tokenize, ""open"", open)(__file__); code=f.read().replace(""\r\n"", ""\n""); f.close(); exec(compile(code, __file__, ""exec"")); ' \; install \; --record '.../install-record.txt' \; --single-version-externally-managed \; --compile; Check the logs for full command output.; ```. I also tried to install using different codes such as:. ```bash; conda install -c conda-forge louvain; ```. There's an error saying that:; PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/787:2039,Deployability,update,update,2039,"128 fig.canvas.print_figure(bytes_io, **kw); 129 data = bytes_io.getvalue(); 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs); 2054 orientation=orientation,; 2055 dryrun=True,; -> 2056 **kwargs); 2057 renderer = self.figure._cachedRenderer; 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs); 525 ; 526 else:; --> 527 FigureCanvasAgg.draw(self); 528 renderer = self.get_renderer(); 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self); 386 self.renderer = self.get_renderer(cleared=True); 387 with RendererAgg.lock:; --> 388 self.figure.draw(self.renderer); 389 # A GUI class may be need to update a window using this draw, so; 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs); 36 renderer.start_filter(); 37 ; ---> 38 return draw(artist, renderer, *args, **kwargs); 39 finally:; 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer); 1707 self.patch.draw(renderer); 1708 mimage._draw_list_compositing_images(; -> 1709 renderer, self, artists, self.suppressComposite); 1710 ; 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite); 133 if not_composite or not has_images:; 134 for a in artists:; --> 135 a.draw(renderer); 136 else:; 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs); 36 renderer.start_filter()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2468,Deployability,patch,patch,2468,"box_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs); 525 ; 526 else:; --> 527 FigureCanvasAgg.draw(self); 528 renderer = self.get_renderer(); 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self); 386 self.renderer = self.get_renderer(cleared=True); 387 with RendererAgg.lock:; --> 388 self.figure.draw(self.renderer); 389 # A GUI class may be need to update a window using this draw, so; 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs); 36 renderer.start_filter(); 37 ; ---> 38 return draw(artist, renderer, *args, **kwargs); 39 finally:; 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer); 1707 self.patch.draw(renderer); 1708 mimage._draw_list_compositing_images(; -> 1709 renderer, self, artists, self.suppressComposite); 1710 ; 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite); 133 if not_composite or not has_images:; 134 for a in artists:; --> 135 a.draw(renderer); 136 else:; 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs); 36 renderer.start_filter(); 37 ; ---> 38 return draw(artist, renderer, *args, **kwargs); 39 finally:; 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer); 290 sorted(self.collections,; 291 key=lambda col: col.do_3d_projection(renderer),; --> 292 reverse=True)):; 293 col.zorder = zorder_offset + i; 294 for i, patch in enumerate(. ~/.local/lib/python3.7/si",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:3425,Deployability,patch,patch,3425,"plotlib/figure.py in draw(self, renderer); 1707 self.patch.draw(renderer); 1708 mimage._draw_list_compositing_images(; -> 1709 renderer, self, artists, self.suppressComposite); 1710 ; 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite); 133 if not_composite or not has_images:; 134 for a in artists:; --> 135 a.draw(renderer); 136 else:; 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs); 36 renderer.start_filter(); 37 ; ---> 38 return draw(artist, renderer, *args, **kwargs); 39 finally:; 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer); 290 sorted(self.collections,; 291 key=lambda col: col.do_3d_projection(renderer),; --> 292 reverse=True)):; 293 col.zorder = zorder_offset + i; 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col); 289 for i, col in enumerate(; 290 sorted(self.collections,; --> 291 key=lambda col: col.do_3d_projection(renderer),; 292 reverse=True)):; 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer); 543 self.set_facecolors(fcs); 544 ; --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else; 546 self._edgecolor3d); 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs); 845 norm = Normalize(min(zs), max(zs)); 846 sats = 1 - norm(zs) * 0.7; --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)); 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]); 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok); 180 [1, 2, 3]",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:1922,Usability,clear,cleared,1922,"-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs); 126 ; 127 bytes_io = BytesIO(); --> 128 fig.canvas.print_figure(bytes_io, **kw); 129 data = bytes_io.getvalue(); 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs); 2054 orientation=orientation,; 2055 dryrun=True,; -> 2056 **kwargs); 2057 renderer = self.figure._cachedRenderer; 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs); 525 ; 526 else:; --> 527 FigureCanvasAgg.draw(self); 528 renderer = self.get_renderer(); 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self); 386 self.renderer = self.get_renderer(cleared=True); 387 with RendererAgg.lock:; --> 388 self.figure.draw(self.renderer); 389 # A GUI class may be need to update a window using this draw, so; 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs); 36 renderer.start_filter(); 37 ; ---> 38 return draw(artist, renderer, *args, **kwargs); 39 finally:; 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer); 1707 self.patch.draw(renderer); 1708 mimage._draw_list_compositing_images(; -> 1709 renderer, self, artists, self.suppressComposite); 1710 ; 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite); 133 if not_composite or not has_images:; 134 for a in artists:; --> 135 a.draw(renderer); 136 else:; 137 # Composite any adjacent images together. ~/.local/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/pull/790:12,Deployability,Update,Update,12,Fixes #762. Update to #764. Just adding a test to make sure the `use_raw` argument does something. I'm opening this a new pull request since git didn't me using other PRs branch name.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/790
https://github.com/scverse/scanpy/pull/790:42,Testability,test,test,42,Fixes #762. Update to #764. Just adding a test to make sure the `use_raw` argument does something. I'm opening this a new pull request since git didn't me using other PRs branch name.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/790
https://github.com/scverse/scanpy/issues/793:82,Deployability,update,update,82,In order to make the learning rate of accessible from scanpy it will be needed to update the dca; version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:38,Security,access,accessible,38,In order to make the learning rate of accessible from scanpy it will be needed to update the dca; version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:21,Usability,learn,learning,21,In order to make the learning rate of accessible from scanpy it will be needed to update the dca; version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:181,Usability,learn,learning,181,In order to make the learning rate of accessible from scanpy it will be needed to update the dca; version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/pull/794:982,Testability,test,tests,982,"This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:; * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) ; * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775).; * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775); * vmax and vmin can be a user defined function. (#775); * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:; - [x] Correct style; - [x] Pass tests; - [x] Add tests for new parameters; - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:999,Testability,test,tests,999,"This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:; * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) ; * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775).; * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775); * vmax and vmin can be a user defined function. (#775); * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:; - [x] Correct style; - [x] Pass tests; - [x] Add tests for new parameters; - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/797:30,Security,hash,hashing,30,Here is a stab at adding cell hashing demultiplexing to scanpy. This will address https://github.com/theislab/scanpy/issues/351.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797
https://github.com/scverse/scanpy/pull/798:244,Deployability,install,installed,244,"No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds?. Package metadata and content will be correctly derived from the git repo’s status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`.; 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy you’re currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`; 2. if `setuptools_scm` can’t be imported or we’re not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:589,Deployability,install,installed,589,"No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds?. Package metadata and content will be correctly derived from the git repo’s status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`.; 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy you’re currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`; 2. if `setuptools_scm` can’t be imported or we’re not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:816,Deployability,install,installed,816,"No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds?. Package metadata and content will be correctly derived from the git repo’s status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`.; 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy you’re currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`; 2. if `setuptools_scm` can’t be imported or we’re not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:102,Testability,log,logic,102,"No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds?. Package metadata and content will be correctly derived from the git repo’s status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`.; 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy you’re currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`; 2. if `setuptools_scm` can’t be imported or we’re not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:64,Usability,simpl,simpler,64,"No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds?. Package metadata and content will be correctly derived from the git repo’s status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`.; 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy you’re currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`; 2. if `setuptools_scm` can’t be imported or we’re not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/issues/800:160,Availability,error,error,160,"@fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python; import scanpy as sc; import numpy as np; from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(); ```. Passing a function. ```python; sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)); ```. <details>; <summary> Traceback </summary>. ```python; ---------------------------------------------------------------------------; UFuncTypeError Traceback (most recent call last); <ipython-input-13-83df06d6a2e8> in <module>; ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs); 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 437 """"""; --> 438 return embedding(adata, 'umap', **kwargs); 439 ; 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 230 ; 231 # check vmin and vmax options; --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector); 233 ; 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector); 390 f""correct format for percentiles.""); 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'; --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:200,Availability,error,error,200,"@fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python; import scanpy as sc; import numpy as np; from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(); ```. Passing a function. ```python; sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)); ```. <details>; <summary> Traceback </summary>. ```python; ---------------------------------------------------------------------------; UFuncTypeError Traceback (most recent call last); <ipython-input-13-83df06d6a2e8> in <module>; ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs); 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 437 """"""; --> 438 return embedding(adata, 'umap', **kwargs); 439 ; 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 230 ; 231 # check vmin and vmax options; --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector); 233 ; 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector); 390 f""correct format for percentiles.""); 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'; --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:246,Testability,log,logic,246,"@fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python; import scanpy as sc; import numpy as np; from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(); ```. Passing a function. ```python; sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)); ```. <details>; <summary> Traceback </summary>. ```python; ---------------------------------------------------------------------------; UFuncTypeError Traceback (most recent call last); <ipython-input-13-83df06d6a2e8> in <module>; ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs); 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 437 """"""; --> 438 return embedding(adata, 'umap', **kwargs); 439 ; 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 230 ; 231 # check vmin and vmax options; --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector); 233 ; 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector); 390 f""correct format for percentiles.""); 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'; --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/pull/802:192,Availability,error,error,192,"This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```; Warning, treated as error:; /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:233,Availability,error,error,233,"This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```; Warning, treated as error:; /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/803:341,Availability,error,error,341,"Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:376,Availability,error,error,376,"Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/issues/909:73,Availability,error,error,73,"Hello.; I need some help with this issue. when I run this line, I got an error. . ```py; sc.pl.paga(; adata,; threshold=0, ; solid_edges='connectivities_tree',; dashed_edges='connectivities', ; root='neoblast 1',; layout='rt_circular',; node_size_scale=0.5,; node_size_power=0.9,; max_edge_width=0.7,; fontsize=3.5,; ); ```. ![error](https://user-images.githubusercontent.com/48261734/63892645-beda1800-c9ad-11e9-8f04-f13a5b7a8812.jpg). thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909
https://github.com/scverse/scanpy/issues/909:327,Availability,error,error,327,"Hello.; I need some help with this issue. when I run this line, I got an error. . ```py; sc.pl.paga(; adata,; threshold=0, ; solid_edges='connectivities_tree',; dashed_edges='connectivities', ; root='neoblast 1',; layout='rt_circular',; node_size_scale=0.5,; node_size_power=0.9,; max_edge_width=0.7,; fontsize=3.5,; ); ```. ![error](https://user-images.githubusercontent.com/48261734/63892645-beda1800-c9ad-11e9-8f04-f13a5b7a8812.jpg). thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909
https://github.com/scverse/scanpy/pull/804:54,Availability,error,error,54,* fixed vmin/vmax for categorical data #800 ; * added error message when vmin is not valid to point out how to format it; * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:124,Deployability,update,updated,124,* fixed vmin/vmax for categorical data #800 ; * added error message when vmin is not valid to point out how to format it; * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:60,Integrability,message,message,60,* fixed vmin/vmax for categorical data #800 ; * added error message when vmin is not valid to point out how to format it; * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:132,Testability,test,test,132,* fixed vmin/vmax for categorical data #800 ; * added error message when vmin is not valid to point out how to format it; * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/805:52,Performance,load,loadings,52,"Currently, we do not show the genes with the lowest loadings in `sc.pl.pca_loadings`. This PRs add an option for that:. ![image](https://user-images.githubusercontent.com/1140359/63976191-c6b5bd00-ca7e-11e9-9173-f04e2a473388.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805
https://github.com/scverse/scanpy/pull/806:226,Usability,intuit,intuitive,226,"This PR does a few things:. - Adds `fontoutline` argument to sc.pl.paga as a convenient shortcut. Similar to https://github.com/theislab/scanpy/pull/640. This can also be achieved via text_kwds but it's super verbose and less intuitive for beginners.; - Removes the usage of empty dictionaries as default arguments, because it might lead to [erroneous results in successive calls](https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments). ; - `sc.pl.paga_compare` now adds `legend_font{size,weight,outline}` arguments to paga_graph_params by default which results in better agreement between the embedding view and paga view.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/806
https://github.com/scverse/scanpy/pull/806:401,Usability,guid,guide,401,"This PR does a few things:. - Adds `fontoutline` argument to sc.pl.paga as a convenient shortcut. Similar to https://github.com/theislab/scanpy/pull/640. This can also be achieved via text_kwds but it's super verbose and less intuitive for beginners.; - Removes the usage of empty dictionaries as default arguments, because it might lead to [erroneous results in successive calls](https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments). ; - `sc.pl.paga_compare` now adds `legend_font{size,weight,outline}` arguments to paga_graph_params by default which results in better agreement between the embedding view and paga view.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/806
https://github.com/scverse/scanpy/issues/807:34,Availability,error,error,34,"While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'.; Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually?. ```pytb; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-191-71b705e00011> in <module>; ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy); 112 directed = False; 113 if not directed: logg.debug(' using the undirected graph'); --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 115 if use_weights:; 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 379 def get_igraph_from_adjacency(adjacency, directed=None):; 380 """"""Get igraph graph from adjacency matrix.""""""; --> 381 import igraph as ig; 382 sources, targets = adjacency.nonzero(); 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>; 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1432,Deployability,upgrade,upgrade,1432,"While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'.; Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually?. ```pytb; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-191-71b705e00011> in <module>; ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy); 112 directed = False; 113 if not directed: logg.debug(' using the undirected graph'); --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 115 if use_weights:; 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 379 def get_igraph_from_adjacency(adjacency, directed=None):; 380 """"""Get igraph graph from adjacency matrix.""""""; --> 381 import igraph as ig; 382 sources, targets = adjacency.nonzero(); 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>; 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1593,Deployability,upgrade,upgrade,1593,"While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'.; Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually?. ```pytb; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-191-71b705e00011> in <module>; ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy); 112 directed = False; 113 if not directed: logg.debug(' using the undirected graph'); --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 115 if use_weights:; 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 379 def get_igraph_from_adjacency(adjacency, directed=None):; 380 """"""Get igraph graph from adjacency matrix.""""""; --> 381 import igraph as ig; 382 sources, targets = adjacency.nonzero(); 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>; 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1309,Safety,avoid,avoid,1309,"While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'.; Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually?. ```pytb; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-191-71b705e00011> in <module>; ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy); 112 directed = False; 113 if not directed: logg.debug(' using the undirected graph'); --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 115 if use_weights:; 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 379 def get_igraph_from_adjacency(adjacency, directed=None):; 380 """"""Get igraph graph from adjacency matrix.""""""; --> 381 import igraph as ig; 382 sources, targets = adjacency.nonzero(); 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>; 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1483,Safety,avoid,avoid,1483,"While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'.; Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually?. ```pytb; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-191-71b705e00011> in <module>; ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy); 112 directed = False; 113 if not directed: logg.debug(' using the undirected graph'); --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 115 if use_weights:; 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 379 def get_igraph_from_adjacency(adjacency, directed=None):; 380 """"""Get igraph graph from adjacency matrix.""""""; --> 381 import igraph as ig; 382 sources, targets = adjacency.nonzero(); 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>; 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:641,Testability,log,logg,641,"While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'.; Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually?. ```pytb; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-191-71b705e00011> in <module>; ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy); 112 directed = False; 113 if not directed: logg.debug(' using the undirected graph'); --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 115 if use_weights:; 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 379 def get_igraph_from_adjacency(adjacency, directed=None):; 380 """"""Get igraph graph from adjacency matrix.""""""; --> 381 import igraph as ig; 382 sources, targets = adjacency.nonzero(); 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>; 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/pull/809:33,Availability,error,error,33,"`sc.pl.clustermap` fails with an error:. ```python; import scanpy as sc; adata = sc.datasets.krumsiek11(); sc.pl.clustermap(adata, obs_keys='cell_type'); ```. Output:. ```python; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-15436b6f0954> in <module>; 3 ; 4 adata = sc.datasets.krumsiek11(); ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds); 777 adata.uns[obs_keys + '_colors'])); 778 row_colors = adata.obs[obs_keys].map(lut); --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds); 780 else:; 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs); 1316 row_colors=row_colors, col_colors=col_colors,; 1317 z_score=z_score, standard_scale=standard_scale,; -> 1318 mask=mask); 1319 ; 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask); 772 ; 773 self.row_colors, self.row_color_labels = \; --> 774 self._preprocess_colors(data, row_colors, axis=0); 775 self.col_colors, self.col_color_labels = \; 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis); 827 # Replace na's with background color; 828 # TODO We should set these to transparent instead; --> 829 colors = colors.fillna('white'); 830 ; 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs); 4343 limit=limit,; 4344 downcast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:940,Availability,mask,mask,940,"`sc.pl.clustermap` fails with an error:. ```python; import scanpy as sc; adata = sc.datasets.krumsiek11(); sc.pl.clustermap(adata, obs_keys='cell_type'); ```. Output:. ```python; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-15436b6f0954> in <module>; 3 ; 4 adata = sc.datasets.krumsiek11(); ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds); 777 adata.uns[obs_keys + '_colors'])); 778 row_colors = adata.obs[obs_keys].map(lut); --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds); 780 else:; 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs); 1316 row_colors=row_colors, col_colors=col_colors,; 1317 z_score=z_score, standard_scale=standard_scale,; -> 1318 mask=mask); 1319 ; 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask); 772 ; 773 self.row_colors, self.row_color_labels = \; --> 774 self._preprocess_colors(data, row_colors, axis=0); 775 self.col_colors, self.col_color_labels = \; 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis); 827 # Replace na's with background color; 828 # TODO We should set these to transparent instead; --> 829 colors = colors.fillna('white'); 830 ; 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs); 4343 limit=limit,; 4344 downcast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1071,Availability,mask,mask,1071,"`sc.pl.clustermap` fails with an error:. ```python; import scanpy as sc; adata = sc.datasets.krumsiek11(); sc.pl.clustermap(adata, obs_keys='cell_type'); ```. Output:. ```python; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-15436b6f0954> in <module>; 3 ; 4 adata = sc.datasets.krumsiek11(); ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds); 777 adata.uns[obs_keys + '_colors'])); 778 row_colors = adata.obs[obs_keys].map(lut); --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds); 780 else:; 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs); 1316 row_colors=row_colors, col_colors=col_colors,; 1317 z_score=z_score, standard_scale=standard_scale,; -> 1318 mask=mask); 1319 ; 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask); 772 ; 773 self.row_colors, self.row_color_labels = \; --> 774 self._preprocess_colors(data, row_colors, axis=0); 775 self.col_colors, self.col_color_labels = \; 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis); 827 # Replace na's with background color; 828 # TODO We should set these to transparent instead; --> 829 colors = colors.fillna('white'); 830 ; 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs); 4343 limit=limit,; 4344 downcast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1076,Availability,mask,mask,1076,"`sc.pl.clustermap` fails with an error:. ```python; import scanpy as sc; adata = sc.datasets.krumsiek11(); sc.pl.clustermap(adata, obs_keys='cell_type'); ```. Output:. ```python; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-15436b6f0954> in <module>; 3 ; 4 adata = sc.datasets.krumsiek11(); ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds); 777 adata.uns[obs_keys + '_colors'])); 778 row_colors = adata.obs[obs_keys].map(lut); --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds); 780 else:; 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs); 1316 row_colors=row_colors, col_colors=col_colors,; 1317 z_score=z_score, standard_scale=standard_scale,; -> 1318 mask=mask); 1319 ; 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask); 772 ; 773 self.row_colors, self.row_color_labels = \; --> 774 self._preprocess_colors(data, row_colors, axis=0); 775 self.col_colors, self.col_color_labels = \; 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis); 827 # Replace na's with background color; 828 # TODO We should set these to transparent instead; --> 829 colors = colors.fillna('white'); 830 ; 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs); 4343 limit=limit,; 4344 downcast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1272,Availability,mask,mask,1272,"-------------------; ValueError Traceback (most recent call last); <ipython-input-2-15436b6f0954> in <module>; 3 ; 4 adata = sc.datasets.krumsiek11(); ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds); 777 adata.uns[obs_keys + '_colors'])); 778 row_colors = adata.obs[obs_keys].map(lut); --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds); 780 else:; 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs); 1316 row_colors=row_colors, col_colors=col_colors,; 1317 z_score=z_score, standard_scale=standard_scale,; -> 1318 mask=mask); 1319 ; 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask); 772 ; 773 self.row_colors, self.row_color_labels = \; --> 774 self._preprocess_colors(data, row_colors, axis=0); 775 self.col_colors, self.col_color_labels = \; 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis); 827 # Replace na's with background color; 828 # TODO We should set these to transparent instead; --> 829 colors = colors.fillna('white'); 830 ; 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs); 4343 limit=limit,; 4344 downcast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 )",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1892,Availability,down,downcast,1892,"ge, row_colors, col_colors, mask, **kwargs); 1316 row_colors=row_colors, col_colors=col_colors,; 1317 z_score=z_score, standard_scale=standard_scale,; -> 1318 mask=mask); 1319 ; 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask); 772 ; 773 self.row_colors, self.row_color_labels = \; --> 774 self._preprocess_colors(data, row_colors, axis=0); 775 self.col_colors, self.col_color_labels = \; 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis); 827 # Replace na's with background color; 828 # TODO We should set these to transparent instead; --> 829 colors = colors.fillna('white'); 830 ; 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs); 4343 limit=limit,; 4344 downcast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, do",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1937,Availability,down,downcast,1937,"ge, row_colors, col_colors, mask, **kwargs); 1316 row_colors=row_colors, col_colors=col_colors,; 1317 z_score=z_score, standard_scale=standard_scale,; -> 1318 mask=mask); 1319 ; 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask); 772 ; 773 self.row_colors, self.row_color_labels = \; --> 774 self._preprocess_colors(data, row_colors, axis=0); 775 self.col_colors, self.col_color_labels = \; 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis); 827 # Replace na's with background color; 828 # TODO We should set these to transparent instead; --> 829 colors = colors.fillna('white'); 830 ; 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs); 4343 limit=limit,; 4344 downcast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, do",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1946,Availability,down,downcast,1946,"ge, row_colors, col_colors, mask, **kwargs); 1316 row_colors=row_colors, col_colors=col_colors,; 1317 z_score=z_score, standard_scale=standard_scale,; -> 1318 mask=mask); 1319 ; 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask); 772 ; 773 self.row_colors, self.row_color_labels = \; --> 774 self._preprocess_colors(data, row_colors, axis=0); 775 self.col_colors, self.col_color_labels = \; 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis); 827 # Replace na's with background color; 828 # TODO We should set these to transparent instead; --> 829 colors = colors.fillna('white'); 830 ; 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs); 4343 limit=limit,; 4344 downcast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, do",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2105,Availability,down,downcast,2105,"eturn plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask); 772 ; 773 self.row_colors, self.row_color_labels = \; --> 774 self._preprocess_colors(data, row_colors, axis=0); 775 self.col_colors, self.col_color_labels = \; 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis); 827 # Replace na's with background color; 828 # TODO We should set these to transparent instead; --> 829 colors = colors.fillna('white'); 830 ; 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs); 4343 limit=limit,; 4344 downcast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=val",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2210,Availability,down,downcast,2210," data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask); 772 ; 773 self.row_colors, self.row_color_labels = \; --> 774 self._preprocess_colors(data, row_colors, axis=0); 775 self.col_colors, self.col_color_labels = \; 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis); 827 # Replace na's with background color; 828 # TODO We should set these to transparent instead; --> 829 colors = colors.fillna('white'); 830 ; 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs); 4343 limit=limit,; 4344 downcast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-pac",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2219,Availability,down,downcast,2219," data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask); 772 ; 773 self.row_colors, self.row_color_labels = \; --> 774 self._preprocess_colors(data, row_colors, axis=0); 775 self.col_colors, self.col_color_labels = \; 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis); 827 # Replace na's with background color; 828 # TODO We should set these to transparent instead; --> 829 colors = colors.fillna('white'); 830 ; 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs); 4343 limit=limit,; 4344 downcast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-pac",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2444,Availability,down,downcast,2444," \; 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis); 827 # Replace na's with background color; 828 # TODO We should set these to transparent instead; --> 829 colors = colors.fillna('white'); 830 ; 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs); 4343 limit=limit,; 4344 downcast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/cat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2911,Availability,down,downcast,2911,"cast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2978,Availability,down,downcast,2978,"cast=downcast,; -> 4345 **kwargs; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3652,Availability,mask,mask,3652,"args; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3233,Integrability,wrap,wrapper,3233,"args; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3367,Integrability,wrap,wrapper,3367,"args; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3846,Modifiability,variab,variable,3846,"args; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3929,Modifiability,variab,variable,3929,"args; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3963,Testability,test,tests,3963,"args; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3890,Usability,simpl,simply,3890,"args; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/issues/811:168,Availability,error,error,168,"Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```; reading 1M_neurons_filtered_gene_bc_matrices_h5.h5; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; (0:01:39); running recipe zheng17; filtered out 3983 genes that are detectedin less than 1 counts; Killed; ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:618,Availability,error,error,618,"Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```; reading 1M_neurons_filtered_gene_bc_matrices_h5.h5; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; (0:01:39); running recipe zheng17; filtered out 3983 genes that are detectedin less than 1 counts; Killed; ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:123,Deployability,release,released,123,"Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```; reading 1M_neurons_filtered_gene_bc_matrices_h5.h5; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; (0:01:39); running recipe zheng17; filtered out 3983 genes that are detectedin less than 1 counts; Killed; ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:233,Modifiability,Variab,Variable,233,"Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```; reading 1M_neurons_filtered_gene_bc_matrices_h5.h5; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; (0:01:39); running recipe zheng17; filtered out 3983 genes that are detectedin less than 1 counts; Killed; ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:385,Safety,detect,detectedin,385,"Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```; reading 1M_neurons_filtered_gene_bc_matrices_h5.h5; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; (0:01:39); running recipe zheng17; filtered out 3983 genes that are detectedin less than 1 counts; Killed; ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/pull/812:197,Testability,test,tests,197,"With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes.; Over-expressed genes are selected with the Mann-Whitney U tests and cell; types are assigned with the hypergeometric test. This function first selects; genes from gene expression data with the Mann Whitney U test, then annotate; them with the hypergeometric test, and finally filter out cell types that; have zero scores for all cells. The results are scores that tell how; probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/812:256,Testability,test,test,256,"With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes.; Over-expressed genes are selected with the Mann-Whitney U tests and cell; types are assigned with the hypergeometric test. This function first selects; genes from gene expression data with the Mann Whitney U test, then annotate; them with the hypergeometric test, and finally filter out cell types that; have zero scores for all cells. The results are scores that tell how; probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/812:347,Testability,test,test,347,"With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes.; Over-expressed genes are selected with the Mann-Whitney U tests and cell; types are assigned with the hypergeometric test. This function first selects; genes from gene expression data with the Mann Whitney U test, then annotate; them with the hypergeometric test, and finally filter out cell types that; have zero scores for all cells. The results are scores that tell how; probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/812:397,Testability,test,test,397,"With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes.; Over-expressed genes are selected with the Mann-Whitney U tests and cell; types are assigned with the hypergeometric test. This function first selects; genes from gene expression data with the Mann Whitney U test, then annotate; them with the hypergeometric test, and finally filter out cell types that; have zero scores for all cells. The results are scores that tell how; probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/813:50,Usability,learn,learning,50,"scanpy currently does not allow to select the dca learning rate. This can create convergence issues (for example dca diverges 60% of the time on Tabula Muris when trained on CPU, but oddly enough converges when trained on GPU)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813
https://github.com/scverse/scanpy/pull/814:50,Deployability,upgrade,upgrade,50,This also highlights a problem: pypairs had a big upgrade in the meantime and we didn’t see it. Its docs are now much better than ours were (and still a bit more complete after this PR) IDK how to prevent this…,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/814
https://github.com/scverse/scanpy/issues/815:840,Usability,learn,learn,840,"Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. ; I tried it using `subplots`, but this is what happens: . ```python; fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)); p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1); p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2); ```; ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions; ```; scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```. ### Full repex; ```python; import scanpy as sc; import numpy as np; from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1); adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) ; p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False); p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) ; fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)); p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1); p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/817:1284,Modifiability,layers,layers,1284,"Hi,. I have sliced some candidate genes (according to my pre-knowledge) from adata, and do sc.tl.rank_genes_groups for adata to check those genes are enriched in which group of cells. But the output rank gene names is wrong, many of the ouptput genes names are not the in adata.var.index after my selection, which should be already excluded by my candidate genes selection. Bellow is my code, seems still use the genes before selection? Can you help me?. Thanks,; Jphe. ```py; adata_raw = adata.copy(); df = pd.read_table('/public/home/jphe/omicsdata/genome/mm10/scTE/atac/candidates.txt', header=None); genes = list(df[0]); genes = [k for k in genes if k in adata.var.index ]; adata = adata[:, genes] # only have ~1000 genes in adata after selection. sc.tl.rank_genes_groups(adata, 'leiden_r0.5', n_genes=20); sc.pl.rank_genes_groups(adata, n_genes=20, show=True). adata; ```; ```; AnnData object with n_obs × n_vars = 53165 × 1080 ; obs: 'batch', 'n_counts', 'n_genes', 'time', 'log_counts', 'mt_frac', 'size_factors', 'leiden_r1', 'leiden_r0.5', 'leiden_r0.1'; var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; uns: 'leiden', 'neighbors', 'pca', 'time_colors', 'leiden_r0.5_colors', 'rank_genes_groups'; obsm: 'X_pca', 'X_tsne', 'X_umap'; varm: 'PCs'; layers: 'counts'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817
https://github.com/scverse/scanpy/pull/819:576,Integrability,message,messages,576,"This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python; import scanpy as sc. sc.settings.verbosity = 3; adata = sc.datasets.pbmc3k(); sc.pp.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata); sc.tl.louvain(adata, resolution=2.0, key_added='newkey'); adata.uns; ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/820:297,Availability,avail,available,297,"`compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python; import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)); lm.rows = adata.uns['neighbors']['knn_indices']; lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']); lm = lm.tocsr(); lm.setdiag(0); lm.eliminate_zeros(); lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm); adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:842,Availability,mask,mask,842,"`compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python; import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)); lm.rows = adata.uns['neighbors']['knn_indices']; lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']); lm = lm.tocsr(); lm.setdiag(0); lm.eliminate_zeros(); lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm); adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:186,Security,access,accessible,186,"`compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python; import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)); lm.rows = adata.uns['neighbors']['knn_indices']; lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']); lm = lm.tocsr(); lm.setdiag(0); lm.eliminate_zeros(); lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm); adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:336,Security,access,access,336,"`compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python; import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)); lm.rows = adata.uns['neighbors']['knn_indices']; lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']); lm = lm.tocsr(); lm.setdiag(0); lm.eliminate_zeros(); lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm); adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/issues/821:64,Deployability,pipeline,pipeline,64,"Hello, scanpy developers, ; Firstly, thank you so much for your pipeline scanpy. It is very useful to me. Secondly, I want to ask a question. Are you going to support Stimulated vs Control analysis? Thank you.; Sincerely yours,; Shangyu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821
https://github.com/scverse/scanpy/pull/824:431,Testability,test,tested,431,"This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:481,Testability,Test,Testing,481,"This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:244,Usability,feedback,feedback,244,"This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/issues/826:130,Integrability,rout,routines,130,"when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use; ```; tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]; ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var); ```; to ""reset"" the `.X` matrix (maybe there's a better way?); or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```; scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:180,Modifiability,variab,variable,180,"when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use; ```; tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]; ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var); ```; to ""reset"" the `.X` matrix (maybe there's a better way?); or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```; scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:238,Modifiability,variab,variable,238,"when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use; ```; tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]; ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var); ```; to ""reset"" the `.X` matrix (maybe there's a better way?); or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```; scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:427,Modifiability,variab,variable,427,"when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use; ```; tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]; ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var); ```; to ""reset"" the `.X` matrix (maybe there's a better way?); or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```; scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:846,Usability,learn,learn,846,"when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use; ```; tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]; ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var); ```; to ""reset"" the `.X` matrix (maybe there's a better way?); or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```; scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/828:258,Modifiability,layers,layers,258,"I think this could use a consolidated effort for consistent behavior. Especially since testing whether it works will probably have some common patterns. In some cases `Raw` will need to be an option. I like the convention of having the arguments `use_raw`, `layers`, and (when appropriate) `obsm_key`/ `varm_key`. With these at most one of the values can be not None, and if all are None (the default) `X` is used. An alternative convention is `use_rep: Optional[str]`. I’m less a fan of this due to potential key collisions. Some relevant issues/ prs: #826 #801 #730",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828
https://github.com/scverse/scanpy/issues/828:87,Testability,test,testing,87,"I think this could use a consolidated effort for consistent behavior. Especially since testing whether it works will probably have some common patterns. In some cases `Raw` will need to be an option. I like the convention of having the arguments `use_raw`, `layers`, and (when appropriate) `obsm_key`/ `varm_key`. With these at most one of the values can be not None, and if all are None (the default) `X` is used. An alternative convention is `use_rep: Optional[str]`. I’m less a fan of this due to potential key collisions. Some relevant issues/ prs: #826 #801 #730",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828
https://github.com/scverse/scanpy/issues/829:37,Availability,error,error,37,"Dear Theis lab,; I get the following error:; `ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (3255,4)`; when running `sc.pl.diffmap(adata, color='leiden', projection='3d', save='_diff_3d.pdf')`. I hope you guys can help me with this.; regards",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/829
https://github.com/scverse/scanpy/pull/830:1780,Availability,avail,available,1780,"This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python; sc.pp.neighbors(adata, method='rapids'); sc.tl.louvain(adata, flavor='rapids'); sc.tl.umap(adata, method='rapids'); ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |; | --------- | ------------ | ------------ | ------- |; | Neighbors | 47 | 15 | 3x |; | Louvain | 70 | 1 | 70x |; | UMAP | 186 | 15 | 12x |. Comments:; * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.); * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate.; * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar.; * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug.; * UMAP computes the nearest neighbors again from scratch - it can’t reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes.; * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any).; * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:909,Safety,detect,detection,909,"This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python; sc.pp.neighbors(adata, method='rapids'); sc.tl.louvain(adata, flavor='rapids'); sc.tl.umap(adata, method='rapids'); ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |; | --------- | ------------ | ------------ | ------- |; | Neighbors | 47 | 15 | 3x |; | Louvain | 70 | 1 | 70x |; | UMAP | 186 | 15 | 12x |. Comments:; * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.); * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate.; * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar.; * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug.; * UMAP computes the nearest neighbors again from scratch - it can’t reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes.; * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any).; * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1418,Security,expose,exposes,1418,"This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python; sc.pp.neighbors(adata, method='rapids'); sc.tl.louvain(adata, flavor='rapids'); sc.tl.umap(adata, method='rapids'); ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |; | --------- | ------------ | ------------ | ------- |; | Neighbors | 47 | 15 | 3x |; | Louvain | 70 | 1 | 70x |; | UMAP | 186 | 15 | 12x |. Comments:; * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.); * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate.; * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar.; * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug.; * UMAP computes the nearest neighbors again from scratch - it can’t reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes.; * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any).; * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1506,Security,expose,exposes,1506,"This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python; sc.pp.neighbors(adata, method='rapids'); sc.tl.louvain(adata, flavor='rapids'); sc.tl.umap(adata, method='rapids'); ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |; | --------- | ------------ | ------------ | ------- |; | Neighbors | 47 | 15 | 3x |; | Louvain | 70 | 1 | 70x |; | UMAP | 186 | 15 | 12x |. Comments:; * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.); * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate.; * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar.; * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug.; * UMAP computes the nearest neighbors again from scratch - it can’t reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes.; * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any).; * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1854,Usability,learn,learning-vm,1854,"This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python; sc.pp.neighbors(adata, method='rapids'); sc.tl.louvain(adata, flavor='rapids'); sc.tl.umap(adata, method='rapids'); ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |; | --------- | ------------ | ------------ | ------- |; | Neighbors | 47 | 15 | 3x |; | Louvain | 70 | 1 | 70x |; | UMAP | 186 | 15 | 12x |. Comments:; * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.); * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate.; * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar.; * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug.; * UMAP computes the nearest neighbors again from scratch - it can’t reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes.; * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any).; * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/issues/832:500,Availability,error,error,500,"I'm using Scanpy with the following software versions:. python==3.7; scanpy==1.4.4; numpy==1.17.2; anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/min",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3652,Availability,error,error,3652,"acked, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 542 return key, value; 543 ; --> 544 key, value = postprocess_reading(key, value); 545 d[key_write] = value; 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value); 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))); 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]; --> 541 value = value.astype(new_dtype); 542 return key, value; 543 . ValueError: invalid shape in fixed-type tuple.; ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3693,Deployability,update,updated,3693,"acked, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 542 return key, value; 543 ; --> 544 key, value = postprocess_reading(key, value); 545 d[key_write] = value; 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value); 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))); 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]; --> 541 value = value.astype(new_dtype); 542 return key, value; 543 . ValueError: invalid shape in fixed-type tuple.; ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:506,Integrability,message,message,506,"I'm using Scanpy with the following software versions:. python==3.7; scanpy==1.4.4; numpy==1.17.2; anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/min",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:242,Performance,load,load,242,"I'm using Scanpy with the following software versions:. python==3.7; scanpy==1.4.4; numpy==1.17.2; anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/min",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1073,Performance,cache,cache,1073," to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = No",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1237,Performance,cache,cache,1237," to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = No",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1243,Performance,cache,cache,1243," to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = No",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1464,Performance,cache,cache,1464," message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {};",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1826,Performance,load,load,1826,"; ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:720,Testability,log,log,720,"I'm using Scanpy with the following software versions:. python==3.7; scanpy==1.4.4; numpy==1.17.2; anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/min",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:731,Testability,TEST,TEST,731,"I'm using Scanpy with the following software versions:. python==3.7; scanpy==1.4.4; numpy==1.17.2; anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/min",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:839,Testability,log,log,839,"I'm using Scanpy with the following software versions:. python==3.7; scanpy==1.4.4; numpy==1.17.2; anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/min",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:843,Testability,log,logstatus,843,"I'm using Scanpy with the following software versions:. python==3.7; scanpy==1.4.4; numpy==1.17.2; anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/min",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1626,Testability,log,logg,1626," message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {};",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/833:230,Availability,error,error,230,"I'm trying to take subgroups from the AnnData object as such:; `adata_1 = adata[adata.obs['louvain'] == '0']`; which works but if I want to get the inverse; `adata_1 = adata[adata.obs['louvain'] != '0']`; it sometimes throws this error only on the second line.; 'IndexError: index 8 is out of bounds for axis 0 with size 8'; After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/834:127,Availability,error,error,127,"Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:; ```pytb; /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key); 280 n_top_genes=n_top_genes,; 281 n_bins=n_bins,; --> 282 flavor=flavor,; 283 ); 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor); 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]; 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower; --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]; 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off; 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898; ```. I run scanpy in Python 3.7 (linux machine) with the following versions:; ```; scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:971,Testability,log,logg,971,"Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:; ```pytb; /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key); 280 n_top_genes=n_top_genes,; 281 n_bins=n_bins,; --> 282 flavor=flavor,; 283 ); 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor); 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]; 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower; --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]; 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off; 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898; ```. I run scanpy in Python 3.7 (linux machine) with the following versions:; ```; scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
