quality_attribute,keyword,matched_word,sentence,source,author,repo,version,wiki,url
Deployability,integrat,integrate,"Hi Samuel,. I'm just reading your notes here and I find them very interesting. I'm at the moment trying to figure out how to solve a very similar problem, how to integrate reduce operation naturally into a Go's FBP data processing pipeline. Something that I'm considering is to specify in each IPs belonging to the same reduce group a reference to a channel that's used to perform the reduce operation. The tricky part is that the process in charge of doing the reduce operation has to maintain state during the operation (cannot be shared) and reset itself or die once it completes and sends the result back to the network. This would mean creating processes on demand per reduce operation or group of IPs to be reduced and I'm not sure how all this would fit within the FBP paradigm. The proposal is still not clear in my head so this is just a vague idea that has come up reading your comments. I'll try to think more about this so I can propose something more elaborated, but I'd love to discuss more about this problem.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/20#issuecomment-272685339
Energy Efficiency,reduce,reduce,"Hi Samuel,. I'm just reading your notes here and I find them very interesting. I'm at the moment trying to figure out how to solve a very similar problem, how to integrate reduce operation naturally into a Go's FBP data processing pipeline. Something that I'm considering is to specify in each IPs belonging to the same reduce group a reference to a channel that's used to perform the reduce operation. The tricky part is that the process in charge of doing the reduce operation has to maintain state during the operation (cannot be shared) and reset itself or die once it completes and sends the result back to the network. This would mean creating processes on demand per reduce operation or group of IPs to be reduced and I'm not sure how all this would fit within the FBP paradigm. The proposal is still not clear in my head so this is just a vague idea that has come up reading your comments. I'll try to think more about this so I can propose something more elaborated, but I'd love to discuss more about this problem.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/20#issuecomment-272685339
Integrability,integrat,integrate,"Hi Samuel,. I'm just reading your notes here and I find them very interesting. I'm at the moment trying to figure out how to solve a very similar problem, how to integrate reduce operation naturally into a Go's FBP data processing pipeline. Something that I'm considering is to specify in each IPs belonging to the same reduce group a reference to a channel that's used to perform the reduce operation. The tricky part is that the process in charge of doing the reduce operation has to maintain state during the operation (cannot be shared) and reset itself or die once it completes and sends the result back to the network. This would mean creating processes on demand per reduce operation or group of IPs to be reduced and I'm not sure how all this would fit within the FBP paradigm. The proposal is still not clear in my head so this is just a vague idea that has come up reading your comments. I'll try to think more about this so I can propose something more elaborated, but I'd love to discuss more about this problem.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/20#issuecomment-272685339
Performance,perform,perform,"Hi Samuel,. I'm just reading your notes here and I find them very interesting. I'm at the moment trying to figure out how to solve a very similar problem, how to integrate reduce operation naturally into a Go's FBP data processing pipeline. Something that I'm considering is to specify in each IPs belonging to the same reduce group a reference to a channel that's used to perform the reduce operation. The tricky part is that the process in charge of doing the reduce operation has to maintain state during the operation (cannot be shared) and reset itself or die once it completes and sends the result back to the network. This would mean creating processes on demand per reduce operation or group of IPs to be reduced and I'm not sure how all this would fit within the FBP paradigm. The proposal is still not clear in my head so this is just a vague idea that has come up reading your comments. I'll try to think more about this so I can propose something more elaborated, but I'd love to discuss more about this problem.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/20#issuecomment-272685339
Usability,clear,clear,"Hi Samuel,. I'm just reading your notes here and I find them very interesting. I'm at the moment trying to figure out how to solve a very similar problem, how to integrate reduce operation naturally into a Go's FBP data processing pipeline. Something that I'm considering is to specify in each IPs belonging to the same reduce group a reference to a channel that's used to perform the reduce operation. The tricky part is that the process in charge of doing the reduce operation has to maintain state during the operation (cannot be shared) and reset itself or die once it completes and sends the result back to the network. This would mean creating processes on demand per reduce operation or group of IPs to be reduced and I'm not sure how all this would fit within the FBP paradigm. The proposal is still not clear in my head so this is just a vague idea that has come up reading your comments. I'll try to think more about this so I can propose something more elaborated, but I'd love to discuss more about this problem.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/20#issuecomment-272685339
Integrability,interface,interface,"Well, we also have:; - [(Almost) ranging over multiple Go channels simultaneously](http://bionics.it/posts/range-over-multiple-go-channels); - The suggestion in #45:; > Use the same IP (at least the same interface) for both files and parameters. This would allow to implement the receiveInputs() method with a merge into a single chan map[string]*IP that any process.Run method would iterate over to generate tasks, (or run inline). It would dramatically simplify the current Process.createTasks() method(!). It would also allow us to keep a full audit trail of where certain parameters are coming from, not only files. ... so it's perhaps not a given that the waitgroup idea is the best.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/22#issuecomment-373871903
Security,audit,audit,"Well, we also have:; - [(Almost) ranging over multiple Go channels simultaneously](http://bionics.it/posts/range-over-multiple-go-channels); - The suggestion in #45:; > Use the same IP (at least the same interface) for both files and parameters. This would allow to implement the receiveInputs() method with a merge into a single chan map[string]*IP that any process.Run method would iterate over to generate tasks, (or run inline). It would dramatically simplify the current Process.createTasks() method(!). It would also allow us to keep a full audit trail of where certain parameters are coming from, not only files. ... so it's perhaps not a given that the waitgroup idea is the best.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/22#issuecomment-373871903
Usability,simpl,simplify,"Well, we also have:; - [(Almost) ranging over multiple Go channels simultaneously](http://bionics.it/posts/range-over-multiple-go-channels); - The suggestion in #45:; > Use the same IP (at least the same interface) for both files and parameters. This would allow to implement the receiveInputs() method with a merge into a single chan map[string]*IP that any process.Run method would iterate over to generate tasks, (or run inline). It would dramatically simplify the current Process.createTasks() method(!). It would also allow us to keep a full audit trail of where certain parameters are coming from, not only files. ... so it's perhaps not a given that the waitgroup idea is the best.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/22#issuecomment-373871903
Availability,robust,robustly,"Hi @valentin-krasontovitsch , thanks for checking out scipipe. What we are doing right now, while pondering the best way to add proper SLURM support, is to just use the prepend feature, and add the SLURM `salloc` command ther (as shown in http://scipipe.org/howtos/hpc/). It actually works very robustly, but it would be sure great to have something a tad bit more integrated. I will check the go SLURM library (wasn't aware of it yet :) ), but I'm also leaning towards keeping everything as simple as possible, fighting a bit to avoid dependencies etc ... as I think this will help make scipipe easier to maintain in the long run, with limited academic funding :P",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/38#issuecomment-357923912
Deployability,integrat,integrated,"Hi @valentin-krasontovitsch , thanks for checking out scipipe. What we are doing right now, while pondering the best way to add proper SLURM support, is to just use the prepend feature, and add the SLURM `salloc` command ther (as shown in http://scipipe.org/howtos/hpc/). It actually works very robustly, but it would be sure great to have something a tad bit more integrated. I will check the go SLURM library (wasn't aware of it yet :) ), but I'm also leaning towards keeping everything as simple as possible, fighting a bit to avoid dependencies etc ... as I think this will help make scipipe easier to maintain in the long run, with limited academic funding :P",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/38#issuecomment-357923912
Integrability,integrat,integrated,"Hi @valentin-krasontovitsch , thanks for checking out scipipe. What we are doing right now, while pondering the best way to add proper SLURM support, is to just use the prepend feature, and add the SLURM `salloc` command ther (as shown in http://scipipe.org/howtos/hpc/). It actually works very robustly, but it would be sure great to have something a tad bit more integrated. I will check the go SLURM library (wasn't aware of it yet :) ), but I'm also leaning towards keeping everything as simple as possible, fighting a bit to avoid dependencies etc ... as I think this will help make scipipe easier to maintain in the long run, with limited academic funding :P",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/38#issuecomment-357923912
Safety,avoid,avoid,"Hi @valentin-krasontovitsch , thanks for checking out scipipe. What we are doing right now, while pondering the best way to add proper SLURM support, is to just use the prepend feature, and add the SLURM `salloc` command ther (as shown in http://scipipe.org/howtos/hpc/). It actually works very robustly, but it would be sure great to have something a tad bit more integrated. I will check the go SLURM library (wasn't aware of it yet :) ), but I'm also leaning towards keeping everything as simple as possible, fighting a bit to avoid dependencies etc ... as I think this will help make scipipe easier to maintain in the long run, with limited academic funding :P",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/38#issuecomment-357923912
Usability,simpl,simple,"Hi @valentin-krasontovitsch , thanks for checking out scipipe. What we are doing right now, while pondering the best way to add proper SLURM support, is to just use the prepend feature, and add the SLURM `salloc` command ther (as shown in http://scipipe.org/howtos/hpc/). It actually works very robustly, but it would be sure great to have something a tad bit more integrated. I will check the go SLURM library (wasn't aware of it yet :) ), but I'm also leaning towards keeping everything as simple as possible, fighting a bit to avoid dependencies etc ... as I think this will help make scipipe easier to maintain in the long run, with limited academic funding :P",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/38#issuecomment-357923912
Integrability,interface,interface,"Hi @kwrobert and thanks for the input and kind words!. This request is timely, as I'm looking at refactoring the IP (possibly corresponding to what you call ""resource""?) into a more composable core, which different types of implementations can extend, via struct embedding. I actually started this, which can be seen in the naming of the IP interface, the BaseIP struct and the FileIP one, in https://github.com/scipipe/scipipe/blob/master/ip.go ... but this is unfinished work, and not really cleaned up in that particular file (be warned!). I have successfully created a re-usable core for Process and Tasks, but the IP one requires a bit more thinking to get right. . There is some really tricky things to think through to get this right. I have some notes spread out over issues and my journal etc. Will see if I can try to collect them in one place, perhaps in this issue. For what you mention about remote task execution, that sounds vaguely a little bit more like [Task Execution Schemes](https://github.com/ga4gh/task-execution-schemas) and the one implementation of a TES-supporting task-running server I know ([Funnel](https://github.com/ga4gh/task-execution-schemas))?. Important to note is that there is a one-to-many relationship between tasks and data, so it's important not to tie the concepts of data and tasks too closely together. This is not always becoming visible in data science workflows in industry where people are often just processing one big table-like dataset at a time, but it shows up in biology where multiple inputs and outputs per task is very common (I have outlined this realization in an old blog post: http://bionics.it/posts/workflows-dataflow-not-task-deps ). So, reg. pull requests, I have to warn that I already have some thoughts about this. OTOH, I will always be interested to compare notes, and approaches, but I will also push really hard to get a really generic solution, according to a few patterns that have emerged. It will for sure help a lot to hav",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380898779
Modifiability,refactor,refactoring,"Hi @kwrobert and thanks for the input and kind words!. This request is timely, as I'm looking at refactoring the IP (possibly corresponding to what you call ""resource""?) into a more composable core, which different types of implementations can extend, via struct embedding. I actually started this, which can be seen in the naming of the IP interface, the BaseIP struct and the FileIP one, in https://github.com/scipipe/scipipe/blob/master/ip.go ... but this is unfinished work, and not really cleaned up in that particular file (be warned!). I have successfully created a re-usable core for Process and Tasks, but the IP one requires a bit more thinking to get right. . There is some really tricky things to think through to get this right. I have some notes spread out over issues and my journal etc. Will see if I can try to collect them in one place, perhaps in this issue. For what you mention about remote task execution, that sounds vaguely a little bit more like [Task Execution Schemes](https://github.com/ga4gh/task-execution-schemas) and the one implementation of a TES-supporting task-running server I know ([Funnel](https://github.com/ga4gh/task-execution-schemas))?. Important to note is that there is a one-to-many relationship between tasks and data, so it's important not to tie the concepts of data and tasks too closely together. This is not always becoming visible in data science workflows in industry where people are often just processing one big table-like dataset at a time, but it shows up in biology where multiple inputs and outputs per task is very common (I have outlined this realization in an old blog post: http://bionics.it/posts/workflows-dataflow-not-task-deps ). So, reg. pull requests, I have to warn that I already have some thoughts about this. OTOH, I will always be interested to compare notes, and approaches, but I will also push really hard to get a really generic solution, according to a few patterns that have emerged. It will for sure help a lot to hav",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380898779
Usability,usab,usable,"Hi @kwrobert and thanks for the input and kind words!. This request is timely, as I'm looking at refactoring the IP (possibly corresponding to what you call ""resource""?) into a more composable core, which different types of implementations can extend, via struct embedding. I actually started this, which can be seen in the naming of the IP interface, the BaseIP struct and the FileIP one, in https://github.com/scipipe/scipipe/blob/master/ip.go ... but this is unfinished work, and not really cleaned up in that particular file (be warned!). I have successfully created a re-usable core for Process and Tasks, but the IP one requires a bit more thinking to get right. . There is some really tricky things to think through to get this right. I have some notes spread out over issues and my journal etc. Will see if I can try to collect them in one place, perhaps in this issue. For what you mention about remote task execution, that sounds vaguely a little bit more like [Task Execution Schemes](https://github.com/ga4gh/task-execution-schemas) and the one implementation of a TES-supporting task-running server I know ([Funnel](https://github.com/ga4gh/task-execution-schemas))?. Important to note is that there is a one-to-many relationship between tasks and data, so it's important not to tie the concepts of data and tasks too closely together. This is not always becoming visible in data science workflows in industry where people are often just processing one big table-like dataset at a time, but it shows up in biology where multiple inputs and outputs per task is very common (I have outlined this realization in an old blog post: http://bionics.it/posts/workflows-dataflow-not-task-deps ). So, reg. pull requests, I have to warn that I already have some thoughts about this. OTOH, I will always be interested to compare notes, and approaches, but I will also push really hard to get a really generic solution, according to a few patterns that have emerged. It will for sure help a lot to hav",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380898779
Deployability,pipeline,pipeline,"Notes would be great! It would help keep everybody on the same page. I was wondering what this IP stuff meant (Information Packet, right?). So I can kind of imagine where this is going (we have various types of IPs flowing through a data pipeline). . So when I use the word Resource, I'm referring to a machine where Tasks can be executed. You have your current local machine (LocalResource) from which you start the pipeline, but maybe you also have SSH access to a powerful workstation somewhere or a virtual machine in the cloud (SSHResource). Finally, maybe you're an academic that has access to a big cluster running the SLURM job scheduler (SLURMResource). Each of these Resources would be able do the following things:. 1) Consume a Task given to it by a Process. 2) Copy any input IPs for the Task to the remote location. This might get tricky if you're using pipes and not files, but you could still stream input data up to the remote resource. It's also fine for there to be many input IPs for a single Task, we just need to copy them all. . 3) Execute the Task on the remote resource. 4) Copy any output IP's back to the local machine you started the pipeline from. . 5) Notify the Process object that the Task has completed. I'm not sure how this is currently handled in the code. Do you check for the presence of specified outputs to signify a task is done?. In my mind, it's really important to avoid any sort of server-based architecture, so the implementation I'm envisioning isn't similar to the links you posted. Setting up and running servers is usually a bit of a pain, especially if you have a bunch of different resources you need to run jobs on. It also feels like overkill for our academic kind of use case where the number of tasks is small compared to some sort of industry use case that has a massive number of tasks to execute. . I also envision SSH as the primary communication protocol to remote Resources. It's simple, secure, and the Go standard library has an implemen",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380907660
Energy Efficiency,power,powerful,"Notes would be great! It would help keep everybody on the same page. I was wondering what this IP stuff meant (Information Packet, right?). So I can kind of imagine where this is going (we have various types of IPs flowing through a data pipeline). . So when I use the word Resource, I'm referring to a machine where Tasks can be executed. You have your current local machine (LocalResource) from which you start the pipeline, but maybe you also have SSH access to a powerful workstation somewhere or a virtual machine in the cloud (SSHResource). Finally, maybe you're an academic that has access to a big cluster running the SLURM job scheduler (SLURMResource). Each of these Resources would be able do the following things:. 1) Consume a Task given to it by a Process. 2) Copy any input IPs for the Task to the remote location. This might get tricky if you're using pipes and not files, but you could still stream input data up to the remote resource. It's also fine for there to be many input IPs for a single Task, we just need to copy them all. . 3) Execute the Task on the remote resource. 4) Copy any output IP's back to the local machine you started the pipeline from. . 5) Notify the Process object that the Task has completed. I'm not sure how this is currently handled in the code. Do you check for the presence of specified outputs to signify a task is done?. In my mind, it's really important to avoid any sort of server-based architecture, so the implementation I'm envisioning isn't similar to the links you posted. Setting up and running servers is usually a bit of a pain, especially if you have a bunch of different resources you need to run jobs on. It also feels like overkill for our academic kind of use case where the number of tasks is small compared to some sort of industry use case that has a massive number of tasks to execute. . I also envision SSH as the primary communication protocol to remote Resources. It's simple, secure, and the Go standard library has an implemen",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380907660
Integrability,protocol,protocol," local machine (LocalResource) from which you start the pipeline, but maybe you also have SSH access to a powerful workstation somewhere or a virtual machine in the cloud (SSHResource). Finally, maybe you're an academic that has access to a big cluster running the SLURM job scheduler (SLURMResource). Each of these Resources would be able do the following things:. 1) Consume a Task given to it by a Process. 2) Copy any input IPs for the Task to the remote location. This might get tricky if you're using pipes and not files, but you could still stream input data up to the remote resource. It's also fine for there to be many input IPs for a single Task, we just need to copy them all. . 3) Execute the Task on the remote resource. 4) Copy any output IP's back to the local machine you started the pipeline from. . 5) Notify the Process object that the Task has completed. I'm not sure how this is currently handled in the code. Do you check for the presence of specified outputs to signify a task is done?. In my mind, it's really important to avoid any sort of server-based architecture, so the implementation I'm envisioning isn't similar to the links you posted. Setting up and running servers is usually a bit of a pain, especially if you have a bunch of different resources you need to run jobs on. It also feels like overkill for our academic kind of use case where the number of tasks is small compared to some sort of industry use case that has a massive number of tasks to execute. . I also envision SSH as the primary communication protocol to remote Resources. It's simple, secure, and the Go standard library has an implementation of it. . Finally, I think this would all require moving `func (t *Task) Execute()` from Task objects into Resource objects. So we would have something like `func (r *Resource) Execute(t Task)` which takes a task and completes all the steps listed above. Does that clarify my thinking? How do feel about this? Do you think it would work in the code base?",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380907660
Safety,avoid,avoid," local machine (LocalResource) from which you start the pipeline, but maybe you also have SSH access to a powerful workstation somewhere or a virtual machine in the cloud (SSHResource). Finally, maybe you're an academic that has access to a big cluster running the SLURM job scheduler (SLURMResource). Each of these Resources would be able do the following things:. 1) Consume a Task given to it by a Process. 2) Copy any input IPs for the Task to the remote location. This might get tricky if you're using pipes and not files, but you could still stream input data up to the remote resource. It's also fine for there to be many input IPs for a single Task, we just need to copy them all. . 3) Execute the Task on the remote resource. 4) Copy any output IP's back to the local machine you started the pipeline from. . 5) Notify the Process object that the Task has completed. I'm not sure how this is currently handled in the code. Do you check for the presence of specified outputs to signify a task is done?. In my mind, it's really important to avoid any sort of server-based architecture, so the implementation I'm envisioning isn't similar to the links you posted. Setting up and running servers is usually a bit of a pain, especially if you have a bunch of different resources you need to run jobs on. It also feels like overkill for our academic kind of use case where the number of tasks is small compared to some sort of industry use case that has a massive number of tasks to execute. . I also envision SSH as the primary communication protocol to remote Resources. It's simple, secure, and the Go standard library has an implementation of it. . Finally, I think this would all require moving `func (t *Task) Execute()` from Task objects into Resource objects. So we would have something like `func (r *Resource) Execute(t Task)` which takes a task and completes all the steps listed above. Does that clarify my thinking? How do feel about this? Do you think it would work in the code base?",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380907660
Security,access,access,"Notes would be great! It would help keep everybody on the same page. I was wondering what this IP stuff meant (Information Packet, right?). So I can kind of imagine where this is going (we have various types of IPs flowing through a data pipeline). . So when I use the word Resource, I'm referring to a machine where Tasks can be executed. You have your current local machine (LocalResource) from which you start the pipeline, but maybe you also have SSH access to a powerful workstation somewhere or a virtual machine in the cloud (SSHResource). Finally, maybe you're an academic that has access to a big cluster running the SLURM job scheduler (SLURMResource). Each of these Resources would be able do the following things:. 1) Consume a Task given to it by a Process. 2) Copy any input IPs for the Task to the remote location. This might get tricky if you're using pipes and not files, but you could still stream input data up to the remote resource. It's also fine for there to be many input IPs for a single Task, we just need to copy them all. . 3) Execute the Task on the remote resource. 4) Copy any output IP's back to the local machine you started the pipeline from. . 5) Notify the Process object that the Task has completed. I'm not sure how this is currently handled in the code. Do you check for the presence of specified outputs to signify a task is done?. In my mind, it's really important to avoid any sort of server-based architecture, so the implementation I'm envisioning isn't similar to the links you posted. Setting up and running servers is usually a bit of a pain, especially if you have a bunch of different resources you need to run jobs on. It also feels like overkill for our academic kind of use case where the number of tasks is small compared to some sort of industry use case that has a massive number of tasks to execute. . I also envision SSH as the primary communication protocol to remote Resources. It's simple, secure, and the Go standard library has an implemen",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380907660
Usability,simpl,simple," local machine (LocalResource) from which you start the pipeline, but maybe you also have SSH access to a powerful workstation somewhere or a virtual machine in the cloud (SSHResource). Finally, maybe you're an academic that has access to a big cluster running the SLURM job scheduler (SLURMResource). Each of these Resources would be able do the following things:. 1) Consume a Task given to it by a Process. 2) Copy any input IPs for the Task to the remote location. This might get tricky if you're using pipes and not files, but you could still stream input data up to the remote resource. It's also fine for there to be many input IPs for a single Task, we just need to copy them all. . 3) Execute the Task on the remote resource. 4) Copy any output IP's back to the local machine you started the pipeline from. . 5) Notify the Process object that the Task has completed. I'm not sure how this is currently handled in the code. Do you check for the presence of specified outputs to signify a task is done?. In my mind, it's really important to avoid any sort of server-based architecture, so the implementation I'm envisioning isn't similar to the links you posted. Setting up and running servers is usually a bit of a pain, especially if you have a bunch of different resources you need to run jobs on. It also feels like overkill for our academic kind of use case where the number of tasks is small compared to some sort of industry use case that has a massive number of tasks to execute. . I also envision SSH as the primary communication protocol to remote Resources. It's simple, secure, and the Go standard library has an implementation of it. . Finally, I think this would all require moving `func (t *Task) Execute()` from Task objects into Resource objects. So we would have something like `func (r *Resource) Execute(t Task)` which takes a task and completes all the steps listed above. Does that clarify my thinking? How do feel about this? Do you think it would work in the code base?",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380907660
Availability,robust,robust," > So when I use the word Resource, I'm referring to a machine where Tasks can be executed. Ah, that makes sense!. > In my mind, it's really important to avoid any sort of server-based architecture, so the implementation I'm envisioning isn't similar to the links you posted. Setting up and running servers is usually a bit of a pain, especially if you have a bunch of different resources you need to run jobs on. It also feels like overkill for our academic kind of use case where the number of tasks is small compared to some sort of industry use case that has a massive number of tasks to execute. Actually very much second this, and I'm happy you're saying it! :) I've been looking at TES / funnel with a kind of worry, since it would make the scipipe code base which is currently only just over 1k LOC, a lot larger and more complex. > Finally, I think this would all require moving func (t *Task) Execute() from Task objects into Resource objects. So we would have something like func (r *Resource) Execute(t Task) which takes a task and completes all the steps listed above. I see what you mean. These are all really interesting ideas I think. I like your focus on simplicity!. Some thoughts coming to mind off arm: ""Resource"" might be a slightly vague term, as it is often used to refer to other things like the data items (which is why I thought you were referring to the IPs (which also might not have the optimal name :D)). Could it be ""Executor"" or similar? A clear name might help think more clearly about how it would fit into the code base as well. I'll need to think more about how it can fit into the code base, but I like the general direction, and would love it if we could find a simple yet robust way of ""light-weight task distribution""! It's been on the wish list for a long time :) (We used the light weight distributor in [luigi](https://github.com/spotify/luigi) a bit, but I think it is a bit more focused on a shared file system (this is what we used) or other data store).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380913501
Deployability,pipeline,pipeline,"> I was wondering what this IP stuff meant (Information Packet, right?). Yes, a slightly awkward bit of flow-based programming lingo. > So I can kind of imagine where this is going (we have various types of IPs flowing through a data pipeline). Exactly. Although, in batch workflows, the IPs are not really long-lived as in the original FBP ideas, but rather short lived, between two processes only, and typically immutable. > So when I use the word Resource, I'm referring to a machine where Tasks can be executed. Ah, that makes sense!. > In my mind, it's really important to avoid any sort of server-based architecture, so the implementation I'm envisioning isn't similar to the links you posted. Setting up and running servers is usually a bit of a pain, especially if you have a bunch of different resources you need to run jobs on. It also feels like overkill for our academic kind of use case where the number of tasks is small compared to some sort of industry use case that has a massive number of tasks to execute. Actually very much second this, and I'm happy you're saying it! :) I've been looking at TES / funnel with a kind of worry, since it would make the scipipe code base which is currently only just over 1k LOC, a lot larger and more complex. > Finally, I think this would all require moving func (t *Task) Execute() from Task objects into Resource objects. So we would have something like func (r *Resource) Execute(t Task) which takes a task and completes all the steps listed above. I see what you mean. These are all really interesting ideas I think. I like your focus on simplicity!. Some thoughts coming to mind off arm: ""Resource"" might be a slightly vague term, as it is often used to refer to other things like the data items (which is why I thought you were referring to the IPs (which also might not have the optimal name :D)). Could it be ""Executor"" or similar? A clear name might help think more clearly about how it would fit into the code base as well. I'll need to ",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380913501
Safety,avoid,avoid,"> I was wondering what this IP stuff meant (Information Packet, right?). Yes, a slightly awkward bit of flow-based programming lingo. > So I can kind of imagine where this is going (we have various types of IPs flowing through a data pipeline). Exactly. Although, in batch workflows, the IPs are not really long-lived as in the original FBP ideas, but rather short lived, between two processes only, and typically immutable. > So when I use the word Resource, I'm referring to a machine where Tasks can be executed. Ah, that makes sense!. > In my mind, it's really important to avoid any sort of server-based architecture, so the implementation I'm envisioning isn't similar to the links you posted. Setting up and running servers is usually a bit of a pain, especially if you have a bunch of different resources you need to run jobs on. It also feels like overkill for our academic kind of use case where the number of tasks is small compared to some sort of industry use case that has a massive number of tasks to execute. Actually very much second this, and I'm happy you're saying it! :) I've been looking at TES / funnel with a kind of worry, since it would make the scipipe code base which is currently only just over 1k LOC, a lot larger and more complex. > Finally, I think this would all require moving func (t *Task) Execute() from Task objects into Resource objects. So we would have something like func (r *Resource) Execute(t Task) which takes a task and completes all the steps listed above. I see what you mean. These are all really interesting ideas I think. I like your focus on simplicity!. Some thoughts coming to mind off arm: ""Resource"" might be a slightly vague term, as it is often used to refer to other things like the data items (which is why I thought you were referring to the IPs (which also might not have the optimal name :D)). Could it be ""Executor"" or similar? A clear name might help think more clearly about how it would fit into the code base as well. I'll need to ",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380913501
Usability,simpl,simplicity," > So when I use the word Resource, I'm referring to a machine where Tasks can be executed. Ah, that makes sense!. > In my mind, it's really important to avoid any sort of server-based architecture, so the implementation I'm envisioning isn't similar to the links you posted. Setting up and running servers is usually a bit of a pain, especially if you have a bunch of different resources you need to run jobs on. It also feels like overkill for our academic kind of use case where the number of tasks is small compared to some sort of industry use case that has a massive number of tasks to execute. Actually very much second this, and I'm happy you're saying it! :) I've been looking at TES / funnel with a kind of worry, since it would make the scipipe code base which is currently only just over 1k LOC, a lot larger and more complex. > Finally, I think this would all require moving func (t *Task) Execute() from Task objects into Resource objects. So we would have something like func (r *Resource) Execute(t Task) which takes a task and completes all the steps listed above. I see what you mean. These are all really interesting ideas I think. I like your focus on simplicity!. Some thoughts coming to mind off arm: ""Resource"" might be a slightly vague term, as it is often used to refer to other things like the data items (which is why I thought you were referring to the IPs (which also might not have the optimal name :D)). Could it be ""Executor"" or similar? A clear name might help think more clearly about how it would fit into the code base as well. I'll need to think more about how it can fit into the code base, but I like the general direction, and would love it if we could find a simple yet robust way of ""light-weight task distribution""! It's been on the wish list for a long time :) (We used the light weight distributor in [luigi](https://github.com/spotify/luigi) a bit, but I think it is a bit more focused on a shared file system (this is what we used) or other data store).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-380913501
Availability,error,error,"Cool! This is exactly the kind of interface I was envisioning. I do have a few thoughts about this. I think the ability to attach multiple `Executor`s to a `Process` would be really cool, and I think the notion of an ""ActiveExecutor"" might limit this capability. Instead, one should be able to add many `Executor`s to a `Process`, and the `Process` becomes responsible for distributing `Tasks` among its attached `Executor`s. This would require some sort of scheduling algorithm to be included in the `Run` method of `Process`, but this could be made a simple as you like (just a simple round robin algorithm to start, and we could get fancy with it later). So to sketch what I'm thinking, an `Executor` should have channels for incoming and completed `Task`s:. ```golang; type Executor interface {; // This function just receives a Task, and returns an error if ; /// the Executor can't take any more tasks at the moment; ReceiveTask(t *Task) Error; // This should be a loop that runs continuously and executes; // tasks as they are received. All completed tasks should; // has the Task.Done signal set; ExecuteIncomingTasks(); // Stop receiving incoming tasks, finish all currently running tasks, ; // cleanup and exit; Close(); }; ```. and a `Process` should have a slice of `Executor`s as an additional member. ```golang; type Process struct {; // other stuff ...; Executors []Executor; }; ```. And I think the `Run` method of `Process` would have something like this:; EDIT: I just realized this *is not* round robin scheduling, and you end up always filling up the first `Executor` before moving on to the others, but hopefully this illustrates the general idea.; ```golang; // Start up ExecuteIncomingTasks for each Executor; for executor := range p.Executors {; go executor.ExecuteIncomingTasks(); defer executor.Close(); }. tasks := []*Task{}; for t := range p.createTasks() {; tasks = append(tasks, t); taskReceived := false; for taskReceived {; for executor := range p.Executors {; // Pass ",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383106434
Deployability,continuous,continuously,"Cool! This is exactly the kind of interface I was envisioning. I do have a few thoughts about this. I think the ability to attach multiple `Executor`s to a `Process` would be really cool, and I think the notion of an ""ActiveExecutor"" might limit this capability. Instead, one should be able to add many `Executor`s to a `Process`, and the `Process` becomes responsible for distributing `Tasks` among its attached `Executor`s. This would require some sort of scheduling algorithm to be included in the `Run` method of `Process`, but this could be made a simple as you like (just a simple round robin algorithm to start, and we could get fancy with it later). So to sketch what I'm thinking, an `Executor` should have channels for incoming and completed `Task`s:. ```golang; type Executor interface {; // This function just receives a Task, and returns an error if ; /// the Executor can't take any more tasks at the moment; ReceiveTask(t *Task) Error; // This should be a loop that runs continuously and executes; // tasks as they are received. All completed tasks should; // has the Task.Done signal set; ExecuteIncomingTasks(); // Stop receiving incoming tasks, finish all currently running tasks, ; // cleanup and exit; Close(); }; ```. and a `Process` should have a slice of `Executor`s as an additional member. ```golang; type Process struct {; // other stuff ...; Executors []Executor; }; ```. And I think the `Run` method of `Process` would have something like this:; EDIT: I just realized this *is not* round robin scheduling, and you end up always filling up the first `Executor` before moving on to the others, but hopefully this illustrates the general idea.; ```golang; // Start up ExecuteIncomingTasks for each Executor; for executor := range p.Executors {; go executor.ExecuteIncomingTasks(); defer executor.Close(); }. tasks := []*Task{}; for t := range p.createTasks() {; tasks = append(tasks, t); taskReceived := false; for taskReceived {; for executor := range p.Executors {; // Pass ",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383106434
Energy Efficiency,schedul,scheduling,"Cool! This is exactly the kind of interface I was envisioning. I do have a few thoughts about this. I think the ability to attach multiple `Executor`s to a `Process` would be really cool, and I think the notion of an ""ActiveExecutor"" might limit this capability. Instead, one should be able to add many `Executor`s to a `Process`, and the `Process` becomes responsible for distributing `Tasks` among its attached `Executor`s. This would require some sort of scheduling algorithm to be included in the `Run` method of `Process`, but this could be made a simple as you like (just a simple round robin algorithm to start, and we could get fancy with it later). So to sketch what I'm thinking, an `Executor` should have channels for incoming and completed `Task`s:. ```golang; type Executor interface {; // This function just receives a Task, and returns an error if ; /// the Executor can't take any more tasks at the moment; ReceiveTask(t *Task) Error; // This should be a loop that runs continuously and executes; // tasks as they are received. All completed tasks should; // has the Task.Done signal set; ExecuteIncomingTasks(); // Stop receiving incoming tasks, finish all currently running tasks, ; // cleanup and exit; Close(); }; ```. and a `Process` should have a slice of `Executor`s as an additional member. ```golang; type Process struct {; // other stuff ...; Executors []Executor; }; ```. And I think the `Run` method of `Process` would have something like this:; EDIT: I just realized this *is not* round robin scheduling, and you end up always filling up the first `Executor` before moving on to the others, but hopefully this illustrates the general idea.; ```golang; // Start up ExecuteIncomingTasks for each Executor; for executor := range p.Executors {; go executor.ExecuteIncomingTasks(); defer executor.Close(); }. tasks := []*Task{}; for t := range p.createTasks() {; tasks = append(tasks, t); taskReceived := false; for taskReceived {; for executor := range p.Executors {; // Pass ",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383106434
Integrability,interface,interface,"Cool! This is exactly the kind of interface I was envisioning. I do have a few thoughts about this. I think the ability to attach multiple `Executor`s to a `Process` would be really cool, and I think the notion of an ""ActiveExecutor"" might limit this capability. Instead, one should be able to add many `Executor`s to a `Process`, and the `Process` becomes responsible for distributing `Tasks` among its attached `Executor`s. This would require some sort of scheduling algorithm to be included in the `Run` method of `Process`, but this could be made a simple as you like (just a simple round robin algorithm to start, and we could get fancy with it later). So to sketch what I'm thinking, an `Executor` should have channels for incoming and completed `Task`s:. ```golang; type Executor interface {; // This function just receives a Task, and returns an error if ; /// the Executor can't take any more tasks at the moment; ReceiveTask(t *Task) Error; // This should be a loop that runs continuously and executes; // tasks as they are received. All completed tasks should; // has the Task.Done signal set; ExecuteIncomingTasks(); // Stop receiving incoming tasks, finish all currently running tasks, ; // cleanup and exit; Close(); }; ```. and a `Process` should have a slice of `Executor`s as an additional member. ```golang; type Process struct {; // other stuff ...; Executors []Executor; }; ```. And I think the `Run` method of `Process` would have something like this:; EDIT: I just realized this *is not* round robin scheduling, and you end up always filling up the first `Executor` before moving on to the others, but hopefully this illustrates the general idea.; ```golang; // Start up ExecuteIncomingTasks for each Executor; for executor := range p.Executors {; go executor.ExecuteIncomingTasks(); defer executor.Close(); }. tasks := []*Task{}; for t := range p.createTasks() {; tasks = append(tasks, t); taskReceived := false; for taskReceived {; for executor := range p.Executors {; // Pass ",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383106434
Testability,log,logic,"s should be a loop that runs continuously and executes; // tasks as they are received. All completed tasks should; // has the Task.Done signal set; ExecuteIncomingTasks(); // Stop receiving incoming tasks, finish all currently running tasks, ; // cleanup and exit; Close(); }; ```. and a `Process` should have a slice of `Executor`s as an additional member. ```golang; type Process struct {; // other stuff ...; Executors []Executor; }; ```. And I think the `Run` method of `Process` would have something like this:; EDIT: I just realized this *is not* round robin scheduling, and you end up always filling up the first `Executor` before moving on to the others, but hopefully this illustrates the general idea.; ```golang; // Start up ExecuteIncomingTasks for each Executor; for executor := range p.Executors {; go executor.ExecuteIncomingTasks(); defer executor.Close(); }. tasks := []*Task{}; for t := range p.createTasks() {; tasks = append(tasks, t); taskReceived := false; for taskReceived {; for executor := range p.Executors {; // Pass in a pointer to a Task so we can set Task.Done; err := executor.ReceiveTask(t) ; // if err == nil, the Task has been received!; // If we got an error back, the executor can't accept any more tasks; // at the moment and we need to try giving this task to another executor; if err == nil { ; taskReceived = true ; break; }; }; }; }. // make sure all tasks are finished, this logic shouldn't change from the existing loop at all; ```; ; And each special `Executor` type would implement `ExecuteIncomingTasks` differently depending on the type of Executor it is (Local, SSH, SLURM, etc). . What have I missed here? Because each `Task` contains its `InIPs` and `OutIPs` it should be possible to do all data transferring within `ExecuteIncomingTasks` using just the information contained in the `Task` itself, right? . BTW, the fact that this should be straightforward to implement is a testament to your design. Well done! Also sorry for the super long comment!",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383106434
Usability,simpl,simple,"Cool! This is exactly the kind of interface I was envisioning. I do have a few thoughts about this. I think the ability to attach multiple `Executor`s to a `Process` would be really cool, and I think the notion of an ""ActiveExecutor"" might limit this capability. Instead, one should be able to add many `Executor`s to a `Process`, and the `Process` becomes responsible for distributing `Tasks` among its attached `Executor`s. This would require some sort of scheduling algorithm to be included in the `Run` method of `Process`, but this could be made a simple as you like (just a simple round robin algorithm to start, and we could get fancy with it later). So to sketch what I'm thinking, an `Executor` should have channels for incoming and completed `Task`s:. ```golang; type Executor interface {; // This function just receives a Task, and returns an error if ; /// the Executor can't take any more tasks at the moment; ReceiveTask(t *Task) Error; // This should be a loop that runs continuously and executes; // tasks as they are received. All completed tasks should; // has the Task.Done signal set; ExecuteIncomingTasks(); // Stop receiving incoming tasks, finish all currently running tasks, ; // cleanup and exit; Close(); }; ```. and a `Process` should have a slice of `Executor`s as an additional member. ```golang; type Process struct {; // other stuff ...; Executors []Executor; }; ```. And I think the `Run` method of `Process` would have something like this:; EDIT: I just realized this *is not* round robin scheduling, and you end up always filling up the first `Executor` before moving on to the others, but hopefully this illustrates the general idea.; ```golang; // Start up ExecuteIncomingTasks for each Executor; for executor := range p.Executors {; go executor.ExecuteIncomingTasks(); defer executor.Close(); }. tasks := []*Task{}; for t := range p.createTasks() {; tasks = append(tasks, t); taskReceived := false; for taskReceived {; for executor := range p.Executors {; // Pass ",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383106434
Availability,avail,available,"Many thanks for the detail input @kwrobert ! Being able to distribute work among multiple executors would indeed be cool!. I still think one needs the ability to quickly switch between running e.g. only locally, and distributing the work to everything available, but I'm sure both of these goals can be combined. In the simplest form, one could just allow `SetActiveExecutor(""*"")`, and that would activate all configured executors ... or something similar. Regarding the implementation of the executors, overall looks legit!. About taking tasks via channels though, I'm still not sure (but still thinking!). Right now, the scheduling logic is guaranteed by each process keeping an idle go-routine per task, waiting for it to return from (blocking) shell commands before proceeding. Handing off a task to an executor might change this logic considerably. Perhaps this is needed to get true scalability (will need to think more), if so be it, but just raising a flag that it might complicate things a bit. Will easily be convinced by well-working code though :)",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383117959
Energy Efficiency,schedul,scheduling,"Many thanks for the detail input @kwrobert ! Being able to distribute work among multiple executors would indeed be cool!. I still think one needs the ability to quickly switch between running e.g. only locally, and distributing the work to everything available, but I'm sure both of these goals can be combined. In the simplest form, one could just allow `SetActiveExecutor(""*"")`, and that would activate all configured executors ... or something similar. Regarding the implementation of the executors, overall looks legit!. About taking tasks via channels though, I'm still not sure (but still thinking!). Right now, the scheduling logic is guaranteed by each process keeping an idle go-routine per task, waiting for it to return from (blocking) shell commands before proceeding. Handing off a task to an executor might change this logic considerably. Perhaps this is needed to get true scalability (will need to think more), if so be it, but just raising a flag that it might complicate things a bit. Will easily be convinced by well-working code though :)",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383117959
Integrability,rout,routine,"Many thanks for the detail input @kwrobert ! Being able to distribute work among multiple executors would indeed be cool!. I still think one needs the ability to quickly switch between running e.g. only locally, and distributing the work to everything available, but I'm sure both of these goals can be combined. In the simplest form, one could just allow `SetActiveExecutor(""*"")`, and that would activate all configured executors ... or something similar. Regarding the implementation of the executors, overall looks legit!. About taking tasks via channels though, I'm still not sure (but still thinking!). Right now, the scheduling logic is guaranteed by each process keeping an idle go-routine per task, waiting for it to return from (blocking) shell commands before proceeding. Handing off a task to an executor might change this logic considerably. Perhaps this is needed to get true scalability (will need to think more), if so be it, but just raising a flag that it might complicate things a bit. Will easily be convinced by well-working code though :)",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383117959
Modifiability,config,configured,"Many thanks for the detail input @kwrobert ! Being able to distribute work among multiple executors would indeed be cool!. I still think one needs the ability to quickly switch between running e.g. only locally, and distributing the work to everything available, but I'm sure both of these goals can be combined. In the simplest form, one could just allow `SetActiveExecutor(""*"")`, and that would activate all configured executors ... or something similar. Regarding the implementation of the executors, overall looks legit!. About taking tasks via channels though, I'm still not sure (but still thinking!). Right now, the scheduling logic is guaranteed by each process keeping an idle go-routine per task, waiting for it to return from (blocking) shell commands before proceeding. Handing off a task to an executor might change this logic considerably. Perhaps this is needed to get true scalability (will need to think more), if so be it, but just raising a flag that it might complicate things a bit. Will easily be convinced by well-working code though :)",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383117959
Performance,scalab,scalability,"Many thanks for the detail input @kwrobert ! Being able to distribute work among multiple executors would indeed be cool!. I still think one needs the ability to quickly switch between running e.g. only locally, and distributing the work to everything available, but I'm sure both of these goals can be combined. In the simplest form, one could just allow `SetActiveExecutor(""*"")`, and that would activate all configured executors ... or something similar. Regarding the implementation of the executors, overall looks legit!. About taking tasks via channels though, I'm still not sure (but still thinking!). Right now, the scheduling logic is guaranteed by each process keeping an idle go-routine per task, waiting for it to return from (blocking) shell commands before proceeding. Handing off a task to an executor might change this logic considerably. Perhaps this is needed to get true scalability (will need to think more), if so be it, but just raising a flag that it might complicate things a bit. Will easily be convinced by well-working code though :)",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383117959
Testability,log,logic,"Many thanks for the detail input @kwrobert ! Being able to distribute work among multiple executors would indeed be cool!. I still think one needs the ability to quickly switch between running e.g. only locally, and distributing the work to everything available, but I'm sure both of these goals can be combined. In the simplest form, one could just allow `SetActiveExecutor(""*"")`, and that would activate all configured executors ... or something similar. Regarding the implementation of the executors, overall looks legit!. About taking tasks via channels though, I'm still not sure (but still thinking!). Right now, the scheduling logic is guaranteed by each process keeping an idle go-routine per task, waiting for it to return from (blocking) shell commands before proceeding. Handing off a task to an executor might change this logic considerably. Perhaps this is needed to get true scalability (will need to think more), if so be it, but just raising a flag that it might complicate things a bit. Will easily be convinced by well-working code though :)",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383117959
Usability,simpl,simplest,"Many thanks for the detail input @kwrobert ! Being able to distribute work among multiple executors would indeed be cool!. I still think one needs the ability to quickly switch between running e.g. only locally, and distributing the work to everything available, but I'm sure both of these goals can be combined. In the simplest form, one could just allow `SetActiveExecutor(""*"")`, and that would activate all configured executors ... or something similar. Regarding the implementation of the executors, overall looks legit!. About taking tasks via channels though, I'm still not sure (but still thinking!). Right now, the scheduling logic is guaranteed by each process keeping an idle go-routine per task, waiting for it to return from (blocking) shell commands before proceeding. Handing off a task to an executor might change this logic considerably. Perhaps this is needed to get true scalability (will need to think more), if so be it, but just raising a flag that it might complicate things a bit. Will easily be convinced by well-working code though :)",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/53#issuecomment-383117959
Availability,error,errors," your question!. I think this is a very relevant question, as python is a much more popular language in data science in general, and in bioinformatics in particular. There are a number of reasons why we switched to Go, after using the python-based [Luigi engine](https://github.com/spotify/luigi), with our helper library [SciLuigi](https://github.com/pharmbio/sciluigi) on top:. ## Performance. Firstly, Go lets us run each task concurrently in a ""light-weight thread"" (coroutine, or goroutine) instead of completely separate python processes. This allows us to keep thousands of tasks in their own goroutines, without performance problems, while in Luigi, we got communication problems between the processes when going over 64 workers. Go is also in general very performant, as being a compiled language, that has gotten a lot of optimization work by Google engineers. This is the primary driving reason for our switch. . Then there are a number of other factors that play in as well:. ## Catching errors early. With Python/Luigi, we often had many small bugs show up only when running a workflow. Sometimes a HPC job would fail on the 7th day of a 7-day run, just because of a simple dict key error. With Go, since it is compiled, we catch much more of these simple bugs already at compile time. ## An excellent dataflow / flow-based programming language. Also, we realized that the concurrency primitives in Go (channels, goroutines etc) makes it really easy to create scheduling code which is very very easy to reason about, by letting the workflow be a dataflow network, that acts as an ""implicity scheduler"". I recommend to read more about [Flow-based programming](https://github.com/samuell/awesome-fbp) to understand the benetifs of this. ## Robustness. Then, our experience with Go after the first experimentation was very positive: Go turned out to be a much more robust and less error-prone language to work with, in the long run. This is due to a number of reasons:. - It is a compiled l",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-391662679
Energy Efficiency,schedul,scheduling,"oroutine) instead of completely separate python processes. This allows us to keep thousands of tasks in their own goroutines, without performance problems, while in Luigi, we got communication problems between the processes when going over 64 workers. Go is also in general very performant, as being a compiled language, that has gotten a lot of optimization work by Google engineers. This is the primary driving reason for our switch. . Then there are a number of other factors that play in as well:. ## Catching errors early. With Python/Luigi, we often had many small bugs show up only when running a workflow. Sometimes a HPC job would fail on the 7th day of a 7-day run, just because of a simple dict key error. With Go, since it is compiled, we catch much more of these simple bugs already at compile time. ## An excellent dataflow / flow-based programming language. Also, we realized that the concurrency primitives in Go (channels, goroutines etc) makes it really easy to create scheduling code which is very very easy to reason about, by letting the workflow be a dataflow network, that acts as an ""implicity scheduler"". I recommend to read more about [Flow-based programming](https://github.com/samuell/awesome-fbp) to understand the benetifs of this. ## Robustness. Then, our experience with Go after the first experimentation was very positive: Go turned out to be a much more robust and less error-prone language to work with, in the long run. This is due to a number of reasons:. - It is a compiled language, catching errors early; - The language is small and simple, meaning that you can often easily understand your code when you return a few weeks later; - A lot of functionality is available in the standard library, lessening the need to rely on external dependencies, that might suddently change or break; - It comes with a lot of important tooling in the official package (built in testing, code formatting, checking, race detection, etc etc). ## More pointers on Go for data scie",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-391662679
Integrability,depend,dependencies,"y small bugs show up only when running a workflow. Sometimes a HPC job would fail on the 7th day of a 7-day run, just because of a simple dict key error. With Go, since it is compiled, we catch much more of these simple bugs already at compile time. ## An excellent dataflow / flow-based programming language. Also, we realized that the concurrency primitives in Go (channels, goroutines etc) makes it really easy to create scheduling code which is very very easy to reason about, by letting the workflow be a dataflow network, that acts as an ""implicity scheduler"". I recommend to read more about [Flow-based programming](https://github.com/samuell/awesome-fbp) to understand the benetifs of this. ## Robustness. Then, our experience with Go after the first experimentation was very positive: Go turned out to be a much more robust and less error-prone language to work with, in the long run. This is due to a number of reasons:. - It is a compiled language, catching errors early; - The language is small and simple, meaning that you can often easily understand your code when you return a few weeks later; - A lot of functionality is available in the standard library, lessening the need to rely on external dependencies, that might suddently change or break; - It comes with a lot of important tooling in the official package (built in testing, code formatting, checking, race detection, etc etc). ## More pointers on Go for data science. For more on other people's experience with Go, compared to e.g. python, for Data Science, I'd recommend @dwhitena's GopherCon talk:. - [GopherCon 2016: Daniel Whitenack - Go for Data Science](https://www.youtube.com/watch?v=D5tDubyXLrQ). It also turns out Go is becoming a popular language for workflow frameworks. We wrote up a post comparing four recent Go-based workflow systems here:. - [More Go-based Workflow Tools in Bioinformatics](http://gopherdata.io/post/more_go_based_workflow_tools_in_bioinformatics/). Ok, that's a few pointers. Hope it helps!",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-391662679
Performance,concurren,concurrently,"Hi @rizplate and thanks for your question!. I think this is a very relevant question, as python is a much more popular language in data science in general, and in bioinformatics in particular. There are a number of reasons why we switched to Go, after using the python-based [Luigi engine](https://github.com/spotify/luigi), with our helper library [SciLuigi](https://github.com/pharmbio/sciluigi) on top:. ## Performance. Firstly, Go lets us run each task concurrently in a ""light-weight thread"" (coroutine, or goroutine) instead of completely separate python processes. This allows us to keep thousands of tasks in their own goroutines, without performance problems, while in Luigi, we got communication problems between the processes when going over 64 workers. Go is also in general very performant, as being a compiled language, that has gotten a lot of optimization work by Google engineers. This is the primary driving reason for our switch. . Then there are a number of other factors that play in as well:. ## Catching errors early. With Python/Luigi, we often had many small bugs show up only when running a workflow. Sometimes a HPC job would fail on the 7th day of a 7-day run, just because of a simple dict key error. With Go, since it is compiled, we catch much more of these simple bugs already at compile time. ## An excellent dataflow / flow-based programming language. Also, we realized that the concurrency primitives in Go (channels, goroutines etc) makes it really easy to create scheduling code which is very very easy to reason about, by letting the workflow be a dataflow network, that acts as an ""implicity scheduler"". I recommend to read more about [Flow-based programming](https://github.com/samuell/awesome-fbp) to understand the benetifs of this. ## Robustness. Then, our experience with Go after the first experimentation was very positive: Go turned out to be a much more robust and less error-prone language to work with, in the long run. This is due to a number of reas",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-391662679
Safety,detect,detection,"y small bugs show up only when running a workflow. Sometimes a HPC job would fail on the 7th day of a 7-day run, just because of a simple dict key error. With Go, since it is compiled, we catch much more of these simple bugs already at compile time. ## An excellent dataflow / flow-based programming language. Also, we realized that the concurrency primitives in Go (channels, goroutines etc) makes it really easy to create scheduling code which is very very easy to reason about, by letting the workflow be a dataflow network, that acts as an ""implicity scheduler"". I recommend to read more about [Flow-based programming](https://github.com/samuell/awesome-fbp) to understand the benetifs of this. ## Robustness. Then, our experience with Go after the first experimentation was very positive: Go turned out to be a much more robust and less error-prone language to work with, in the long run. This is due to a number of reasons:. - It is a compiled language, catching errors early; - The language is small and simple, meaning that you can often easily understand your code when you return a few weeks later; - A lot of functionality is available in the standard library, lessening the need to rely on external dependencies, that might suddently change or break; - It comes with a lot of important tooling in the official package (built in testing, code formatting, checking, race detection, etc etc). ## More pointers on Go for data science. For more on other people's experience with Go, compared to e.g. python, for Data Science, I'd recommend @dwhitena's GopherCon talk:. - [GopherCon 2016: Daniel Whitenack - Go for Data Science](https://www.youtube.com/watch?v=D5tDubyXLrQ). It also turns out Go is becoming a popular language for workflow frameworks. We wrote up a post comparing four recent Go-based workflow systems here:. - [More Go-based Workflow Tools in Bioinformatics](http://gopherdata.io/post/more_go_based_workflow_tools_in_bioinformatics/). Ok, that's a few pointers. Hope it helps!",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-391662679
Testability,test,testing,"y small bugs show up only when running a workflow. Sometimes a HPC job would fail on the 7th day of a 7-day run, just because of a simple dict key error. With Go, since it is compiled, we catch much more of these simple bugs already at compile time. ## An excellent dataflow / flow-based programming language. Also, we realized that the concurrency primitives in Go (channels, goroutines etc) makes it really easy to create scheduling code which is very very easy to reason about, by letting the workflow be a dataflow network, that acts as an ""implicity scheduler"". I recommend to read more about [Flow-based programming](https://github.com/samuell/awesome-fbp) to understand the benetifs of this. ## Robustness. Then, our experience with Go after the first experimentation was very positive: Go turned out to be a much more robust and less error-prone language to work with, in the long run. This is due to a number of reasons:. - It is a compiled language, catching errors early; - The language is small and simple, meaning that you can often easily understand your code when you return a few weeks later; - A lot of functionality is available in the standard library, lessening the need to rely on external dependencies, that might suddently change or break; - It comes with a lot of important tooling in the official package (built in testing, code formatting, checking, race detection, etc etc). ## More pointers on Go for data science. For more on other people's experience with Go, compared to e.g. python, for Data Science, I'd recommend @dwhitena's GopherCon talk:. - [GopherCon 2016: Daniel Whitenack - Go for Data Science](https://www.youtube.com/watch?v=D5tDubyXLrQ). It also turns out Go is becoming a popular language for workflow frameworks. We wrote up a post comparing four recent Go-based workflow systems here:. - [More Go-based Workflow Tools in Bioinformatics](http://gopherdata.io/post/more_go_based_workflow_tools_in_bioinformatics/). Ok, that's a few pointers. Hope it helps!",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-391662679
Usability,simpl,simple," in particular. There are a number of reasons why we switched to Go, after using the python-based [Luigi engine](https://github.com/spotify/luigi), with our helper library [SciLuigi](https://github.com/pharmbio/sciluigi) on top:. ## Performance. Firstly, Go lets us run each task concurrently in a ""light-weight thread"" (coroutine, or goroutine) instead of completely separate python processes. This allows us to keep thousands of tasks in their own goroutines, without performance problems, while in Luigi, we got communication problems between the processes when going over 64 workers. Go is also in general very performant, as being a compiled language, that has gotten a lot of optimization work by Google engineers. This is the primary driving reason for our switch. . Then there are a number of other factors that play in as well:. ## Catching errors early. With Python/Luigi, we often had many small bugs show up only when running a workflow. Sometimes a HPC job would fail on the 7th day of a 7-day run, just because of a simple dict key error. With Go, since it is compiled, we catch much more of these simple bugs already at compile time. ## An excellent dataflow / flow-based programming language. Also, we realized that the concurrency primitives in Go (channels, goroutines etc) makes it really easy to create scheduling code which is very very easy to reason about, by letting the workflow be a dataflow network, that acts as an ""implicity scheduler"". I recommend to read more about [Flow-based programming](https://github.com/samuell/awesome-fbp) to understand the benetifs of this. ## Robustness. Then, our experience with Go after the first experimentation was very positive: Go turned out to be a much more robust and less error-prone language to work with, in the long run. This is due to a number of reasons:. - It is a compiled language, catching errors early; - The language is small and simple, meaning that you can often easily understand your code when you return a few weeks ",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-391662679
Energy Efficiency,schedul,scheduling,"#### Performance; Python also have light-weight threads, eventlets, gevets etc. Unless it is a non-blocking app, mostly for udp type processing i.e. voice or video, where data loss is acceptable, using ""light-weight thread"" (coroutine, or goroutine) which assume that each unit of work is very very short, is really not a good solution. Threads are good for workload that is blocking , for any amount of time, which is the case for all data processing applications. So I do not buy that for processing large amount of scientific data we need eventloop type of processing. Event type processing is only good for small payload udp type processing where processing is finished in few millisecond i.e. chat/voice/video-frames etc. Coroutines are not created for data flow scheduling, they were created for scheduling small processor workload on different **cores** of cpu, this is not the scheduling that luigi does. #### Bugs/Errors/Roubustness etc; Python has mature, decades old, battle tested frameworks and runtime. The reason for popularity of Python is that it is the easiest language, easier that javascript or anything else. The difficulty is not that it is easier to debug in python vs go. It is that distributed systems are hard. I am very familiar with luigi, ariflow, joblib and all the animals in hadoop zoo. The reason is that it is difficult to program and understand and do logging in distributed systems. I am not aware of any good distributed frameworks in go except some maybe some Amazon tools, everything else is written in Java/JVM due to maturity of ecosystem. . #### Libraries; Python has a huge ecosystem, python std library is very mature, more mature than Java. Most of the code in scientific and data world is written in python. It will be a huge rewriting effort or working with bad libraries (similar to pathetic state of libraries in javascript ecosystem). I will not be using it or recommending it, however do keep at it though , for your learning experience.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-392589326
Testability,test,tested,"#### Performance; Python also have light-weight threads, eventlets, gevets etc. Unless it is a non-blocking app, mostly for udp type processing i.e. voice or video, where data loss is acceptable, using ""light-weight thread"" (coroutine, or goroutine) which assume that each unit of work is very very short, is really not a good solution. Threads are good for workload that is blocking , for any amount of time, which is the case for all data processing applications. So I do not buy that for processing large amount of scientific data we need eventloop type of processing. Event type processing is only good for small payload udp type processing where processing is finished in few millisecond i.e. chat/voice/video-frames etc. Coroutines are not created for data flow scheduling, they were created for scheduling small processor workload on different **cores** of cpu, this is not the scheduling that luigi does. #### Bugs/Errors/Roubustness etc; Python has mature, decades old, battle tested frameworks and runtime. The reason for popularity of Python is that it is the easiest language, easier that javascript or anything else. The difficulty is not that it is easier to debug in python vs go. It is that distributed systems are hard. I am very familiar with luigi, ariflow, joblib and all the animals in hadoop zoo. The reason is that it is difficult to program and understand and do logging in distributed systems. I am not aware of any good distributed frameworks in go except some maybe some Amazon tools, everything else is written in Java/JVM due to maturity of ecosystem. . #### Libraries; Python has a huge ecosystem, python std library is very mature, more mature than Java. Most of the code in scientific and data world is written in python. It will be a huge rewriting effort or working with bad libraries (similar to pathetic state of libraries in javascript ecosystem). I will not be using it or recommending it, however do keep at it though , for your learning experience.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-392589326
Usability,learn,learning,"#### Performance; Python also have light-weight threads, eventlets, gevets etc. Unless it is a non-blocking app, mostly for udp type processing i.e. voice or video, where data loss is acceptable, using ""light-weight thread"" (coroutine, or goroutine) which assume that each unit of work is very very short, is really not a good solution. Threads are good for workload that is blocking , for any amount of time, which is the case for all data processing applications. So I do not buy that for processing large amount of scientific data we need eventloop type of processing. Event type processing is only good for small payload udp type processing where processing is finished in few millisecond i.e. chat/voice/video-frames etc. Coroutines are not created for data flow scheduling, they were created for scheduling small processor workload on different **cores** of cpu, this is not the scheduling that luigi does. #### Bugs/Errors/Roubustness etc; Python has mature, decades old, battle tested frameworks and runtime. The reason for popularity of Python is that it is the easiest language, easier that javascript or anything else. The difficulty is not that it is easier to debug in python vs go. It is that distributed systems are hard. I am very familiar with luigi, ariflow, joblib and all the animals in hadoop zoo. The reason is that it is difficult to program and understand and do logging in distributed systems. I am not aware of any good distributed frameworks in go except some maybe some Amazon tools, everything else is written in Java/JVM due to maturity of ecosystem. . #### Libraries; Python has a huge ecosystem, python std library is very mature, more mature than Java. Most of the code in scientific and data world is written in python. It will be a huge rewriting effort or working with bad libraries (similar to pathetic state of libraries in javascript ecosystem). I will not be using it or recommending it, however do keep at it though , for your learning experience.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-392589326
Energy Efficiency,schedul,scheduling,"Thanks for your honest feedback @rizplate. A few comments:. > Python also have light-weight threads, eventlets, gevets etc. Unless it is a non-blocking app, mostly for udp type processing i.e. voice or video, where data loss is acceptable, using ""light-weight thread"" (coroutine, or goroutine) which assume that each unit of work is very very short, is really not a good solution. Threads are good for workload that is blocking , for any amount of time, which is the case for all data processing applications. So I do not buy that for processing large amount of scientific data we need eventloop type of processing. Event type processing is only good for small payload udp type processing where processing is finished in few millisecond i.e. chat/voice/video-frames etc. Coroutines are not created for data flow scheduling, they were created for scheduling small processor workload on different cores of cpu, this is not the scheduling that luigi does. Python coroutines and Go's goroutines are very different under the hood. Python coroutines all happen on the same core, while Go automatically multiplexes goroutines onto physical cores. For blocking system calls, it automatically creates extra threads, so as not to block the main program. This means that blocking operations are not suited for python's coroutines, but work very well with Go's goroutines. > I am not aware of any good distributed frameworks in go except some maybe some Amazon tools, everything else is written in Java/JVM due to maturity of ecosystem. For distributed frameworks, I suggest checking out: [Nats](https://nats.io/), [Vice](https://github.com/matryer/vice) and possibly [Glow](https://github.com/chrislusf/glow) and [Gleam](https://github.com/chrislusf/gleam). (For workflows, I already referred to a post mentioning [Pachyderm](http://pachyderm.io), [Reflow](https://github.com/grailbio/reflow) and [AWE](https://github.com/MG-RAST/AWE)). . > It will be a huge rewriting effort or working with bad libraries (simil",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-392601359
Performance,perform,performant,"appen on the same core, while Go automatically multiplexes goroutines onto physical cores. For blocking system calls, it automatically creates extra threads, so as not to block the main program. This means that blocking operations are not suited for python's coroutines, but work very well with Go's goroutines. > I am not aware of any good distributed frameworks in go except some maybe some Amazon tools, everything else is written in Java/JVM due to maturity of ecosystem. For distributed frameworks, I suggest checking out: [Nats](https://nats.io/), [Vice](https://github.com/matryer/vice) and possibly [Glow](https://github.com/chrislusf/glow) and [Gleam](https://github.com/chrislusf/gleam). (For workflows, I already referred to a post mentioning [Pachyderm](http://pachyderm.io), [Reflow](https://github.com/grailbio/reflow) and [AWE](https://github.com/MG-RAST/AWE)). . > It will be a huge rewriting effort or working with bad libraries (similar to pathetic state of libraries in javascript ecosystem). Go has a very extensive set of functionality in its standard library. As a small example, I was [blown away by the excellent XML parsing support](http://bionics.it/posts/parsing-drugbank-xml-or-any-large-xml-file-in-streaming-mode-in-go), which was more powerful and performant (while simple), than anything I've worked with in Python. It also has a fast growing [ecosystem of data science libraries](https://github.com/gopherdata/resources/tree/master/tooling). > I will not be using it or recommending it, however do keep at it though , for your learning experience. I agree that probably a lot of people will stick to Python for a long time to come, because of its easier syntax. I primarily develop this tool to solve my/our own problems though, and went with Go because of a frustration with Python (as much as I love the language as such). SciPipe is as such, my own dream-tool for building workflows. If anybody else finds it useful, that'll be bonus points :slightly_smiling_face:",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-392601359
Usability,feedback,feedback,"Thanks for your honest feedback @rizplate. A few comments:. > Python also have light-weight threads, eventlets, gevets etc. Unless it is a non-blocking app, mostly for udp type processing i.e. voice or video, where data loss is acceptable, using ""light-weight thread"" (coroutine, or goroutine) which assume that each unit of work is very very short, is really not a good solution. Threads are good for workload that is blocking , for any amount of time, which is the case for all data processing applications. So I do not buy that for processing large amount of scientific data we need eventloop type of processing. Event type processing is only good for small payload udp type processing where processing is finished in few millisecond i.e. chat/voice/video-frames etc. Coroutines are not created for data flow scheduling, they were created for scheduling small processor workload on different cores of cpu, this is not the scheduling that luigi does. Python coroutines and Go's goroutines are very different under the hood. Python coroutines all happen on the same core, while Go automatically multiplexes goroutines onto physical cores. For blocking system calls, it automatically creates extra threads, so as not to block the main program. This means that blocking operations are not suited for python's coroutines, but work very well with Go's goroutines. > I am not aware of any good distributed frameworks in go except some maybe some Amazon tools, everything else is written in Java/JVM due to maturity of ecosystem. For distributed frameworks, I suggest checking out: [Nats](https://nats.io/), [Vice](https://github.com/matryer/vice) and possibly [Glow](https://github.com/chrislusf/glow) and [Gleam](https://github.com/chrislusf/gleam). (For workflows, I already referred to a post mentioning [Pachyderm](http://pachyderm.io), [Reflow](https://github.com/grailbio/reflow) and [AWE](https://github.com/MG-RAST/AWE)). . > It will be a huge rewriting effort or working with bad libraries (simil",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-392601359
Availability,error,errors,"Hi,; I am getting below errors when running the file; download.SetPathStatic undefined (type *scipipe.Process has no field or method SetPathStatic); ./drug_feature_extract.go:24:7: unzip.SetPathStatic undefined (type *scipipe.Process has no field or method SetPathStatic); ./drug_feature_extract.go:28:10: xmlToTSV.SetPathExtend undefined (type *scipipe.Process has no field or method SetPathExtend). Any guidance will be really helpful. Thanks,; Rajesh",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-468100726
Usability,guid,guidance,"Hi,; I am getting below errors when running the file; download.SetPathStatic undefined (type *scipipe.Process has no field or method SetPathStatic); ./drug_feature_extract.go:24:7: unzip.SetPathStatic undefined (type *scipipe.Process has no field or method SetPathStatic); ./drug_feature_extract.go:28:10: xmlToTSV.SetPathExtend undefined (type *scipipe.Process has no field or method SetPathExtend). Any guidance will be really helpful. Thanks,; Rajesh",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/59#issuecomment-468100726
Availability,down,downsides,"Well, also, I'm not yet sure what are the pros and cons of process-level logging. In a way I think it sounds interesting ... but it should also be possible to filter out log rows for a specific process rather easily using Unix tools such as grep. But, if there are no clear downsides, I can see that process-level logging would be quite useful.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/73#issuecomment-478933344
Testability,log,logging,"Well, also, I'm not yet sure what are the pros and cons of process-level logging. In a way I think it sounds interesting ... but it should also be possible to filter out log rows for a specific process rather easily using Unix tools such as grep. But, if there are no clear downsides, I can see that process-level logging would be quite useful.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/73#issuecomment-478933344
Usability,clear,clear,"Well, also, I'm not yet sure what are the pros and cons of process-level logging. In a way I think it sounds interesting ... but it should also be possible to filter out log rows for a specific process rather easily using Unix tools such as grep. But, if there are no clear downsides, I can see that process-level logging would be quite useful.",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/73#issuecomment-478933344
Deployability,update,update,"7%`.; > The diff coverage is `84.61%`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/82/graphs/tree.svg?width=650&token=t9PLztMgha&height=150&src=pr)](https://codecov.io/gh/scipipe/scipipe/pull/82?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #82 +/- ##; ===========================================; - Coverage 61.69% 59.98% -1.71% ; ===========================================; Files 24 23 -1 ; Lines 1817 1757 -60 ; ===========================================; - Hits 1121 1054 -67 ; - Misses 627 636 +9 ; + Partials 69 67 -2; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/82?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [process.go](https://codecov.io/gh/scipipe/scipipe/pull/82/diff?src=pr&el=tree#diff-cHJvY2Vzcy5nbw==) | `86.24% <84.61%> (+0.14%)` | :arrow_up: |; | [components/param\_source.go](https://codecov.io/gh/scipipe/scipipe/pull/82/diff?src=pr&el=tree#diff-Y29tcG9uZW50cy9wYXJhbV9zb3VyY2UuZ28=) | `0% <0%> (-100%)` | :arrow_down: |; | [utils.go](https://codecov.io/gh/scipipe/scipipe/pull/82/diff?src=pr&el=tree#diff-dXRpbHMuZ28=) | `61.76% <0%> (-6.53%)` | :arrow_down: |; | [components/param\_combinator.go](https://codecov.io/gh/scipipe/scipipe/pull/82/diff?src=pr&el=tree#diff-Y29tcG9uZW50cy9wYXJhbV9jb21iaW5hdG9yLmdv) | | |; | [task.go](https://codecov.io/gh/scipipe/scipipe/pull/82/diff?src=pr&el=tree#diff-dGFzay5nbw==) | `74.88% <0%> (+1.13%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/82?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/82?src=pr&el=footer). Last update [34de367...8be2feb](https://codecov.io/gh/scipipe/scipipe/pull/82?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/82#issuecomment-502111444
Usability,learn,learn,"7%`.; > The diff coverage is `84.61%`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/82/graphs/tree.svg?width=650&token=t9PLztMgha&height=150&src=pr)](https://codecov.io/gh/scipipe/scipipe/pull/82?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #82 +/- ##; ===========================================; - Coverage 61.69% 59.98% -1.71% ; ===========================================; Files 24 23 -1 ; Lines 1817 1757 -60 ; ===========================================; - Hits 1121 1054 -67 ; - Misses 627 636 +9 ; + Partials 69 67 -2; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/82?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [process.go](https://codecov.io/gh/scipipe/scipipe/pull/82/diff?src=pr&el=tree#diff-cHJvY2Vzcy5nbw==) | `86.24% <84.61%> (+0.14%)` | :arrow_up: |; | [components/param\_source.go](https://codecov.io/gh/scipipe/scipipe/pull/82/diff?src=pr&el=tree#diff-Y29tcG9uZW50cy9wYXJhbV9zb3VyY2UuZ28=) | `0% <0%> (-100%)` | :arrow_down: |; | [utils.go](https://codecov.io/gh/scipipe/scipipe/pull/82/diff?src=pr&el=tree#diff-dXRpbHMuZ28=) | `61.76% <0%> (-6.53%)` | :arrow_down: |; | [components/param\_combinator.go](https://codecov.io/gh/scipipe/scipipe/pull/82/diff?src=pr&el=tree#diff-Y29tcG9uZW50cy9wYXJhbV9jb21iaW5hdG9yLmdv) | | |; | [task.go](https://codecov.io/gh/scipipe/scipipe/pull/82/diff?src=pr&el=tree#diff-dGFzay5nbw==) | `74.88% <0%> (+1.13%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/82?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/82?src=pr&el=footer). Last update [34de367...8be2feb](https://codecov.io/gh/scipipe/scipipe/pull/82?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/82#issuecomment-502111444
Deployability,update,update,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=h1) Report; > Merging [#83](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=desc) into [develop](https://codecov.io/gh/scipipe/scipipe/commit/359427b219297ae43b28a0b0412351af643028ea?src=pr&el=desc) will **increase** coverage by `0.18%`.; > The diff coverage is `92.85%`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/83/graphs/tree.svg?width=650&token=t9PLztMgha&height=150&src=pr)](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #83 +/- ##; ===========================================; + Coverage 61.73% 61.92% +0.18% ; ===========================================; Files 24 24 ; Lines 1819 1828 +9 ; ===========================================; + Hits 1123 1132 +9 ; Misses 627 627 ; Partials 69 69; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [process.go](https://codecov.io/gh/scipipe/scipipe/pull/83/diff?src=pr&el=tree#diff-cHJvY2Vzcy5nbw==) | `86.86% <92.85%> (+0.62%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=footer). Last update [359427b...1207663](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/83#issuecomment-502175448
Usability,learn,learn,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=h1) Report; > Merging [#83](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=desc) into [develop](https://codecov.io/gh/scipipe/scipipe/commit/359427b219297ae43b28a0b0412351af643028ea?src=pr&el=desc) will **increase** coverage by `0.18%`.; > The diff coverage is `92.85%`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/83/graphs/tree.svg?width=650&token=t9PLztMgha&height=150&src=pr)](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #83 +/- ##; ===========================================; + Coverage 61.73% 61.92% +0.18% ; ===========================================; Files 24 24 ; Lines 1819 1828 +9 ; ===========================================; + Hits 1123 1132 +9 ; Misses 627 627 ; Partials 69 69; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [process.go](https://codecov.io/gh/scipipe/scipipe/pull/83/diff?src=pr&el=tree#diff-cHJvY2Vzcy5nbw==) | `86.86% <92.85%> (+0.62%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=footer). Last update [359427b...1207663](https://codecov.io/gh/scipipe/scipipe/pull/83?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/83#issuecomment-502175448
Deployability,update,update,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/84?src=pr&el=h1) Report; > Merging [#84](https://codecov.io/gh/scipipe/scipipe/pull/84?src=pr&el=desc) into [develop](https://codecov.io/gh/scipipe/scipipe/commit/59b22b2a29c6b2648ffec157a191fa2c10cc7fc7?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/84/graphs/tree.svg?width=650&token=t9PLztMgha&height=150&src=pr)](https://codecov.io/gh/scipipe/scipipe/pull/84?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #84 +/- ##; ========================================; Coverage 61.73% 61.73% ; ========================================; Files 24 24 ; Lines 1819 1819 ; ========================================; Hits 1123 1123 ; Misses 627 627 ; Partials 69 69; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/84?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/84?src=pr&el=footer). Last update [59b22b2...ca6dc34](https://codecov.io/gh/scipipe/scipipe/pull/84?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/84#issuecomment-502406307
Usability,learn,learn,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/84?src=pr&el=h1) Report; > Merging [#84](https://codecov.io/gh/scipipe/scipipe/pull/84?src=pr&el=desc) into [develop](https://codecov.io/gh/scipipe/scipipe/commit/59b22b2a29c6b2648ffec157a191fa2c10cc7fc7?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/84/graphs/tree.svg?width=650&token=t9PLztMgha&height=150&src=pr)](https://codecov.io/gh/scipipe/scipipe/pull/84?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #84 +/- ##; ========================================; Coverage 61.73% 61.73% ; ========================================; Files 24 24 ; Lines 1819 1819 ; ========================================; Hits 1123 1123 ; Misses 627 627 ; Partials 69 69; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/84?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/84?src=pr&el=footer). Last update [59b22b2...ca6dc34](https://codecov.io/gh/scipipe/scipipe/pull/84?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/84#issuecomment-502406307
Deployability,update,update,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=h1) Report; > Merging [#85](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=desc) into [develop](https://codecov.io/gh/scipipe/scipipe/commit/1e9114281545b90aa9d9abb1fc067fef5ab3e190?src=pr&el=desc) will **increase** coverage by `0.92%`.; > The diff coverage is `100%`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/85/graphs/tree.svg?width=650&token=t9PLztMgha&height=150&src=pr)](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #85 +/- ##; ===========================================; + Coverage 61.92% 62.85% +0.92% ; ===========================================; Files 24 24 ; Lines 1828 1828 ; ===========================================; + Hits 1132 1149 +17 ; + Misses 627 610 -17 ; Partials 69 69; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [process.go](https://codecov.io/gh/scipipe/scipipe/pull/85/diff?src=pr&el=tree#diff-cHJvY2Vzcy5nbw==) | `86.86% <100%> (ø)` | :arrow_up: |; | [components/maptotags.go](https://codecov.io/gh/scipipe/scipipe/pull/85/diff?src=pr&el=tree#diff-Y29tcG9uZW50cy9tYXB0b3RhZ3MuZ28=) | `100% <0%> (+100%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=footer). Last update [1e91142...ad33b41](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/85#issuecomment-503240584
Usability,learn,learn,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=h1) Report; > Merging [#85](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=desc) into [develop](https://codecov.io/gh/scipipe/scipipe/commit/1e9114281545b90aa9d9abb1fc067fef5ab3e190?src=pr&el=desc) will **increase** coverage by `0.92%`.; > The diff coverage is `100%`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/85/graphs/tree.svg?width=650&token=t9PLztMgha&height=150&src=pr)](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #85 +/- ##; ===========================================; + Coverage 61.92% 62.85% +0.92% ; ===========================================; Files 24 24 ; Lines 1828 1828 ; ===========================================; + Hits 1132 1149 +17 ; + Misses 627 610 -17 ; Partials 69 69; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [process.go](https://codecov.io/gh/scipipe/scipipe/pull/85/diff?src=pr&el=tree#diff-cHJvY2Vzcy5nbw==) | `86.86% <100%> (ø)` | :arrow_up: |; | [components/maptotags.go](https://codecov.io/gh/scipipe/scipipe/pull/85/diff?src=pr&el=tree#diff-Y29tcG9uZW50cy9tYXB0b3RhZ3MuZ28=) | `100% <0%> (+100%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=footer). Last update [1e91142...ad33b41](https://codecov.io/gh/scipipe/scipipe/pull/85?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/85#issuecomment-503240584
Deployability,update,update,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/97?src=pr&el=h1) Report; > Merging [#97](https://codecov.io/gh/scipipe/scipipe/pull/97?src=pr&el=desc) into [master](https://codecov.io/gh/scipipe/scipipe/commit/ffe2752e57b3e688f382005e7438bc35f5fa6961&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/97/graphs/tree.svg?width=650&height=150&src=pr&token=t9PLztMgha)](https://codecov.io/gh/scipipe/scipipe/pull/97?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #97 +/- ##; =======================================; Coverage 67.30% 67.30% ; =======================================; Files 24 24 ; Lines 1841 1841 ; =======================================; Hits 1239 1239 ; Misses 598 598 ; Partials 4 4 ; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/97?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/97?src=pr&el=footer). Last update [ffe2752...12b47b3](https://codecov.io/gh/scipipe/scipipe/pull/97?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/97#issuecomment-627231429
Usability,learn,learn,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/97?src=pr&el=h1) Report; > Merging [#97](https://codecov.io/gh/scipipe/scipipe/pull/97?src=pr&el=desc) into [master](https://codecov.io/gh/scipipe/scipipe/commit/ffe2752e57b3e688f382005e7438bc35f5fa6961&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/97/graphs/tree.svg?width=650&height=150&src=pr&token=t9PLztMgha)](https://codecov.io/gh/scipipe/scipipe/pull/97?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #97 +/- ##; =======================================; Coverage 67.30% 67.30% ; =======================================; Files 24 24 ; Lines 1841 1841 ; =======================================; Hits 1239 1239 ; Misses 598 598 ; Partials 4 4 ; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/97?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/97?src=pr&el=footer). Last update [ffe2752...12b47b3](https://codecov.io/gh/scipipe/scipipe/pull/97?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/97#issuecomment-627231429
Deployability,update,update,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/98?src=pr&el=h1) Report; > Merging [#98](https://codecov.io/gh/scipipe/scipipe/pull/98?src=pr&el=desc) into [master](https://codecov.io/gh/scipipe/scipipe/commit/632fdb12a67927c9be41629c737573ecda35d9f7&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/98/graphs/tree.svg?width=650&height=150&src=pr&token=t9PLztMgha)](https://codecov.io/gh/scipipe/scipipe/pull/98?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #98 +/- ##; =======================================; Coverage 67.30% 67.30% ; =======================================; Files 24 24 ; Lines 1841 1841 ; =======================================; Hits 1239 1239 ; Misses 598 598 ; Partials 4 4 ; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/98?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/98?src=pr&el=footer). Last update [632fdb1...e0bca6f](https://codecov.io/gh/scipipe/scipipe/pull/98?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/98#issuecomment-633133481
Usability,learn,learn,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/98?src=pr&el=h1) Report; > Merging [#98](https://codecov.io/gh/scipipe/scipipe/pull/98?src=pr&el=desc) into [master](https://codecov.io/gh/scipipe/scipipe/commit/632fdb12a67927c9be41629c737573ecda35d9f7&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/98/graphs/tree.svg?width=650&height=150&src=pr&token=t9PLztMgha)](https://codecov.io/gh/scipipe/scipipe/pull/98?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #98 +/- ##; =======================================; Coverage 67.30% 67.30% ; =======================================; Files 24 24 ; Lines 1841 1841 ; =======================================; Hits 1239 1239 ; Misses 598 598 ; Partials 4 4 ; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/98?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/98?src=pr&el=footer). Last update [632fdb1...e0bca6f](https://codecov.io/gh/scipipe/scipipe/pull/98?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/98#issuecomment-633133481
Usability,simpl,simply,There was a bit of work on similar things at some point but nothing finished as far as I know. I think in theory there is nothing stopping it except that there is implementation to do. But Samuel know better. ; I am simply running in one giant pod on our Kubernetes cluster but that does not scale beyond what I am already doing.,MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/110#issuecomment-705373067
Deployability,update,update,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=h1) Report; > Merging [#112](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=desc) into [master](https://codecov.io/gh/scipipe/scipipe/commit/141aa8db40c76b34735ce1a44304907acd244bca?el=desc) will **increase** coverage by `0.08%`.; > The diff coverage is `100.00%`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/112/graphs/tree.svg?width=650&height=150&src=pr&token=t9PLztMgha)](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #112 +/- ##; ==========================================; + Coverage 67.84% 67.92% +0.08% ; ==========================================; Files 24 24 ; Lines 1502 1506 +4 ; ==========================================; + Hits 1019 1023 +4 ; Misses 479 479 ; Partials 4 4 ; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [common.go](https://codecov.io/gh/scipipe/scipipe/pull/112/diff?src=pr&el=tree#diff-Y29tbW9uLmdv) | `82.00% <100.00%> (+1.56%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=footer). Last update [141aa8d...b1f8792](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/112#issuecomment-707156361
Usability,learn,learn,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=h1) Report; > Merging [#112](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=desc) into [master](https://codecov.io/gh/scipipe/scipipe/commit/141aa8db40c76b34735ce1a44304907acd244bca?el=desc) will **increase** coverage by `0.08%`.; > The diff coverage is `100.00%`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/112/graphs/tree.svg?width=650&height=150&src=pr&token=t9PLztMgha)](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #112 +/- ##; ==========================================; + Coverage 67.84% 67.92% +0.08% ; ==========================================; Files 24 24 ; Lines 1502 1506 +4 ; ==========================================; + Hits 1019 1023 +4 ; Misses 479 479 ; Partials 4 4 ; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [common.go](https://codecov.io/gh/scipipe/scipipe/pull/112/diff?src=pr&el=tree#diff-Y29tbW9uLmdv) | `82.00% <100.00%> (+1.56%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=footer). Last update [141aa8d...b1f8792](https://codecov.io/gh/scipipe/scipipe/pull/112?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/112#issuecomment-707156361
Availability,down,downloading,"Hi @nyue and thanks for trying out scipipe!. What is your current workflow for testing changes? Did you install scipipe via the `go get` command mainly?. I realize that the project is missing a contributors guide(!). The workflow I have been using the most, is to go where scipipe is installed via `go get`, and replace the scipipe folder there with a git cloned version. This might only work if you have set the GOROOT and GOPATH environment variables manually, and I think the Go project has been moving away from that, so this might not be the best way for the future. I have to check into that. But to specify how I have been doing it:. - I have been installing Go by downloading the tarball into my home folder, and unpacking it into `~/go`.; - I have been putting my own Go projects in `~/proj/go`; - I have then been setting GOROOT and GOPATH as follows:; ```; GOROOT=~/go; GOPATH=~/proj/go; ```; - Then, when I do `go get github.com/scipipe/scipipe/...`, it ends up in `~/proj/go/src/github.com/scipipe/scipipe`; - I then have replaced this folder with the one I get by cloning the git repo into that very folder.; - After each change in the scipipe source, I think I typically need to run ; ```; go install github.com/scipipe/scipipe/...; ```; ... to have those changes build and installed into the build libraries in `~/go/proj/pkg` (if I recall correctly).; - Then, those changes should be available to my local workflow repos, wherever I store them. So, the proper workflow for the future might or might not be some version of the above. Hope this helps, until we have a proper contributors guide!",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/113#issuecomment-772679872
Deployability,install,install,"Hi @nyue and thanks for trying out scipipe!. What is your current workflow for testing changes? Did you install scipipe via the `go get` command mainly?. I realize that the project is missing a contributors guide(!). The workflow I have been using the most, is to go where scipipe is installed via `go get`, and replace the scipipe folder there with a git cloned version. This might only work if you have set the GOROOT and GOPATH environment variables manually, and I think the Go project has been moving away from that, so this might not be the best way for the future. I have to check into that. But to specify how I have been doing it:. - I have been installing Go by downloading the tarball into my home folder, and unpacking it into `~/go`.; - I have been putting my own Go projects in `~/proj/go`; - I have then been setting GOROOT and GOPATH as follows:; ```; GOROOT=~/go; GOPATH=~/proj/go; ```; - Then, when I do `go get github.com/scipipe/scipipe/...`, it ends up in `~/proj/go/src/github.com/scipipe/scipipe`; - I then have replaced this folder with the one I get by cloning the git repo into that very folder.; - After each change in the scipipe source, I think I typically need to run ; ```; go install github.com/scipipe/scipipe/...; ```; ... to have those changes build and installed into the build libraries in `~/go/proj/pkg` (if I recall correctly).; - Then, those changes should be available to my local workflow repos, wherever I store them. So, the proper workflow for the future might or might not be some version of the above. Hope this helps, until we have a proper contributors guide!",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/113#issuecomment-772679872
Modifiability,variab,variables,"Hi @nyue and thanks for trying out scipipe!. What is your current workflow for testing changes? Did you install scipipe via the `go get` command mainly?. I realize that the project is missing a contributors guide(!). The workflow I have been using the most, is to go where scipipe is installed via `go get`, and replace the scipipe folder there with a git cloned version. This might only work if you have set the GOROOT and GOPATH environment variables manually, and I think the Go project has been moving away from that, so this might not be the best way for the future. I have to check into that. But to specify how I have been doing it:. - I have been installing Go by downloading the tarball into my home folder, and unpacking it into `~/go`.; - I have been putting my own Go projects in `~/proj/go`; - I have then been setting GOROOT and GOPATH as follows:; ```; GOROOT=~/go; GOPATH=~/proj/go; ```; - Then, when I do `go get github.com/scipipe/scipipe/...`, it ends up in `~/proj/go/src/github.com/scipipe/scipipe`; - I then have replaced this folder with the one I get by cloning the git repo into that very folder.; - After each change in the scipipe source, I think I typically need to run ; ```; go install github.com/scipipe/scipipe/...; ```; ... to have those changes build and installed into the build libraries in `~/go/proj/pkg` (if I recall correctly).; - Then, those changes should be available to my local workflow repos, wherever I store them. So, the proper workflow for the future might or might not be some version of the above. Hope this helps, until we have a proper contributors guide!",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/113#issuecomment-772679872
Testability,test,testing,"Hi @nyue and thanks for trying out scipipe!. What is your current workflow for testing changes? Did you install scipipe via the `go get` command mainly?. I realize that the project is missing a contributors guide(!). The workflow I have been using the most, is to go where scipipe is installed via `go get`, and replace the scipipe folder there with a git cloned version. This might only work if you have set the GOROOT and GOPATH environment variables manually, and I think the Go project has been moving away from that, so this might not be the best way for the future. I have to check into that. But to specify how I have been doing it:. - I have been installing Go by downloading the tarball into my home folder, and unpacking it into `~/go`.; - I have been putting my own Go projects in `~/proj/go`; - I have then been setting GOROOT and GOPATH as follows:; ```; GOROOT=~/go; GOPATH=~/proj/go; ```; - Then, when I do `go get github.com/scipipe/scipipe/...`, it ends up in `~/proj/go/src/github.com/scipipe/scipipe`; - I then have replaced this folder with the one I get by cloning the git repo into that very folder.; - After each change in the scipipe source, I think I typically need to run ; ```; go install github.com/scipipe/scipipe/...; ```; ... to have those changes build and installed into the build libraries in `~/go/proj/pkg` (if I recall correctly).; - Then, those changes should be available to my local workflow repos, wherever I store them. So, the proper workflow for the future might or might not be some version of the above. Hope this helps, until we have a proper contributors guide!",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/113#issuecomment-772679872
Usability,guid,guide,"Hi @nyue and thanks for trying out scipipe!. What is your current workflow for testing changes? Did you install scipipe via the `go get` command mainly?. I realize that the project is missing a contributors guide(!). The workflow I have been using the most, is to go where scipipe is installed via `go get`, and replace the scipipe folder there with a git cloned version. This might only work if you have set the GOROOT and GOPATH environment variables manually, and I think the Go project has been moving away from that, so this might not be the best way for the future. I have to check into that. But to specify how I have been doing it:. - I have been installing Go by downloading the tarball into my home folder, and unpacking it into `~/go`.; - I have been putting my own Go projects in `~/proj/go`; - I have then been setting GOROOT and GOPATH as follows:; ```; GOROOT=~/go; GOPATH=~/proj/go; ```; - Then, when I do `go get github.com/scipipe/scipipe/...`, it ends up in `~/proj/go/src/github.com/scipipe/scipipe`; - I then have replaced this folder with the one I get by cloning the git repo into that very folder.; - After each change in the scipipe source, I think I typically need to run ; ```; go install github.com/scipipe/scipipe/...; ```; ... to have those changes build and installed into the build libraries in `~/go/proj/pkg` (if I recall correctly).; - Then, those changes should be available to my local workflow repos, wherever I store them. So, the proper workflow for the future might or might not be some version of the above. Hope this helps, until we have a proper contributors guide!",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/113#issuecomment-772679872
Usability,guid,guide,"Regarding the contributors' guide, are you planning a separate document or are you thinking of an additional section in the current README.md file ?",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/113#issuecomment-772726201
Deployability,update,update,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/114?src=pr&el=h1) Report; > Merging [#114](https://codecov.io/gh/scipipe/scipipe/pull/114?src=pr&el=desc) (f4f16e4) into [master](https://codecov.io/gh/scipipe/scipipe/commit/4128b9ee070481bc09061abf9a3ebcb65c3917b0?el=desc) (4128b9e) will **not change** coverage.; > The diff coverage is `n/a`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/114/graphs/tree.svg?width=650&height=150&src=pr&token=t9PLztMgha)](https://codecov.io/gh/scipipe/scipipe/pull/114?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #114 +/- ##; =======================================; Coverage 67.92% 67.92% ; =======================================; Files 24 24 ; Lines 1506 1506 ; =======================================; Hits 1023 1023 ; Misses 479 479 ; Partials 4 4 ; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/114?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/114?src=pr&el=footer). Last update [4128b9e...f4f16e4](https://codecov.io/gh/scipipe/scipipe/pull/114?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/114#issuecomment-772958089
Usability,learn,learn,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/114?src=pr&el=h1) Report; > Merging [#114](https://codecov.io/gh/scipipe/scipipe/pull/114?src=pr&el=desc) (f4f16e4) into [master](https://codecov.io/gh/scipipe/scipipe/commit/4128b9ee070481bc09061abf9a3ebcb65c3917b0?el=desc) (4128b9e) will **not change** coverage.; > The diff coverage is `n/a`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/114/graphs/tree.svg?width=650&height=150&src=pr&token=t9PLztMgha)](https://codecov.io/gh/scipipe/scipipe/pull/114?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #114 +/- ##; =======================================; Coverage 67.92% 67.92% ; =======================================; Files 24 24 ; Lines 1506 1506 ; =======================================; Hits 1023 1023 ; Misses 479 479 ; Partials 4 4 ; ```. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/114?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/114?src=pr&el=footer). Last update [4128b9e...f4f16e4](https://codecov.io/gh/scipipe/scipipe/pull/114?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/114#issuecomment-772958089
Deployability,update,update,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=h1) Report; > Merging [#116](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=desc) (39a3f97) into [master](https://codecov.io/gh/scipipe/scipipe/commit/a85326dd3931a092710e128710cca50f7e4d843b?el=desc) (a85326d) will **increase** coverage by `0.06%`.; > The diff coverage is `87.50%`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/116/graphs/tree.svg?width=650&height=150&src=pr&token=t9PLztMgha)](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #116 +/- ##; ==========================================; + Coverage 67.92% 67.98% +0.06% ; ==========================================; Files 24 25 +1 ; Lines 1506 1512 +6 ; ==========================================; + Hits 1023 1028 +5 ; - Misses 479 480 +1 ; Partials 4 4 ; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [process.go](https://codecov.io/gh/scipipe/scipipe/pull/116/diff?src=pr&el=tree#diff-cHJvY2Vzcy5nbw==) | `89.03% <ø> (ø)` | |; | [settings.go](https://codecov.io/gh/scipipe/scipipe/pull/116/diff?src=pr&el=tree#diff-c2V0dGluZ3MuZ28=) | `83.33% <83.33%> (ø)` | |; | [port.go](https://codecov.io/gh/scipipe/scipipe/pull/116/diff?src=pr&el=tree#diff-cG9ydC5nbw==) | `79.03% <100.00%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=footer). Last update [943de2b...39a3f97](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/116#issuecomment-793116476
Usability,learn,learn,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=h1) Report; > Merging [#116](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=desc) (39a3f97) into [master](https://codecov.io/gh/scipipe/scipipe/commit/a85326dd3931a092710e128710cca50f7e4d843b?el=desc) (a85326d) will **increase** coverage by `0.06%`.; > The diff coverage is `87.50%`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/116/graphs/tree.svg?width=650&height=150&src=pr&token=t9PLztMgha)](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #116 +/- ##; ==========================================; + Coverage 67.92% 67.98% +0.06% ; ==========================================; Files 24 25 +1 ; Lines 1506 1512 +6 ; ==========================================; + Hits 1023 1028 +5 ; - Misses 479 480 +1 ; Partials 4 4 ; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [process.go](https://codecov.io/gh/scipipe/scipipe/pull/116/diff?src=pr&el=tree#diff-cHJvY2Vzcy5nbw==) | `89.03% <ø> (ø)` | |; | [settings.go](https://codecov.io/gh/scipipe/scipipe/pull/116/diff?src=pr&el=tree#diff-c2V0dGluZ3MuZ28=) | `83.33% <83.33%> (ø)` | |; | [port.go](https://codecov.io/gh/scipipe/scipipe/pull/116/diff?src=pr&el=tree#diff-cG9ydC5nbw==) | `79.03% <100.00%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=footer). Last update [943de2b...39a3f97](https://codecov.io/gh/scipipe/scipipe/pull/116?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/116#issuecomment-793116476
Deployability,update,update,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=h1) Report; > Merging [#122](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=desc) (abb623f) into [master](https://codecov.io/gh/scipipe/scipipe/commit/766b9708f4d2991acb1744efbe93eac6f9fc28f2?el=desc) (766b970) will **increase** coverage by `0.39%`.; > The diff coverage is `84.21%`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/122/graphs/tree.svg?width=650&height=150&src=pr&token=t9PLztMgha)](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #122 +/- ##; ==========================================; + Coverage 67.98% 68.38% +0.39% ; ==========================================; Files 25 26 +1 ; Lines 1512 1550 +38 ; ==========================================; + Hits 1028 1060 +32 ; - Misses 480 486 +6 ; Partials 4 4 ; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [components/utils.go](https://codecov.io/gh/scipipe/scipipe/pull/122/diff?src=pr&el=tree#diff-Y29tcG9uZW50cy91dGlscy5nbw==) | `41.66% <50.00%> (+41.66%)` | :arrow_up: |; | [components/ip\_selector\_sync.go](https://codecov.io/gh/scipipe/scipipe/pull/122/diff?src=pr&el=tree#diff-Y29tcG9uZW50cy9pcF9zZWxlY3Rvcl9zeW5jLmdv) | `96.42% <96.42%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=footer). Last update [766b970...abb623f](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/122#issuecomment-801497094
Usability,learn,learn,"# [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=h1) Report; > Merging [#122](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=desc) (abb623f) into [master](https://codecov.io/gh/scipipe/scipipe/commit/766b9708f4d2991acb1744efbe93eac6f9fc28f2?el=desc) (766b970) will **increase** coverage by `0.39%`.; > The diff coverage is `84.21%`. [![Impacted file tree graph](https://codecov.io/gh/scipipe/scipipe/pull/122/graphs/tree.svg?width=650&height=150&src=pr&token=t9PLztMgha)](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #122 +/- ##; ==========================================; + Coverage 67.98% 68.38% +0.39% ; ==========================================; Files 25 26 +1 ; Lines 1512 1550 +38 ; ==========================================; + Hits 1028 1060 +32 ; - Misses 480 486 +6 ; Partials 4 4 ; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [components/utils.go](https://codecov.io/gh/scipipe/scipipe/pull/122/diff?src=pr&el=tree#diff-Y29tcG9uZW50cy91dGlscy5nbw==) | `41.66% <50.00%> (+41.66%)` | :arrow_up: |; | [components/ip\_selector\_sync.go](https://codecov.io/gh/scipipe/scipipe/pull/122/diff?src=pr&el=tree#diff-Y29tcG9uZW50cy9pcF9zZWxlY3Rvcl9zeW5jLmdv) | `96.42% <96.42%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=footer). Last update [766b970...abb623f](https://codecov.io/gh/scipipe/scipipe/pull/122?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/122#issuecomment-801497094
Deployability,update,update,"t=comment&utm_campaign=pr+comments&utm_term=scipipe)](https://codecov.io/gh/scipipe/scipipe/pull/141?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe). ```diff; @@ Coverage Diff @@; ## master #141 +/- ##; =======================================; Coverage 68.09% 68.09% ; =======================================; Files 26 26 ; Lines 2106 2106 ; =======================================; Hits 1434 1434 ; Misses 666 666 ; Partials 6 6 ; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/141?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe) | Coverage Δ | |; |---|---|---|; | [task.go](https://codecov.io/gh/scipipe/scipipe/pull/141/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe#diff-dGFzay5nbw==) | `81.94% <0.00%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/141?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/141?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe). Last update [48b1386...12f822a](https://codecov.io/gh/scipipe/scipipe/pull/141?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/141#issuecomment-910594431
Usability,learn,learn,"t=comment&utm_campaign=pr+comments&utm_term=scipipe)](https://codecov.io/gh/scipipe/scipipe/pull/141?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe). ```diff; @@ Coverage Diff @@; ## master #141 +/- ##; =======================================; Coverage 68.09% 68.09% ; =======================================; Files 26 26 ; Lines 2106 2106 ; =======================================; Hits 1434 1434 ; Misses 666 666 ; Partials 6 6 ; ```. | [Impacted Files](https://codecov.io/gh/scipipe/scipipe/pull/141?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe) | Coverage Δ | |; |---|---|---|; | [task.go](https://codecov.io/gh/scipipe/scipipe/pull/141/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe#diff-dGFzay5nbw==) | `81.94% <0.00%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/scipipe/scipipe/pull/141?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/scipipe/scipipe/pull/141?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe). Last update [48b1386...12f822a](https://codecov.io/gh/scipipe/scipipe/pull/141?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe).",MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/141#issuecomment-910594431
Testability,log,log,I am more of a user of SciPipe than a developer but I mainly look at log files and output files. My workflows contains many steps which write to file so I simply look at how many files has been created which gives me a feeling for how the execution is going... What are the features you want? I had a look at the README files of those projects but they don't specify any features of the projects just talk about which workflow tools they are built for...,MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/149#issuecomment-1161372677
Usability,simpl,simply,I am more of a user of SciPipe than a developer but I mainly look at log files and output files. My workflows contains many steps which write to file so I simply look at how many files has been created which gives me a feeling for how the execution is going... What are the features you want? I had a look at the README files of those projects but they don't specify any features of the projects just talk about which workflow tools they are built for...,MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/issues/149#issuecomment-1161372677
Testability,test,tests,## [Codecov](https://app.codecov.io/gh/scipipe/scipipe/pull/160?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe) Report; All modified and coverable lines are covered by tests :white_check_mark:; > Project coverage is 60.50%. Comparing base [(`4d29035`)](https://app.codecov.io/gh/scipipe/scipipe/commit/4d29035e8538d7d5362227eff037a1dd22d1e186?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe) to head [(`ccf3fad`)](https://app.codecov.io/gh/scipipe/scipipe/commit/ccf3fad9ed35d9ade2941cc56595675b69f370fb?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## master #160 +/- ##; =======================================; Coverage 60.50% 60.50% ; =======================================; Files 15 15 ; Lines 676 676 ; =======================================; Hits 409 409 ; Misses 266 266 ; Partials 1 1 ; ```. </details>. [:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/scipipe/scipipe/pull/160?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe). ; :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe).,MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/160#issuecomment-2287676961
Usability,feedback,feedback,## [Codecov](https://app.codecov.io/gh/scipipe/scipipe/pull/160?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe) Report; All modified and coverable lines are covered by tests :white_check_mark:; > Project coverage is 60.50%. Comparing base [(`4d29035`)](https://app.codecov.io/gh/scipipe/scipipe/commit/4d29035e8538d7d5362227eff037a1dd22d1e186?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe) to head [(`ccf3fad`)](https://app.codecov.io/gh/scipipe/scipipe/commit/ccf3fad9ed35d9ade2941cc56595675b69f370fb?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## master #160 +/- ##; =======================================; Coverage 60.50% 60.50% ; =======================================; Files 15 15 ; Lines 676 676 ; =======================================; Hits 409 409 ; Misses 266 266 ; Partials 1 1 ; ```. </details>. [:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/scipipe/scipipe/pull/160?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe). ; :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scipipe).,MatchSource.ISSUE_COMMENT,scipipe,scipipe,v0.12.0,https://scipipe.org,https://github.com/scipipe/scipipe/pull/160#issuecomment-2287676961
