id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228541825:324,Availability,avail,available,324,"Hi - is this in 0.19 proper or 0.19_hotfix? I believe this was fixed already in the latter. If you're using the former try the latter, if you're using the latter then clearly I'm wrong :). I'll also point a finger at develop which is radically different (many problems fixed, will have its own new problems) which should be available as 0.20 this week",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228541825
https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228541825:167,Usability,clear,clearly,167,"Hi - is this in 0.19 proper or 0.19_hotfix? I believe this was fixed already in the latter. If you're using the former try the latter, if you're using the latter then clearly I'm wrong :). I'll also point a finger at develop which is radically different (many problems fixed, will have its own new problems) which should be available as 0.20 this week",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228541825
https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228572361:79,Availability,error,error,79,"I was using 0.19 proper. I compiled the 0.19_hotfix branch, and I got the same error.; I complied the develop branch, and this problem appears resolved... Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228572361
https://github.com/broadinstitute/cromwell/issues/1071#issuecomment-229115974:37,Availability,error,errors,37,"Hello,. I can better assist you with errors like this over on the [WDL website](https://software.broadinstitute.org/wdl/index.php), specifically the [Ask the WDL team](http://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) section. Oftentimes, you can find a solution to your error, or perhaps you will find documents that answer your question on the website. As such, we prefer to answer questions, or file bug reports (if needed) from the forum. In your particular case, I would suggest asking a new question in the Ask the WDL team forum. Let me know if you need anything else.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071#issuecomment-229115974
https://github.com/broadinstitute/cromwell/issues/1071#issuecomment-229115974:291,Availability,error,error,291,"Hello,. I can better assist you with errors like this over on the [WDL website](https://software.broadinstitute.org/wdl/index.php), specifically the [Ask the WDL team](http://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) section. Oftentimes, you can find a solution to your error, or perhaps you will find documents that answer your question on the website. As such, we prefer to answer questions, or file bug reports (if needed) from the forum. In your particular case, I would suggest asking a new question in the Ask the WDL team forum. Let me know if you need anything else.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071#issuecomment-229115974
https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230903835:96,Modifiability,config,config,96,"@djhshih I don't think it fully explains it, but the path to the localization strategies in the config file has changed.; `-Dbackend.shared-filesystem.localization.0=soft-link` won't have any effect.; Each backend now has its own `config` stanza. For local backend that would be ; `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link`; Could you try with that and see if you have the same problem ?; I'm still confused as why some of them are soft-linked and some of them aren't. I think logging when a localization strategy fails would also be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230903835
https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230903835:231,Modifiability,config,config,231,"@djhshih I don't think it fully explains it, but the path to the localization strategies in the config file has changed.; `-Dbackend.shared-filesystem.localization.0=soft-link` won't have any effect.; Each backend now has its own `config` stanza. For local backend that would be ; `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link`; Could you try with that and see if you have the same problem ?; I'm still confused as why some of them are soft-linked and some of them aren't. I think logging when a localization strategy fails would also be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230903835
https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230903835:308,Modifiability,config,config,308,"@djhshih I don't think it fully explains it, but the path to the localization strategies in the config file has changed.; `-Dbackend.shared-filesystem.localization.0=soft-link` won't have any effect.; Each backend now has its own `config` stanza. For local backend that would be ; `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link`; Could you try with that and see if you have the same problem ?; I'm still confused as why some of them are soft-linked and some of them aren't. I think logging when a localization strategy fails would also be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230903835
https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230903835:512,Testability,log,logging,512,"@djhshih I don't think it fully explains it, but the path to the localization strategies in the config file has changed.; `-Dbackend.shared-filesystem.localization.0=soft-link` won't have any effect.; Each backend now has its own `config` stanza. For local backend that would be ; `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link`; Could you try with that and see if you have the same problem ?; I'm still confused as why some of them are soft-linked and some of them aren't. I think logging when a localization strategy fails would also be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230903835
https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938:190,Availability,error,error,190,"Sorry. I didn't realize the configuration has changed between 0.19/0.19_hotfix and 0.20. What happened is that the config flag was ignored (though it would perhaps be preferable to throw an error or at least a warning.) and the cromwell tried each localization strategy until one succeeded, as described in the documentation. In my run, the files that were hard-linked live on the same storage device.; The files that were soft-linked live on another storage device, presumably because hard-linking failed. Using the new flag `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link` ensures that all files are soft-linked as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938
https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938:28,Deployability,configurat,configuration,28,"Sorry. I didn't realize the configuration has changed between 0.19/0.19_hotfix and 0.20. What happened is that the config flag was ignored (though it would perhaps be preferable to throw an error or at least a warning.) and the cromwell tried each localization strategy until one succeeded, as described in the documentation. In my run, the files that were hard-linked live on the same storage device.; The files that were soft-linked live on another storage device, presumably because hard-linking failed. Using the new flag `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link` ensures that all files are soft-linked as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938
https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938:28,Modifiability,config,configuration,28,"Sorry. I didn't realize the configuration has changed between 0.19/0.19_hotfix and 0.20. What happened is that the config flag was ignored (though it would perhaps be preferable to throw an error or at least a warning.) and the cromwell tried each localization strategy until one succeeded, as described in the documentation. In my run, the files that were hard-linked live on the same storage device.; The files that were soft-linked live on another storage device, presumably because hard-linking failed. Using the new flag `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link` ensures that all files are soft-linked as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938
https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938:115,Modifiability,config,config,115,"Sorry. I didn't realize the configuration has changed between 0.19/0.19_hotfix and 0.20. What happened is that the config flag was ignored (though it would perhaps be preferable to throw an error or at least a warning.) and the cromwell tried each localization strategy until one succeeded, as described in the documentation. In my run, the files that were hard-linked live on the same storage device.; The files that were soft-linked live on another storage device, presumably because hard-linking failed. Using the new flag `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link` ensures that all files are soft-linked as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938
https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938:553,Modifiability,config,config,553,"Sorry. I didn't realize the configuration has changed between 0.19/0.19_hotfix and 0.20. What happened is that the config flag was ignored (though it would perhaps be preferable to throw an error or at least a warning.) and the cromwell tried each localization strategy until one succeeded, as described in the documentation. In my run, the files that were hard-linked live on the same storage device.; The files that were soft-linked live on another storage device, presumably because hard-linking failed. Using the new flag `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link` ensures that all files are soft-linked as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-229630583:17,Energy Efficiency,reduce,reduce,17,Good refactor to reduce future headaches/bugs/effort,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-229630583
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-229630583:5,Modifiability,refactor,refactor,5,Good refactor to reduce future headaches/bugs/effort,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-229630583
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-230929279:34,Safety,avoid,avoid,34,"I picked up this ticket mostly to avoid getting in other people's way, but having looked at if for a while I'm not seeing much value in this. The default_runtime_attributes feature itself is valuable and working, with Centaur test coverage. The backends aren't really re-implementing default runtime attributes, nearly all the heavy lifting is being done by `RuntimeAttributesDefault`. The backends currently do have to be aware of the existence of the default runtime attributes feature but that doesn't really seem so bad. Unassigning and returning to the bottom of the 0.21 pile, recommending for demotion to a lesser pile or outright closure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-230929279
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-230929279:226,Testability,test,test,226,"I picked up this ticket mostly to avoid getting in other people's way, but having looked at if for a while I'm not seeing much value in this. The default_runtime_attributes feature itself is valuable and working, with Centaur test coverage. The backends aren't really re-implementing default runtime attributes, nearly all the heavy lifting is being done by `RuntimeAttributesDefault`. The backends currently do have to be aware of the existence of the default runtime attributes feature but that doesn't really seem so bad. Unassigning and returning to the bottom of the 0.21 pile, recommending for demotion to a lesser pile or outright closure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-230929279
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:495,Availability,down,down,495,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:387,Modifiability,refactor,refactor,387,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:334,Security,validat,validation,334,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:399,Security,expose,expose,399,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:722,Security,validat,validation,722,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:557,Usability,clear,clear,557,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247:1447,Deployability,update,updated,1447,"Per standup this morning we've decided to close this. The requested functionality is problematic on a few levels:. JSON runtime attributes are currently interpreted as WdlExpressions after going through coercion, which presently is a backend-dependent process as only the backend knows the accepted runtime attribute coercions. While currently it might appear safe to treat the default runtime attribute JSON values as WDL with a `WdlExpression.fromString`, that's an assumption of WDL / JSON expression equivalence not made elsewhere in Cromwell and it seems questionable to pioneer that here. Currently Cromwell's backend assignments happen at initialization time, but in the future Cromwell's backend dispatch is likely to become more sophisticated and dynamic. It therefore wouldn't be possible to say at workflow initialization time the backend to which a call was fated, which would mean the default runtime attribute handling would need to happen in the various `FooRuntimeAttributes` classes as it does now. The refactor proposed here would actually remove the default runtime attribute handling at call execution time and therefore make dynamic dispatch more difficult to add in the future. Finally, while it appears technically possible to doctor tasks with workflow option-derived default runtime attributes in `MaterializeWorkflowDescriptorActor`, this makes for some pretty hacky code. I discovered at least 3 spots that needed to be updated:; - backendAssignment values; - NamespaceWithWorkflow -> tasks; - NamespaceWithWorkflow -> workflow -> children -> tasks. That last item was particularly hideous since `Workflow#children` is explicitly write-once. I got around this with subclassing, but I felt bad about myself afterward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247:242,Integrability,depend,dependent,242,"Per standup this morning we've decided to close this. The requested functionality is problematic on a few levels:. JSON runtime attributes are currently interpreted as WdlExpressions after going through coercion, which presently is a backend-dependent process as only the backend knows the accepted runtime attribute coercions. While currently it might appear safe to treat the default runtime attribute JSON values as WDL with a `WdlExpression.fromString`, that's an assumption of WDL / JSON expression equivalence not made elsewhere in Cromwell and it seems questionable to pioneer that here. Currently Cromwell's backend assignments happen at initialization time, but in the future Cromwell's backend dispatch is likely to become more sophisticated and dynamic. It therefore wouldn't be possible to say at workflow initialization time the backend to which a call was fated, which would mean the default runtime attribute handling would need to happen in the various `FooRuntimeAttributes` classes as it does now. The refactor proposed here would actually remove the default runtime attribute handling at call execution time and therefore make dynamic dispatch more difficult to add in the future. Finally, while it appears technically possible to doctor tasks with workflow option-derived default runtime attributes in `MaterializeWorkflowDescriptorActor`, this makes for some pretty hacky code. I discovered at least 3 spots that needed to be updated:; - backendAssignment values; - NamespaceWithWorkflow -> tasks; - NamespaceWithWorkflow -> workflow -> children -> tasks. That last item was particularly hideous since `Workflow#children` is explicitly write-once. I got around this with subclassing, but I felt bad about myself afterward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247:1020,Modifiability,refactor,refactor,1020,"Per standup this morning we've decided to close this. The requested functionality is problematic on a few levels:. JSON runtime attributes are currently interpreted as WdlExpressions after going through coercion, which presently is a backend-dependent process as only the backend knows the accepted runtime attribute coercions. While currently it might appear safe to treat the default runtime attribute JSON values as WDL with a `WdlExpression.fromString`, that's an assumption of WDL / JSON expression equivalence not made elsewhere in Cromwell and it seems questionable to pioneer that here. Currently Cromwell's backend assignments happen at initialization time, but in the future Cromwell's backend dispatch is likely to become more sophisticated and dynamic. It therefore wouldn't be possible to say at workflow initialization time the backend to which a call was fated, which would mean the default runtime attribute handling would need to happen in the various `FooRuntimeAttributes` classes as it does now. The refactor proposed here would actually remove the default runtime attribute handling at call execution time and therefore make dynamic dispatch more difficult to add in the future. Finally, while it appears technically possible to doctor tasks with workflow option-derived default runtime attributes in `MaterializeWorkflowDescriptorActor`, this makes for some pretty hacky code. I discovered at least 3 spots that needed to be updated:; - backendAssignment values; - NamespaceWithWorkflow -> tasks; - NamespaceWithWorkflow -> workflow -> children -> tasks. That last item was particularly hideous since `Workflow#children` is explicitly write-once. I got around this with subclassing, but I felt bad about myself afterward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247:360,Safety,safe,safe,360,"Per standup this morning we've decided to close this. The requested functionality is problematic on a few levels:. JSON runtime attributes are currently interpreted as WdlExpressions after going through coercion, which presently is a backend-dependent process as only the backend knows the accepted runtime attribute coercions. While currently it might appear safe to treat the default runtime attribute JSON values as WDL with a `WdlExpression.fromString`, that's an assumption of WDL / JSON expression equivalence not made elsewhere in Cromwell and it seems questionable to pioneer that here. Currently Cromwell's backend assignments happen at initialization time, but in the future Cromwell's backend dispatch is likely to become more sophisticated and dynamic. It therefore wouldn't be possible to say at workflow initialization time the backend to which a call was fated, which would mean the default runtime attribute handling would need to happen in the various `FooRuntimeAttributes` classes as it does now. The refactor proposed here would actually remove the default runtime attribute handling at call execution time and therefore make dynamic dispatch more difficult to add in the future. Finally, while it appears technically possible to doctor tasks with workflow option-derived default runtime attributes in `MaterializeWorkflowDescriptorActor`, this makes for some pretty hacky code. I discovered at least 3 spots that needed to be updated:; - backendAssignment values; - NamespaceWithWorkflow -> tasks; - NamespaceWithWorkflow -> workflow -> children -> tasks. That last item was particularly hideous since `Workflow#children` is explicitly write-once. I got around this with subclassing, but I felt bad about myself afterward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247
https://github.com/broadinstitute/cromwell/issues/1079#issuecomment-228875121:67,Modifiability,config,config,67,"An example of this is submit a workflow with a ""user-with-refresh"" config and forget the ""refresh_token"" workflow option",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1079#issuecomment-228875121
https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229134405:193,Deployability,update,update,193,"@mcovarr correct, the more general problem exists throughout cromwell though and I didn't want to get into a rabbit hole with this but after digging a bit more it's actually not that bad. I'll update the PR with some better crash handling instead",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229134405
https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229348436:14,Testability,test,test,14,"re-👍, pending test passes",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229348436
https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314:61,Availability,failure,failures,61,Going to merge. Centaur has been updated to address the test failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314
https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314:33,Deployability,update,updated,33,Going to merge. Centaur has been updated to address the test failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314
https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314:56,Testability,test,test,56,Going to merge. Centaur has been updated to address the test failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314
https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-229573997:296,Energy Efficiency,efficient,efficient,296,"+1; we like to compose WDL workflows using import of many atomic tasks, some of these will be very simple like generating a UUID or calling mkdir, some will be long running computations. Being able to specify the short tasks to run locally and handing the longer ones off to the cluster would be efficient.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-229573997
https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-229573997:99,Usability,simpl,simple,99,"+1; we like to compose WDL workflows using import of many atomic tasks, some of these will be very simple like generating a UUID or calling mkdir, some will be long running computations. Being able to specify the short tasks to run locally and handing the longer ones off to the cluster would be efficient.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-229573997
https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-253986986:5,Deployability,update,update,5,"I'll update the issue title... this isn't about SGE per-se, but rather about running local + some other back end in a single workflow. Google JES is fine for that. Just to further clarify, this doesn't require being able to schlep files between the two (although it would be great to know if that works!) but as @delocalizer mentioned, a use case of running something locally that generates a non-file type (e.g. string) that can be passed as input to a task on another backend would be the thing to verify or get working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-253986986
https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-253987047:22,Testability,test,test,22,... but if we want to test SGE... Lee would be a good contact,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-253987047
https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-270158225:34,Testability,test,test,34,"There exists a Centaur `local_gcs.test` which creates a file in GCS using the JES backend, then reads it using a GCS-savvy Local backend. Does that meet the requirements of this ticket?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-270158225
https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-276574494:43,Deployability,release,release,43,"This tests as working for me at least with release 24; I can specify ""Local"" or my custom ""PBS"" backend (essentially a modified SGE configured backend) in a task `runtime` block and it behaves as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-276574494
https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-276574494:132,Modifiability,config,configured,132,"This tests as working for me at least with release 24; I can specify ""Local"" or my custom ""PBS"" backend (essentially a modified SGE configured backend) in a task `runtime` block and it behaves as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-276574494
https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-276574494:5,Testability,test,tests,5,"This tests as working for me at least with release 24; I can specify ""Local"" or my custom ""PBS"" backend (essentially a modified SGE configured backend) in a task `runtime` block and it behaves as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-276574494
https://github.com/broadinstitute/cromwell/issues/1087#issuecomment-229145028:52,Security,validat,validate,52,Spark Backend for the PBE (develop) branch. Need to validate Cromwell's task description for different spark execution modes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1087#issuecomment-229145028
https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229210088:37,Availability,error,errors,37,This has the potential to reduce QPS errors enormously.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229210088
https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229210088:26,Energy Efficiency,reduce,reduce,26,This has the potential to reduce QPS errors enormously.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229210088
https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152:128,Integrability,message,message,128,"The simplest way this can be done is to use [Google Cloud Pub/Sub](https://cloud.google.com/pubsub/overview), and then have the message pushed to all subscribed VMs since one can get a list of Operations ([globally](https://cloud.google.com/compute/docs/reference/latest/globalOperations/list), [by region](https://cloud.google.com/compute/docs/reference/latest/regionOperations/list) or [zone](https://cloud.google.com/compute/docs/reference/latest/zoneOperations/list)). Then based on the message each individual VM can query itself, and provide the Operation resource status to a central location (Web service, Google Storage, etc). For more information, here are a link:. https://cloud.google.com/pubsub/overview. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152
https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152:491,Integrability,message,message,491,"The simplest way this can be done is to use [Google Cloud Pub/Sub](https://cloud.google.com/pubsub/overview), and then have the message pushed to all subscribed VMs since one can get a list of Operations ([globally](https://cloud.google.com/compute/docs/reference/latest/globalOperations/list), [by region](https://cloud.google.com/compute/docs/reference/latest/regionOperations/list) or [zone](https://cloud.google.com/compute/docs/reference/latest/zoneOperations/list)). Then based on the message each individual VM can query itself, and provide the Operation resource status to a central location (Web service, Google Storage, etc). For more information, here are a link:. https://cloud.google.com/pubsub/overview. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152
https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152:4,Usability,simpl,simplest,4,"The simplest way this can be done is to use [Google Cloud Pub/Sub](https://cloud.google.com/pubsub/overview), and then have the message pushed to all subscribed VMs since one can get a list of Operations ([globally](https://cloud.google.com/compute/docs/reference/latest/globalOperations/list), [by region](https://cloud.google.com/compute/docs/reference/latest/regionOperations/list) or [zone](https://cloud.google.com/compute/docs/reference/latest/zoneOperations/list)). Then based on the message each individual VM can query itself, and provide the Operation resource status to a central location (Web service, Google Storage, etc). For more information, here are a link:. https://cloud.google.com/pubsub/overview. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152
https://github.com/broadinstitute/cromwell/pull/1092#issuecomment-229399616:57,Testability,test,tests,57,Sorry about all the comments regarding the philosophy of tests! I personally think they're important but they are ToL so I won't hold back the 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1092/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1092#issuecomment-229399616
https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229388655:80,Testability,test,test,80,"I though the advantage of tagging them was that we could exclude them from ""sbt test"" while still having them ""unignored"" so they don't count against the ""ignored"" count.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229388655
https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229391214:125,Deployability,patch,patch,125,"Assuming `PostMVP` tagged tests are filtered out of `sbt test`, then they should never be _added_ to the ignored count. This patch therefore only adds to the ignored count for `sbt alltests:test`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229391214
https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229391214:26,Testability,test,tests,26,"Assuming `PostMVP` tagged tests are filtered out of `sbt test`, then they should never be _added_ to the ignored count. This patch therefore only adds to the ignored count for `sbt alltests:test`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229391214
https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229391214:57,Testability,test,test,57,"Assuming `PostMVP` tagged tests are filtered out of `sbt test`, then they should never be _added_ to the ignored count. This patch therefore only adds to the ignored count for `sbt alltests:test`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229391214
https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229391214:190,Testability,test,test,190,"Assuming `PostMVP` tagged tests are filtered out of `sbt test`, then they should never be _added_ to the ignored count. This patch therefore only adds to the ignored count for `sbt alltests:test`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229391214
https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235597055:36,Testability,test,tests,36,"The success rate on the non-Centaur tests is getting pretty close to zero, can we discuss taking this one off HOLD?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235597055
https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235602241:33,Testability,test,tests,33,"As discussed in person IMO these tests should be reworked to have Main talking to some sort of mock such that it's just checking that the correct signals are being sent in to the SingleWorkflow (or whatever, w/o looking at it what I just said might not be 100% the right thing). I was planning on doing this in the near future if no one else did.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235602241
https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235602241:95,Testability,mock,mock,95,"As discussed in person IMO these tests should be reworked to have Main talking to some sort of mock such that it's just checking that the correct signals are being sent in to the SingleWorkflow (or whatever, w/o looking at it what I just said might not be 100% the right thing). I was planning on doing this in the near future if no one else did.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235602241
https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235603369:64,Testability,test,tests,64,"I was assuming the root cause of this was the same as the other tests failing in Travis, is that not the case?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235603369
https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235613640:69,Availability,failure,failure,69,Discussed in person but probably not (assuming that the two modes of failure are what I was seeing a couple of weeks ago),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235613640
https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-238392294:60,Testability,test,tests,60,"This no longer exists as MainSpec no longer exists, and the tests which replaced it are no longer async-y and such",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-238392294
https://github.com/broadinstitute/cromwell/issues/1097#issuecomment-229629709:21,Deployability,update,update,21,"@geoffjentry can you update the ticket with more info? Where is this parameter used, and what is the effect of this being ignored?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1097#issuecomment-229629709
https://github.com/broadinstitute/cromwell/issues/1097#issuecomment-230538183:36,Modifiability,config,config,36,Thanks. Let's make this tunable via config as part of this ticket,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1097#issuecomment-230538183
https://github.com/broadinstitute/cromwell/pull/1099#issuecomment-229972340:92,Deployability,a/b,a/broadinstitute,92,"I think in April we chose Option 1 with a sprinkling of Option 3 in https://docs.google.com/a/broadinstitute.com/document/d/1feRDusWXQQ2pJ03sNHTNmrrnnwL3y-vtyF1fv_RdogU/edit?usp=sharing whereas this is clearly Option 2... (I'm not saying it's wrong, this seems to address all of the ""cons"" as I saw them with gusto... just pointing it out :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1099#issuecomment-229972340
https://github.com/broadinstitute/cromwell/pull/1099#issuecomment-229972340:202,Usability,clear,clearly,202,"I think in April we chose Option 1 with a sprinkling of Option 3 in https://docs.google.com/a/broadinstitute.com/document/d/1feRDusWXQQ2pJ03sNHTNmrrnnwL3y-vtyF1fv_RdogU/edit?usp=sharing whereas this is clearly Option 2... (I'm not saying it's wrong, this seems to address all of the ""cons"" as I saw them with gusto... just pointing it out :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1099#issuecomment-229972340
https://github.com/broadinstitute/cromwell/pull/1100#issuecomment-229752230:19,Deployability,update,update,19,@francares Can you update the story on waffle (#884) and move it to in-review?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1100#issuecomment-229752230
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813:351,Availability,recover,recovery,351,"Hey Conrad - Thanks, this is awesome. To give some insight on how things are playing out in the hopefully-not-too-long-term, we're planning on cutting an alpha release of the PBE stuff imminently (perhaps today?) which is something we feel is stable enough to start poking at but is missing a few features we need in our production use cases (restart/recovery, call caching), backends other than local/JES, and with some known warts we need to hammer out. I'm guessing you're looking at roughly a month for something more stable than that, although I'm famous for my ""about a month"" predictions. However, since you're already pretty up to speed with what's going on, I'd say that the 0.20 alpha should be stable enough to work up a backend. It'd at least be a good test case as someone who _did_ figure out how to make one in the old system if the new system is inscrutable or not. In terms of what to do with this PR, I'll somewhat leave it up to you. We're hoping to close the 0.19 books as much as possible once the alpha thing is out, but if you feel like it'll provide value to folks over the next month or so I'm happy to take some time to review it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813:160,Deployability,release,release,160,"Hey Conrad - Thanks, this is awesome. To give some insight on how things are playing out in the hopefully-not-too-long-term, we're planning on cutting an alpha release of the PBE stuff imminently (perhaps today?) which is something we feel is stable enough to start poking at but is missing a few features we need in our production use cases (restart/recovery, call caching), backends other than local/JES, and with some known warts we need to hammer out. I'm guessing you're looking at roughly a month for something more stable than that, although I'm famous for my ""about a month"" predictions. However, since you're already pretty up to speed with what's going on, I'd say that the 0.20 alpha should be stable enough to work up a backend. It'd at least be a good test case as someone who _did_ figure out how to make one in the old system if the new system is inscrutable or not. In terms of what to do with this PR, I'll somewhat leave it up to you. We're hoping to close the 0.19 books as much as possible once the alpha thing is out, but if you feel like it'll provide value to folks over the next month or so I'm happy to take some time to review it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813:351,Safety,recover,recovery,351,"Hey Conrad - Thanks, this is awesome. To give some insight on how things are playing out in the hopefully-not-too-long-term, we're planning on cutting an alpha release of the PBE stuff imminently (perhaps today?) which is something we feel is stable enough to start poking at but is missing a few features we need in our production use cases (restart/recovery, call caching), backends other than local/JES, and with some known warts we need to hammer out. I'm guessing you're looking at roughly a month for something more stable than that, although I'm famous for my ""about a month"" predictions. However, since you're already pretty up to speed with what's going on, I'd say that the 0.20 alpha should be stable enough to work up a backend. It'd at least be a good test case as someone who _did_ figure out how to make one in the old system if the new system is inscrutable or not. In terms of what to do with this PR, I'll somewhat leave it up to you. We're hoping to close the 0.19 books as much as possible once the alpha thing is out, but if you feel like it'll provide value to folks over the next month or so I'm happy to take some time to review it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813:583,Safety,predict,predictions,583,"Hey Conrad - Thanks, this is awesome. To give some insight on how things are playing out in the hopefully-not-too-long-term, we're planning on cutting an alpha release of the PBE stuff imminently (perhaps today?) which is something we feel is stable enough to start poking at but is missing a few features we need in our production use cases (restart/recovery, call caching), backends other than local/JES, and with some known warts we need to hammer out. I'm guessing you're looking at roughly a month for something more stable than that, although I'm famous for my ""about a month"" predictions. However, since you're already pretty up to speed with what's going on, I'd say that the 0.20 alpha should be stable enough to work up a backend. It'd at least be a good test case as someone who _did_ figure out how to make one in the old system if the new system is inscrutable or not. In terms of what to do with this PR, I'll somewhat leave it up to you. We're hoping to close the 0.19 books as much as possible once the alpha thing is out, but if you feel like it'll provide value to folks over the next month or so I'm happy to take some time to review it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813:765,Testability,test,test,765,"Hey Conrad - Thanks, this is awesome. To give some insight on how things are playing out in the hopefully-not-too-long-term, we're planning on cutting an alpha release of the PBE stuff imminently (perhaps today?) which is something we feel is stable enough to start poking at but is missing a few features we need in our production use cases (restart/recovery, call caching), backends other than local/JES, and with some known warts we need to hammer out. I'm guessing you're looking at roughly a month for something more stable than that, although I'm famous for my ""about a month"" predictions. However, since you're already pretty up to speed with what's going on, I'd say that the 0.20 alpha should be stable enough to work up a backend. It'd at least be a good test case as someone who _did_ figure out how to make one in the old system if the new system is inscrutable or not. In terms of what to do with this PR, I'll somewhat leave it up to you. We're hoping to close the 0.19 books as much as possible once the alpha thing is out, but if you feel like it'll provide value to folks over the next month or so I'm happy to take some time to review it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-230188328:198,Deployability,release,release,198,"Thanks for the insight Jeff.Let's just leave it then if the PBE stuff is happening so soon, I don't want to waste your time. I'd be very happy to be a guinea pig for making a backend using an early release of the new design. Cromwell rocks b.t.w.; and thanks for making it open!; cheers,C",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-230188328
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254153089:165,Deployability,release,release,165,[sorry for resurrecting this old thread]. Is Cromwell now stable enough for this kind of contributions? I see that the big backend changes were included in the 0.21 release.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254153089
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254165500:113,Deployability,configurat,configuration,113,"@ALTree if you're referring specifically to PBS, that can now be supported with a little kludging purely through configuration. The only feature of PBS cluster that needs special treatment is the handling of stderr and stdout, which by default on PBS are copied to the execution directory only _after_ the job completes. [The config file in this gist](https://gist.github.com/delocalizer/fa29139675fb4118e908a4c80249dffb) works for me. Note that it requires that PBS_JOBDIR (the user's home directory by default, but can be a custom value) is shared across compute nodes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254165500
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254165500:113,Modifiability,config,configuration,113,"@ALTree if you're referring specifically to PBS, that can now be supported with a little kludging purely through configuration. The only feature of PBS cluster that needs special treatment is the handling of stderr and stdout, which by default on PBS are copied to the execution directory only _after_ the job completes. [The config file in this gist](https://gist.github.com/delocalizer/fa29139675fb4118e908a4c80249dffb) works for me. Note that it requires that PBS_JOBDIR (the user's home directory by default, but can be a custom value) is shared across compute nodes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254165500
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254165500:326,Modifiability,config,config,326,"@ALTree if you're referring specifically to PBS, that can now be supported with a little kludging purely through configuration. The only feature of PBS cluster that needs special treatment is the handling of stderr and stdout, which by default on PBS are copied to the execution directory only _after_ the job completes. [The config file in this gist](https://gist.github.com/delocalizer/fa29139675fb4118e908a4c80249dffb) works for me. Note that it requires that PBS_JOBDIR (the user's home directory by default, but can be a custom value) is shared across compute nodes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254165500
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254206921:149,Deployability,configurat,configuration,149,"@delocalizer ah, thanks. I didn't realize the new backend system was this flexible (the `README` is a little opaque). Thank you for sharing your PBS configuration file! I confirm it works for me, too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254206921
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254206921:74,Modifiability,flexible,flexible,74,"@delocalizer ah, thanks. I didn't realize the new backend system was this flexible (the `README` is a little opaque). Thank you for sharing your PBS configuration file! I confirm it works for me, too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254206921
https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254206921:149,Modifiability,config,configuration,149,"@delocalizer ah, thanks. I didn't realize the new backend system was this flexible (the `README` is a little opaque). Thank you for sharing your PBS configuration file! I confirm it works for me, too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254206921
https://github.com/broadinstitute/cromwell/pull/1108#issuecomment-229980714:61,Integrability,message,message,61,Let's reply with honesty rather than sending an unrecognised message to an actor which probably has nothing to do with call caching anymore,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1108#issuecomment-229980714
https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230055984:46,Testability,test,test,46,I don't know the scope of DockerTest but this test the only thing it does is to check the docker command generated for the job. It does not execute anything.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230055984
https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230058582:157,Availability,avail,available,157,"@francares @cjllanwarne I think the actual issue here is that the ""/tmp"" assertions on lines 260 and 261 always fail on Mac, regardless of whether Docker is available or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230058582
https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230058582:73,Testability,assert,assertions,73,"@francares @cjllanwarne I think the actual issue here is that the ""/tmp"" assertions on lines 260 and 261 always fail on Mac, regardless of whether Docker is available or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230058582
https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230490075:654,Availability,echo,echo,654,"@francares the result when run on a mac:. ```; ""#!/bin/sh; cd local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello; docker run -w /workingDir -v /Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello/var/folders/c4/_dbcj9012gl6lbjsvd22_m21hj9ndk/T:/Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello/var/folders/c4/_dbcj9012gl6lbjsvd22_m21hj9ndk/T:ro -v /Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello:/outputDir --rm ubuntu/latest ; echo /Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello/var/folders/c4/_dbcj9012gl6lbjsvd22_m21hj9ndk/T/testFile977065604273058878.out; echo $? > rc"" did not contain ""/tmp:"" (HtCondorJobExecutionActorSpec.scala:261); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230490075
https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230490075:855,Availability,echo,echo,855,"@francares the result when run on a mac:. ```; ""#!/bin/sh; cd local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello; docker run -w /workingDir -v /Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello/var/folders/c4/_dbcj9012gl6lbjsvd22_m21hj9ndk/T:/Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello/var/folders/c4/_dbcj9012gl6lbjsvd22_m21hj9ndk/T:ro -v /Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello:/outputDir --rm ubuntu/latest ; echo /Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello/var/folders/c4/_dbcj9012gl6lbjsvd22_m21hj9ndk/T/testFile977065604273058878.out; echo $? > rc"" did not contain ""/tmp:"" (HtCondorJobExecutionActorSpec.scala:261); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230490075
https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230491411:130,Testability,test,test,130,@francares @mcovarr I've modified this (fairly bluntly however) so that it passes on mac. If you have a suggestion for a stronger test case let me know,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230491411
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230512806:33,Testability,log,logic,33,"(Specifically, this is the retry logic which supports the preemptible VM retries)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230512806
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714:862,Availability,recover,recover,862,"@pgrosu It's in our internal space, however I can give you the gist. We're doing a few things at once ...; - Make workflow submission async. Submitted workflows go into a new store and the WorkflowManagerActor can pull them as necessary. Within the store they'll be marked as either Submitted, Running or Restartable. The latter is a state which is assigned to any workflow in Running state when the system comes online; - The EngineJobExecutionActor (EJEA above) sits between the WorkflowExecutionActor and the BackendJobExecutionActor, and will manage engine-side knowledge in a persisted store. The combination of this and the above will allow us to bring back what we call the 'restart' functionality - i.e. pick up a running workflow from the engine side but not reattach to running backend jobs; - Less hashed out at the moment, if a backend will support 'recover' functionality (attaching to the backend jobs, we'll implement this in as many of our own backends as we can), the backend will need to manage its own information, e.g. using the KV store",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714:862,Safety,recover,recover,862,"@pgrosu It's in our internal space, however I can give you the gist. We're doing a few things at once ...; - Make workflow submission async. Submitted workflows go into a new store and the WorkflowManagerActor can pull them as necessary. Within the store they'll be marked as either Submitted, Running or Restartable. The latter is a state which is assigned to any workflow in Running state when the system comes online; - The EngineJobExecutionActor (EJEA above) sits between the WorkflowExecutionActor and the BackendJobExecutionActor, and will manage engine-side knowledge in a persisted store. The combination of this and the above will allow us to bring back what we call the 'restart' functionality - i.e. pick up a running workflow from the engine side but not reattach to running backend jobs; - Less hashed out at the moment, if a backend will support 'recover' functionality (attaching to the backend jobs, we'll implement this in as many of our own backends as we can), the backend will need to manage its own information, e.g. using the KV store",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714:809,Security,hash,hashed,809,"@pgrosu It's in our internal space, however I can give you the gist. We're doing a few things at once ...; - Make workflow submission async. Submitted workflows go into a new store and the WorkflowManagerActor can pull them as necessary. Within the store they'll be marked as either Submitted, Running or Restartable. The latter is a state which is assigned to any workflow in Running state when the system comes online; - The EngineJobExecutionActor (EJEA above) sits between the WorkflowExecutionActor and the BackendJobExecutionActor, and will manage engine-side knowledge in a persisted store. The combination of this and the above will allow us to bring back what we call the 'restart' functionality - i.e. pick up a running workflow from the engine side but not reattach to running backend jobs; - Less hashed out at the moment, if a backend will support 'recover' functionality (attaching to the backend jobs, we'll implement this in as many of our own backends as we can), the backend will need to manage its own information, e.g. using the KV store",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1335,Availability,recover,recovery,1335,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1483,Availability,recover,recovery,1483,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1239,Energy Efficiency,schedul,scheduling,1239,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1197,Integrability,depend,dependencies,1197,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1394,Integrability,protocol,protocol,1394,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1994,Integrability,depend,depending,1994,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1723,Modifiability,sandbox,sandbox,1723,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1142,Performance,queue,queue,1142,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1335,Safety,recover,recovery,1335,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1483,Safety,recover,recovery,1483,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371
https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1723,Testability,sandbox,sandbox,1723,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371
https://github.com/broadinstitute/cromwell/issues/1119#issuecomment-239529170:0,Integrability,Depend,Depends,0,Depends on #1280,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1119#issuecomment-239529170
https://github.com/broadinstitute/cromwell/issues/1120#issuecomment-232426738:170,Modifiability,config,config,170,Comments from the FireCloud/GOTC beta test experience. > Workflow options file key change: Had to change defaultRuntimeOptions to default_runtime_attributes.; > Cromwell config typo: Had to change JesBackendLifecycleFactory to JesBackendLifecycleActorFactory.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1120#issuecomment-232426738
https://github.com/broadinstitute/cromwell/issues/1120#issuecomment-232426738:38,Testability,test,test,38,Comments from the FireCloud/GOTC beta test experience. > Workflow options file key change: Had to change defaultRuntimeOptions to default_runtime_attributes.; > Cromwell config typo: Had to change JesBackendLifecycleFactory to JesBackendLifecycleActorFactory.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1120#issuecomment-232426738
https://github.com/broadinstitute/cromwell/issues/1121#issuecomment-230542274:0,Integrability,Depend,Depends,0,Depends on #1115,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1121#issuecomment-230542274
https://github.com/broadinstitute/cromwell/issues/1123#issuecomment-230548868:0,Integrability,Depend,Depends,0,Depends on #1116,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1123#issuecomment-230548868
https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453:1415,Availability,alive,alive,1415,"More info on the current issue:; - The local backend, running on the Config backend, supports running commands with or without docker.; - However the Config backend only allows `run-in-background` to be set for _both_ with and without docker.; - When `run-in-background` is set to true, the process id of the backgrounded process is stored for later killing.; - One cannot halt a docker container using [just `kill`](https://www.fpcomplete.com/blog/2016/10/docker-demons-pid1-orphans-zombies-signals). This gets ignored by the linux kernal and leaves the container running:; > If you must send a SIGKILL to your process, use the docker kill command instead; - `docker kill` takes in the container id, not a pid.; - `docker run -d` outputs the container id, detaching the process.; - For this detached docker, one should set `run-in-background = false`, the opposite of Local without docker. Workaround-- make the runtime attribute ""docker"" mandatory and run docker in detached mode:. ```; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = false. runtime-attributes = """"""; String docker; String? docker_user; """""". submit-docker = """"""; docker run \; -d --rm \; ${""--user "" + docker_user} \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash -c '/bin/bash ${script} > ${out} 2> ${err} < /dev/null'; """""". kill = ""docker kill ${job_id}""; check-alive = ""docker ps ${job_id}""; job-id-regex = ""([0-9a-f]+)""; }; }; ```. Actual fix-- Refactor the config backend, better separating the docker vs. non-docker config. Only `submit` vs. `submit-docker` are differentiated currently, but `run-in-background`, `job-id-regex`, `kill`, etc. are all different for docker vs. non-docker. **TL;DR: The local backend should run the docker container in the background, capture the docker container id from stdout, and later issue a `docker kill <container id>`.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453
https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453:69,Modifiability,Config,Config,69,"More info on the current issue:; - The local backend, running on the Config backend, supports running commands with or without docker.; - However the Config backend only allows `run-in-background` to be set for _both_ with and without docker.; - When `run-in-background` is set to true, the process id of the backgrounded process is stored for later killing.; - One cannot halt a docker container using [just `kill`](https://www.fpcomplete.com/blog/2016/10/docker-demons-pid1-orphans-zombies-signals). This gets ignored by the linux kernal and leaves the container running:; > If you must send a SIGKILL to your process, use the docker kill command instead; - `docker kill` takes in the container id, not a pid.; - `docker run -d` outputs the container id, detaching the process.; - For this detached docker, one should set `run-in-background = false`, the opposite of Local without docker. Workaround-- make the runtime attribute ""docker"" mandatory and run docker in detached mode:. ```; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = false. runtime-attributes = """"""; String docker; String? docker_user; """""". submit-docker = """"""; docker run \; -d --rm \; ${""--user "" + docker_user} \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash -c '/bin/bash ${script} > ${out} 2> ${err} < /dev/null'; """""". kill = ""docker kill ${job_id}""; check-alive = ""docker ps ${job_id}""; job-id-regex = ""([0-9a-f]+)""; }; }; ```. Actual fix-- Refactor the config backend, better separating the docker vs. non-docker config. Only `submit` vs. `submit-docker` are differentiated currently, but `run-in-background`, `job-id-regex`, `kill`, etc. are all different for docker vs. non-docker. **TL;DR: The local backend should run the docker container in the background, capture the docker container id from stdout, and later issue a `docker kill <container id>`.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453
https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453:150,Modifiability,Config,Config,150,"More info on the current issue:; - The local backend, running on the Config backend, supports running commands with or without docker.; - However the Config backend only allows `run-in-background` to be set for _both_ with and without docker.; - When `run-in-background` is set to true, the process id of the backgrounded process is stored for later killing.; - One cannot halt a docker container using [just `kill`](https://www.fpcomplete.com/blog/2016/10/docker-demons-pid1-orphans-zombies-signals). This gets ignored by the linux kernal and leaves the container running:; > If you must send a SIGKILL to your process, use the docker kill command instead; - `docker kill` takes in the container id, not a pid.; - `docker run -d` outputs the container id, detaching the process.; - For this detached docker, one should set `run-in-background = false`, the opposite of Local without docker. Workaround-- make the runtime attribute ""docker"" mandatory and run docker in detached mode:. ```; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = false. runtime-attributes = """"""; String docker; String? docker_user; """""". submit-docker = """"""; docker run \; -d --rm \; ${""--user "" + docker_user} \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash -c '/bin/bash ${script} > ${out} 2> ${err} < /dev/null'; """""". kill = ""docker kill ${job_id}""; check-alive = ""docker ps ${job_id}""; job-id-regex = ""([0-9a-f]+)""; }; }; ```. Actual fix-- Refactor the config backend, better separating the docker vs. non-docker config. Only `submit` vs. `submit-docker` are differentiated currently, but `run-in-background`, `job-id-regex`, `kill`, etc. are all different for docker vs. non-docker. **TL;DR: The local backend should run the docker container in the background, capture the docker container id from stdout, and later issue a `docker kill <container id>`.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453
https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453:1041,Modifiability,config,config,1041,"More info on the current issue:; - The local backend, running on the Config backend, supports running commands with or without docker.; - However the Config backend only allows `run-in-background` to be set for _both_ with and without docker.; - When `run-in-background` is set to true, the process id of the backgrounded process is stored for later killing.; - One cannot halt a docker container using [just `kill`](https://www.fpcomplete.com/blog/2016/10/docker-demons-pid1-orphans-zombies-signals). This gets ignored by the linux kernal and leaves the container running:; > If you must send a SIGKILL to your process, use the docker kill command instead; - `docker kill` takes in the container id, not a pid.; - `docker run -d` outputs the container id, detaching the process.; - For this detached docker, one should set `run-in-background = false`, the opposite of Local without docker. Workaround-- make the runtime attribute ""docker"" mandatory and run docker in detached mode:. ```; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = false. runtime-attributes = """"""; String docker; String? docker_user; """""". submit-docker = """"""; docker run \; -d --rm \; ${""--user "" + docker_user} \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash -c '/bin/bash ${script} > ${out} 2> ${err} < /dev/null'; """""". kill = ""docker kill ${job_id}""; check-alive = ""docker ps ${job_id}""; job-id-regex = ""([0-9a-f]+)""; }; }; ```. Actual fix-- Refactor the config backend, better separating the docker vs. non-docker config. Only `submit` vs. `submit-docker` are differentiated currently, but `run-in-background`, `job-id-regex`, `kill`, etc. are all different for docker vs. non-docker. **TL;DR: The local backend should run the docker container in the background, capture the docker container id from stdout, and later issue a `docker kill <container id>`.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453
https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453:1048,Modifiability,Config,ConfigBackendLifecycleActorFactory,1048,"More info on the current issue:; - The local backend, running on the Config backend, supports running commands with or without docker.; - However the Config backend only allows `run-in-background` to be set for _both_ with and without docker.; - When `run-in-background` is set to true, the process id of the backgrounded process is stored for later killing.; - One cannot halt a docker container using [just `kill`](https://www.fpcomplete.com/blog/2016/10/docker-demons-pid1-orphans-zombies-signals). This gets ignored by the linux kernal and leaves the container running:; > If you must send a SIGKILL to your process, use the docker kill command instead; - `docker kill` takes in the container id, not a pid.; - `docker run -d` outputs the container id, detaching the process.; - For this detached docker, one should set `run-in-background = false`, the opposite of Local without docker. Workaround-- make the runtime attribute ""docker"" mandatory and run docker in detached mode:. ```; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = false. runtime-attributes = """"""; String docker; String? docker_user; """""". submit-docker = """"""; docker run \; -d --rm \; ${""--user "" + docker_user} \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash -c '/bin/bash ${script} > ${out} 2> ${err} < /dev/null'; """""". kill = ""docker kill ${job_id}""; check-alive = ""docker ps ${job_id}""; job-id-regex = ""([0-9a-f]+)""; }; }; ```. Actual fix-- Refactor the config backend, better separating the docker vs. non-docker config. Only `submit` vs. `submit-docker` are differentiated currently, but `run-in-background`, `job-id-regex`, `kill`, etc. are all different for docker vs. non-docker. **TL;DR: The local backend should run the docker container in the background, capture the docker container id from stdout, and later issue a `docker kill <container id>`.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453
https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453:1085,Modifiability,config,config,1085,"More info on the current issue:; - The local backend, running on the Config backend, supports running commands with or without docker.; - However the Config backend only allows `run-in-background` to be set for _both_ with and without docker.; - When `run-in-background` is set to true, the process id of the backgrounded process is stored for later killing.; - One cannot halt a docker container using [just `kill`](https://www.fpcomplete.com/blog/2016/10/docker-demons-pid1-orphans-zombies-signals). This gets ignored by the linux kernal and leaves the container running:; > If you must send a SIGKILL to your process, use the docker kill command instead; - `docker kill` takes in the container id, not a pid.; - `docker run -d` outputs the container id, detaching the process.; - For this detached docker, one should set `run-in-background = false`, the opposite of Local without docker. Workaround-- make the runtime attribute ""docker"" mandatory and run docker in detached mode:. ```; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = false. runtime-attributes = """"""; String docker; String? docker_user; """""". submit-docker = """"""; docker run \; -d --rm \; ${""--user "" + docker_user} \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash -c '/bin/bash ${script} > ${out} 2> ${err} < /dev/null'; """""". kill = ""docker kill ${job_id}""; check-alive = ""docker ps ${job_id}""; job-id-regex = ""([0-9a-f]+)""; }; }; ```. Actual fix-- Refactor the config backend, better separating the docker vs. non-docker config. Only `submit` vs. `submit-docker` are differentiated currently, but `run-in-background`, `job-id-regex`, `kill`, etc. are all different for docker vs. non-docker. **TL;DR: The local backend should run the docker container in the background, capture the docker container id from stdout, and later issue a `docker kill <container id>`.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453
https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453:1500,Modifiability,Refactor,Refactor,1500,"More info on the current issue:; - The local backend, running on the Config backend, supports running commands with or without docker.; - However the Config backend only allows `run-in-background` to be set for _both_ with and without docker.; - When `run-in-background` is set to true, the process id of the backgrounded process is stored for later killing.; - One cannot halt a docker container using [just `kill`](https://www.fpcomplete.com/blog/2016/10/docker-demons-pid1-orphans-zombies-signals). This gets ignored by the linux kernal and leaves the container running:; > If you must send a SIGKILL to your process, use the docker kill command instead; - `docker kill` takes in the container id, not a pid.; - `docker run -d` outputs the container id, detaching the process.; - For this detached docker, one should set `run-in-background = false`, the opposite of Local without docker. Workaround-- make the runtime attribute ""docker"" mandatory and run docker in detached mode:. ```; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = false. runtime-attributes = """"""; String docker; String? docker_user; """""". submit-docker = """"""; docker run \; -d --rm \; ${""--user "" + docker_user} \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash -c '/bin/bash ${script} > ${out} 2> ${err} < /dev/null'; """""". kill = ""docker kill ${job_id}""; check-alive = ""docker ps ${job_id}""; job-id-regex = ""([0-9a-f]+)""; }; }; ```. Actual fix-- Refactor the config backend, better separating the docker vs. non-docker config. Only `submit` vs. `submit-docker` are differentiated currently, but `run-in-background`, `job-id-regex`, `kill`, etc. are all different for docker vs. non-docker. **TL;DR: The local backend should run the docker container in the background, capture the docker container id from stdout, and later issue a `docker kill <container id>`.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453
https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453:1513,Modifiability,config,config,1513,"More info on the current issue:; - The local backend, running on the Config backend, supports running commands with or without docker.; - However the Config backend only allows `run-in-background` to be set for _both_ with and without docker.; - When `run-in-background` is set to true, the process id of the backgrounded process is stored for later killing.; - One cannot halt a docker container using [just `kill`](https://www.fpcomplete.com/blog/2016/10/docker-demons-pid1-orphans-zombies-signals). This gets ignored by the linux kernal and leaves the container running:; > If you must send a SIGKILL to your process, use the docker kill command instead; - `docker kill` takes in the container id, not a pid.; - `docker run -d` outputs the container id, detaching the process.; - For this detached docker, one should set `run-in-background = false`, the opposite of Local without docker. Workaround-- make the runtime attribute ""docker"" mandatory and run docker in detached mode:. ```; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = false. runtime-attributes = """"""; String docker; String? docker_user; """""". submit-docker = """"""; docker run \; -d --rm \; ${""--user "" + docker_user} \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash -c '/bin/bash ${script} > ${out} 2> ${err} < /dev/null'; """""". kill = ""docker kill ${job_id}""; check-alive = ""docker ps ${job_id}""; job-id-regex = ""([0-9a-f]+)""; }; }; ```. Actual fix-- Refactor the config backend, better separating the docker vs. non-docker config. Only `submit` vs. `submit-docker` are differentiated currently, but `run-in-background`, `job-id-regex`, `kill`, etc. are all different for docker vs. non-docker. **TL;DR: The local backend should run the docker container in the background, capture the docker container id from stdout, and later issue a `docker kill <container id>`.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453
https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453:1573,Modifiability,config,config,1573,"More info on the current issue:; - The local backend, running on the Config backend, supports running commands with or without docker.; - However the Config backend only allows `run-in-background` to be set for _both_ with and without docker.; - When `run-in-background` is set to true, the process id of the backgrounded process is stored for later killing.; - One cannot halt a docker container using [just `kill`](https://www.fpcomplete.com/blog/2016/10/docker-demons-pid1-orphans-zombies-signals). This gets ignored by the linux kernal and leaves the container running:; > If you must send a SIGKILL to your process, use the docker kill command instead; - `docker kill` takes in the container id, not a pid.; - `docker run -d` outputs the container id, detaching the process.; - For this detached docker, one should set `run-in-background = false`, the opposite of Local without docker. Workaround-- make the runtime attribute ""docker"" mandatory and run docker in detached mode:. ```; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = false. runtime-attributes = """"""; String docker; String? docker_user; """""". submit-docker = """"""; docker run \; -d --rm \; ${""--user "" + docker_user} \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash -c '/bin/bash ${script} > ${out} 2> ${err} < /dev/null'; """""". kill = ""docker kill ${job_id}""; check-alive = ""docker ps ${job_id}""; job-id-regex = ""([0-9a-f]+)""; }; }; ```. Actual fix-- Refactor the config backend, better separating the docker vs. non-docker config. Only `submit` vs. `submit-docker` are differentiated currently, but `run-in-background`, `job-id-regex`, `kill`, etc. are all different for docker vs. non-docker. **TL;DR: The local backend should run the docker container in the background, capture the docker container id from stdout, and later issue a `docker kill <container id>`.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453
https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-323854577:64,Modifiability,refactor,refactoring,64,@kshakir do you have an idea of the effort for your Actual Fix: refactoring the config backend to separate out the Docker vs. Non-docker config?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-323854577
https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-323854577:80,Modifiability,config,config,80,@kshakir do you have an idea of the effort for your Actual Fix: refactoring the config backend to separate out the Docker vs. Non-docker config?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-323854577
https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-323854577:137,Modifiability,config,config,137,@kshakir do you have an idea of the effort for your Actual Fix: refactoring the config backend to separate out the Docker vs. Non-docker config?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-323854577
https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230785869:28,Modifiability,config,config,28,"- Still need to add the new config options to the README; - Something in the tests is broken, still working on that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230785869
https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230785869:77,Testability,test,tests,77,"- Still need to add the new config options to the README; - Something in the tests is broken, still working on that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230785869
https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230829446:116,Performance,perform,performant,116,"As per a convo with @mcovarr I'm going to switch out the mutable map for an immutable list structure. It'll be less performant but a lot cleaner, and it'd be easy to switch out if performance every did become an issue here. To paraphrase Miguel, something else likely will have blown up in Cromwell prior to performance being an issue here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230829446
https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230829446:180,Performance,perform,performance,180,"As per a convo with @mcovarr I'm going to switch out the mutable map for an immutable list structure. It'll be less performant but a lot cleaner, and it'd be easy to switch out if performance every did become an issue here. To paraphrase Miguel, something else likely will have blown up in Cromwell prior to performance being an issue here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230829446
https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230829446:308,Performance,perform,performance,308,"As per a convo with @mcovarr I'm going to switch out the mutable map for an immutable list structure. It'll be less performant but a lot cleaner, and it'd be easy to switch out if performance every did become an issue here. To paraphrase Miguel, something else likely will have blown up in Cromwell prior to performance being an issue here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230829446
https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230930417:190,Testability,test,tests,190,"@mcovarr @cjllanwarne This PR is now more-or-less in the state it was this morning, albeit with a lot of stuff changed as per your request. It's a lot different now so worth a re-look (unit tests are still borked, and this time it's sans centaur-ing as well, but eh)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230930417
https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-231114604:54,Energy Efficiency,green,green,54,:+1: reaffirmed though it would be really nice to see green builds before merge 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-231114604
https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-230869079:38,Availability,failure,failure,38,what was the scenario that caused the failure (so we can reproduce)? bad credential for a remote db? botched config?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-230869079
https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-230869079:109,Modifiability,config,config,109,what was the scenario that caused the failure (so we can reproduce)? bad credential for a remote db? botched config?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-230869079
https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-231123371:50,Security,firewall,firewall,50,"Here's another repro. We had a new database whose firewall wasn't set up to let this particular Cromwell instance talk to it. Cromwell starts up and appears to be running but there isn't any mention in the log that there's a problem - and we don't get to the ""Running with database"" <connection string> log line. It just quietly fails.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-231123371
https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-231123371:206,Testability,log,log,206,"Here's another repro. We had a new database whose firewall wasn't set up to let this particular Cromwell instance talk to it. Cromwell starts up and appears to be running but there isn't any mention in the log that there's a problem - and we don't get to the ""Running with database"" <connection string> log line. It just quietly fails.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-231123371
https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-231123371:303,Testability,log,log,303,"Here's another repro. We had a new database whose firewall wasn't set up to let this particular Cromwell instance talk to it. Cromwell starts up and appears to be running but there isn't any mention in the log that there's a problem - and we don't get to the ""Running with database"" <connection string> log line. It just quietly fails.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-231123371
https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-230914991:15,Testability,test,test,15,:+1: very nice test!. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1130/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-230914991
https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-230923488:37,Availability,failure,failure,37,"👍 from me. If you want, I can re-add failure retry support into my PR and then one of us can revisit it on the EJEA side. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1130/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-230923488
https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-231509284:14,Availability,ping,pinging,14,@Horneth Just pinging to see what the status is on this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-231509284
https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-230915259:67,Deployability,configurat,configurations,67,"The motivation for this is that I would like FireCloud/GAWB method configurations to include brief descriptions of declared input and output parameters. It would be great if these description fields could be initialized by metadata contained within a workflow's WDL file. I was thinking we could leverage the parameter_meta sections, but currently that section is only supported in the task definition block, and only for input parameters. I don't, however, want to request a specific solution....the use and expansion of the parameter_meta section may not be the answer. My primary goal is to be able to initialize the input/output parameter descriptions with strings drawn from the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-230915259
https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-230915259:67,Modifiability,config,configurations,67,"The motivation for this is that I would like FireCloud/GAWB method configurations to include brief descriptions of declared input and output parameters. It would be great if these description fields could be initialized by metadata contained within a workflow's WDL file. I was thinking we could leverage the parameter_meta sections, but currently that section is only supported in the task definition block, and only for input parameters. I don't, however, want to request a specific solution....the use and expansion of the parameter_meta section may not be the answer. My primary goal is to be able to initialize the input/output parameter descriptions with strings drawn from the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-230915259
https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-276483565:119,Safety,safe,safely,119,"You can now supply `meta` blocks at a workflow level, and those can contain outputs as well as inputs so I think I can safely close this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-276483565
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660:365,Availability,avail,available,365,"@geoffjentry ; **Prerequisite**: We have Spark cluster (3 nodes = 1 master, rest worker including master), Docker on all three nodes, Hdfs file system (3 nodes = 1 Namenode, rest Data nodes including NN) Spark app build locally in Docker repository. This app (spark_hdfs.jar) has two main entry points 1) Word count 2) Vowel count as main class. App consumes input available on Hdfs (assumption pre-loaded) and produces output to Hdfs. . **WDL:**. ```; task spark {; command {; /opt/spark/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.intel.spark.poc.nfs.SparkVowelLine --master spark://10.0.1.22:7077 /app/spark_hdfs.jar hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output; }. output {; File empty = stdout(); }. runtime {; docker: ""sparkapp""; }. }. workflow test {; call spark; }. ```. So the app is available inside the docker container that has base image with Spark environment(i.e. Spark driver + hadoop connector) that will connect to Spark master on host machine within the cluster to submit job, reads input from hdfs file system and turn them into RDDs and distribute the work to workers with the help of master and at the end write output to hdfs. . Following are the arguments to the app ; `hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660:858,Availability,avail,available,858,"@geoffjentry ; **Prerequisite**: We have Spark cluster (3 nodes = 1 master, rest worker including master), Docker on all three nodes, Hdfs file system (3 nodes = 1 Namenode, rest Data nodes including NN) Spark app build locally in Docker repository. This app (spark_hdfs.jar) has two main entry points 1) Word count 2) Vowel count as main class. App consumes input available on Hdfs (assumption pre-loaded) and produces output to Hdfs. . **WDL:**. ```; task spark {; command {; /opt/spark/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.intel.spark.poc.nfs.SparkVowelLine --master spark://10.0.1.22:7077 /app/spark_hdfs.jar hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output; }. output {; File empty = stdout(); }. runtime {; docker: ""sparkapp""; }. }. workflow test {; call spark; }. ```. So the app is available inside the docker container that has base image with Spark environment(i.e. Spark driver + hadoop connector) that will connect to Spark master on host machine within the cluster to submit job, reads input from hdfs file system and turn them into RDDs and distribute the work to workers with the help of master and at the end write output to hdfs. . Following are the arguments to the app ; `hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660:399,Performance,load,loaded,399,"@geoffjentry ; **Prerequisite**: We have Spark cluster (3 nodes = 1 master, rest worker including master), Docker on all three nodes, Hdfs file system (3 nodes = 1 Namenode, rest Data nodes including NN) Spark app build locally in Docker repository. This app (spark_hdfs.jar) has two main entry points 1) Word count 2) Vowel count as main class. App consumes input available on Hdfs (assumption pre-loaded) and produces output to Hdfs. . **WDL:**. ```; task spark {; command {; /opt/spark/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.intel.spark.poc.nfs.SparkVowelLine --master spark://10.0.1.22:7077 /app/spark_hdfs.jar hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output; }. output {; File empty = stdout(); }. runtime {; docker: ""sparkapp""; }. }. workflow test {; call spark; }. ```. So the app is available inside the docker container that has base image with Spark environment(i.e. Spark driver + hadoop connector) that will connect to Spark master on host machine within the cluster to submit job, reads input from hdfs file system and turn them into RDDs and distribute the work to workers with the help of master and at the end write output to hdfs. . Following are the arguments to the app ; `hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660:668,Testability,test,test,668,"@geoffjentry ; **Prerequisite**: We have Spark cluster (3 nodes = 1 master, rest worker including master), Docker on all three nodes, Hdfs file system (3 nodes = 1 Namenode, rest Data nodes including NN) Spark app build locally in Docker repository. This app (spark_hdfs.jar) has two main entry points 1) Word count 2) Vowel count as main class. App consumes input available on Hdfs (assumption pre-loaded) and produces output to Hdfs. . **WDL:**. ```; task spark {; command {; /opt/spark/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.intel.spark.poc.nfs.SparkVowelLine --master spark://10.0.1.22:7077 /app/spark_hdfs.jar hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output; }. output {; File empty = stdout(); }. runtime {; docker: ""sparkapp""; }. }. workflow test {; call spark; }. ```. So the app is available inside the docker container that has base image with Spark environment(i.e. Spark driver + hadoop connector) that will connect to Spark master on host machine within the cluster to submit job, reads input from hdfs file system and turn them into RDDs and distribute the work to workers with the help of master and at the end write output to hdfs. . Following are the arguments to the app ; `hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660:816,Testability,test,test,816,"@geoffjentry ; **Prerequisite**: We have Spark cluster (3 nodes = 1 master, rest worker including master), Docker on all three nodes, Hdfs file system (3 nodes = 1 Namenode, rest Data nodes including NN) Spark app build locally in Docker repository. This app (spark_hdfs.jar) has two main entry points 1) Word count 2) Vowel count as main class. App consumes input available on Hdfs (assumption pre-loaded) and produces output to Hdfs. . **WDL:**. ```; task spark {; command {; /opt/spark/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.intel.spark.poc.nfs.SparkVowelLine --master spark://10.0.1.22:7077 /app/spark_hdfs.jar hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output; }. output {; File empty = stdout(); }. runtime {; docker: ""sparkapp""; }. }. workflow test {; call spark; }. ```. So the app is available inside the docker container that has base image with Spark environment(i.e. Spark driver + hadoop connector) that will connect to Spark master on host machine within the cluster to submit job, reads input from hdfs file system and turn them into RDDs and distribute the work to workers with the help of master and at the end write output to hdfs. . Following are the arguments to the app ; `hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660:1296,Testability,test,test,1296,"@geoffjentry ; **Prerequisite**: We have Spark cluster (3 nodes = 1 master, rest worker including master), Docker on all three nodes, Hdfs file system (3 nodes = 1 Namenode, rest Data nodes including NN) Spark app build locally in Docker repository. This app (spark_hdfs.jar) has two main entry points 1) Word count 2) Vowel count as main class. App consumes input available on Hdfs (assumption pre-loaded) and produces output to Hdfs. . **WDL:**. ```; task spark {; command {; /opt/spark/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.intel.spark.poc.nfs.SparkVowelLine --master spark://10.0.1.22:7077 /app/spark_hdfs.jar hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output; }. output {; File empty = stdout(); }. runtime {; docker: ""sparkapp""; }. }. workflow test {; call spark; }. ```. So the app is available inside the docker container that has base image with Spark environment(i.e. Spark driver + hadoop connector) that will connect to Spark master on host machine within the cluster to submit job, reads input from hdfs file system and turn them into RDDs and distribute the work to workers with the help of master and at the end write output to hdfs. . Following are the arguments to the app ; `hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340:223,Modifiability,variab,variable,223,"hey @jainh -- Building on @geoffjentry 's question, and after reading through the code here... I'm not sure what about this PR is actually Spark'ified. It seems like a copy and paste of the HtCondor backend (including some variable names!) and then using that to run a command inside a docker to connect to a pre-existing spark cluster with preloaded data. All this could already be done using any one of the docker-based back ends (e.g. Local or even Google). Am I missing something here? If not, it would be great to see more of a roadmap about where you see this going because it's not clear from the code here what direction it could take. The concept of running/orchestrating Spark jobs via Cromwell is an important one and could take many, many forms so clarifying which approach this is taking (and making that clear in naming/location/etc) is important. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340:589,Usability,clear,clear,589,"hey @jainh -- Building on @geoffjentry 's question, and after reading through the code here... I'm not sure what about this PR is actually Spark'ified. It seems like a copy and paste of the HtCondor backend (including some variable names!) and then using that to run a command inside a docker to connect to a pre-existing spark cluster with preloaded data. All this could already be done using any one of the docker-based back ends (e.g. Local or even Google). Am I missing something here? If not, it would be great to see more of a roadmap about where you see this going because it's not clear from the code here what direction it could take. The concept of running/orchestrating Spark jobs via Cromwell is an important one and could take many, many forms so clarifying which approach this is taking (and making that clear in naming/location/etc) is important. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340:818,Usability,clear,clear,818,"hey @jainh -- Building on @geoffjentry 's question, and after reading through the code here... I'm not sure what about this PR is actually Spark'ified. It seems like a copy and paste of the HtCondor backend (including some variable names!) and then using that to run a command inside a docker to connect to a pre-existing spark cluster with preloaded data. All this could already be done using any one of the docker-based back ends (e.g. Local or even Google). Am I missing something here? If not, it would be great to see more of a roadmap about where you see this going because it's not clear from the code here what direction it could take. The concept of running/orchestrating Spark jobs via Cromwell is an important one and could take many, many forms so clarifying which approach this is taking (and making that clear in naming/location/etc) is important. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231182019:267,Deployability,configurat,configuration,267,"@kcibul ; I am missing few things in the implementation like the way to construct Spark command inside Spark backend for this PR, Let me add that to make it Spark'ified. It should be same like Htcondor but entertain Runtime attributes like executor-memory, cores and configuration based master to execute Spark job both in Dockerized or non-Dockerized mode. ; So let me close this pull request (**I will not merge it** ) and create a new one after changes. My goal is to create Spark backend to run Spark jobs in standalone cluster mode for this PR, then later to add other Resource manager related support. ; Does it make sense ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231182019
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231182019:267,Modifiability,config,configuration,267,"@kcibul ; I am missing few things in the implementation like the way to construct Spark command inside Spark backend for this PR, Let me add that to make it Spark'ified. It should be same like Htcondor but entertain Runtime attributes like executor-memory, cores and configuration based master to execute Spark job both in Dockerized or non-Dockerized mode. ; So let me close this pull request (**I will not merge it** ) and create a new one after changes. My goal is to create Spark backend to run Spark jobs in standalone cluster mode for this PR, then later to add other Resource manager related support. ; Does it make sense ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231182019
https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231094276:7,Energy Efficiency,green,green,7,I know green requested this so they'll be happy but should also let blue know as well,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231094276
https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231123975:46,Deployability,hotfix,hotfix,46,"Per @vivster7 the bug report was against 0.19 hotfix, not develop. Not that this shouldn't be fixed here too. 😄 . @kcibul It seems #794 is also labeled 0.21 but actually applies to 0.19 hotfix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231123975
https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231123975:186,Deployability,hotfix,hotfix,186,"Per @vivster7 the bug report was against 0.19 hotfix, not develop. Not that this shouldn't be fixed here too. 😄 . @kcibul It seems #794 is also labeled 0.21 but actually applies to 0.19 hotfix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231123975
https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231140664:67,Deployability,hotfix,hotfix,67,"^ I would add that its not terribly important that it get fixed in hotfix, if thats what you're suggesting. Just that it gets fixed in some future version of cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231140664
https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231374697:44,Safety,avoid,avoid,44,"@cjllanwarne Firecloud needs it, this is to avoid cromwell to always require a google auth (even when running only local backend).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231374697
https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231378728:29,Modifiability,config,config,29,"really clear comments in the config file, nice",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231378728
https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231378728:7,Usability,clear,clear,7,"really clear comments in the config file, nice",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231378728
https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231626677:155,Safety,avoid,avoid,155,"@yfarjoun -- can you give a little more context on this? These jobs will just fail upon localization right? or does something else happen that you want to avoid?. Completing this sentence ""this is important because ..."" would be a good formula!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231626677
https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231627459:47,Security,validat,validation,47,The former. He was looking for a backend-aware validation type behavior,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231627459
https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231711585:39,Availability,error,error,39,"Actually, the main problem is that the error is so cryptic one cannot tell; that a file is causing the problem, nor which file it is, even if one had; an inkling that it's a missing file problem. so I have to resort to divide; and conquer in order to identify the missing file...and that's a pain. On Sun, Jul 10, 2016 at 10:08 PM, Jeff Gentry notifications@github.com; wrote:. > The former. He was looking for a backend-aware validation type behavior; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231627459,; > or mute the thread; > https://github.com/notifications/unsubscribe/ACnk0hYeQnaJcsNEVJlnxrz9-tA880TLks5qUaWXgaJpZM4JHehH; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231711585
https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231711585:427,Security,validat,validation,427,"Actually, the main problem is that the error is so cryptic one cannot tell; that a file is causing the problem, nor which file it is, even if one had; an inkling that it's a missing file problem. so I have to resort to divide; and conquer in order to identify the missing file...and that's a pain. On Sun, Jul 10, 2016 at 10:08 PM, Jeff Gentry notifications@github.com; wrote:. > The former. He was looking for a backend-aware validation type behavior; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231627459,; > or mute the thread; > https://github.com/notifications/unsubscribe/ACnk0hYeQnaJcsNEVJlnxrz9-tA880TLks5qUaWXgaJpZM4JHehH; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231711585
https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231715169:9,Availability,error,error,9,IIRC the error in question was passing the 403 Forbidden back from Google,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231715169
https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231720082:195,Availability,error,error,195,"Indeed. but the problem was that we couldn't tell that a file was missing; and indeed which file it was. On Mon, Jul 11, 2016 at 8:03 AM, Jeff Gentry notifications@github.com; wrote:. > IIRC the error in question was passing the 403 Forbidden back from Google; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231715169,; > or mute the thread; > https://github.com/notifications/unsubscribe/ACnk0ps25RuScsJmjoD9M1qUPteP2aLqks5qUjD_gaJpZM4JHehH; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231720082
https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231720259:363,Availability,error,error,363,"...so if the 403 comes back with that information, that's great!. On Mon, Jul 11, 2016 at 8:27 AM, Yossi Farjoun farjoun@broadinstitute.org; wrote:. > Indeed. but the problem was that we couldn't tell that a file was missing; > and indeed which file it was.; > ; > On Mon, Jul 11, 2016 at 8:03 AM, Jeff Gentry notifications@github.com; > wrote:; > ; > > IIRC the error in question was passing the 403 Forbidden back from Google; > > ; > > —; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231715169,; > > or mute the thread; > > https://github.com/notifications/unsubscribe/ACnk0ps25RuScsJmjoD9M1qUPteP2aLqks5qUjD_gaJpZM4JHehH; > > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231720259
https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-276483224:18,Availability,error,error,18,Closing since the error message now contains (a) that a file was missing (b) the appropriate file name,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-276483224
https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-276483224:24,Integrability,message,message,24,Closing since the error message now contains (a) that a file was missing (b) the appropriate file name,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-276483224
https://github.com/broadinstitute/cromwell/issues/1139#issuecomment-323855111:83,Safety,abort,abort,83,"@kcibul From what I understand, it is still a frequent problem that P.API does not abort workflows even though Cromwell asked. . @geoffjentry do you have an idea about the effort involved to make this fix? . @abaumann Do we have any data (from FC) with how often this is happening?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139#issuecomment-323855111
https://github.com/broadinstitute/cromwell/issues/1139#issuecomment-323877156:38,Safety,abort,aborts,38,"This doc predates that ""we should fix aborts"" google doc and is effectively a subset of that. I say effectively in that the specifics of what this ticket are asking for might be different from that doc, but that doc should be the authoritative one of the two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139#issuecomment-323877156
https://github.com/broadinstitute/cromwell/issues/1140#issuecomment-231414741:157,Deployability,configurat,configuration,157,"Cromwell localizes every input file in the call directory when the task gets run. You can specify how it gets localized (soft-link, hard-link, copy) in the [configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/application.conf#L148). Does this make your workflow / call fail ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1140#issuecomment-231414741
https://github.com/broadinstitute/cromwell/issues/1140#issuecomment-231414741:157,Modifiability,config,configuration,157,"Cromwell localizes every input file in the call directory when the task gets run. You can specify how it gets localized (soft-link, hard-link, copy) in the [configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/application.conf#L148). Does this make your workflow / call fail ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1140#issuecomment-231414741
https://github.com/broadinstitute/cromwell/pull/1141#issuecomment-231486265:37,Integrability,depend,dependency,37,"Got over aggressive with last minute dependency thinning, removing some reflective dependencies for tests, but the concept is what you see there. Fixing locally, will confirm via travis, then merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141#issuecomment-231486265
https://github.com/broadinstitute/cromwell/pull/1141#issuecomment-231486265:83,Integrability,depend,dependencies,83,"Got over aggressive with last minute dependency thinning, removing some reflective dependencies for tests, but the concept is what you see there. Fixing locally, will confirm via travis, then merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141#issuecomment-231486265
https://github.com/broadinstitute/cromwell/pull/1141#issuecomment-231486265:100,Testability,test,tests,100,"Got over aggressive with last minute dependency thinning, removing some reflective dependencies for tests, but the concept is what you see there. Fixing locally, will confirm via travis, then merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141#issuecomment-231486265
https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875:281,Safety,timeout,timeouts,281,"Reviewer wheel hasn't been rolled-- but IMHO the tag name is overly specific. I'd say perhaps even just use the `PostMVP` tag or `ignore` with a TODO as to what's going on, until we come up with a fix for our supervision model / retries for initializing the database. `MainSpec`'s timeouts happen more frequently, but in my repeated tests the corpse services _seemed_ to be killing [other tests too](https://s3.amazonaws.com/archive.travis-ci.org/jobs/143093948/log.txt) in Travis:; - `SingleToArrayCoercionSpec`; - `EmptyOutputSpec`; - `InputLocalizationWorkflowSpec`; - `LocalBackendSpec`; - `BadTaskOutputWorkflowSpec`; - `ReadTsvWorkflowSpec`; - `GlobbingWorkflowSpec`; - `MultiLineCommandWorkflowSpec`; - `FileSizeWorkflowSpec`; - `WriteTsvSpec`; - `WriteLinesSpec`; - `CromwellApiServiceSpec`; - … plus (at least) one spec that seems to be zombieing the entire test suite such that it times out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875
https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875:333,Testability,test,tests,333,"Reviewer wheel hasn't been rolled-- but IMHO the tag name is overly specific. I'd say perhaps even just use the `PostMVP` tag or `ignore` with a TODO as to what's going on, until we come up with a fix for our supervision model / retries for initializing the database. `MainSpec`'s timeouts happen more frequently, but in my repeated tests the corpse services _seemed_ to be killing [other tests too](https://s3.amazonaws.com/archive.travis-ci.org/jobs/143093948/log.txt) in Travis:; - `SingleToArrayCoercionSpec`; - `EmptyOutputSpec`; - `InputLocalizationWorkflowSpec`; - `LocalBackendSpec`; - `BadTaskOutputWorkflowSpec`; - `ReadTsvWorkflowSpec`; - `GlobbingWorkflowSpec`; - `MultiLineCommandWorkflowSpec`; - `FileSizeWorkflowSpec`; - `WriteTsvSpec`; - `WriteLinesSpec`; - `CromwellApiServiceSpec`; - … plus (at least) one spec that seems to be zombieing the entire test suite such that it times out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875
https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875:389,Testability,test,tests,389,"Reviewer wheel hasn't been rolled-- but IMHO the tag name is overly specific. I'd say perhaps even just use the `PostMVP` tag or `ignore` with a TODO as to what's going on, until we come up with a fix for our supervision model / retries for initializing the database. `MainSpec`'s timeouts happen more frequently, but in my repeated tests the corpse services _seemed_ to be killing [other tests too](https://s3.amazonaws.com/archive.travis-ci.org/jobs/143093948/log.txt) in Travis:; - `SingleToArrayCoercionSpec`; - `EmptyOutputSpec`; - `InputLocalizationWorkflowSpec`; - `LocalBackendSpec`; - `BadTaskOutputWorkflowSpec`; - `ReadTsvWorkflowSpec`; - `GlobbingWorkflowSpec`; - `MultiLineCommandWorkflowSpec`; - `FileSizeWorkflowSpec`; - `WriteTsvSpec`; - `WriteLinesSpec`; - `CromwellApiServiceSpec`; - … plus (at least) one spec that seems to be zombieing the entire test suite such that it times out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875
https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875:462,Testability,log,log,462,"Reviewer wheel hasn't been rolled-- but IMHO the tag name is overly specific. I'd say perhaps even just use the `PostMVP` tag or `ignore` with a TODO as to what's going on, until we come up with a fix for our supervision model / retries for initializing the database. `MainSpec`'s timeouts happen more frequently, but in my repeated tests the corpse services _seemed_ to be killing [other tests too](https://s3.amazonaws.com/archive.travis-ci.org/jobs/143093948/log.txt) in Travis:; - `SingleToArrayCoercionSpec`; - `EmptyOutputSpec`; - `InputLocalizationWorkflowSpec`; - `LocalBackendSpec`; - `BadTaskOutputWorkflowSpec`; - `ReadTsvWorkflowSpec`; - `GlobbingWorkflowSpec`; - `MultiLineCommandWorkflowSpec`; - `FileSizeWorkflowSpec`; - `WriteTsvSpec`; - `WriteLinesSpec`; - `CromwellApiServiceSpec`; - … plus (at least) one spec that seems to be zombieing the entire test suite such that it times out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875
https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875:867,Testability,test,test,867,"Reviewer wheel hasn't been rolled-- but IMHO the tag name is overly specific. I'd say perhaps even just use the `PostMVP` tag or `ignore` with a TODO as to what's going on, until we come up with a fix for our supervision model / retries for initializing the database. `MainSpec`'s timeouts happen more frequently, but in my repeated tests the corpse services _seemed_ to be killing [other tests too](https://s3.amazonaws.com/archive.travis-ci.org/jobs/143093948/log.txt) in Travis:; - `SingleToArrayCoercionSpec`; - `EmptyOutputSpec`; - `InputLocalizationWorkflowSpec`; - `LocalBackendSpec`; - `BadTaskOutputWorkflowSpec`; - `ReadTsvWorkflowSpec`; - `GlobbingWorkflowSpec`; - `MultiLineCommandWorkflowSpec`; - `FileSizeWorkflowSpec`; - `WriteTsvSpec`; - `WriteLinesSpec`; - `CromwellApiServiceSpec`; - … plus (at least) one spec that seems to be zombieing the entire test suite such that it times out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-268376374:120,Testability,assert,assert,120,"With the conditionals, this feels more like it would be better as an ""add a `fail` method"" ticket rather than explicit `assert` statements?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-268376374
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-268403788:97,Testability,log,logs,97,"The difference is that as a fail method I would need to explicitly declare; as output all of the logs, while this way I can keep the logs in the; execution directory and still operate on them within the task. On Tue, Dec 20, 2016 at 5:21 PM, Chris Llanwarne <notifications@github.com>; wrote:. > With the conditionals, this feels more like it would be better as an ""add; > a fail method"" ticket rather than explicit assert statements?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-268376374>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0sVSZCSrgixKUYGIl6G-SeikVDKbks5rKFT6gaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-268403788
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-268403788:133,Testability,log,logs,133,"The difference is that as a fail method I would need to explicitly declare; as output all of the logs, while this way I can keep the logs in the; execution directory and still operate on them within the task. On Tue, Dec 20, 2016 at 5:21 PM, Chris Llanwarne <notifications@github.com>; wrote:. > With the conditionals, this feels more like it would be better as an ""add; > a fail method"" ticket rather than explicit assert statements?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-268376374>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0sVSZCSrgixKUYGIl6G-SeikVDKbks5rKFT6gaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-268403788
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-268403788:416,Testability,assert,assert,416,"The difference is that as a fail method I would need to explicitly declare; as output all of the logs, while this way I can keep the logs in the; execution directory and still operate on them within the task. On Tue, Dec 20, 2016 at 5:21 PM, Chris Llanwarne <notifications@github.com>; wrote:. > With the conditionals, this feels more like it would be better as an ""add; > a fail method"" ticket rather than explicit assert statements?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-268376374>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0sVSZCSrgixKUYGIl6G-SeikVDKbks5rKFT6gaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-268403788
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-275787429:91,Availability,error,errors,91,"Not really. The problem is that then the command has to be written very; carefully so that errors in intermediate steps propagate...we already have; this problem, but this will make it worse. On Fri, Jan 27, 2017 at 4:54 PM, Chris Llanwarne <notifications@github.com>; wrote:. > @yfarjoun <https://github.com/yfarjoun> Is this good enough as an interim?; >; > task foo {; > command {; > # do some stuff...; > # verify and write results to verification.txt; > }; > output {; > # normal outputs; > # Boolean verification = read_bool(verification.txt); > }; > }; >; > task fail {; > command {; > # something guaranteed to fail; > }; > }; >; > workflow bar {; > call foo; > if (!foo.verification) { call fail }; > }; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-275785501>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0qC2URNFfJfnNoDOm6_RCQtKt4p_ks5rWmejgaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-275787429
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986:112,Testability,assert,assert,112,"Sounds like some syntactic sugars are expected to simplify the fail method declaration. In addition, will such 'assert' be dynamic (in run-time) or static (in parse-time, before running any task)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986:50,Usability,simpl,simplify,50,"Sounds like some syntactic sugars are expected to simplify the fail method declaration. In addition, will such 'assert' be dynamic (in run-time) or static (in parse-time, before running any task)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:366,Availability,failure,failure,366,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:68,Security,validat,validation,68,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:190,Security,validat,validation,190,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:331,Security,validat,validate,331,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:25,Testability,assert,assert,25,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:355,Testability,assert,assertion,355,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:579,Testability,assert,assert,579,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:508,Usability,simpl,simplify,508,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301:105,Availability,echo,echo,105,"I see. My usual ""workaround"" for such fail (but continue) is like this:. ```; task foo {; 	command {; 		(echo foo; false) || (echo 1>&2 MSG; true); 	}; }. workflow test {; 	call foo; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301:126,Availability,echo,echo,126,"I see. My usual ""workaround"" for such fail (but continue) is like this:. ```; task foo {; 	command {; 		(echo foo; false) || (echo 1>&2 MSG; true); 	}; }. workflow test {; 	call foo; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301:164,Testability,test,test,164,"I see. My usual ""workaround"" for such fail (but continue) is like this:. ```; task foo {; 	command {; 		(echo foo; false) || (echo 1>&2 MSG; true); 	}; }. workflow test {; 	call foo; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276424241:268,Availability,echo,echo,268,"Right, but I want to know what happened...not that a tool fails silently... On Tue, Jan 31, 2017 at 11:46 AM, Linlin Yan <notifications@github.com>; wrote:. > I see. My usual ""workaround"" for such fail (but continue) is like this:; >; > task foo {; > 	command {; > 		(echo foo; false) || (echo 1>&2 MSG; true); > 	}; > }; >; > workflow test {; > 	call foo; > }; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0izLbBIY8rqgekOd7mNujSZ-DIRiks5rX2VXgaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276424241
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276424241:289,Availability,echo,echo,289,"Right, but I want to know what happened...not that a tool fails silently... On Tue, Jan 31, 2017 at 11:46 AM, Linlin Yan <notifications@github.com>; wrote:. > I see. My usual ""workaround"" for such fail (but continue) is like this:; >; > task foo {; > 	command {; > 		(echo foo; false) || (echo 1>&2 MSG; true); > 	}; > }; >; > workflow test {; > 	call foo; > }; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0izLbBIY8rqgekOd7mNujSZ-DIRiks5rX2VXgaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276424241
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276424241:336,Testability,test,test,336,"Right, but I want to know what happened...not that a tool fails silently... On Tue, Jan 31, 2017 at 11:46 AM, Linlin Yan <notifications@github.com>; wrote:. > I see. My usual ""workaround"" for such fail (but continue) is like this:; >; > task foo {; > 	command {; > 		(echo foo; false) || (echo 1>&2 MSG; true); > 	}; > }; >; > workflow test {; > 	call foo; > }; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0izLbBIY8rqgekOd7mNujSZ-DIRiks5rX2VXgaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276424241
https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-320692423:68,Testability,test,tests,68,@geoffjentry is this still a useful ticket? Are there still PostMVP tests or have they been cleaned up in the last year?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-320692423
https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-320693216:161,Testability,test,tests,161,"@katevoss I've said that it is not useful for quite some time now. Every time I bring it up other people disagree with removing it. Yes, there are still ignored tests but I'm using the model of ""if you haven't touched it by now it's not useful and you're never going to come back to it"". . I'd suggest we set a TTL for this ticket and anyone who feels strongly about these tests can effort to enable them between now and then.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-320693216
https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-320693216:373,Testability,test,tests,373,"@katevoss I've said that it is not useful for quite some time now. Every time I bring it up other people disagree with removing it. Yes, there are still ignored tests but I'm using the model of ""if you haven't touched it by now it's not useful and you're never going to come back to it"". . I'd suggest we set a TTL for this ticket and anyone who feels strongly about these tests can effort to enable them between now and then.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-320693216
https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-323857855:29,Testability,test,tests,29,"If we don't care about those tests then IMHO this ticket should become ""delete all that PostMVP stuff you don't care about"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-323857855
https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-323858568:56,Testability,test,tests,56,"My long held stance is that if anyone cared about those tests they'd have unignored them by now. . Imo the options are ""someone who cares fix this immediately"" or what @mcovarr said",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-323858568
https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232176168:33,Testability,test,test,33,"@Horneth In general, how did you test/benchmark this stuff?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232176168
https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232176168:38,Testability,benchmark,benchmark,38,"@Horneth In general, how did you test/benchmark this stuff?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232176168
https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232186556:63,Security,access,access,63,"I flooded the Local backend with scattered sleeps and tried to access metadata, status, submit workflows etc...; I'd like to try more benchmarky things though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232186556
https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232186556:134,Testability,benchmark,benchmarky,134,"I flooded the Local backend with scattered sleeps and tried to access metadata, status, submit workflows etc...; I'd like to try more benchmarky things though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232186556
https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232187693:119,Integrability,depend,dependent,119,Just a heads up that the changes incoming on `mlc_existence` as currently structured move the execution of workflow-ID-dependent APIs (which is most of them) from `CromwellApiService` to `EngineMetadataServiceActor`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232187693
https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232191521:98,Safety,avoid,avoid,98,@mcovarr I was just thinking that this PR probably warrants careful timing & some coordination to avoid a giant rebase hell for everyone,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232191521
https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232975937:20,Performance,cache,cache,20,:+1: with existence cache removal. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1152/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232975937
https://github.com/broadinstitute/cromwell/issues/1156#issuecomment-252258248:54,Availability,error,error,54,"Confirmed that this issue has been resolved, a proper error pops up when missing a required refresh token.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156#issuecomment-252258248
https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419608:173,Availability,down,down,173,"What are the other 2 commands ? It's a known issue that if the commands are very fast to execute, the monitoring.log is not flushed before it's delocalized and the VM shuts down, and it ends up empty.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419608
https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419608:102,Energy Efficiency,monitor,monitoring,102,"What are the other 2 commands ? It's a known issue that if the commands are very fast to execute, the monitoring.log is not flushed before it's delocalized and the VM shuts down, and it ends up empty.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419608
https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419608:113,Testability,log,log,113,"What are the other 2 commands ? It's a known issue that if the commands are very fast to execute, the monitoring.log is not flushed before it's delocalized and the VM shuts down, and it ends up empty.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419608
https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419836:39,Deployability,update,update,39,they're `grep` and `wc` commands. I'll update the documentation accordingly.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419836
https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-233072259:47,Deployability,update,update,47,closing -- sounds like this is a documentation update (which is being handled). Please re-open if there is still a bug here,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-233072259
https://github.com/broadinstitute/cromwell/issues/1162#issuecomment-234028065:0,Integrability,Depend,Depends,0,Depends on (or maybe even duplicates) #658,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1162#issuecomment-234028065
https://github.com/broadinstitute/cromwell/issues/1165#issuecomment-253934964:71,Testability,log,logic,71,erm yeah sorry about that 😦 . Currently the `withRecognizedWorkflowId` logic in `CromwellApiService` is using a callback to communicate with the service registry actor. This ticket was meant to suggest that this communication be kept actor-based instead.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1165#issuecomment-253934964
https://github.com/broadinstitute/cromwell/issues/1167#issuecomment-253881280:68,Deployability,update,update,68,"One could make the argument that it'd hose people who don't want to update their options files, but I know that's not really the answer :'(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1167#issuecomment-253881280
https://github.com/broadinstitute/cromwell/issues/1167#issuecomment-275786330:107,Modifiability,refactor,refactor,107,I'm gonna close this. There's no reason to disrupt this before we undergo the big-scary runtime-attributes-refactor,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1167#issuecomment-275786330
https://github.com/broadinstitute/cromwell/pull/1170#issuecomment-233487066:115,Modifiability,refactor,refactored,115,Looks good. When the SGE backend makes its much anticipated return there are some bits here that could probably be refactored to be shared. :+1:. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1170/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1170#issuecomment-233487066
https://github.com/broadinstitute/cromwell/issues/1172#issuecomment-233343249:0,Integrability,Depend,Depends,0,Depends on #1115,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1172#issuecomment-233343249
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-323876960:122,Testability,test,test,122,It has not. It requires someone spending the time to figure out how to set up an ephemeral SGE cluster in travis for each test run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-323876960
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324096040:79,Modifiability,config,config,79,"Also of note - at the time (probably) we had an ""SGE backend"". Now we have the config backend. So we could do the same with e.g. LSF. Outside of Broad we probably have more LSF users than SGE users. Inside Broad it'd be nearly 100% SGE. OTOH I don't know how well our SGE stuff works with UGER so perhaps not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324096040
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:895,Deployability,install,installer,895,"> Are there a lot of users on SGE?. I would also ask the methods team, say ldgauthier or LeeTL1220. > It's probably more a ""we don't know how"" than ""if we did, it'd be a lot of work"". Yep, we are firmly in the camp of ""we don't know how"", with a heap of ""we never rtfm'ed'. There are a number of examples out there, and folks probably willing to help us, we just haven't prioritized this ticket. I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:968,Deployability,install,installed,968,"> Are there a lot of users on SGE?. I would also ask the methods team, say ldgauthier or LeeTL1220. > It's probably more a ""we don't know how"" than ""if we did, it'd be a lot of work"". Yep, we are firmly in the camp of ""we don't know how"", with a heap of ""we never rtfm'ed'. There are a number of examples out there, and folks probably willing to help us, we just haven't prioritized this ticket. I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:1811,Energy Efficiency,schedul,schedulers,1811,"I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the docker image / setup script like we have for Funnel. > I don't know how well our SGE stuff works with UGER so perhaps not. Cromwell works great on BITS' newer UGE cluster named ""UGER"". I use cromwell frequently with `concurrent-job-limit` set to 900 due to our resource caps. **TL;DR Getting grid engine test support setup for a Broad-like environment is possible, just hasn't been a priority.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:1966,Energy Efficiency,schedul,scheduler,1966,"I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the docker image / setup script like we have for Funnel. > I don't know how well our SGE stuff works with UGER so perhaps not. Cromwell works great on BITS' newer UGE cluster named ""UGER"". I use cromwell frequently with `concurrent-job-limit` set to 900 due to our resource caps. **TL;DR Getting grid engine test support setup for a Broad-like environment is possible, just hasn't been a priority.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:680,Modifiability,layers,layers,680,"> Are there a lot of users on SGE?. I would also ask the methods team, say ldgauthier or LeeTL1220. > It's probably more a ""we don't know how"" than ""if we did, it'd be a lot of work"". Yep, we are firmly in the camp of ""we don't know how"", with a heap of ""we never rtfm'ed'. There are a number of examples out there, and folks probably willing to help us, we just haven't prioritized this ticket. I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:2218,Performance,concurren,concurrent-job-limit,2218,"I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the docker image / setup script like we have for Funnel. > I don't know how well our SGE stuff works with UGER so perhaps not. Cromwell works great on BITS' newer UGE cluster named ""UGER"". I use cromwell frequently with `concurrent-job-limit` set to 900 due to our resource caps. **TL;DR Getting grid engine test support setup for a Broad-like environment is possible, just hasn't been a priority.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:2305,Testability,test,test,2305,"I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the docker image / setup script like we have for Funnel. > I don't know how well our SGE stuff works with UGER so perhaps not. Cromwell works great on BITS' newer UGE cluster named ""UGER"". I use cromwell frequently with `concurrent-job-limit` set to 900 due to our resource caps. **TL;DR Getting grid engine test support setup for a Broad-like environment is possible, just hasn't been a priority.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806:117,Modifiability,config,config,117,"@ldgauthier and @Leetl1220 do you know how many users use Cromwell with SGE?. As a **SGE user**, I want to **the SGE config to be tested in Centaur**, so that I can **avoid regressions**.; - Effort: **Medium to Large**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806:167,Safety,avoid,avoid,167,"@ldgauthier and @Leetl1220 do you know how many users use Cromwell with SGE?. As a **SGE user**, I want to **the SGE config to be tested in Centaur**, so that I can **avoid regressions**.; - Effort: **Medium to Large**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806:222,Safety,Risk,Risk,222,"@ldgauthier and @Leetl1220 do you know how many users use Cromwell with SGE?. As a **SGE user**, I want to **the SGE config to be tested in Centaur**, so that I can **avoid regressions**.; - Effort: **Medium to Large**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806
https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806:130,Testability,test,tested,130,"@ldgauthier and @Leetl1220 do you know how many users use Cromwell with SGE?. As a **SGE user**, I want to **the SGE config to be tested in Centaur**, so that I can **avoid regressions**.; - Effort: **Medium to Large**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806
https://github.com/broadinstitute/cromwell/pull/1183#issuecomment-233726634:65,Modifiability,config,config,65,@Horneth I'm adding you as a reviewer so you can confirm the new config file being used -- multiBackend.conf has the changes you expect :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1183#issuecomment-233726634
https://github.com/broadinstitute/cromwell/issues/1190#issuecomment-289556748:16,Integrability,depend,dependency,16,I don't see the dependency anymore so it looks good to me !,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1190#issuecomment-289556748
https://github.com/broadinstitute/cromwell/issues/1191#issuecomment-234341313:130,Usability,clear,clearly,130,"@cjllanwarne Is anything listening to them currently? If not, is this the right answer or would it be to remove the acks? They're clearly not being used by the WEA or the WMA for anything.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1191#issuecomment-234341313
https://github.com/broadinstitute/cromwell/issues/1191#issuecomment-234342288:66,Testability,test,tests,66,Insert my rant about needing to add crap into real code to enable tests 😡,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1191#issuecomment-234342288
https://github.com/broadinstitute/cromwell/pull/1192#issuecomment-234070556:174,Usability,undo,undo,174,Welcoming all comers as this is hefty but @Horneth in particular - a lot of the rebase-a-palooza involved changes you had made so an extra pair of eyes to make sure I didn't undo something would be good.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1192#issuecomment-234070556
https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235593390:45,Availability,recover,recover,45,Yes this pretty much always happens with the recover code I merged to develop yesterday.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235593390
https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235593390:45,Safety,recover,recover,45,Yes this pretty much always happens with the recover code I merged to develop yesterday.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235593390
https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544:1914,Availability,down,down,1914,"tched; 2016-07-27 16:48:50,689 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Restarting workflow UUID(c6eb4949-cb81-4a56-b3de-11b1cde3e13e); 2016-07-27 16:48:50,693 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Successfully started WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e; 2016-07-27 16:48:50,773 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-07-27 16:48:51,258 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: Call-to-Backend assignments: hello.hello -> JES; 2016-07-27 16:48:51,284 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-07-27 16:48:51,291 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-07-27 16:48:51,320 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowInitializationActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-07-27 16:48:51,631 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowInitializationActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: State is now terminal. Shutting down.; 2016-07-27 16:48:51,637 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from InitializingWorkflowState to ExecutingWor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544
https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544:2618,Availability,down,down,2618,"ackend assignments: hello.hello -> JES; 2016-07-27 16:48:51,284 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-07-27 16:48:51,291 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-07-27 16:48:51,320 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowInitializationActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-07-27 16:48:51,631 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowInitializationActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: State is now terminal. Shutting down.; 2016-07-27 16:48:51,637 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from InitializingWorkflowState to ExecutingWorkflowState; 2016-07-27 16:48:51,663 cromwell-system-akka.dispatchers.engine-dispatcher-16 INFO - WorkflowExecutionActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: Starting calls: hello.hello:NA:1; 2016-07-27 16:48:51,670 cromwell-system-akka.dispatchers.engine-dispatcher-16 INFO - WorkflowExecutionActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: WorkflowExecutionActor [UUID(c6eb4949)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-07-27 16:48:53,508 INFO - JesRun [UUID(c6eb4949)hello.hello:NA:1]: JES Run ID is operations/EMKB-PDiKhiz_dqSktq89pQBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-07-27 16:48:53,610 cromwell-system-akka.dispatchers.backend-dispatcher-17 INFO - $a [UUID(c6eb4949)hello.hello:NA:1]: ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544
https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544:715,Deployability,release,released,715,"I can't seem to reproduce this - I have a workflow running, stop cromwell, restart it, and no matter how many times I do it I don't see anything wrong with the sequence of events:. ```. 2016-07-27 16:48:47,144 INFO - Slf4jLogger started; 2016-07-27 16:48:48,456 cromwell-system-akka.actor.default-dispatcher-3 INFO - Bound to /0.0.0.0:8009; 2016-07-27 16:48:48,460 ForkJoinPool-2-worker-15 INFO - Cromwell service started...; 2016-07-27 16:48:48,703 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell_new; 2016-07-27 16:48:50,251 INFO - Reading from cromwell_new.DATABASECHANGELOG; 2016-07-27 16:48:50,337 INFO - Successfully acquired change log lock; 2016-07-27 16:48:50,400 INFO - Successfully released change log lock; 2016-07-27 16:48:50,689 cromwell-system-akka.actor.default-dispatcher-3 INFO - 1 new workflows fetched; 2016-07-27 16:48:50,689 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Restarting workflow UUID(c6eb4949-cb81-4a56-b3de-11b1cde3e13e); 2016-07-27 16:48:50,693 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Successfully started WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e; 2016-07-27 16:48:50,773 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-07-27 16:48:51,258 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: Call-to-Backend assignments: hello.hello -> JES; 2016-07-27 16:48:51,284 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-07-27 16:48:51,291 cromwell-system-akka.dispatchers.engine-dispatcher-15 INF",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544
https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544:661,Testability,log,log,661,"I can't seem to reproduce this - I have a workflow running, stop cromwell, restart it, and no matter how many times I do it I don't see anything wrong with the sequence of events:. ```. 2016-07-27 16:48:47,144 INFO - Slf4jLogger started; 2016-07-27 16:48:48,456 cromwell-system-akka.actor.default-dispatcher-3 INFO - Bound to /0.0.0.0:8009; 2016-07-27 16:48:48,460 ForkJoinPool-2-worker-15 INFO - Cromwell service started...; 2016-07-27 16:48:48,703 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell_new; 2016-07-27 16:48:50,251 INFO - Reading from cromwell_new.DATABASECHANGELOG; 2016-07-27 16:48:50,337 INFO - Successfully acquired change log lock; 2016-07-27 16:48:50,400 INFO - Successfully released change log lock; 2016-07-27 16:48:50,689 cromwell-system-akka.actor.default-dispatcher-3 INFO - 1 new workflows fetched; 2016-07-27 16:48:50,689 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Restarting workflow UUID(c6eb4949-cb81-4a56-b3de-11b1cde3e13e); 2016-07-27 16:48:50,693 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Successfully started WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e; 2016-07-27 16:48:50,773 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-07-27 16:48:51,258 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: Call-to-Backend assignments: hello.hello -> JES; 2016-07-27 16:48:51,284 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-07-27 16:48:51,291 cromwell-system-akka.dispatchers.engine-dispatcher-15 INF",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544
https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544:731,Testability,log,log,731,"I can't seem to reproduce this - I have a workflow running, stop cromwell, restart it, and no matter how many times I do it I don't see anything wrong with the sequence of events:. ```. 2016-07-27 16:48:47,144 INFO - Slf4jLogger started; 2016-07-27 16:48:48,456 cromwell-system-akka.actor.default-dispatcher-3 INFO - Bound to /0.0.0.0:8009; 2016-07-27 16:48:48,460 ForkJoinPool-2-worker-15 INFO - Cromwell service started...; 2016-07-27 16:48:48,703 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell_new; 2016-07-27 16:48:50,251 INFO - Reading from cromwell_new.DATABASECHANGELOG; 2016-07-27 16:48:50,337 INFO - Successfully acquired change log lock; 2016-07-27 16:48:50,400 INFO - Successfully released change log lock; 2016-07-27 16:48:50,689 cromwell-system-akka.actor.default-dispatcher-3 INFO - 1 new workflows fetched; 2016-07-27 16:48:50,689 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Restarting workflow UUID(c6eb4949-cb81-4a56-b3de-11b1cde3e13e); 2016-07-27 16:48:50,693 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Successfully started WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e; 2016-07-27 16:48:50,773 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-07-27 16:48:51,258 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: Call-to-Backend assignments: hello.hello -> JES; 2016-07-27 16:48:51,284 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-07-27 16:48:51,291 cromwell-system-akka.dispatchers.engine-dispatcher-15 INF",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544
https://github.com/broadinstitute/cromwell/pull/1198#issuecomment-235020776:67,Usability,feedback,feedback,67,👍 assuming Travis is happy. Re-assign back if/when requesting more feedback. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1198/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1198#issuecomment-235020776
https://github.com/broadinstitute/cromwell/pull/1199#issuecomment-234843562:102,Deployability,integrat,integrated,102,"Overall, a code-based start of discussion. See also individual commits messages. Some or all could be integrated into #1198.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1199#issuecomment-234843562
https://github.com/broadinstitute/cromwell/pull/1199#issuecomment-234843562:71,Integrability,message,messages,71,"Overall, a code-based start of discussion. See also individual commits messages. Some or all could be integrated into #1198.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1199#issuecomment-234843562
https://github.com/broadinstitute/cromwell/pull/1199#issuecomment-234843562:102,Integrability,integrat,integrated,102,"Overall, a code-based start of discussion. See also individual commits messages. Some or all could be integrated into #1198.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1199#issuecomment-234843562
https://github.com/broadinstitute/cromwell/pull/1206#issuecomment-235476027:39,Energy Efficiency,green,green,39,Feel free to merge as soon as we get 4 green checkboxes. I know I'd love to get this in my other branch.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1206#issuecomment-235476027
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-323857659:201,Safety,risk,risks,201,@LeeTL1220 is this still something you want? Can you explain a bit more about your use case for wanting to override a runtime attribute?. @geoffjentry what would the effort be to add this feature? Any risks in adding it?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-323857659
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-324084418:62,Modifiability,variab,variables,62,"As a **user running workflows**, I want to **override runtime variables**, so that I can **change hardcoded runtime attributes when someone else wrote the method**.; - Effort: **Small**; - Risk: **Small**; - Carefully vet a good solution; - Avoid breaking changes; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-324084418
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-324084418:189,Safety,Risk,Risk,189,"As a **user running workflows**, I want to **override runtime variables**, so that I can **change hardcoded runtime attributes when someone else wrote the method**.; - Effort: **Small**; - Risk: **Small**; - Carefully vet a good solution; - Avoid breaking changes; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-324084418
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-324084418:241,Safety,Avoid,Avoid,241,"As a **user running workflows**, I want to **override runtime variables**, so that I can **change hardcoded runtime attributes when someone else wrote the method**.; - Effort: **Small**; - Risk: **Small**; - Carefully vet a good solution; - Avoid breaking changes; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-324084418
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325718645:290,Deployability,pipeline,pipeline,290,"@geoffjentry This is my github ID. :). @katevoss We've been using parameters to tasks to override runtime attributes. However, I am not sure how this affects call caching, it is very clunky, and the standard names my group uses may different from another. This lattermost is a headache for pipeline engineers. Anyway, it would be nice to have a mechanism for overriding runtime attributes, mostly for users, not developers. This would cover times where WDL writers have hardcoded runtime attributes that do not fit a user's need. For me, this is no longer high priority... . Though is another issue needed for these parameters that I use adversely affecting call caching (e.g. my `preemptible_attempts` parameter is only used in the runtime block and should not cause a cache miss if it is changed)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325718645
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325718645:770,Performance,cache,cache,770,"@geoffjentry This is my github ID. :). @katevoss We've been using parameters to tasks to override runtime attributes. However, I am not sure how this affects call caching, it is very clunky, and the standard names my group uses may different from another. This lattermost is a headache for pipeline engineers. Anyway, it would be nice to have a mechanism for overriding runtime attributes, mostly for users, not developers. This would cover times where WDL writers have hardcoded runtime attributes that do not fit a user's need. For me, this is no longer high priority... . Though is another issue needed for these parameters that I use adversely affecting call caching (e.g. my `preemptible_attempts` parameter is only used in the runtime block and should not cause a cache miss if it is changed)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325718645
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325784438:29,Performance,cache,cache,29,"@geoffjentry will a job call cache if a user overrides runtime attributes? ; @LeeTL1220 What do you mean by your last comment? Do you mean that there are certain parameters that cause a cache-miss when you change them, but you want them to cache-hit? That might be a different issue, I can create it if you want to explain it to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325784438
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325784438:186,Performance,cache,cache-miss,186,"@geoffjentry will a job call cache if a user overrides runtime attributes? ; @LeeTL1220 What do you mean by your last comment? Do you mean that there are certain parameters that cause a cache-miss when you change them, but you want them to cache-hit? That might be a different issue, I can create it if you want to explain it to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325784438
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325784438:240,Performance,cache,cache-hit,240,"@geoffjentry will a job call cache if a user overrides runtime attributes? ; @LeeTL1220 What do you mean by your last comment? Do you mean that there are certain parameters that cause a cache-miss when you change them, but you want them to cache-hit? That might be a different issue, I can create it if you want to explain it to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325784438
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325789100:222,Performance,cache,cache,222,"@katevoss in a world where our runtime attrs weren't fubar we should be caching on what values were actually used. For instance if a user wants to swap in their own docker image or change the number of CPUs it should only cache to their variant. And yeah, @LeeTL1220 brings up a good point - there are some parameters which should never cause the outcome to change but do get counted against caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325789100
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325842489:520,Performance,cache,cache,520,"@katevoss @geoffjentry <https://github.com/geoffjentry> I do not know what; those runtime attributes would be. Someone on red team would be much; better suited to answer that. On Tue, Aug 29, 2017 at 4:18 PM, Jeff Gentry <notifications@github.com>; wrote:. > @katevoss <https://github.com/katevoss> in a world where our runtime; > attrs weren't fubar we should be caching on what values were actually used.; > For instance if a user wants to swap in their own docker image or change; > the number of CPUs it should only cache to their variant.; >; > And yeah, @LeeTL1220 <https://github.com/leetl1220> brings up a good; > point - there are some parameters which should never cause the outcome to; > change but do get counted against caching.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325789100>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk8uWAvdQtT2jHiCFk3jR73-uU4Zkks5sdHIsgaJpZM4JWcLP>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325842489
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511067941:384,Deployability,configurat,configuration,384,I am running into an issue whereby this feature would be helpful. I am developing workflows using CWL but one of the command line tools being used in CWL requires the runtime attribute 'bootDiskSizeGb' to be set to 100 instead of 10 for this particular task. As CWL doesn't have an equivalent attribute the only way I can set this is in the 'default-runtime-attributes' section of my configuration file but now all tasks for the workflow will use 100GB for their boot disk size instead of the single task that actually requires it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511067941
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511067941:384,Modifiability,config,configuration,384,I am running into an issue whereby this feature would be helpful. I am developing workflows using CWL but one of the command line tools being used in CWL requires the runtime attribute 'bootDiskSizeGb' to be set to 100 instead of 10 for this particular task. As CWL doesn't have an equivalent attribute the only way I can set this is in the 'default-runtime-attributes' section of my configuration file but now all tasks for the workflow will use 100GB for their boot disk size instead of the single task that actually requires it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511067941
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511141850:28,Deployability,update,update,28,"@geoffjentry thanks for the update, I'll keep an eye on the pull request.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511141850
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511142579:43,Usability,clear,clear,43,"@ctjoreilly Sorry, I should have been more clear, that change wouldn't affect your problem as it's WDL only. While it's a known issue for CWL support it doesn't look like there's an issue for **that** problem. It's worth opening a new ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511142579
https://github.com/broadinstitute/cromwell/issues/1211#issuecomment-250264649:20,Modifiability,config,configs,20,I have come to like configs (https://github.com/kxbmap/configs) but another one I liked using was ficus (https://github.com/iheartradio/ficus). Nothing is going to be a complete replacement for the structure we have built up in lenthall but it should help ease the custom code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1211#issuecomment-250264649
https://github.com/broadinstitute/cromwell/issues/1211#issuecomment-250264649:55,Modifiability,config,configs,55,I have come to like configs (https://github.com/kxbmap/configs) but another one I liked using was ficus (https://github.com/iheartradio/ficus). Nothing is going to be a complete replacement for the structure we have built up in lenthall but it should help ease the custom code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1211#issuecomment-250264649
https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235719480:28,Deployability,hotfix,hotfix,28,👍 . Does this need to be on hotfix as well? @jsotobroad @kcibul . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1213/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235719480
https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235719562:54,Energy Efficiency,green,green,54,Actually I think @vivster7 was the person to tag from green,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235719562
https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235720261:58,Testability,test,testing,58,Nope. Hot fix works as expected. This bug was found while testing .20,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235720261
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235933879:215,Availability,robust,robust,215,"OOI, what's the use-case for this? I'm not saying this code change is bad (as a code change it looks totally fine) but the description is flagging up warning signs in my head. The WDL spec claims that workflows are robust to calls being specified in **any order** since the DAG is 100% implied by inter-call dependencies rather than list-order. Eg this should be fine:. ```; workflow x {; call b { input: i = a.i }; call a; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235933879
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235933879:308,Integrability,depend,dependencies,308,"OOI, what's the use-case for this? I'm not saying this code change is bad (as a code change it looks totally fine) but the description is flagging up warning signs in my head. The WDL spec claims that workflows are robust to calls being specified in **any order** since the DAG is 100% implied by inter-call dependencies rather than list-order. Eg this should be fine:. ```; workflow x {; call b { input: i = a.i }; call a; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235933879
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235934992:10,Availability,echo,echo,10,"@jainh To echo what @cjllanwarne said, there is _no_ implied order in WDL, it's a pure dataflow. Any backend which requires an ordering beyond the dependency graph is implementing things incorrectly. If I'm misunderstanding what you're trying to do here, let me know - it's possible that @cjllanwarne totally biased my thinking :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235934992
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235934992:147,Integrability,depend,dependency,147,"@jainh To echo what @cjllanwarne said, there is _no_ implied order in WDL, it's a pure dataflow. Any backend which requires an ordering beyond the dependency graph is implementing things incorrectly. If I'm misunderstanding what you're trying to do here, let me know - it's possible that @cjllanwarne totally biased my thinking :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235934992
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235959600:91,Integrability,depend,dependencies,91,"@cjllanwarne As you mentioned ""order is defined in WDL and DAG gets created based on inter-dependencies"" is true during execution of the task but not during initialization phase, hence that order is required during initialization as well. . @geoffjentry If i understand correctly there is no graph being received by the backend during initialization it is `Map[Call, String]` to identifies all calls @ backend and then ultimately `Seq[Call]`, what is expected from engine if user specifies calls as @cjllanwarne mentioned in above wdl example (a then b) that order is preserved and pass through. . RP backend in CCC depends upon order of calls during initialization as well therefore that change is required.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235959600
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235959600:616,Integrability,depend,depends,616,"@cjllanwarne As you mentioned ""order is defined in WDL and DAG gets created based on inter-dependencies"" is true during execution of the task but not during initialization phase, hence that order is required during initialization as well. . @geoffjentry If i understand correctly there is no graph being received by the backend during initialization it is `Map[Call, String]` to identifies all calls @ backend and then ultimately `Seq[Call]`, what is expected from engine if user specifies calls as @cjllanwarne mentioned in above wdl example (a then b) that order is preserved and pass through. . RP backend in CCC depends upon order of calls during initialization as well therefore that change is required.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235959600
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235961731:91,Availability,robust,robust,91,"@jainh I would therefore suggest you want to sort the list yourself to make that guarantee robust, rather than assume it's already correctly ordered in the WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235961731
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235976923:19,Usability,clear,clear,19,"@jainh Just so I'm clear, the WDL example that @cjllanwarne posted above would still work correctly, right? (I believe that's the case, just being cautious here)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235976923
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236009348:134,Integrability,depend,dependencies,134,"@cjllanwarne Sort does not solve the purpose here because it is not what backend expecting and want,However it is expecting the right dependencies as specified in the wdl. . Because wdl writer may not right alphabetically or numeric order for call name it could be something logical name or else. I still don't get how sorting would work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236009348
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236009348:275,Testability,log,logical,275,"@cjllanwarne Sort does not solve the purpose here because it is not what backend expecting and want,However it is expecting the right dependencies as specified in the wdl. . Because wdl writer may not right alphabetically or numeric order for call name it could be something logical name or else. I still don't get how sorting would work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236009348
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236012262:18,Usability,clear,clear,18,"@jainh just to be clear. I don't think your code breaks anything but I also don't think it can be comprehensively correct. Here's why- Imagine two nearly the same WDL files:. ```; workflow ordered {; call a; call b { input: i = a.i }; }; ```. and:. ```; workflow unordered {; call b { input: i = a.i }; call a; }; ```. Now, if I understand you correctly, in both cases you want to pass to the initialiser the same list: List(a, b) since a should in initialised before b. However, if you just create a ListMap as your code does, it will pick the same order from the WDL file. So here's what happens:. | Workflow | List that you want | List that your code makes | Match |; | --- | --- | --- | --- |; | ordered | `List(a, b)` | `List(a, b)` | 🆗 |; | unordered | `List(a, b)` | `List(b, a)` | 🚫 |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236012262
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236050063:139,Integrability,depend,dependency,139,"@cjllanwarne I agree now that fix will not work, but what i don't see we can perform Topological sort because we have seq of calls instead dependency graph ( Correct me if i am wrong?).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236050063
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236050063:77,Performance,perform,perform,77,"@cjllanwarne I agree now that fix will not work, but what i don't see we can perform Topological sort because we have seq of calls instead dependency graph ( Correct me if i am wrong?).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236050063
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236149002:13,Integrability,depend,dependencies,13,"You have the dependencies in `Call#prerequisiteCallNames`, so you certainly _could_ perform a Topological sort. But TBH a backend having a requirement for a topologically ordered list of calls seems kind of sketchy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236149002
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236149002:84,Performance,perform,perform,84,"You have the dependencies in `Call#prerequisiteCallNames`, so you certainly _could_ perform a Topological sort. But TBH a backend having a requirement for a topologically ordered list of calls seems kind of sketchy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236149002
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236381218:211,Deployability,integrat,integration,211,"@mcovarr, @cjllanwarne agree on your comments, I implemented a PoC on graph topology creation using topological sort algorithm based on DFS calculation at the beginning of this year. The idea was to propose the integration of that concept to Cromwell after PBE.; Coming back to this issue, If we need to recreate a DAG in the backend side it means there is something wrong with that backend (we know it). I explained why we try to do so in the Waffle.io ticket but I think we can explain better in the next meeting we may have. Anyways @jainh will not proceed with this change.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236381218
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236381218:211,Integrability,integrat,integration,211,"@mcovarr, @cjllanwarne agree on your comments, I implemented a PoC on graph topology creation using topological sort algorithm based on DFS calculation at the beginning of this year. The idea was to propose the integration of that concept to Cromwell after PBE.; Coming back to this issue, If we need to recreate a DAG in the backend side it means there is something wrong with that backend (we know it). I explained why we try to do so in the Waffle.io ticket but I think we can explain better in the next meeting we may have. Anyways @jainh will not proceed with this change.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236381218
https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235935544:106,Integrability,depend,dependencies,106,@francares Why do you view this as a bug? WDL explicitly states that there is no natural order outside of dependencies. Tagging our illustrious PO @kcibul just for record keeping,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235935544
https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235939607:130,Integrability,depend,dependency,130,"I take that back, the spec might not explicitly state this. However, that has always been the intention - the only ""order"" is via dependency",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235939607
https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235952476:453,Deployability,release,release,453,"@geoffjentry Current state of Reliance Point backend, that is being implemented here, requires during initialization to know the order of calls execution. This is because the component behind this backend needs to know basic execution order since it still does not understand WDL therefore it can't read a complete DAG. ; I thought this was a bug due to the loose of oder when a fold or groupBy is applied in map of call -> backend.; We have a possible release date (given by the team who is developing it) of the RP component for beginning next year which will support WDL spec but for now we need to support this.; Please let me know if we can keep this change in the engine since it does not hurt. Later when RP WDL version is in place we revert this change.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235952476
https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235972513:191,Integrability,contract,contract,191,"Tagging @jainh here just so I can collapse the conversation into a single place & not both here & PR #1216 . I'm fine w/ this since it shouldn't affect the outside behavior - technically the contract from Cromwell is that it is free to launch any runnable call at any point it wants. I do want to make sure that the ""WDL order"" change is never documented anywhere so that we're free to change it at any time, but IIRC #1216 doesn't do that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235972513
https://github.com/broadinstitute/cromwell/issues/1217#issuecomment-236262636:93,Modifiability,Config,Config,93,"@kshakir mentioned in #1202 that one will soon be able to specify runtime attributes via the Config backend and not through code. Any implementation of this will work for us, provided that:; 1. One can specify arbitrary runtime attributes with values that can contain any characters, including nested double quotes (escaped if necessary).; 2. Arbitrary runtime attributes can be specified both within a single task in a WDL file, and in a workflow options JSON file. For example, using runtime attributes ""-app"" (application profile), ""-q"" (queue), ""-M"" (memory), ""-n"" (processors) and ""-R"" (resource requirements), the submit command line structure for LSF would be of the form:. `bsub -app large -q idle -M 125829120 -n 16,16 -R ""swp > 15 && span[hosts=1]"" -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /bin/sh ${script}`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1217#issuecomment-236262636
https://github.com/broadinstitute/cromwell/issues/1217#issuecomment-236262636:541,Performance,queue,queue,541,"@kshakir mentioned in #1202 that one will soon be able to specify runtime attributes via the Config backend and not through code. Any implementation of this will work for us, provided that:; 1. One can specify arbitrary runtime attributes with values that can contain any characters, including nested double quotes (escaped if necessary).; 2. Arbitrary runtime attributes can be specified both within a single task in a WDL file, and in a workflow options JSON file. For example, using runtime attributes ""-app"" (application profile), ""-q"" (queue), ""-M"" (memory), ""-n"" (processors) and ""-R"" (resource requirements), the submit command line structure for LSF would be of the form:. `bsub -app large -q idle -M 125829120 -n 16,16 -R ""swp > 15 && span[hosts=1]"" -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /bin/sh ${script}`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1217#issuecomment-236262636
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:759,Integrability,message,messages,759,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:95,Modifiability,extend,extended,95,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:599,Modifiability,extend,extends,599,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:833,Modifiability,refactor,refactored,833,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:937,Modifiability,rewrite,rewrites,937,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:529,Usability,simpl,simplified,529,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-332394280:140,Modifiability,refactor,refactoring,140,"I'd call this a ""medium"" effort for a developer. It's not a quick fix, while not as much as an effort as ""implement CWL"". A couple weeks of refactoring, a round of testing and team review, then another two weeks to polish it off.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-332394280
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-332394280:164,Testability,test,testing,164,"I'd call this a ""medium"" effort for a developer. It's not a quick fix, while not as much as an effort as ""implement CWL"". A couple weeks of refactoring, a round of testing and team review, then another two weeks to polish it off.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-332394280
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-344404227:231,Safety,Risk,Risk,231,"As a **user developing a backend for Cromwell**, I want **the execution actor naming and system to be simple and concise**, so that I can **more easily work with Cromwell, and don't add unnecessary code**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Small**; - Not to devalue cleaning up tech debt, but there are higher value items on our docket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-344404227
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-344404227:102,Usability,simpl,simple,102,"As a **user developing a backend for Cromwell**, I want **the execution actor naming and system to be simple and concise**, so that I can **more easily work with Cromwell, and don't add unnecessary code**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Small**; - Not to devalue cleaning up tech debt, but there are higher value items on our docket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-344404227
https://github.com/broadinstitute/cromwell/issues/1219#issuecomment-237553061:64,Integrability,synchroniz,synchronizing,64,"@mcovarr So there seems to be some Liquibase issues in terms of synchronizing the column names. If we're going to do this as a part of 0.21, I might have @kshakir refine this ticket in terms of what steps we have to take to successfully change column names without disrupting users too much.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1219#issuecomment-237553061
https://github.com/broadinstitute/cromwell/issues/1219#issuecomment-237705566:36,Modifiability,rewrite,rewrite,36,"Not sure if you're referring to my ""rewrite"" of a liquibase change log, but that was already merged to develop. Besides that fixed issue, there are further technical issues that I'm aware of blocking us from changing column names. Just use [`<renameColumn`](http://www.liquibase.org/documentation/changes/rename_column.html), like we have in a few cases in the changelog history already.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1219#issuecomment-237705566
https://github.com/broadinstitute/cromwell/issues/1219#issuecomment-237705566:67,Testability,log,log,67,"Not sure if you're referring to my ""rewrite"" of a liquibase change log, but that was already merged to develop. Besides that fixed issue, there are further technical issues that I'm aware of blocking us from changing column names. Just use [`<renameColumn`](http://www.liquibase.org/documentation/changes/rename_column.html), like we have in a few cases in the changelog history already.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1219#issuecomment-237705566
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-236272656:0,Deployability,update,update,0,update title to include triage. Just because someone put in a PBE TODO doesn't mean it absolutely has to be done. Use good judgement ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-236272656
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2112,Deployability,toggle,toggles,2112," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2556,Deployability,toggle,toggles,2556," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:227,Energy Efficiency,Adapt,Adapt,227,"I find 18 occurrences. `./backend/src/main/scala/cromwell/backend/ExecutionHash.scala: // TODO: PBE: ideally hashes should be deterministic; ./backend/src/test/scala/cromwell/backend/caching/CachingConfigSpec.scala:// TODO PBE Adapt to how new caching works, but the test logic should not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/sr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:1746,Integrability,depend,dependencies,1746,"well/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:227,Modifiability,Adapt,Adapt,227,"I find 18 occurrences. `./backend/src/main/scala/cromwell/backend/ExecutionHash.scala: // TODO: PBE: ideally hashes should be deterministic; ./backend/src/test/scala/cromwell/backend/caching/CachingConfigSpec.scala:// TODO PBE Adapt to how new caching works, but the test logic should not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/sr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2185,Performance,cache,cache,2185," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2629,Performance,cache,cache,2629," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2823,Performance,race condition,race condition,2823," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2787,Safety,abort,abort,2787," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2981,Safety,abort,abort,2981," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:3012,Safety,abort,abort,3012," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:109,Security,hash,hashes,109,"I find 18 occurrences. `./backend/src/main/scala/cromwell/backend/ExecutionHash.scala: // TODO: PBE: ideally hashes should be deterministic; ./backend/src/test/scala/cromwell/backend/caching/CachingConfigSpec.scala:// TODO PBE Adapt to how new caching works, but the test logic should not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/sr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:155,Testability,test,test,155,"I find 18 occurrences. `./backend/src/main/scala/cromwell/backend/ExecutionHash.scala: // TODO: PBE: ideally hashes should be deterministic; ./backend/src/test/scala/cromwell/backend/caching/CachingConfigSpec.scala:// TODO PBE Adapt to how new caching works, but the test logic should not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/sr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:267,Testability,test,test,267,"I find 18 occurrences. `./backend/src/main/scala/cromwell/backend/ExecutionHash.scala: // TODO: PBE: ideally hashes should be deterministic; ./backend/src/test/scala/cromwell/backend/caching/CachingConfigSpec.scala:// TODO PBE Adapt to how new caching works, but the test logic should not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/sr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:272,Testability,log,logic,272,"I find 18 occurrences. `./backend/src/main/scala/cromwell/backend/ExecutionHash.scala: // TODO: PBE: ideally hashes should be deterministic; ./backend/src/test/scala/cromwell/backend/caching/CachingConfigSpec.scala:// TODO PBE Adapt to how new caching works, but the test logic should not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/sr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:322,Testability,test,test,322,"I find 18 occurrences. `./backend/src/main/scala/cromwell/backend/ExecutionHash.scala: // TODO: PBE: ideally hashes should be deterministic; ./backend/src/test/scala/cromwell/backend/caching/CachingConfigSpec.scala:// TODO PBE Adapt to how new caching works, but the test logic should not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/sr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:1099,Testability,test,test,1099,"rministic; ./backend/src/test/scala/cromwell/backend/caching/CachingConfigSpec.scala:// TODO PBE Adapt to how new caching works, but the test logic should not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:1251,Testability,test,test,1251,"ld not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:1386,Testability,test,test,1386,"his after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:1476,Testability,test,tests,1476,"ell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2223,Testability,test,test,2223," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2360,Testability,mock,mocks,2360," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2667,Testability,test,test,2667," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2763,Testability,test,test,2763," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2890,Testability,test,test,2890," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:1079,Availability,failure,failures,1079,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:982,Energy Efficiency,energy,energy,982,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:550,Integrability,message,messages,550,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:742,Integrability,message,message,742,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:299,Performance,cache,cache,299,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:442,Performance,cache,cache,442,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:578,Performance,cache,cache,578,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:658,Performance,cache,cache,658,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:899,Performance,cache,cache,899,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:769,Safety,avoid,avoid,769,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:358,Security,hash,hashing,358,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:403,Security,hash,hashing,403,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:509,Security,hash,hashes,509,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880
https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-237313379:139,Availability,down,down,139,This strawman was brave... but it wasn't good enough. Since this no longer represents our current thinking I'm going to take this strawman down.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-237313379
https://github.com/broadinstitute/cromwell/issues/1230#issuecomment-236367111:75,Usability,simpl,simple,75,"@kshakir noted in the design doc:. PLEASE be sure to not use the name as a simple string. It's a tuple3 of (namespace, repository, tag). For example ""ubuntu"" == ""library/ubuntu:latest"". Not specifying a slash implies ""library/"", and not specifying a colon implies "":latest"". . https://docs.docker.com/docker-hub/repos/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230#issuecomment-236367111
https://github.com/broadinstitute/cromwell/issues/1236#issuecomment-242167982:31,Performance,cache,cache,31,"Just a comment when there is a cache _hit_: For intermediate files (outputs of one task going into another), on local and SGE backends, we should not copy the files (symlink or hardlink would be preferred). Just to conserve storage, CPU, and time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1236#issuecomment-242167982
https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236930516:218,Modifiability,enhance,enhancements,218,"K.I.S.S. indeed. 👍 to the code diff. Your comments do help, but I'm still only at about 75% in understanding of what initialization actors can and cannot validate currently. I'm fine if folks file issues with example ""enhancements"" for the future. One could also write a large suite of tests with runtime attribute expressions that should and should not validate, but that's another ticket to be prioritized. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1240/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236930516
https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236930516:154,Security,validat,validate,154,"K.I.S.S. indeed. 👍 to the code diff. Your comments do help, but I'm still only at about 75% in understanding of what initialization actors can and cannot validate currently. I'm fine if folks file issues with example ""enhancements"" for the future. One could also write a large suite of tests with runtime attribute expressions that should and should not validate, but that's another ticket to be prioritized. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1240/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236930516
https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236930516:354,Security,validat,validate,354,"K.I.S.S. indeed. 👍 to the code diff. Your comments do help, but I'm still only at about 75% in understanding of what initialization actors can and cannot validate currently. I'm fine if folks file issues with example ""enhancements"" for the future. One could also write a large suite of tests with runtime attribute expressions that should and should not validate, but that's another ticket to be prioritized. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1240/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236930516
https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236930516:286,Testability,test,tests,286,"K.I.S.S. indeed. 👍 to the code diff. Your comments do help, but I'm still only at about 75% in understanding of what initialization actors can and cannot validate currently. I'm fine if folks file issues with example ""enhancements"" for the future. One could also write a large suite of tests with runtime attribute expressions that should and should not validate, but that's another ticket to be prioritized. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1240/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236930516
https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236984808:59,Usability,clear,clear,59,"@kshakir I think I might be able to make the comments more clear, I'll give that a try. @cjllanwarne Can't be better than the Spirit of 1776. 🇺🇸 🎆",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236984808
https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-253933381:197,Performance,perform,performed,197,cf: `WorkflowManagerActor`:. ```; [190] jobStoreActor ! RegisterWorkflowCompleted(workflowId); ...; [196] workflowStore ! WorkflowStoreActor.RemoveWorkflow(workflowId); ```. Ideally these would be performed in order rather than simultaneously.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-253933381
https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-304958771:107,Availability,recover,recovered,107,"Just to clarify, I think the motivation here was to make sure the workflow that went terminal does not get recovered and a job that already ran once gets re-run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-304958771
https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-304958771:107,Safety,recover,recovered,107,"Just to clarify, I think the motivation here was to make sure the workflow that went terminal does not get recovered and a job that already ran once gets re-run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-304958771
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:446,Availability,down,down,446,"In a scatter of 20500 shards, we ran a task that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:471,Availability,error,errors,471,"In a scatter of 20500 shards, we ran a task that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6745,Availability,down,down,6745,"mwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:11238,Availability,error,errors,11238,ute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:933) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:927) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. We suspect that of all the retries from the errors above plus the new succeeded tasks that now needed post processing created a backlog that may have cause Cromwell to become sluggish and eventually run out of memory. It may have also been the opposite direction where Cromwell becoming sluggish caused these errors to start popping up. @dshiga any additions to explain what we saw when you launched the 20k callset with 3000 intervals?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:11503,Availability,error,errors,11503,ute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:933) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:927) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. We suspect that of all the retries from the errors above plus the new succeeded tasks that now needed post processing created a backlog that may have cause Cromwell to become sluggish and eventually run out of memory. It may have also been the opposite direction where Cromwell becoming sluggish caused these errors to start popping up. @dshiga any additions to explain what we saw when you launched the 20k callset with 3000 intervals?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:1136,Integrability,protocol,protocol,1136,"b where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:1229,Integrability,protocol,protocol,1229,"nt relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$3.apply(GcsFileSystemProvider.scala:242) ~[cromwell.jar:0.19",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:1363,Integrability,protocol,protocol,1363,"r about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$3.apply(GcsFileSystemProvider.scala:242) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$3.apply(GcsFileSystemProvider.scala:242) ~[cromwell.jar",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8732,Integrability,protocol,protocol,8732,LSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(R,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8825,Integrability,protocol,protocol,8825,r.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8959,Integrability,protocol,protocol,8959,shaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwel,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:876,Performance,perform,performInitialHandshake,876,"In a scatter of 20500 shards, we ran a task that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:5888,Performance,concurren,concurrent,5888,engine$backend$jes$JesBackend$$outputLookup(JesBackend.scala:695) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$getOutputFoldingFunction$1.apply(JesBackend.scala:675) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$getOutputFoldingFunction$1.apply(JesBackend.scala:674) ~[cromwell.jar:0.19]; at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.scala:84) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:665) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketIm,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:5999,Performance,concurren,concurrent,5999,"end.jes.JesBackend$$anonfun$getOutputFoldingFunction$1.apply(JesBackend.scala:675) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$getOutputFoldingFunction$1.apply(JesBackend.scala:674) ~[cromwell.jar:0.19]; at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.scala:84) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:665) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-syst",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6306,Performance,concurren,concurrent,6306,"cala:124) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.scala:84) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:665) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6399,Performance,concurren,concurrent,6399,"cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:665) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6504,Performance,concurren,concurrent,6504,"l.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6601,Performance,concurren,concurrent,6601,"d.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:705) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8472,Performance,perform,performInitialHandshake,8472,ite(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparse,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:10382,Performance,concurren,concurrent,10382,mwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:933) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:927) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. We suspect that of all the retries from the errors above plus the new succeeded tasks that now needed post processing created a backlog that may have cause Cromwell to become sluggish and eventu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:10492,Performance,concurren,concurrent,10492,ractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:933) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:927) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. We suspect that of all the retries from the errors above plus the new succeeded tasks that now needed post processing created a backlog that may have cause Cromwell to become sluggish and eventually run out of memory. It may have also been the opposite direction where Cromwell becoming sluggish caused t,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:10797,Performance,concurren,concurrent,10797,ute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:933) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:927) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. We suspect that of all the retries from the errors above plus the new succeeded tasks that now needed post processing created a backlog that may have cause Cromwell to become sluggish and eventually run out of memory. It may have also been the opposite direction where Cromwell becoming sluggish caused these errors to start popping up. @dshiga any additions to explain what we saw when you launched the 20k callset with 3000 intervals?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:10890,Performance,concurren,concurrent,10890,ute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:933) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:927) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. We suspect that of all the retries from the errors above plus the new succeeded tasks that now needed post processing created a backlog that may have cause Cromwell to become sluggish and eventually run out of memory. It may have also been the opposite direction where Cromwell becoming sluggish caused these errors to start popping up. @dshiga any additions to explain what we saw when you launched the 20k callset with 3000 intervals?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:10995,Performance,concurren,concurrent,10995,ute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:933) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:927) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. We suspect that of all the retries from the errors above plus the new succeeded tasks that now needed post processing created a backlog that may have cause Cromwell to become sluggish and eventually run out of memory. It may have also been the opposite direction where Cromwell becoming sluggish caused these errors to start popping up. @dshiga any additions to explain what we saw when you launched the 20k callset with 3000 intervals?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:11092,Performance,concurren,concurrent,11092,ute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:933) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:927) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. We suspect that of all the retries from the errors above plus the new succeeded tasks that now needed post processing created a backlog that may have cause Cromwell to become sluggish and eventually run out of memory. It may have also been the opposite direction where Cromwell becoming sluggish caused these errors to start popping up. @dshiga any additions to explain what we saw when you launched the 20k callset with 3000 intervals?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:764,Security,secur,security,764,"In a scatter of 20500 shards, we ran a task that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:849,Security,secur,security,849,"In a scatter of 20500 shards, we ran a task that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:948,Security,secur,security,948,"In a scatter of 20500 shards, we ran a task that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:1038,Security,secur,security,1038,"k that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(Abstr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6770,Security,secur,security,6770,"gine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.j",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6845,Security,secur,security,6845,"la:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7379,Security,secur,security,7379,"0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7463,Security,secur,security,7463,":1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7541,Security,secur,security,7541,"r(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.se",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7635,Security,secur,security,7635,"ad.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.securi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7721,Security,secur,security,7721,"tion: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.ne",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7807,Security,secur,security,7807,".java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7897,Security,secur,security,7897,":973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHtt",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8004,Security,secur,security,8004,"akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(Http",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8101,Security,secur,security,8101,ception: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8196,Security,secur,security,8196,; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpReques,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8276,Security,secur,security,8276,.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.g,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8359,Security,secur,security,8359,8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClient,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8445,Security,secur,security,8445,0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.A,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8544,Security,secur,security,8544,y.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.1,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8634,Security,secur,security,8634,urity.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(Abstr,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:485,Testability,log,logs,485,"In a scatter of 20500 shards, we ran a task that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696:142,Availability,avail,available,142,"@jsotobroad As scatter/gather seem to be concurrency events I think you're running into the maximum IOPS (Input/Output Operations Per Second) available from Google. The simulatneous Google disk IOPS are as follows, based on the following link and shown in the table below:. https://cloud.google.com/compute/docs/disks/performance#type_comparison. | Read | Write |; | --- | --- |; | 3000 IOPS | 0 IOPS |; | 2250 IOPS | 3750 IOPS |; | 1500 IOPS | 7500 IOPS |; | 750 IOPS | 11250 IOPS |; | 0 IOPS | 15000 IOPS |. There are other ways this might be tackled where the IOPS is throttled, or a few architectural changes would need to be done where it would write locally to the VM and then transferred over in a preemptive way. There is a microservices approach, but that is a major architectural change for Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696:41,Performance,concurren,concurrency,41,"@jsotobroad As scatter/gather seem to be concurrency events I think you're running into the maximum IOPS (Input/Output Operations Per Second) available from Google. The simulatneous Google disk IOPS are as follows, based on the following link and shown in the table below:. https://cloud.google.com/compute/docs/disks/performance#type_comparison. | Read | Write |; | --- | --- |; | 3000 IOPS | 0 IOPS |; | 2250 IOPS | 3750 IOPS |; | 1500 IOPS | 7500 IOPS |; | 750 IOPS | 11250 IOPS |; | 0 IOPS | 15000 IOPS |. There are other ways this might be tackled where the IOPS is throttled, or a few architectural changes would need to be done where it would write locally to the VM and then transferred over in a preemptive way. There is a microservices approach, but that is a major architectural change for Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696:318,Performance,perform,performance,318,"@jsotobroad As scatter/gather seem to be concurrency events I think you're running into the maximum IOPS (Input/Output Operations Per Second) available from Google. The simulatneous Google disk IOPS are as follows, based on the following link and shown in the table below:. https://cloud.google.com/compute/docs/disks/performance#type_comparison. | Read | Write |; | --- | --- |; | 3000 IOPS | 0 IOPS |; | 2250 IOPS | 3750 IOPS |; | 1500 IOPS | 7500 IOPS |; | 750 IOPS | 11250 IOPS |; | 0 IOPS | 15000 IOPS |. There are other ways this might be tackled where the IOPS is throttled, or a few architectural changes would need to be done where it would write locally to the VM and then transferred over in a preemptive way. There is a microservices approach, but that is a major architectural change for Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696
https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696:571,Performance,throttle,throttled,571,"@jsotobroad As scatter/gather seem to be concurrency events I think you're running into the maximum IOPS (Input/Output Operations Per Second) available from Google. The simulatneous Google disk IOPS are as follows, based on the following link and shown in the table below:. https://cloud.google.com/compute/docs/disks/performance#type_comparison. | Read | Write |; | --- | --- |; | 3000 IOPS | 0 IOPS |; | 2250 IOPS | 3750 IOPS |; | 1500 IOPS | 7500 IOPS |; | 750 IOPS | 11250 IOPS |; | 0 IOPS | 15000 IOPS |. There are other ways this might be tackled where the IOPS is throttled, or a few architectural changes would need to be done where it would write locally to the VM and then transferred over in a preemptive way. There is a microservices approach, but that is a major architectural change for Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696
https://github.com/broadinstitute/cromwell/issues/1251#issuecomment-253931720:33,Security,hash,hashes,33,"I believe this was only used for hashes, which are no longer part of Cromwells v21+",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1251#issuecomment-253931720
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:708,Availability,failure,failure,708,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:778,Availability,error,error,778,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:2026,Availability,echo,echoHelloWorld,2026,"w workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignments: printHelloAndGoodbye.echoHelloWorld -> Jes; 2016-09-09 15:50:56,282 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-09-09 15:50:56,286 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-09-09 15:50:56,326 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-09-09 15:50:58,078 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is now terminal. Shutting down.; 2016-09-09 15:50:58,084 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:2308,Availability,down,down,2308,"9-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignments: printHelloAndGoodbye.echoHelloWorld -> Jes; 2016-09-09 15:50:56,282 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-09-09 15:50:56,286 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-09-09 15:50:56,326 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-09-09 15:50:58,078 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is now terminal. Shutting down.; 2016-09-09 15:50:58,084 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from InitializingWorkflowState to ExecutingWor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:3012,Availability,down,down,3012,"tHelloAndGoodbye.echoHelloWorld -> Jes; 2016-09-09 15:50:56,282 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-09-09 15:50:56,286 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-09-09 15:50:56,326 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-09-09 15:50:58,078 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is now terminal. Shutting down.; 2016-09-09 15:50:58,084 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from InitializingWorkflowState to ExecutingWorkflowState; 2016-09-09 15:50:58,130 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Starting calls: printHelloAndGoodbye.echoHelloWorld:NA:1; 2016-09-09 15:50:58,138 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - EJEA_aed1aad8:printHelloAndGoodbye.echoHelloWorld:-1:1: Effective call caching mode: CallCachingOff; 2016-09-09 15:50:58,139 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-disp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:3445,Availability,echo,echoHelloWorld,3445,"ing from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-09-09 15:50:56,326 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-09-09 15:50:58,078 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is now terminal. Shutting down.; 2016-09-09 15:50:58,084 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from InitializingWorkflowState to ExecutingWorkflowState; 2016-09-09 15:50:58,130 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Starting calls: printHelloAndGoodbye.echoHelloWorld:NA:1; 2016-09-09 15:50:58,138 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - EJEA_aed1aad8:printHelloAndGoodbye.echoHelloWorld:-1:1: Effective call caching mode: CallCachingOff; 2016-09-09 15:50:58,139 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:3586,Availability,echo,echoHelloWorld,3586,"rkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-09-09 15:50:58,078 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is now terminal. Shutting down.; 2016-09-09 15:50:58,084 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from InitializingWorkflowState to ExecutingWorkflowState; 2016-09-09 15:50:58,130 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Starting calls: printHelloAndGoodbye.echoHelloWorld:NA:1; 2016-09-09 15:50:58,138 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - EJEA_aed1aad8:printHelloAndGoodbye.echoHelloWorld:-1:1: Effective call caching mode: CallCachingOff; 2016-09-09 15:50:58,139 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4694,Availability,ERROR,ERROR,4694,"ecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:N",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4822,Availability,echo,echoHelloWorld,4822,"aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4943,Availability,echo,echoHelloWorld,4943,"2 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:5173,Availability,echo,echoHelloWorld,5173,"s.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] received an unhandled message: SucceededResponse(printHelloAndGoodbye.echoHelloWorld:-1:1,Some(0),Map()) in state: WorkflowExecutionA",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:5265,Availability,echo,echoHelloWorld,5265,"er-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] received an unhandled message: SucceededResponse(printHelloAndGoodbye.echoHelloWorld:-1:1,Some(0),Map()) in state: WorkflowExecutionAbortingState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:5449,Availability,echo,echoHelloWorld,5449,"er-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] received an unhandled message: SucceededResponse(printHelloAndGoodbye.echoHelloWorld:-1:1,Some(0),Map()) in state: WorkflowExecutionAbortingState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:5541,Availability,echo,echoHelloWorld,5541,"er-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] received an unhandled message: SucceededResponse(printHelloAndGoodbye.echoHelloWorld:-1:1,Some(0),Map()) in state: WorkflowExecutionAbortingState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:5731,Availability,echo,echoHelloWorld,5731,"er-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] received an unhandled message: SucceededResponse(printHelloAndGoodbye.echoHelloWorld:-1:1,Some(0),Map()) in state: WorkflowExecutionAbortingState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:5823,Availability,echo,echoHelloWorld,5823,"er-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] received an unhandled message: SucceededResponse(printHelloAndGoodbye.echoHelloWorld:-1:1,Some(0),Map()) in state: WorkflowExecutionAbortingState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:6156,Availability,echo,echoHelloWorld,6156,"er-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] received an unhandled message: SucceededResponse(printHelloAndGoodbye.echoHelloWorld:-1:1,Some(0),Map()) in state: WorkflowExecutionAbortingState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:796,Integrability,message,messages,796,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4713,Integrability,message,message,4713,"ecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:N",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:6108,Integrability,message,message,6108,"er-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] received an unhandled message: SucceededResponse(printHelloAndGoodbye.echoHelloWorld:-1:1,Some(0),Map()) in state: WorkflowExecutionAbortingState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:529,Performance,race condition,race condition,529,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:184,Safety,abort,abort,184,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:230,Safety,abort,abort,230,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:283,Safety,abort,abort,283,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:395,Safety,abort,abort,395,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:731,Safety,abort,abort,731,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4290,Safety,Abort,Abort,4290,"8,130 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Starting calls: printHelloAndGoodbye.echoHelloWorld:NA:1; 2016-09-09 15:50:58,138 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - EJEA_aed1aad8:printHelloAndGoodbye.echoHelloWorld:-1:1: Effective call caching mode: CallCachingOff; 2016-09-09 15:50:58,139 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodby",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4306,Safety,Abort,Aborting,4306,"wExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Starting calls: printHelloAndGoodbye.echoHelloWorld:NA:1; 2016-09-09 15:50:58,138 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - EJEA_aed1aad8:printHelloAndGoodbye.echoHelloWorld:-1:1: Effective call caching mode: CallCachingOff; 2016-09-09 15:50:58,139 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733
https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237932020:62,Modifiability,inherit,inheritance,62,"IMO this is an example where composition would be better than inheritance. If we just passed a general-purpose `KeyValueServiceActor` a `databaseLike: KeyValueDatabaseInterface` at construction time (for us, an SQL-backed database), this might look a lot nicer",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237932020
https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237934228:179,Modifiability,refactor,refactoring,179,@cjllanwarne I think that should be the second step to do (I was referring previously to that with a data abstraction layer -DAL- implementation) but It may involve a lot of more refactoring since you will need to modify database package to make it generic.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237934228
https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237934690:191,Modifiability,layers,layers,191,"@francares oh ok, as long as it's on the long range plan. Incidentally, the other DB components (job store, workflow store) use a composition model without going too overboard on abstraction layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237934690
https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563:55,Integrability,depend,dependency,55,"@cjllanwarne Yes, but for example Workflow store has a dependency on WorkflowStoreSqlDatabase trait, so that means you can not use a NoSQLDatabase impl.; In order to allow WorkflowStore to do that you will need to implement a generic interface to work with different specializations of dbs and for that you will need to define a DAL and perform a bigger refactoring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563
https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563:234,Integrability,interface,interface,234,"@cjllanwarne Yes, but for example Workflow store has a dependency on WorkflowStoreSqlDatabase trait, so that means you can not use a NoSQLDatabase impl.; In order to allow WorkflowStore to do that you will need to implement a generic interface to work with different specializations of dbs and for that you will need to define a DAL and perform a bigger refactoring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563
https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563:354,Modifiability,refactor,refactoring,354,"@cjllanwarne Yes, but for example Workflow store has a dependency on WorkflowStoreSqlDatabase trait, so that means you can not use a NoSQLDatabase impl.; In order to allow WorkflowStore to do that you will need to implement a generic interface to work with different specializations of dbs and for that you will need to define a DAL and perform a bigger refactoring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563
https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563:337,Performance,perform,perform,337,"@cjllanwarne Yes, but for example Workflow store has a dependency on WorkflowStoreSqlDatabase trait, so that means you can not use a NoSQLDatabase impl.; In order to allow WorkflowStore to do that you will need to implement a generic interface to work with different specializations of dbs and for that you will need to define a DAL and perform a bigger refactoring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:74,Deployability,configurat,configuration,74,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:170,Deployability,configurat,configuration,170,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:405,Deployability,configurat,configuration,405,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:551,Deployability,update,update,551,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:941,Deployability,update,update,941,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:240,Integrability,message,message,240,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:74,Modifiability,config,configuration,74,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:170,Modifiability,config,configuration,170,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:375,Modifiability,config,config,375,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:405,Modifiability,config,configuration,405,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:492,Modifiability,config,config,492,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:600,Modifiability,config,config,600,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:952,Modifiability,config,configure,952,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849
https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-324165426:42,Modifiability,refactor,refactoring,42,@mcovarr is this solved by your excellent refactoring of the CLI?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-324165426
https://github.com/broadinstitute/cromwell/issues/1261#issuecomment-238340358:29,Usability,clear,clear,29,Ok. Perfect. There was not a clear disclaimer about Windows. Sorry,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261#issuecomment-238340358
https://github.com/broadinstitute/cromwell/issues/1264#issuecomment-303756279:159,Integrability,depend,dependency,159,"According to the [jetbrains](http://www.jetbrains.org/intellij/sdk/docs/basics/getting_started/plugin_compatibility.html) website, we might just need to add a dependency and hey-presto, it'll be picked up for all other jetbrains IDEs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1264#issuecomment-303756279
https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238699654:5,Testability,test,tests,5,Some tests for the new db stuff?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238699654
https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238703519:12,Testability,test,testing,12,@mcovarr re testing - I would agree that some CallCaching tests would be nice but... I would argue that this DB connection layer wouldn't be an obvious candidate for unit tests. I actually think perhaps enhancing the EJEA tests would be more worthwhile but definitely some centaur tests. I can write up a ticket for those,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238703519
https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238703519:58,Testability,test,tests,58,@mcovarr re testing - I would agree that some CallCaching tests would be nice but... I would argue that this DB connection layer wouldn't be an obvious candidate for unit tests. I actually think perhaps enhancing the EJEA tests would be more worthwhile but definitely some centaur tests. I can write up a ticket for those,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238703519
https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238703519:171,Testability,test,tests,171,@mcovarr re testing - I would agree that some CallCaching tests would be nice but... I would argue that this DB connection layer wouldn't be an obvious candidate for unit tests. I actually think perhaps enhancing the EJEA tests would be more worthwhile but definitely some centaur tests. I can write up a ticket for those,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238703519
https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238703519:222,Testability,test,tests,222,@mcovarr re testing - I would agree that some CallCaching tests would be nice but... I would argue that this DB connection layer wouldn't be an obvious candidate for unit tests. I actually think perhaps enhancing the EJEA tests would be more worthwhile but definitely some centaur tests. I can write up a ticket for those,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238703519
https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238703519:281,Testability,test,tests,281,@mcovarr re testing - I would agree that some CallCaching tests would be nice but... I would argue that this DB connection layer wouldn't be an obvious candidate for unit tests. I actually think perhaps enhancing the EJEA tests would be more worthwhile but definitely some centaur tests. I can write up a ticket for those,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238703519
https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238709074:41,Testability,test,tests,41,@cjllanwarne there _are_ some CC centaur tests FWIW,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238709074
https://github.com/broadinstitute/cromwell/issues/1269#issuecomment-238569423:48,Testability,test,tests,48,"For now in the code I'm working on, I'm marking tests which I find highly suspect w/ a ""FLAGGED"" comment w/ some explanation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269#issuecomment-238569423
https://github.com/broadinstitute/cromwell/issues/1271#issuecomment-253887769:24,Deployability,release,release,24,yes this is in the 0.22 release,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1271#issuecomment-253887769
https://github.com/broadinstitute/cromwell/issues/1272#issuecomment-238694173:4,Availability,error,error,4,"The error you see in centaur looks like. > Metadata mismatch for failures.0.message - expected: ""Task invalid_runtime_attributes has an invalid runtime attribute continueOnReturnCode = \""oops\"""" but got: ""None.get""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272#issuecomment-238694173
https://github.com/broadinstitute/cromwell/issues/1272#issuecomment-238694173:65,Availability,failure,failures,65,"The error you see in centaur looks like. > Metadata mismatch for failures.0.message - expected: ""Task invalid_runtime_attributes has an invalid runtime attribute continueOnReturnCode = \""oops\"""" but got: ""None.get""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272#issuecomment-238694173
https://github.com/broadinstitute/cromwell/issues/1272#issuecomment-238694173:76,Integrability,message,message,76,"The error you see in centaur looks like. > Metadata mismatch for failures.0.message - expected: ""Task invalid_runtime_attributes has an invalid runtime attribute continueOnReturnCode = \""oops\"""" but got: ""None.get""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272#issuecomment-238694173
https://github.com/broadinstitute/cromwell/pull/1273#issuecomment-238712758:31,Performance,concurren,concurrency,31,"I made some comments regarding concurrency & thread safety. Those weren't a roundabout way of me saying I thought there _was_ a problem, rather I just wanted to make sure that was thought through due to the way that's being called. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1273/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1273#issuecomment-238712758
https://github.com/broadinstitute/cromwell/pull/1273#issuecomment-238712758:52,Safety,safe,safety,52,"I made some comments regarding concurrency & thread safety. Those weren't a roundabout way of me saying I thought there _was_ a problem, rather I just wanted to make sure that was thought through due to the way that's being called. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1273/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1273#issuecomment-238712758
https://github.com/broadinstitute/cromwell/issues/1276#issuecomment-239225597:48,Performance,cache,cacheBetweenWF,48,"There are already call caching WFs in centaur - cacheBetweenWF, cacheWithinWF, readFromCache, writeToCache. All four are currently set to 'ignore' though, so they're not run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1276#issuecomment-239225597
https://github.com/broadinstitute/cromwell/issues/1276#issuecomment-239225597:64,Performance,cache,cacheWithinWF,64,"There are already call caching WFs in centaur - cacheBetweenWF, cacheWithinWF, readFromCache, writeToCache. All four are currently set to 'ignore' though, so they're not run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1276#issuecomment-239225597
https://github.com/broadinstitute/cromwell/pull/1277#issuecomment-239000015:17,Testability,test,test,17,":+1: delta your ""test at scale"" item. 😄 Very cool!. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1277/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1277#issuecomment-239000015
https://github.com/broadinstitute/cromwell/pull/1277#issuecomment-243643308:304,Integrability,wrap,wrapper,304,"Re: the `hsqldb compliant` commit: There should be multiple ways to check at runtime the database type, and then customize the code/SQL within the `CustomTaskChange`. For example, matching on the `liquibase.database.Database`, or go further and use the `Database.getDatabaseProductName`. The latter is a wrapper to `java.sql.Connection.getMetadata.getDatabaseProductName`. For HSQL it should return `HsqlDatabaseProperties.PRODUCT_NAME`, or the hard coded string `""MySQL""`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1277#issuecomment-243643308
https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-238893135:419,Integrability,synchroniz,synchronizing,419,"Hi Jeff,. But the Google Storage backend endpoints still operate on the idea of an exponential backoff, as both are REST-based:. https://cloud.google.com/storage/docs/xml-api/overview. https://cloud.google.com/storage/docs/json_api/. So there is no way around needing to wait. Maybe having an exponential backoff would help instead of the `Thread.sleep(retryInterval.toMillis.toInt)` of 500 milliseconds, as now you're synchronizing a subset of threads to wake up at the same time - becoming similar to a [Thundering herd problem](https://en.wikipedia.org/wiki/Thundering_herd_problem) - which in this case is a subcluster of threads that would succeed instead of just one. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-238893135
https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-238992929:29,Energy Efficiency,schedul,scheduler,29,"Exactly. When you use Akka's scheduler it's not blocking a thread, which leads to much happiness.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-238992929
https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-276488183:48,Performance,scalab,scalability,48,"Closing as a duplicate of newer, more ambition, scalability tickets",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-276488183
https://github.com/broadinstitute/cromwell/issues/1282#issuecomment-239239863:21,Security,threat,threatstack,21,"Apropos: http://blog.threatstack.com/useful-scalac-options-for-better-scala-development-part-1. Although as @mcovarr pointed out, IntelliJ does a good job of flagging this stuff",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1282#issuecomment-239239863
https://github.com/broadinstitute/cromwell/pull/1286#issuecomment-239452983:8,Usability,clear,clear,8,👍 looks clear!. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1286/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1286#issuecomment-239452983
https://github.com/broadinstitute/cromwell/pull/1286#issuecomment-239455401:106,Availability,down,down,106,"I'm going to take a quick stab at grabbing the WFID, a thought I had last night is it'd be badass to suck down the centaur log and emit it directly. If it is indeed a pain I'll just merge this for now",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1286#issuecomment-239455401
https://github.com/broadinstitute/cromwell/pull/1286#issuecomment-239455401:123,Testability,log,log,123,"I'm going to take a quick stab at grabbing the WFID, a thought I had last night is it'd be badass to suck down the centaur log and emit it directly. If it is indeed a pain I'll just merge this for now",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1286#issuecomment-239455401
https://github.com/broadinstitute/cromwell/issues/1287#issuecomment-248626136:97,Testability,test,tested,97,"This may already be addressed for SGE. I.e. Control-c may already work; fine for SGE. I have not tested it for SGE. On Sep 21, 2016 10:07 AM, ""kcibul"" notifications@github.com wrote:. > I get this for local backends... but can you explain what you're seeing; > with SGE backend in this regard?; > ; > ---; > ; > Kristian Cibulskis; > Chief Architect, Data Sciences & Data Engineering; > Broad Institute of MIT and Harvard; > kcibul@broadinstitute.org; > ; > On Wed, Sep 21, 2016 at 10:03 AM, Lee Lichtenstein <; > notifications@github.com; > ; > > wrote:; > > ; > > @kcibul https://github.com/kcibul This one is a big problem,; > > especially; > > when running on local backends and SGE backends. Is it fixed in 0.21?; > > ; > > —; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > <https://github.com/broadinstitute/cromwell/issues/1287#issuecomment-; > > 248621800>,; > > or mute the thread; > > <https://github.com/notifications/unsubscribe-auth/ABW4g8EGJxLgZgZ2jM4-; > > Esblwm61N2NPks5qsTkigaJpZM4JieEi>; > > .; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1287#issuecomment-248623111,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk_MtGoyQtYWsy5Nus0IFhvz0ct6Xks5qsTo4gaJpZM4JieEi; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1287#issuecomment-248626136
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239537352:184,Deployability,pipeline,pipelines,184,"Hi @dinvlad ...; - The ""Docker"" backend referenced in application.conf is just using straight docker on the host machine. There's also ""JES"" backend which is using the google genomics pipelines API, which is effectively docker-as-a-service; - There are indeed plans to support both AWS and Azure. Some (probably crude at first) support for one of the two is expected within a couple of months. Over the course of then ext few quarters we expect to support both as well as other cloud vendors as well. In terms of how they'd be done, the answer is It Depends. There's the budding [GA4GH Task Execution API](https://github.com/ga4gh/task-execution-schemas) which was heavily inspired by the google genomics pipeline API. Our hope is to see other cloud vendors support this API, which would make our lives easier. Assuming that doens't happen, we've experimented a bit with Azure, and the remote docker approach has made the most sense. The actual outcome is not set in stone at the moment. Does this answer your questions?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239537352
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239537352:705,Deployability,pipeline,pipeline,705,"Hi @dinvlad ...; - The ""Docker"" backend referenced in application.conf is just using straight docker on the host machine. There's also ""JES"" backend which is using the google genomics pipelines API, which is effectively docker-as-a-service; - There are indeed plans to support both AWS and Azure. Some (probably crude at first) support for one of the two is expected within a couple of months. Over the course of then ext few quarters we expect to support both as well as other cloud vendors as well. In terms of how they'd be done, the answer is It Depends. There's the budding [GA4GH Task Execution API](https://github.com/ga4gh/task-execution-schemas) which was heavily inspired by the google genomics pipeline API. Our hope is to see other cloud vendors support this API, which would make our lives easier. Assuming that doens't happen, we've experimented a bit with Azure, and the remote docker approach has made the most sense. The actual outcome is not set in stone at the moment. Does this answer your questions?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239537352
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239537352:550,Integrability,Depend,Depends,550,"Hi @dinvlad ...; - The ""Docker"" backend referenced in application.conf is just using straight docker on the host machine. There's also ""JES"" backend which is using the google genomics pipelines API, which is effectively docker-as-a-service; - There are indeed plans to support both AWS and Azure. Some (probably crude at first) support for one of the two is expected within a couple of months. Over the course of then ext few quarters we expect to support both as well as other cloud vendors as well. In terms of how they'd be done, the answer is It Depends. There's the budding [GA4GH Task Execution API](https://github.com/ga4gh/task-execution-schemas) which was heavily inspired by the google genomics pipeline API. Our hope is to see other cloud vendors support this API, which would make our lives easier. Assuming that doens't happen, we've experimented a bit with Azure, and the remote docker approach has made the most sense. The actual outcome is not set in stone at the moment. Does this answer your questions?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239537352
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121:186,Energy Efficiency,schedul,schedulers,186,"Thank you!. A related question is - could the existing APIs be used to make custom implementations of ""task call""?; In particular, AWS ECS offers a very simple way to create custom task schedulers, which could process an API call with relative ease. One could then schedule a task on ECS instead of JES based on that API call.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121:265,Energy Efficiency,schedul,schedule,265,"Thank you!. A related question is - could the existing APIs be used to make custom implementations of ""task call""?; In particular, AWS ECS offers a very simple way to create custom task schedulers, which could process an API call with relative ease. One could then schedule a task on ECS instead of JES based on that API call.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121:153,Usability,simpl,simple,153,"Thank you!. A related question is - could the existing APIs be used to make custom implementations of ""task call""?; In particular, AWS ECS offers a very simple way to create custom task schedulers, which could process an API call with relative ease. One could then schedule a task on ECS instead of JES based on that API call.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239899256:68,Integrability,interface,interfaces,68,"@geoffjentry I think what I was looking for is defined in the Scala interfaces for backends. I.e. I could create a custom backend to support AWS by implementing those (so it wouldn't be a web API, just an application PI).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239899256
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-240273099:26,Testability,test,test,26,Great! Looking forward to test early builds.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-240273099
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-418414639:514,Usability,usab,usability,514,"@EvanTheB Without going too far into the details we do hope to see Azure support in the coming quarter or so. Regarding k8s, you might want to look at the work on the [TESK backend](https://github.com/broadinstitute/cromwell/pull/4008) which supports ELIXIR's [TESK](https://github.com/EMBL-EBI-TSI/TESK) project. It's still early days there (e.g. you are limited to using `FTP` based file transfer, which introduces a host of issues) but I'm sure they'd love more people helping them to move that project towards usability",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-418414639
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-553080157:38,Availability,avail,available,38,"Hi everyone, Cromwell on Azure is now available here:. https://github.com/microsoft/CromwellOnAzure. Please let me know if you have any questions or issues. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-553080157
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-257604448:329,Availability,failure,failure,329,"Further discussed points:; - The delete endpoint will be implemented by immediately marking a workflow as effectively ""to be deleted""; - An actor will occasionally sweep through these ""to be deleted"" workflows and remove the previously discussed bullet points; - Upon success, the workflow will be gone from the database; - Upon failure, the workflow will be left in a ""failed to delete"" state in the database; - One may re-mark an existing workflow as ""to be deleted"" by re-using the delete endpoint; - If a workflow output is already deleted, we won't try to re-delete it. However--; - As the refresh token will not be available, output files that were generated with refresh tokens will _not_ be able to be deleted. These types of errors, while expected, will require more planning on how to handle these permissions issues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-257604448
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-257604448:621,Availability,avail,available,621,"Further discussed points:; - The delete endpoint will be implemented by immediately marking a workflow as effectively ""to be deleted""; - An actor will occasionally sweep through these ""to be deleted"" workflows and remove the previously discussed bullet points; - Upon success, the workflow will be gone from the database; - Upon failure, the workflow will be left in a ""failed to delete"" state in the database; - One may re-mark an existing workflow as ""to be deleted"" by re-using the delete endpoint; - If a workflow output is already deleted, we won't try to re-delete it. However--; - As the refresh token will not be available, output files that were generated with refresh tokens will _not_ be able to be deleted. These types of errors, while expected, will require more planning on how to handle these permissions issues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-257604448
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-257604448:734,Availability,error,errors,734,"Further discussed points:; - The delete endpoint will be implemented by immediately marking a workflow as effectively ""to be deleted""; - An actor will occasionally sweep through these ""to be deleted"" workflows and remove the previously discussed bullet points; - Upon success, the workflow will be gone from the database; - Upon failure, the workflow will be left in a ""failed to delete"" state in the database; - One may re-mark an existing workflow as ""to be deleted"" by re-using the delete endpoint; - If a workflow output is already deleted, we won't try to re-delete it. However--; - As the refresh token will not be available, output files that were generated with refresh tokens will _not_ be able to be deleted. These types of errors, while expected, will require more planning on how to handle these permissions issues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-257604448
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-325483057:167,Safety,Risk,Risk,167,"As a **workflow runner**, I want **to delete information I no longer need from a workflow**, so that I can **stop paying for storage for it**.; - Effort: **Large**; - Risk: **Medium to Large**; - - implications for call caching?; - - provenance?; - Business value: **Medium to Large**; - - big cost-saver",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-325483057
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653:326,Performance,cache,cache,326,"This one comes up *a lot*, and will even more once people can see how much they are spending on intermediate results in FireCloud. I wanted to ask about the effort -- why large? Alex has already implemented the logic for what to delete by looking at the workflow metadata, and if we simply had an endpoint to ""evict from call cache"" we could build this outside the system pretty easily. Basically, iterate through a workflow and for every ""output"" that is not part of the workflow ""output"", you rm the file and evict the call from the task that produced it (or maybe any task that uses that output?). What are you thinking of that I'm missing? Or maybe I'm interpreting ""Large"" as being bigger than you are?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653:211,Testability,log,logic,211,"This one comes up *a lot*, and will even more once people can see how much they are spending on intermediate results in FireCloud. I wanted to ask about the effort -- why large? Alex has already implemented the logic for what to delete by looking at the workflow metadata, and if we simply had an endpoint to ""evict from call cache"" we could build this outside the system pretty easily. Basically, iterate through a workflow and for every ""output"" that is not part of the workflow ""output"", you rm the file and evict the call from the task that produced it (or maybe any task that uses that output?). What are you thinking of that I'm missing? Or maybe I'm interpreting ""Large"" as being bigger than you are?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653:283,Usability,simpl,simply,283,"This one comes up *a lot*, and will even more once people can see how much they are spending on intermediate results in FireCloud. I wanted to ask about the effort -- why large? Alex has already implemented the logic for what to delete by looking at the workflow metadata, and if we simply had an endpoint to ""evict from call cache"" we could build this outside the system pretty easily. Basically, iterate through a workflow and for every ""output"" that is not part of the workflow ""output"", you rm the file and evict the call from the task that produced it (or maybe any task that uses that output?). What are you thinking of that I'm missing? Or maybe I'm interpreting ""Large"" as being bigger than you are?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329037148:326,Security,attack,attack,326,"It just goes back to the point of the metadata architecture. It's entire intention is to be write once and then read only. People should also not be blindly removing things from it either. Totally agree w/ your 2nd paragraph but the ""remove stuff from metadata"" option is a pretty fundamental change - there are other ways to attack it if it's an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329037148
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329038021:326,Performance,cache,cache,326,"Yes -- my original comment was because of the inability to remove files created via refresh token. I think then you asked why FireCloud couldn't implement the delete functionality themselves... which they could but that seems brittle and better to centralize (e.g. they would have to crawl the metadata, evict things from the cache, etc).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329038021
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283:434,Deployability,pipeline,pipeline,434,"Thinking about this a little more, I just added this comment to Kate's doc:; ""FWIW, I don’t necessarily need Cromwell to do the delete for me in my use case. I’m happy to do it if I have all the files at hand that I want to delete. This is important because I don’t really want an all-or-nothing delete. I usually want to delete the call cached intermediate results, but not the final outputs of the workflow (and I note that not all pipeline developers list the outputs in the wdl -- I just learned this yesterday from Megan -- so I would like some control over the delete in many cases).""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283:338,Performance,cache,cached,338,"Thinking about this a little more, I just added this comment to Kate's doc:; ""FWIW, I don’t necessarily need Cromwell to do the delete for me in my use case. I’m happy to do it if I have all the files at hand that I want to delete. This is important because I don’t really want an all-or-nothing delete. I usually want to delete the call cached intermediate results, but not the final outputs of the workflow (and I note that not all pipeline developers list the outputs in the wdl -- I just learned this yesterday from Megan -- so I would like some control over the delete in many cases).""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283:492,Usability,learn,learned,492,"Thinking about this a little more, I just added this comment to Kate's doc:; ""FWIW, I don’t necessarily need Cromwell to do the delete for me in my use case. I’m happy to do it if I have all the files at hand that I want to delete. This is important because I don’t really want an all-or-nothing delete. I usually want to delete the call cached intermediate results, but not the final outputs of the workflow (and I note that not all pipeline developers list the outputs in the wdl -- I just learned this yesterday from Megan -- so I would like some control over the delete in many cases).""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329581495:303,Performance,cache,cache,303,"After much discussion and clarification (thanks @geoffjentry ), these are the 2 use cases relating to Deleting and Cleaning up files after running a workflow:; * Use case 1: I want to clean up intermediary files from my workflow, but keep all outputs. I understand that I will no longer be able to call cache on this workflow.; * Use case 2: I want to delete all files related to my workflow, I will never need it again. This includes outputs. I will not be able to call cache on this workflow. There are a few remaining questions to answer but we are getting closer. Thank you all for your input in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329581495
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329581495:471,Performance,cache,cache,471,"After much discussion and clarification (thanks @geoffjentry ), these are the 2 use cases relating to Deleting and Cleaning up files after running a workflow:; * Use case 1: I want to clean up intermediary files from my workflow, but keep all outputs. I understand that I will no longer be able to call cache on this workflow.; * Use case 2: I want to delete all files related to my workflow, I will never need it again. This includes outputs. I will not be able to call cache on this workflow. There are a few remaining questions to answer but we are getting closer. Thank you all for your input in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329581495
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-417331227:36,Deployability,release,released,36,@ernoc We hope to have this feature released in the next few months that involves being able to cleanup workflow outputs!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-417331227
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-417436977:322,Performance,cache,cache,322,We're using lifecycle policy on GCS buckets to automatically remove all data older than 2 days. That seems to work fine. We calculated that the cost of our workflows is equal to the cost of keeping the intermediate data for ~10 days in Regional storage (so it is cheaper to re-calculate everything than to restore it from cache after 10 days). So here we're just agreeing to pay ~20% overhead.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-417436977
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-424981111:145,Performance,cache,cache,145,AC: ; 1. Delete endpoint that targets deleting intermediate output files.; 2. Cleanup of those intermediate outputs if they were utilized by the cache store.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-424981111
https://github.com/broadinstitute/cromwell/pull/1293#issuecomment-240136679:21,Deployability,update,updated,21,👍 swagger yaml to be updated. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1293/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1293#issuecomment-240136679
https://github.com/broadinstitute/cromwell/issues/1294#issuecomment-240113302:71,Usability,usab,usable,71,What's the state of backends that support this API? Is the RI complete/usable?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1294#issuecomment-240113302
https://github.com/broadinstitute/cromwell/issues/1296#issuecomment-315172239:211,Performance,bottleneck,bottleneck,211,"As I commented above, the tricky part is determining where to put this stuff such that it a) makes sense and b) people will remember it is there. I don't really care, I lean towards a private repo, but the real bottleneck at the time (as I remember asking in person as well) was getting people to opine, and no one did.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1296#issuecomment-315172239
https://github.com/broadinstitute/cromwell/issues/1296#issuecomment-315180988:60,Modifiability,config,config,60,Could be that @davidbernick has something to offer here for config/secrets management,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1296#issuecomment-315180988
https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240549174:127,Security,hash,hashes,127,"`HASH_VALUE` in `CALL_CACHING_HASH` is `VARCHAR(255)`, so we shouldn't have this particular problem. But given that the Docker hashes we generate are functions of Docker image names and those seem to have the potential to be very long, we might want to think about an even larger field.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240549174
https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240592996:207,Security,hash,hashes,207,"+1 for 1024. On Aug 17, 2016 5:09 PM, ""mcovarr"" notifications@github.com wrote:. > HASH_VALUE in CALL_CACHING_HASH is VARCHAR(255), so we shouldn't have; > this particular problem. But given that the Docker hashes we generate are; > functions of Docker image names and those seem to have the potential to be; > very long, we might want to think about an even larger field.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240549174,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXkyM0b_81HFK68jJ5Hn71QHaO9qOwks5qg3h-gaJpZM4JmwYu; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240592996
https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240594318:392,Security,hash,hashes,392,"We're getting into automatic docker tag generation, so I could see getting; beyond 255. On Aug 17, 2016 8:47 PM, ""Lee Lichtenstein"" lichtens@broadinstitute.org; wrote:. > +1 for 1024; > ; > On Aug 17, 2016 5:09 PM, ""mcovarr"" notifications@github.com wrote:; > ; > > HASH_VALUE in CALL_CACHING_HASH is VARCHAR(255), so we shouldn't have; > > this particular problem. But given that the Docker hashes we generate are; > > functions of Docker image names and those seem to have the potential to be; > > very long, we might want to think about an even larger field.; > > ; > > —; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240549174,; > > or mute the thread; > > https://github.com/notifications/unsubscribe-auth/ACDXkyM0b_81HFK68jJ5Hn71QHaO9qOwks5qg3h-gaJpZM4JmwYu; > > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240594318
https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-253928605:36,Security,hash,hash,36,This is handled differently now. We hash the runtime attribute `docker` value into a fixed-length MD5 string,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-253928605
https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267175875:238,Availability,echo,echo,238,"@LeeTL1220 I tried this task below and although it's not the same exact type of syntax as what you used, I was able to use a task input variable inside of my glob: . `task createFileArray {; String dir = ""out""; command <<<; mkdir ${dir}; echo ""hullo"" > ${dir}/hello.txt; echo ""buh-bye"" > ${dir}/ciao.txt; sleep 2; >>>; output {; Array[File] out = glob(""out/*.txt"") ; Array[File] out2 = glob(dir + ""/*.txt""); #Array[File] out3 = glob(""${out}/*.txt""); }; runtime {docker:""ubuntu:latest""}; }`; out and out2 are valid task outputs, but not out3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267175875
https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267175875:271,Availability,echo,echo,271,"@LeeTL1220 I tried this task below and although it's not the same exact type of syntax as what you used, I was able to use a task input variable inside of my glob: . `task createFileArray {; String dir = ""out""; command <<<; mkdir ${dir}; echo ""hullo"" > ${dir}/hello.txt; echo ""buh-bye"" > ${dir}/ciao.txt; sleep 2; >>>; output {; Array[File] out = glob(""out/*.txt"") ; Array[File] out2 = glob(dir + ""/*.txt""); #Array[File] out3 = glob(""${out}/*.txt""); }; runtime {docker:""ubuntu:latest""}; }`; out and out2 are valid task outputs, but not out3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267175875
https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267175875:136,Modifiability,variab,variable,136,"@LeeTL1220 I tried this task below and although it's not the same exact type of syntax as what you used, I was able to use a task input variable inside of my glob: . `task createFileArray {; String dir = ""out""; command <<<; mkdir ${dir}; echo ""hullo"" > ${dir}/hello.txt; echo ""buh-bye"" > ${dir}/ciao.txt; sleep 2; >>>; output {; Array[File] out = glob(""out/*.txt"") ; Array[File] out2 = glob(dir + ""/*.txt""); #Array[File] out3 = glob(""${out}/*.txt""); }; runtime {docker:""ubuntu:latest""}; }`; out and out2 are valid task outputs, but not out3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267175875
https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267410928:18,Availability,error,errored,18,"I *think* that it errored out while I was still testing local backend, so I; never tested JES. On Thu, Dec 15, 2016 at 11:28 AM, Ruchi <notifications@github.com> wrote:. > @LeeTL1220-- was your example WDL run locally or on JES?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267372631>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7xuRrOQ7Sac2mRulJkYLAIMPnD_ks5rIWqQgaJpZM4JmxQ5>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267410928
https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267410928:48,Testability,test,testing,48,"I *think* that it errored out while I was still testing local backend, so I; never tested JES. On Thu, Dec 15, 2016 at 11:28 AM, Ruchi <notifications@github.com> wrote:. > @LeeTL1220-- was your example WDL run locally or on JES?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267372631>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7xuRrOQ7Sac2mRulJkYLAIMPnD_ks5rIWqQgaJpZM4JmxQ5>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267410928
https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267410928:83,Testability,test,tested,83,"I *think* that it errored out while I was still testing local backend, so I; never tested JES. On Thu, Dec 15, 2016 at 11:28 AM, Ruchi <notifications@github.com> wrote:. > @LeeTL1220-- was your example WDL run locally or on JES?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267372631>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7xuRrOQ7Sac2mRulJkYLAIMPnD_ks5rIWqQgaJpZM4JmxQ5>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267410928
https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285371298:39,Availability,error,error,39,@LeeTL1220 I'm unable to reproduce the error you saw. Have you seen the same issue with more recent versions of Cromwell? What version were you using when you saw the initial error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285371298
https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285371298:175,Availability,error,error,175,@LeeTL1220 I'm unable to reproduce the error you saw. Have you seen the same issue with more recent versions of Cromwell? What version were you using when you saw the initial error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285371298
https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285409887:192,Availability,error,error,192,"This is pretty old and may have already been fixed. On Mar 9, 2017 09:48, ""Ruchi"" <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> I'm unable to reproduce the; > error you saw. Have you seen the same issue with more recent versions of; > Cromwell? What version were you using when you saw the initial error?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285371298>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk782vulMdWbl5LYOzgueOZhAz_3bks5rkBEtgaJpZM4JmxQ5>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285409887
https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285409887:331,Availability,error,error,331,"This is pretty old and may have already been fixed. On Mar 9, 2017 09:48, ""Ruchi"" <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> I'm unable to reproduce the; > error you saw. Have you seen the same issue with more recent versions of; > Cromwell? What version were you using when you saw the initial error?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285371298>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk782vulMdWbl5LYOzgueOZhAz_3bks5rkBEtgaJpZM4JmxQ5>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285409887
https://github.com/broadinstitute/cromwell/pull/1310#issuecomment-241475681:16,Testability,test,test,16,Could you add a test case for this fix to prevent regression?. Otherwise 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1310/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1310#issuecomment-241475681
https://github.com/broadinstitute/cromwell/pull/1310#issuecomment-241476680:112,Testability,test,tests,112,"@cjllanwarne not sure what you mean? This isn't really a fix so much as a simplification, and there are already tests in `WdlValueBuilderSpec` which thankfully continue to pass. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1310#issuecomment-241476680
https://github.com/broadinstitute/cromwell/pull/1310#issuecomment-241476680:74,Usability,simpl,simplification,74,"@cjllanwarne not sure what you mean? This isn't really a fix so much as a simplification, and there are already tests in `WdlValueBuilderSpec` which thankfully continue to pass. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1310#issuecomment-241476680
https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-324418249:77,Safety,risk,risks,77,"What is the optimal order, and what would the effort be to implement it? Any risks?. Finally, how often do we get questions and complaints about it today? Do you expect that to go up in the near future?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-324418249
https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335814075:346,Integrability,message,messages,346,"As a **user running workflows**, I want to **be able to specify the backend in workflow options**, so that **Cromwell only uses the default backend when no other is specified, and notifies the user that it is using the default**. - Effort: X-Small to Small; - Risk: Small; - Note that some WDL may break, consider ways to deprecate (with warning messages in the WDLs).; - Business value: Small, for now; - This may change as Cromwell supports more backends and more users operate in a multi-backend environment",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335814075
https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335814075:260,Safety,Risk,Risk,260,"As a **user running workflows**, I want to **be able to specify the backend in workflow options**, so that **Cromwell only uses the default backend when no other is specified, and notifies the user that it is using the default**. - Effort: X-Small to Small; - Risk: Small; - Note that some WDL may break, consider ways to deprecate (with warning messages in the WDLs).; - Business value: Small, for now; - This may change as Cromwell supports more backends and more users operate in a multi-backend environment",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335814075
https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335815243:37,Safety,risk,risk,37,it's trivial to do. there's a slight risk involved in that theoretically some WDL would be broken but there's a 100% chance those WDLs become broken in the future anyways.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335815243
https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-424937274:281,Deployability,configurat,configuration,281,"@geoffjentry -- as of Cromwell 35, `backend` key in the workflow options is honored above the default backend. In case of the workflow option asking for a backend that doesn't exit, Cromwell explicitly fails with:. `Backend for call <call-name> ('<backend-name') not registered in configuration file...`. I believe this issue has been resolved with these changes. Feel free to re-open if something was missed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-424937274
https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-424937274:281,Modifiability,config,configuration,281,"@geoffjentry -- as of Cromwell 35, `backend` key in the workflow options is honored above the default backend. In case of the workflow option asking for a backend that doesn't exit, Cromwell explicitly fails with:. `Backend for call <call-name> ('<backend-name') not registered in configuration file...`. I believe this issue has been resolved with these changes. Feel free to re-open if something was missed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-424937274
https://github.com/broadinstitute/cromwell/issues/1314#issuecomment-241831906:52,Testability,test,test,52,"Please move the ""assign default runtime attributes"" test from MWDAspec to JobPreparationActorSpec (or wherever appropriate).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1314#issuecomment-241831906
https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-313485510:17,Deployability,update,update,17,@cjllanwarne any update on this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-313485510
https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-324166084:91,Security,hash,hash,91,"I would check with @Horneth - I know CC is now batching things into some maximum number of hash lookups at a time, but whether it gives up if it finds it can’t continue I’m not sure?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-324166084
https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-324170143:57,Security,hash,hash,57,It won't start new lookups but it won't cancel the other hash lookups in the same batch either.; With the current implementation doing that (cancelling lookups for files in the same batch) would be pretty hard.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-324170143
https://github.com/broadinstitute/cromwell/pull/1319#issuecomment-241748965:76,Availability,alive,alive,76,Global comment: I think based on a problem we saw last week that the `check-alive` command in `application.conf` is wrong for SGE. Should be `qstat -j ${job_id}`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1319#issuecomment-241748965
https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-289828222:35,Energy Efficiency,Green,Green,35,@ktibbett and @jsotobroad does the Green team use this API? Will you need it in order to adopt Cromwell 25?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-289828222
https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324205740:551,Testability,log,logic,551,"Since I couldn't remember what these endpoints even *did* I had to go [dig into ye olde wayback machine, aka gmail](https://github.com/broadinstitute/cromwell/pull/306/files). As far as i'm concerned this is along the lines of those ""PBE TODO"" and similar tickets - I don't remember why we never put this piece of the puzzle back together but clearly it isn't some burning desire. I think I've heard people ask about this functionality once or twice and it gets met with a fuzzy ""yeah, i think we used to have that?"" but as I said, don't remember our logic. My $0.02 is to just close this until such a day that people are asking for this again. I realize that's a shockingly hot take considering I just likened it to my stance on other PBE TODO type things :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324205740
https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324205740:343,Usability,clear,clearly,343,"Since I couldn't remember what these endpoints even *did* I had to go [dig into ye olde wayback machine, aka gmail](https://github.com/broadinstitute/cromwell/pull/306/files). As far as i'm concerned this is along the lines of those ""PBE TODO"" and similar tickets - I don't remember why we never put this piece of the puzzle back together but clearly it isn't some burning desire. I think I've heard people ask about this functionality once or twice and it gets met with a fuzzy ""yeah, i think we used to have that?"" but as I said, don't remember our logic. My $0.02 is to just close this until such a day that people are asking for this again. I realize that's a shockingly hot take considering I just likened it to my stance on other PBE TODO type things :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324205740
https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324207364:228,Integrability,rout,routing,228,"IIRC the main driver for this was to be able to turn off cache copying. Google bills a bucket owner for egress and not the account copying out of the bucket, so a bucket owner is potentially at the mercy of Cromwell's cache hit routing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324207364
https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324207364:57,Performance,cache,cache,57,"IIRC the main driver for this was to be able to turn off cache copying. Google bills a bucket owner for egress and not the account copying out of the bucket, so a bucket owner is potentially at the mercy of Cromwell's cache hit routing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324207364
https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324207364:218,Performance,cache,cache,218,"IIRC the main driver for this was to be able to turn off cache copying. Google bills a bucket owner for egress and not the account copying out of the bucket, so a bucket owner is potentially at the mercy of Cromwell's cache hit routing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324207364
https://github.com/broadinstitute/cromwell/pull/1326#issuecomment-242146999:88,Security,hash,hash,88,"sooo we were saying in standup today we probably want to do away with that whole Docker hash lookup business, which would seem to have more than a bit of impact on this PR... 😦",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326#issuecomment-242146999
https://github.com/broadinstitute/cromwell/issues/1329#issuecomment-324165601:23,Testability,test,tests,23,@Horneth Are these the tests you implemented?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1329#issuecomment-324165601
https://github.com/broadinstitute/cromwell/issues/1329#issuecomment-324170461:4,Testability,test,tests,4,The tests implemented are the second version @geoffjentry described (the one he says not preferring 😄 ). The first one has not been done.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1329#issuecomment-324170461
https://github.com/broadinstitute/cromwell/issues/1329#issuecomment-324172692:46,Availability,down,down,46,"Which is amusing @Horneth because when we sat down to discuss it a few months ago I had switched to the side of chaos while others were advocating order ;) But really we've got the best of both worlds right now IMO, this is closeable",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1329#issuecomment-324172692
https://github.com/broadinstitute/cromwell/issues/1332#issuecomment-242266539:1005,Integrability,interoperab,interoperability,1005,"![19grnh](https://cloud.githubusercontent.com/assets/791985/17954234/7ba0b698-6a47-11e6-873c-c0e60ca163e1.jpg). I'd be game if all `java.nio.Path`s across cromwell, including the engine, all backends, services, etc. were always absolute. A relative path appearing in a web response, a shell script, or even the logs would then be considered a bug. In JES, there are internal private methods such as `JesAsyncBackendJobExecutionActor.relativeLocalizationPath` that create relative paths, but these relative paths should not be externally visible. `JesAsyncBackendJobExecutionActor.jesInputsFromWdlFiles` should be creating absolute paths. Over in the SFS, the `SharedFileSystemAsyncJobExecutionActor` doesn't create relative paths, but still inconsistently calls `Path.toAbsolutePath` in various places. More usage of `better.files` instead of `java.nio.Paths.get()` would help us from omitting calls to `Path.toAbsolutePath`, since [better's `.path` member](https://github.com/pathikrit/better-files#java-interoperability) creates absolute `java.nio.Path`s (for [now](https://github.com/pathikrit/better-files/issues/48#issuecomment-157837460)). **TL;DR This might also be a problem elsewhere, including the SFS backend.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1332#issuecomment-242266539
https://github.com/broadinstitute/cromwell/issues/1332#issuecomment-242266539:311,Testability,log,logs,311,"![19grnh](https://cloud.githubusercontent.com/assets/791985/17954234/7ba0b698-6a47-11e6-873c-c0e60ca163e1.jpg). I'd be game if all `java.nio.Path`s across cromwell, including the engine, all backends, services, etc. were always absolute. A relative path appearing in a web response, a shell script, or even the logs would then be considered a bug. In JES, there are internal private methods such as `JesAsyncBackendJobExecutionActor.relativeLocalizationPath` that create relative paths, but these relative paths should not be externally visible. `JesAsyncBackendJobExecutionActor.jesInputsFromWdlFiles` should be creating absolute paths. Over in the SFS, the `SharedFileSystemAsyncJobExecutionActor` doesn't create relative paths, but still inconsistently calls `Path.toAbsolutePath` in various places. More usage of `better.files` instead of `java.nio.Paths.get()` would help us from omitting calls to `Path.toAbsolutePath`, since [better's `.path` member](https://github.com/pathikrit/better-files#java-interoperability) creates absolute `java.nio.Path`s (for [now](https://github.com/pathikrit/better-files/issues/48#issuecomment-157837460)). **TL;DR This might also be a problem elsewhere, including the SFS backend.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1332#issuecomment-242266539
https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242259156:400,Availability,down,down,400,"> I'm trying to figure out how to get [travis?] to redo the travis build. Yeah, PRs 1333 and this 1334 currently have the same git hash. Try updating to a new git hash by ""touching"" the commit, and then force pushing:. ``` bash; git checkout jg_haircut_for_testkitspec && \; git commit --amend -C HEAD --date=now && \; git push -f origin HEAD; ```. NOTE: Add `-q` after each `git <command>` to quiet down the output.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242259156
https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242259156:131,Security,hash,hash,131,"> I'm trying to figure out how to get [travis?] to redo the travis build. Yeah, PRs 1333 and this 1334 currently have the same git hash. Try updating to a new git hash by ""touching"" the commit, and then force pushing:. ``` bash; git checkout jg_haircut_for_testkitspec && \; git commit --amend -C HEAD --date=now && \; git push -f origin HEAD; ```. NOTE: Add `-q` after each `git <command>` to quiet down the output.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242259156
https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242259156:163,Security,hash,hash,163,"> I'm trying to figure out how to get [travis?] to redo the travis build. Yeah, PRs 1333 and this 1334 currently have the same git hash. Try updating to a new git hash by ""touching"" the commit, and then force pushing:. ``` bash; git checkout jg_haircut_for_testkitspec && \; git commit --amend -C HEAD --date=now && \; git push -f origin HEAD; ```. NOTE: Add `-q` after each `git <command>` to quiet down the output.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242259156
https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242275943:288,Deployability,update,updated,288,"Upon closer inspection, I think Travis is doing the right thing by not currently running `centaurLocal` for the ""push"", but only running it for the ""pr"". The `.travis.yml` in the current ""push""ed commit doesn't specify to run `centaurLocal`. Meanwhile the develop branch has already been updated, meaning that if-this-""pr""-were-merged `centaurLocal` will run. Rebase onto develop, picking up the `.travis.yml` changes, and the ""push"" build should also run `centaurLocal`, probably?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242275943
https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242435068:67,Testability,test,test,67,"~~spring~~ summer cleaning (?). The way I reviewed this is use the test names to identify the wdl/cromwell behavior being tested, so I'm not really looking super deep into what was removed (with the understanding that Travis will catch all that). I had some general comments but none of them require immediate action. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1334/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242435068
https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242435068:122,Testability,test,tested,122,"~~spring~~ summer cleaning (?). The way I reviewed this is use the test names to identify the wdl/cromwell behavior being tested, so I'm not really looking super deep into what was removed (with the understanding that Travis will catch all that). I had some general comments but none of them require immediate action. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1334/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242435068
https://github.com/broadinstitute/cromwell/issues/1335#issuecomment-325452489:173,Safety,Risk,Risk,173,"As a **WDL user**, I want **declare a parameter once for many tasks**, so that I **don't have to alias the parameter for each aliased task**.; - Effort: **Small-Medium**; - Risk: **Small**; - Business value: **Small**; @vdauwera please chime in if you disagree",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1335#issuecomment-325452489
https://github.com/broadinstitute/cromwell/issues/1336#issuecomment-324172419:77,Testability,test,test,77,"@katevoss As @mcovarr alludes to, this is a spiritual cousin of that ignored test ticket. A holdover from a time long enough past that the likelihood of any ticket still mattering closely approximates zero. @mcovarr ; ![images](https://user-images.githubusercontent.com/961771/29590873-106c9d7e-876a-11e7-94bc-2fb749479b5e.jpg)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1336#issuecomment-324172419
https://github.com/broadinstitute/cromwell/pull/1338#issuecomment-242276606:10,Usability,feedback,feedback,10,"Ready for feedback, but don't merge until #1326 is merged and conflicts are resolved.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338#issuecomment-242276606
https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512:63,Deployability,integrat,integration,63,@iyanuobidele can you please pull it and try one more round of integration testing with the Spark cluster ? ; @geoffjentry : I rebased it with the develop branch let me know how does it look ? I will merge it then. Thank you.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512
https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512:63,Integrability,integrat,integration,63,@iyanuobidele can you please pull it and try one more round of integration testing with the Spark cluster ? ; @geoffjentry : I rebased it with the develop branch let me know how does it look ? I will merge it then. Thank you.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512
https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512:75,Testability,test,testing,75,@iyanuobidele can you please pull it and try one more round of integration testing with the Spark cluster ? ; @geoffjentry : I rebased it with the develop branch let me know how does it look ? I will merge it then. Thank you.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512
https://github.com/broadinstitute/cromwell/pull/1340#issuecomment-242796644:11,Modifiability,enhance,enhancements,11,"The future enhancements of `WdlFile` affecting this migrator still makes me uneasy, but this code looks good as is. 👍 . Also I'm assuming this won't get merged before the other migration PR?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1340/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1340#issuecomment-242796644
https://github.com/broadinstitute/cromwell/pull/1340#issuecomment-243482423:134,Energy Efficiency,green,green,134,"Just a heads up, after talking with @Horneth and @cjllanwarne I'm planning to merge this before Thibault's PR if/when these builds go green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1340#issuecomment-243482423
https://github.com/broadinstitute/cromwell/issues/1342#issuecomment-268380661:205,Testability,log,logic,205,"I think we could do this pretty neatly with a `glob()` like function (maybe `optional(""out.txt"")`) to get around the JES restraints of having to declare file outputs upfront. . In fact the entire handling logic could be the same, just at the last minute we would convert the array into an Option as long as it has exactly 1 element.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1342#issuecomment-268380661
https://github.com/broadinstitute/cromwell/issues/1343#issuecomment-268012330:210,Availability,down,down,210,"This task is covered by the DSDE-Docs Epic ""[Update Cromwell Documentation](https://github.com/broadinstitute/dsde-docs/issues/1514)"", which includes moving most of the content to the WDL website and [slimming down the README](https://github.com/broadinstitute/dsde-docs/issues/1515). Closing this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1343#issuecomment-268012330
https://github.com/broadinstitute/cromwell/issues/1343#issuecomment-268012330:45,Deployability,Update,Update,45,"This task is covered by the DSDE-Docs Epic ""[Update Cromwell Documentation](https://github.com/broadinstitute/dsde-docs/issues/1514)"", which includes moving most of the content to the WDL website and [slimming down the README](https://github.com/broadinstitute/dsde-docs/issues/1515). Closing this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1343#issuecomment-268012330
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242777349:94,Integrability,wrap,wrap,94,"I like the idea here but I think we're probably now over the boundary of ""do it sync and then wrap the result in Future.successful"". A hidden cost to `isAlive` is that we have to create a shell process and wait for it to complete every time we query a job's status using it. @geoffjentry @kshakir if we have to fire up a new shell process for every status query, I will re-suggest my previous idea of putting the whole thing behind a routed actor pool so that we can at least rate-limit it... 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242777349
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242777349:434,Integrability,rout,routed,434,"I like the idea here but I think we're probably now over the boundary of ""do it sync and then wrap the result in Future.successful"". A hidden cost to `isAlive` is that we have to create a shell process and wait for it to complete every time we query a job's status using it. @geoffjentry @kshakir if we have to fire up a new shell process for every status query, I will re-suggest my previous idea of putting the whole thing behind a routed actor pool so that we can at least rate-limit it... 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242777349
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242800302:123,Availability,down,down,123,"Looking at the gitter convo, the problem looks more like a case of overzealous logging - could we just drop it from `info` down to `debug`? I'm curious why it was `info` in the first place tbh",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242800302
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242800302:79,Testability,log,logging,79,"Looking at the gitter convo, the problem looks more like a case of overzealous logging - could we just drop it from `info` down to `debug`? I'm curious why it was `info` in the first place tbh",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242800302
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:129,Availability,robust,robust,129,"Well if it's doing it only when the job is finished I would even make it `warn` instead of `info`. > Right now a workflow is not robust enough to run it on a/our HPC, see also on gitte:; > Peter van 't Hof @ffinfo Aug 26 19:05; > ye I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1456,Availability,recover,recovering,1456,"ymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1197,Energy Efficiency,reduce,reduce,1197," I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:734,Performance,Queue,Queue,734,"Well if it's doing it only when the job is finished I would even make it `warn` instead of `info`. > Right now a workflow is not robust enough to run it on a/our HPC, see also on gitte:; > Peter van 't Hof @ffinfo Aug 26 19:05; > ye I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:979,Performance,queue,queue,979,"Well if it's doing it only when the job is finished I would even make it `warn` instead of `info`. > Right now a workflow is not robust enough to run it on a/our HPC, see also on gitte:; > Peter van 't Hof @ffinfo Aug 26 19:05; > ye I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1208,Performance,load,load,1208," I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1567,Performance,Queue,Queue,1567,"t's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise directly. I can try to look into a lfs.drama backend. If it's possible to check running jobs after restart this would be nice to have, but ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:367,Safety,detect,detect,367,"Well if it's doing it only when the job is finished I would even make it `warn` instead of `info`. > Right now a workflow is not robust enough to run it on a/our HPC, see also on gitte:; > Peter van 't Hof @ffinfo Aug 26 19:05; > ye I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1456,Safety,recover,recovering,1456,"ymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1859,Testability,log,logs,1859," > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise directly. I can try to look into a lfs.drama backend. If it's possible to check running jobs after restart this would be nice to have, but need to look into this. What to do with this PR i leave up to you guys, merging it or not does not really matter for me at this point ;).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:2340,Testability,test,test,2340," > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise directly. I can try to look into a lfs.drama backend. If it's possible to check running jobs after restart this would be nice to have, but need to look into this. What to do with this PR i leave up to you guys, merging it or not does not really matter for me at this point ;).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:2299,Usability,usab,usable,2299," > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise directly. I can try to look into a lfs.drama backend. If it's possible to check running jobs after restart this would be nice to have, but need to look into this. What to do with this PR i leave up to you guys, merging it or not does not really matter for me at this point ;).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243102881:229,Modifiability,config,config,229,Maybe this is a workaround till there is a actor pool in place to replace this? Tested this on SGE and it seems to work. Now isAlive is only executing a shell command once each 20 seconds. This 20 seconds can maybe be inside the config but first want to know what you guys think of this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243102881
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243102881:80,Testability,Test,Tested,80,Maybe this is a workaround till there is a actor pool in place to replace this? Tested this on SGE and it seems to work. Now isAlive is only executing a shell command once each 20 seconds. This 20 seconds can maybe be inside the config but first want to know what you guys think of this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243102881
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051:333,Modifiability,config,config-file,333,"~~@ffinfo just to be clear - unless the script explodes in a **really** unusual way, you should _always_ eventually get a return code file. So for us, waiting for it to appear is sufficient. (e.g. even if a command fails, we still get the return code file).~~. ~~In fact, in previous Cromwell versions (before making these backends ""config-file based"") our hard-coded local and SGE backends waited for the return code file and never checked `isAlive` apart from at restart-time.~~. ~~Just so I can understand, is there a specific reason that you don't believe that an `rc` file will eventually be produced for your situation? (and if so, could we maybe try to protect that instead?)~~. EDIT: I just read the gitter log you copied above. I think I see now!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051:715,Testability,log,log,715,"~~@ffinfo just to be clear - unless the script explodes in a **really** unusual way, you should _always_ eventually get a return code file. So for us, waiting for it to appear is sufficient. (e.g. even if a command fails, we still get the return code file).~~. ~~In fact, in previous Cromwell versions (before making these backends ""config-file based"") our hard-coded local and SGE backends waited for the return code file and never checked `isAlive` apart from at restart-time.~~. ~~Just so I can understand, is there a specific reason that you don't believe that an `rc` file will eventually be produced for your situation? (and if so, could we maybe try to protect that instead?)~~. EDIT: I just read the gitter log you copied above. I think I see now!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051:21,Usability,clear,clear,21,"~~@ffinfo just to be clear - unless the script explodes in a **really** unusual way, you should _always_ eventually get a return code file. So for us, waiting for it to appear is sufficient. (e.g. even if a command fails, we still get the return code file).~~. ~~In fact, in previous Cromwell versions (before making these backends ""config-file based"") our hard-coded local and SGE backends waited for the return code file and never checked `isAlive` apart from at restart-time.~~. ~~Just so I can understand, is there a specific reason that you don't believe that an `rc` file will eventually be produced for your situation? (and if so, could we maybe try to protect that instead?)~~. EDIT: I just read the gitter log you copied above. I think I see now!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243148664:172,Integrability,rout,routing,172,"From my point of view, I'd recommend merging this one ~~as is~~ once the tests are happy. If we start seeing issues with too many shells being spawned we can look again at routing the shell requests. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1346/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243148664
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243148664:73,Testability,test,tests,73,"From my point of view, I'd recommend merging this one ~~as is~~ once the tests are happy. If we start seeing issues with too many shells being spawned we can look again at routing the shell requests. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1346/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243148664
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:518,Availability,recover,recover,518,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:594,Availability,alive,alive,594,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:653,Availability,alive,alive,653,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:677,Availability,alive,alive,677,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:731,Availability,alive,alive,731,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:765,Availability,alive,alive,765,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2553,Availability,error,error,2553,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:56,Deployability,patch,patch,56,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:828,Deployability,configurat,configuration,828,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2446,Deployability,configurat,configuration,2446,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:528,Energy Efficiency,schedul,scheduleOnce,528,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:684,Energy Efficiency,schedul,scheduleOnce,684,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:923,Energy Efficiency,schedul,scheduled,923,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:974,Energy Efficiency,schedul,scheduled,974,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1918,Energy Efficiency,schedul,scheduleOnce,1918," - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2270,Energy Efficiency,schedul,scheduler,2270,"- `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2474,Energy Efficiency,schedul,scheduling,2474,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:544,Integrability,message,message,544,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:613,Integrability,message,message,613,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:700,Integrability,message,message,700,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:933,Integrability,message,messages,933,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:957,Integrability,message,message,957,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:828,Modifiability,config,configuration,828,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2446,Modifiability,config,configuration,2446,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2500,Modifiability,config,configurable,2500,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2596,Modifiability,config,configurable,2596,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1240,Performance,cache,cache,1240,"he moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1259,Performance,cache,cache,1259,"unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1304,Performance,cache,cache,1304," look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatt",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1329,Performance,cache,cache,1329,"in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:518,Safety,recover,recover,518,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1943,Testability,test,tests,1943,"eduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2021,Testability,test,tests,2021,"few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suit",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2384,Usability,simpl,simplify,2384,"- `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:53,Deployability,release,release,53,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:257,Energy Efficiency,green,green,257,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:399,Modifiability,config,config,399,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:317,Performance,latency,latency,317,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:251,Testability,test,tests,251,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442
https://github.com/broadinstitute/cromwell/issues/1347#issuecomment-247151430:36,Deployability,release,release,36,This is going to be part of the .21 release.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1347#issuecomment-247151430
https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-325671888:314,Security,validat,validation,314,"Sure... any time a file is listed twice in the input. This actually does; happen. In cancer, we often have several tumor samples, but only one; normal. But we make a pair for each tumor sample (paired with the same; normal), so the normal would get localized multiple times. When I filed; this, I was working on a validation for our clinical work and that one uses; the same normal for about 9 pairs. On Tue, Aug 22, 2017 at 6:11 PM, Kate Voss <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/leetl1220> can you explain the exact use; > case for when there are two copies of a file?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-324166111>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk19JlQd7ln14Z5sg60SKZcBHgrviks5sa1H1gaJpZM4JuaZs>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-325671888
https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-328201762:174,Energy Efficiency,charge,charged,174,"As a **workflow runner running on Local**, I want **Cromwell to localize one copy of a file from the cloud when I use it multiple times in my workflow**, so that I **am only charged egress to local once**.; - Effort: **?** @geoffjentry ; - Risk: **?** @geoffjentry ; - Be careful that inputs aren't being modified in place before allowing them to be used again; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-328201762
https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-328201762:240,Safety,Risk,Risk,240,"As a **workflow runner running on Local**, I want **Cromwell to localize one copy of a file from the cloud when I use it multiple times in my workflow**, so that I **am only charged egress to local once**.; - Effort: **?** @geoffjentry ; - Risk: **?** @geoffjentry ; - Be careful that inputs aren't being modified in place before allowing them to be used again; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-328201762
https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-424954064:344,Performance,optimiz,optimization,344,"@LeeTL1220 whats the driving force behind wanting to run with GCS files locally? It would seem most cost effective (by not having to pay egress at all) to run compute to in GCP. However, if you want to use on-prem resources specifically, then simply make a copy a local copy of workflow inputs to start off with. I think this is an interesting optimization but not really a widespread use case and adds more complexity to how Cromwell has to handle localization strategies. I'm inclined to close this issue but feel free to re-open if this essential to your flow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-424954064
https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-424954064:243,Usability,simpl,simply,243,"@LeeTL1220 whats the driving force behind wanting to run with GCS files locally? It would seem most cost effective (by not having to pay egress at all) to run compute to in GCP. However, if you want to use on-prem resources specifically, then simply make a copy a local copy of workflow inputs to start off with. I think this is an interesting optimization but not really a widespread use case and adds more complexity to how Cromwell has to handle localization strategies. I'm inclined to close this issue but feel free to re-open if this essential to your flow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-424954064
https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:77,Availability,ERROR,ERROR,77,"```; 2016-08-26 16:15:29,393 cromwell-system-akka.actor.default-dispatcher-3 ERROR - JobStoreReadFailure; java.lang.RuntimeException: JobStoreResultSimpletonEntry(metrics,""gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/5b7f21d8-0a96-4bbe-a96b-aa68c0454fd7/call-CollectQualityYieldMetrics/shard-8/HV2J2CCXX.8.Pond-555028.unmapped.quality_yield_metrics"",File,892420,Some(287367)): unrecognized WDL type: File; at cromwell.jobstore.SqlJobStore.cromwell$jobstore$SqlJobStore$$toSimpleton(SqlJobStore.scala:59) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5$$anonfun$3.apply(SqlJobStore.scala:69) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5$$anonfun$3.apply(SqlJobStore.scala:69) ~[classes/:na]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.Iterator$class.foreach(Iterator.scala:742) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:69) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.Option.map(Option.scala:146) ~[scala-library-2.11.7.jar:1.0.0-M1]; at crom",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472
https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:2460,Performance,concurren,concurrent,2460,t scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:69) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.Option.map(Option.scala:146) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472
https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:2558,Performance,concurren,concurrent,2558,:1.0.0-M1]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:69) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.Option.map(Option.scala:146) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472
https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:2656,Performance,concurren,concurrent,2656,1.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:69) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.Option.map(Option.scala:146) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472
https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:3274,Performance,concurren,concurrent,3274,(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.7.jar:na]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472
https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:3724,Performance,concurren,concurrent,3724,(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.7.jar:na]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472
https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:3827,Performance,concurren,concurrent,3827,(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.7.jar:na]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472
https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:3942,Performance,concurren,concurrent,3942,(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.7.jar:na]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472
https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:4049,Performance,concurren,concurrent,4049,(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.7.jar:na]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.7.jar:na]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472
https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242883334:93,Performance,cache,cache,93,"Sorry, I'm not sure what you mean. For context, Ruchi has some forthcoming work for the call cache result stuff that will want to do exactly the same db simpleton -> WdlValueSimpleton conversions as the job store.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242883334
https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242883334:153,Usability,simpl,simpleton,153,"Sorry, I'm not sure what you mean. For context, Ruchi has some forthcoming work for the call cache result stuff that will want to do exactly the same db simpleton -> WdlValueSimpleton conversions as the job store.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242883334
https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242928077:352,Modifiability,refactor,refactorings,352,"No worries. I'll submit a later suggestion inside a code PR. There would be no traits under `package cromwell.database.sql.tables`, because MySQL, HSQLDB, etc. don't have traits. I'm desperately trying to keep the `database` layer modeling what's-in-the-database, versus modeling what-cromwell-would-like-to-see, ahead of Slick 4, doobie, or any other refactorings.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242928077
https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242928245:39,Integrability,message,message,39,"Btw, the closes ""Closes"" in the commit message and the PR name are pointing to the wrong id.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242928245
https://github.com/broadinstitute/cromwell/pull/1352#issuecomment-243175320:3,Testability,Mock,MockitoSugar,3,"So MockitoSugar still exists, it just needs to be imported from elsewhere?. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1352/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1352#issuecomment-243175320
https://github.com/broadinstitute/cromwell/issues/1354#issuecomment-276494116:92,Modifiability,config,config,92,This exists in C22+:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354#issuecomment-276494116
https://github.com/broadinstitute/cromwell/issues/1354#issuecomment-276494116:102,Performance,concurren,concurrent-job-limit,102,This exists in C22+:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354#issuecomment-276494116
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920:5,Availability,error,error,5,That error is a failure of jes to initialize a vm for you in the first place. There are no logs to write beyond what's in there. I just took a quick glance but it looks like you're specifying a file which doesn't exist,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920:16,Availability,failure,failure,16,That error is a failure of jes to initialize a vm for you in the first place. There are no logs to write beyond what's in there. I just took a quick glance but it looks like you're specifying a file which doesn't exist,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920:91,Testability,log,logs,91,That error is a failure of jes to initialize a vm for you in the first place. There are no logs to write beyond what's in there. I just took a quick glance but it looks like you're specifying a file which doesn't exist,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366:36,Availability,error,error,36,"Where did you look? The file in the error message is the output of a task. On Tue, Aug 30, 2016 at 9:37 AM, Jeff Gentry notifications@github.com; wrote:. > That error is a failure of jes to initialize a vm for you in the first; > place. There are no logs to write beyond what's in there. I just took a; > quick glance but it looks like you're specifying a file which doesn't exist; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk0bYjGuwmLKdzJkzsIk8YUxr-Ee8ks5qlDIygaJpZM4JwiCG; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366:161,Availability,error,error,161,"Where did you look? The file in the error message is the output of a task. On Tue, Aug 30, 2016 at 9:37 AM, Jeff Gentry notifications@github.com; wrote:. > That error is a failure of jes to initialize a vm for you in the first; > place. There are no logs to write beyond what's in there. I just took a; > quick glance but it looks like you're specifying a file which doesn't exist; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk0bYjGuwmLKdzJkzsIk8YUxr-Ee8ks5qlDIygaJpZM4JwiCG; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366:172,Availability,failure,failure,172,"Where did you look? The file in the error message is the output of a task. On Tue, Aug 30, 2016 at 9:37 AM, Jeff Gentry notifications@github.com; wrote:. > That error is a failure of jes to initialize a vm for you in the first; > place. There are no logs to write beyond what's in there. I just took a; > quick glance but it looks like you're specifying a file which doesn't exist; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk0bYjGuwmLKdzJkzsIk8YUxr-Ee8ks5qlDIygaJpZM4JwiCG; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366:42,Integrability,message,message,42,"Where did you look? The file in the error message is the output of a task. On Tue, Aug 30, 2016 at 9:37 AM, Jeff Gentry notifications@github.com; wrote:. > That error is a failure of jes to initialize a vm for you in the first; > place. There are no logs to write beyond what's in there. I just took a; > quick glance but it looks like you're specifying a file which doesn't exist; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk0bYjGuwmLKdzJkzsIk8YUxr-Ee8ks5qlDIygaJpZM4JwiCG; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366:250,Testability,log,logs,250,"Where did you look? The file in the error message is the output of a task. On Tue, Aug 30, 2016 at 9:37 AM, Jeff Gentry notifications@github.com; wrote:. > That error is a failure of jes to initialize a vm for you in the first; > place. There are no logs to write beyond what's in there. I just took a; > quick glance but it looks like you're specifying a file which doesn't exist; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk0bYjGuwmLKdzJkzsIk8YUxr-Ee8ks5qlDIygaJpZM4JwiCG; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243854114:162,Availability,failure,failure,162,`Failed to delocalize files` Looks like JES couldn't delocalize a file that was expected as an output.; The task probably failed to produce that output hence the failure.; I thought the logs were copied regardless in that case but apparently not..,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243854114
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243854114:186,Testability,log,logs,186,`Failed to delocalize files` Looks like JES couldn't delocalize a file that was expected as an output.; The task probably failed to produce that output hence the failure.; I thought the logs were copied regardless in that case but apparently not..,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243854114
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243855172:323,Availability,failure,failure,323,"This is old and can be closed. @Thib already explained how to do examine; log files. On Wed, Aug 31, 2016 at 2:20 PM, Thib notifications@github.com wrote:. > Failed to delocalize files Looks like JES couldn't delocalize a file that; > was expected as an output.; > The task probably failed to produce that output hence the failure.; > I thought the logs were copied regardless in that case but apparently not..; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243854114,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk-NAFrpk76r_5joMUbWTtGqkJvXAks5qlcXqgaJpZM4JwiCG; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243855172
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243855172:74,Testability,log,log,74,"This is old and can be closed. @Thib already explained how to do examine; log files. On Wed, Aug 31, 2016 at 2:20 PM, Thib notifications@github.com wrote:. > Failed to delocalize files Looks like JES couldn't delocalize a file that; > was expected as an output.; > The task probably failed to produce that output hence the failure.; > I thought the logs were copied regardless in that case but apparently not..; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243854114,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk-NAFrpk76r_5joMUbWTtGqkJvXAks5qlcXqgaJpZM4JwiCG; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243855172
https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243855172:349,Testability,log,logs,349,"This is old and can be closed. @Thib already explained how to do examine; log files. On Wed, Aug 31, 2016 at 2:20 PM, Thib notifications@github.com wrote:. > Failed to delocalize files Looks like JES couldn't delocalize a file that; > was expected as an output.; > The task probably failed to produce that output hence the failure.; > I thought the logs were copied regardless in that case but apparently not..; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243854114,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk-NAFrpk76r_5joMUbWTtGqkJvXAks5qlcXqgaJpZM4JwiCG; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243855172
https://github.com/broadinstitute/cromwell/issues/1358#issuecomment-253926761:355,Availability,echo,echo,355,"- If the job itself fails to start, this can be reported back immediately by Cromwell (although please do raise this as a separate issue with a reproducing WDL if that's what you mean).; - If Cromwell can start the job but the bash script will fail immediately, then that's out of Cromwell's hands.; - The overhead of running jobs on JES means that even `echo hello` has a minimum overhead of around 5-10 minutes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1358#issuecomment-253926761
https://github.com/broadinstitute/cromwell/pull/1359#issuecomment-243457214:24,Testability,test,test,24,Did you also check the `test` conf file? And the vaulty ones (if we even still have them)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1359#issuecomment-243457214
https://github.com/broadinstitute/cromwell/pull/1365#issuecomment-245036424:7,Security,Hash,HashError,7,A few `HashError` catches and a few extra Event possibilities `when(BackendIsCopyingCachedOutputs)` but mainly this looks good.; Happy to give this a thumb 👍 assuming those are added. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1365/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1365#issuecomment-245036424
https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-269263625:54,Modifiability,config,configurable,54,If this is ever implemented it should be `DEBUG` or a configurable option w/ a warning about the potential performance impact,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-269263625
https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-269263625:107,Performance,perform,performance,107,If this is ever implemented it should be `DEBUG` or a configurable option w/ a warning about the potential performance impact,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-269263625
https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324470513:30,Testability,log,logging,30,@dvoet do you have any better logging these days? Can you tell me more about what you are looking for when you need the Google logs?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324470513
https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324470513:127,Testability,log,logs,127,@dvoet do you have any better logging these days? Can you tell me more about what you are looking for when you need the Google logs?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324470513
https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324470893:26,Integrability,message,message,26,"He wants us to emit a log message on every HTTP call to a google service, and we still don't do that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324470893
https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324470893:22,Testability,log,log,22,"He wants us to emit a log message on every HTTP call to a google service, and we still don't do that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324470893
https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324490086:222,Testability,log,log,222,"Fwiw this *is* a good idea for the sort of production purpose @dvoet has in mind. The issue is that we'd have an angry mob of other users if we ever did this. . This goes into that ""we need to not emit a one size fits all log"" topic",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324490086
https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-325461974:22,Testability,Log,Logging,22,Tracking this in the [Logging Spec document](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-325461974
https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-325462508:152,Safety,Risk,Risk,152,"As a **super-user**, I want **to see every call Cromwell made to Google**, so that I can **debug what is wrong with Cromwell**.; - Effort: **Small**; - Risk: **Medium**; - - Need to make sure regular users don't see these logs, so that these are not visible by default.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-325462508
https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-325462508:222,Testability,log,logs,222,"As a **super-user**, I want **to see every call Cromwell made to Google**, so that I can **debug what is wrong with Cromwell**.; - Effort: **Small**; - Risk: **Medium**; - - Need to make sure regular users don't see these logs, so that these are not visible by default.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-325462508
https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244488817:81,Usability,guid,guide-,81,Actually now that I look at http://doc.akka.io/docs/akka/2.4.0/project/migration-guide-2.3.x-2.4.x.html I might take a spin around our codebase before merging,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244488817
https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:83,Availability,error,error,83,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823
https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:514,Availability,error,error,514,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823
https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:341,Deployability,update,updates,341,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823
https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:383,Deployability,update,updates,383,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823
https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:395,Deployability,update,update,395,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823
https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:412,Integrability,depend,dependencies,412,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823
https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:560,Integrability,message,message,560,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823
https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:569,Safety,timeout,timeout,569,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823
https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:579,Safety,timeout,timeout,579,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823
https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:621,Safety,timeout,timeout,621,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823
https://github.com/broadinstitute/cromwell/issues/1374#issuecomment-245370201:353,Modifiability,refactor,refactorings,353,One of the things I'd like to come out of the upcoming JeffDoc is identify some spots where perhaps we could take advantage of streams instead of actors. I feel like a lot of these little operations winging around the periphery would fit that model - even if at first there's not much of a need for backpressure management (but it could lead to further refactorings where it did indeed help),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1374#issuecomment-245370201
https://github.com/broadinstitute/cromwell/issues/1376#issuecomment-289825174:8,Safety,abort,aborts,8,"Closed, aborts will someday change this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1376#issuecomment-289825174
https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245602916:101,Performance,load,loaded,101,Actually now that I think about it this is possible w/o a code change. The service registry stuff is loaded in dynamically by the conf file so you'd just need to not include a metadata service in your conf,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245602916
https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245732160:388,Performance,load,loaded,388,"This would be excellent if true, let's just confirm that it is!. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Sep 8, 2016 at 9:48 AM, Jeff Gentry notifications@github.com; wrote:. > Actually now that I think about it this is possible w/o a code change. The; > service registry stuff is loaded in dynamically by the conf file so you'd; > just need to not include a metadata service in your conf; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245602916,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4g3bDS29DaHzNNAVesYu_UT85_bhCks5qoBIrgaJpZM4J3efD; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245732160
https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245821145:143,Availability,ping,pings,143,"I take it all back, I think we need this functionality. The metadata based REST queries will go through the MetadataBuilderActor which in turn pings the MetadataService. Having the MetadataService implies having the summarizer. Pinging @Horneth just to double check me on this as I believe he's the one who put that stuff together.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245821145
https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245821145:228,Availability,Ping,Pinging,228,"I take it all back, I think we need this functionality. The metadata based REST queries will go through the MetadataBuilderActor which in turn pings the MetadataService. Having the MetadataService implies having the summarizer. Pinging @Horneth just to double check me on this as I believe he's the one who put that stuff together.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245821145
https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245940665:82,Modifiability,config,config,82,Might be nice to disable this by setting the refresh duration (which is already a config value) to zero (or infinite),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245940665
https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950069:368,Safety,avoid,avoiding,368,"@geoffjentry I believe that is correct.; Just to get some clarification on this - why does the summary refresh actor need to be disabled to use cromwell as read-only ? I understand that the refresh actor writes to the database, but in very low amounts (1 line per workflow), and its purpose (as I understand it, @mcovarr ?) is to help relieve the metadata endpoint by avoiding recomputing the current status on every call, which would be useful for a read-only cromwell ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950069
https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950671:49,Safety,avoid,avoid,49,"So if you stand up several cromwells, how do you avoid them all trying to; do the summary stuff and clobber eachother? That's the purpose of this; ticket. All the readers would read the summary, but only one should be; writing it. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Fri, Sep 9, 2016 at 11:37 AM, Thib notifications@github.com wrote:. > @geoffjentry https://github.com/geoffjentry I believe that is correct.; > Just to get some clarification on this - why does the summary refresh; > actor need to be disabled to use cromwell as read-only ? I understand that; > the refresh actor writes to the database, but in very low amounts (1 line; > per workflow), and its purpose (as I understand it, @mcovarr; > https://github.com/mcovarr ?) is to help relieve the metadata endpoint; > by avoiding recomputing the current status on every call, which would be; > useful for a read-only cromwell ?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950069,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4g3Ta6a_kXzY1BGl8L_aAEr5duoTpks5qoX0hgaJpZM4J3efD; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950671
https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950671:886,Safety,avoid,avoiding,886,"So if you stand up several cromwells, how do you avoid them all trying to; do the summary stuff and clobber eachother? That's the purpose of this; ticket. All the readers would read the summary, but only one should be; writing it. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Fri, Sep 9, 2016 at 11:37 AM, Thib notifications@github.com wrote:. > @geoffjentry https://github.com/geoffjentry I believe that is correct.; > Just to get some clarification on this - why does the summary refresh; > actor need to be disabled to use cromwell as read-only ? I understand that; > the refresh actor writes to the database, but in very low amounts (1 line; > per workflow), and its purpose (as I understand it, @mcovarr; > https://github.com/mcovarr ?) is to help relieve the metadata endpoint; > by avoiding recomputing the current status on every call, which would be; > useful for a read-only cromwell ?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950069,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4g3Ta6a_kXzY1BGl8L_aAEr5duoTpks5qoX0hgaJpZM4J3efD; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950671
https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245486795:12,Deployability,patch,patch,12,Will push a patch updating the lenthall and wdl4s dependencies-- once those PRs get one more thumb.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245486795
https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245486795:50,Integrability,depend,dependencies,50,Will push a patch updating the lenthall and wdl4s dependencies-- once those PRs get one more thumb.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245486795
https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423:39,Availability,failure,failures,39,wdl4s and lenthall updated. Fixed test failures and re-singletoned the factory hashing pools.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423
https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423:19,Deployability,update,updated,19,wdl4s and lenthall updated. Fixed test failures and re-singletoned the factory hashing pools.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423
https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423:79,Security,hash,hashing,79,wdl4s and lenthall updated. Fixed test failures and re-singletoned the factory hashing pools.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423
https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423:34,Testability,test,test,34,wdl4s and lenthall updated. Fixed test failures and re-singletoned the factory hashing pools.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423
https://github.com/broadinstitute/cromwell/issues/1380#issuecomment-245645479:62,Availability,failure,failure-mode,62,"config.file is loaded, but . > workflow-options {; > workflow-failure-mode: ""ContinueWhilePossible""; > }. does not work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1380#issuecomment-245645479
https://github.com/broadinstitute/cromwell/issues/1380#issuecomment-245645479:0,Modifiability,config,config,0,"config.file is loaded, but . > workflow-options {; > workflow-failure-mode: ""ContinueWhilePossible""; > }. does not work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1380#issuecomment-245645479
https://github.com/broadinstitute/cromwell/issues/1380#issuecomment-245645479:15,Performance,load,loaded,15,"config.file is loaded, but . > workflow-options {; > workflow-failure-mode: ""ContinueWhilePossible""; > }. does not work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1380#issuecomment-245645479
https://github.com/broadinstitute/cromwell/pull/1391#issuecomment-246059054:178,Testability,log,log,178,Hm... seems like the workflow id parsing under centaurJes maybe isn't working. Will debug this later:. ```; +EXIT_CODE=0; ++grep 'SingleWorkflowRunnerActor: Workflow submitted ' log.txt; ++perl -pe 's/\e\[?.*?[\@-~]//g'; ++cut -f7 '-d '; +export WORKFLOW_ID=Workflow; +WORKFLOW_ID=Workflow; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1391#issuecomment-246059054
https://github.com/broadinstitute/cromwell/pull/1391#issuecomment-246084273:125,Testability,log,logging,125,"More googling to do. The assembly merge is resulting in the original akka conf overriding ours, and removing the custom akka logging values.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1391#issuecomment-246084273
https://github.com/broadinstitute/cromwell/pull/1392#issuecomment-246056672:23,Deployability,update,updated,23,Centaur should also be updated run the two tests in #1383 before closing the ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1392#issuecomment-246056672
https://github.com/broadinstitute/cromwell/pull/1392#issuecomment-246056672:43,Testability,test,tests,43,Centaur should also be updated run the two tests in #1383 before closing the ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1392#issuecomment-246056672
https://github.com/broadinstitute/cromwell/pull/1392#issuecomment-246056803:69,Testability,test,tested,69,"👍 For this PR, but not for closing the ticket until this is actually tested. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1392/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1392#issuecomment-246056803
https://github.com/broadinstitute/cromwell/pull/1394#issuecomment-246362005:30,Energy Efficiency,green,green,30,Waiting for the tests to turn green-- will take off the EarlyFeedbackRequested label at that point.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1394#issuecomment-246362005
https://github.com/broadinstitute/cromwell/pull/1394#issuecomment-246362005:16,Testability,test,tests,16,Waiting for the tests to turn green-- will take off the EarlyFeedbackRequested label at that point.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1394#issuecomment-246362005
https://github.com/broadinstitute/cromwell/pull/1394#issuecomment-247705936:36,Testability,test,tests,36,👍 assuming we'll do some real scale tests to check it works for big sizes. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1394/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1394#issuecomment-247705936
https://github.com/broadinstitute/cromwell/issues/1395#issuecomment-256901209:249,Deployability,release,release,249,"@dvoet the reason for not having been done yet was that the primary motivating use case (or problem turner upper as it were) was GOTC who were able to work around it by not globbing in the first place. I believe this will be part of the upcoming 23 release. re S3, good news there for you, we have to do our own localization so don't need to rely on others to tell us what's going on ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395#issuecomment-256901209
https://github.com/broadinstitute/cromwell/issues/1395#issuecomment-256933084:143,Safety,timeout,timeouts,143,"A better design (which maybe requires a JES ask) is to have JES report back _exactly_ what was uploaded. Then there will be no uncertainty, no timeouts and no issues with eventual consistency being to eventual.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395#issuecomment-256933084
https://github.com/broadinstitute/cromwell/issues/1396#issuecomment-324467643:0,Safety,Abort,Abort,0,"Abort is broken, it is true. Closing this issue in favor of tracking all the Abort-related work in the [Google Doc](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1396#issuecomment-324467643
https://github.com/broadinstitute/cromwell/issues/1396#issuecomment-324467643:77,Safety,Abort,Abort-related,77,"Abort is broken, it is true. Closing this issue in favor of tracking all the Abort-related work in the [Google Doc](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1396#issuecomment-324467643
https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246718747:63,Energy Efficiency,green,green,63,:+1: you'll need to rebase on develop to get your builds to go green (zone issue). [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1403/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246718747
https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246796358:190,Deployability,configurat,configuration,190,Just thinking out loud - there seems to be a lot of duplication between this backend and the configurable shared-filesystem backend. ; I don't know whether you've tried expressing this as a configuration file backend?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246796358
https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246796358:93,Modifiability,config,configurable,93,Just thinking out loud - there seems to be a lot of duplication between this backend and the configurable shared-filesystem backend. ; I don't know whether you've tried expressing this as a configuration file backend?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246796358
https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246796358:190,Modifiability,config,configuration,190,Just thinking out loud - there seems to be a lot of duplication between this backend and the configurable shared-filesystem backend. ; I don't know whether you've tried expressing this as a configuration file backend?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246796358
https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246858765:160,Energy Efficiency,adapt,adapt,160,"@cjllanwarne Things in this backend long existed before sfs backend came into being, and we didn't look into it yet. Good point though, I think we might try to adapt this to the sfs backend some time in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246858765
https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246858765:160,Modifiability,adapt,adapt,160,"@cjllanwarne Things in this backend long existed before sfs backend came into being, and we didn't look into it yet. Good point though, I think we might try to adapt this to the sfs backend some time in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246858765
https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-246728088:233,Modifiability,config,config,233,"I'm guessing that this isn't running a build of a recent version of develop?. We have not automated publishing of develop builds of cromwell, so can you build a jar from the latest source code using `sbt assembly`, and then try your config again? If that doesn't work, and there's no private information inside you config file, please post the backend stanza from the application.conf so that we may continue debugging your issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-246728088
https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-246728088:315,Modifiability,config,config,315,"I'm guessing that this isn't running a build of a recent version of develop?. We have not automated publishing of develop builds of cromwell, so can you build a jar from the latest source code using `sbt assembly`, and then try your config again? If that doesn't work, and there's no private information inside you config file, please post the backend stanza from the application.conf so that we may continue debugging your issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-246728088
https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-246820302:165,Deployability,release,release,165,"@sndrtj To follow up a bit on what @kshakir says here, for any users who do not need call caching I'd highly recommend working with the develop branch. We expect to release Any Day Now (he's been saying for a few weeks) and while it'll have its own problems with life it's already demonstrated itself to be superior in just about every way to 0.19",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-246820302
https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-247018681:36,Availability,down,downloading,36,Great. Building the code instead of downloading the latest release works like a charm :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-247018681
https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-247018681:59,Deployability,release,release,59,Great. Building the code instead of downloading the latest release works like a charm :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-247018681
https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-324468545:114,Safety,Abort,Abort,114,"@mcovarr can you explain more what this issue is? Alternatively, I can close it and you're welcome to add to the [Abort spec](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-324468545
https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-342589399:34,Safety,abort,aborted,34,"Cromwell now claims a workflow is aborted when it has confirmation from JES that all jobs have a terminal status. This does not necessarily mean that JES did successfully abort all of them, but that's an issue we should bring up to Google if it happens. I'm not sure what this ticket is suggesting to do so I won't close :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-342589399
https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-342589399:171,Safety,abort,abort,171,"Cromwell now claims a workflow is aborted when it has confirmation from JES that all jobs have a terminal status. This does not necessarily mean that JES did successfully abort all of them, but that's an issue we should bring up to Google if it happens. I'm not sure what this ticket is suggesting to do so I won't close :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-342589399
https://github.com/broadinstitute/cromwell/pull/1411#issuecomment-246837777:141,Testability,log,logic,141,I do think that in practice this is unlikely to matter for a while and this isn't the long term right solution IMO. That said I also see the logic as to why it was the way it was (even if it was in an incorrect location) . tl;dr I don't think any of this is a huge deal at the moment so i'm cool with whatever 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1411/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1411#issuecomment-246837777
https://github.com/broadinstitute/cromwell/pull/1411#issuecomment-246895349:300,Availability,Ping,Pinging,300,"ToL: I'm fine if these actors hang as children off the initialization actor or some other _actor_, but not as singletons-in-the-system-jvm-referenced-from-the-non-actor-blaf. Relatedly, the name blaf is a slight misnomer, at it actually creates props, not actor refs, so maybe it should be the blpf? Pinging #1377 to record some of this as food for later thought.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1411#issuecomment-246895349
https://github.com/broadinstitute/cromwell/issues/1412#issuecomment-246819803:36,Usability,clear,clear,36,That name was just to make my point clear. I don't care what it's called.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412#issuecomment-246819803
https://github.com/broadinstitute/cromwell/issues/1412#issuecomment-327924055:106,Safety,avoid,avoid,106,"As a **WDL runner**, I want **to include a directory with index file(s) with my inputs**, so that I can **avoid doing it manually**.; - Effort: **? ** @geoffjentry ; - Risk: ** ? ** @geoffjentry ; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412#issuecomment-327924055
https://github.com/broadinstitute/cromwell/issues/1412#issuecomment-327924055:168,Safety,Risk,Risk,168,"As a **WDL runner**, I want **to include a directory with index file(s) with my inputs**, so that I can **avoid doing it manually**.; - Effort: **? ** @geoffjentry ; - Risk: ** ? ** @geoffjentry ; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412#issuecomment-327924055
https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324468923:31,Energy Efficiency,reduce,reduce,31,"@geoffjentry Why do we need to reduce the scope of EJEA? What's the current problem? ; We do have the i/o actor, does that help?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324468923
https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324470912:48,Energy Efficiency,reduce,reduced,48,What is the affect of the current EJEA versus a reduced EJEA on Cromwell and the user? What do you mean by developer sanity? What would the effort be to reduce EJEA?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324470912
https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324470912:153,Energy Efficiency,reduce,reduce,153,What is the affect of the current EJEA versus a reduced EJEA on Cromwell and the user? What do you mean by developer sanity? What would the effort be to reduce EJEA?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324470912
https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-325650373:98,Safety,Risk,Risk,98,"It's hard to say what the impact is, right now it is likely an annoyance. - Effort: **Medium**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-325650373
https://github.com/broadinstitute/cromwell/issues/1414#issuecomment-314539221:39,Safety,abort,aborting,39,Another recent example of some strange aborting behavior: http://gatkforums.broadinstitute.org/gatk/discussion/comment/40215,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414#issuecomment-314539221
https://github.com/broadinstitute/cromwell/issues/1414#issuecomment-324470022:155,Safety,Abort,Aborts,155,"I'm going to add these notes to the [Spec Doc](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit) and close this issue. Aborts need some serious thought and work, to be tracked in the spec doc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414#issuecomment-324470022
https://github.com/broadinstitute/cromwell/pull/1421#issuecomment-247414190:107,Integrability,message,messages,107,👍 assuming tests all pass. I feel like a while back some specs were checking waiting on certain `info` log messages. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1421/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1421#issuecomment-247414190
https://github.com/broadinstitute/cromwell/pull/1421#issuecomment-247414190:11,Testability,test,tests,11,👍 assuming tests all pass. I feel like a while back some specs were checking waiting on certain `info` log messages. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1421/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1421#issuecomment-247414190
https://github.com/broadinstitute/cromwell/pull/1421#issuecomment-247414190:103,Testability,log,log,103,👍 assuming tests all pass. I feel like a while back some specs were checking waiting on certain `info` log messages. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1421/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1421#issuecomment-247414190
https://github.com/broadinstitute/cromwell/pull/1421#issuecomment-247454623:48,Testability,test,test,48,I'm going to make a new PR once I've sorted out test progression without these logs,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1421#issuecomment-247454623
https://github.com/broadinstitute/cromwell/pull/1421#issuecomment-247454623:79,Testability,log,logs,79,I'm going to make a new PR once I've sorted out test progression without these logs,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1421#issuecomment-247454623
https://github.com/broadinstitute/cromwell/pull/1424#issuecomment-247664605:57,Deployability,update,updated,57,I don't know why Github conversation screen is not being updated with new commits.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1424#issuecomment-247664605
https://github.com/broadinstitute/cromwell/issues/1426#issuecomment-247630452:571,Safety,detect,detects,571,"Is there a typo in the better behavior? Should the job continue WITH; relocalizing (since the input.bam file is partially copied?. On Fri, Sep 16, 2016 at 11:20 AM, kshakir notifications@github.com wrote:. > The zeroth localizers checks to see if a file exists before re-localizing.; > The copy localizer should therefore copy-to-temp-then-rename.; > ; > Current broken behavior:; > - Run SGE task with a large input.bam and copy localization; > - Cromwell starts copying to <call_root>/input.bam; > - Kill the job during localization; > - Restart cromwell; > - Cromwell detects the partial <call_root>/input.bam exists.; > - The job continues without relocalizing.; > ; > Better behavior:; > - Run SGE task with a large input.bam and copy localization; > - Cromwell starts copying to <call_root>/input.bam.tmp; > - Kill the job during localization; > - Restart cromwell; > - Cromwell doesn't detects the partial <call_root>/input.bam exists.; > - The job continues without relocalizing.; > ; > And when cromwell isn't killed:; > - Run SGE task with a large input.bam and copy localization; > - Cromwell starts copying to <call_root>/input.bam.tmp, ensuring to; > overwrite previous results; > - When copying finishes rename <call_root>/input.bam.tmp to; > <call_root>/input.bam; > - The job continues; > ; > NOTE: Most people do not like copying inputs anyway, so this hasn't been a; > major issue.; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1426, or mute the; > thread; > https://github.com/notifications/unsubscribe-auth/ABW4g1ZnxF4tyJPhaY4gsjrShn6qz9Vyks5qqrOxgaJpZM4J_DUj; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1426#issuecomment-247630452
https://github.com/broadinstitute/cromwell/issues/1426#issuecomment-247630452:893,Safety,detect,detects,893,"Is there a typo in the better behavior? Should the job continue WITH; relocalizing (since the input.bam file is partially copied?. On Fri, Sep 16, 2016 at 11:20 AM, kshakir notifications@github.com wrote:. > The zeroth localizers checks to see if a file exists before re-localizing.; > The copy localizer should therefore copy-to-temp-then-rename.; > ; > Current broken behavior:; > - Run SGE task with a large input.bam and copy localization; > - Cromwell starts copying to <call_root>/input.bam; > - Kill the job during localization; > - Restart cromwell; > - Cromwell detects the partial <call_root>/input.bam exists.; > - The job continues without relocalizing.; > ; > Better behavior:; > - Run SGE task with a large input.bam and copy localization; > - Cromwell starts copying to <call_root>/input.bam.tmp; > - Kill the job during localization; > - Restart cromwell; > - Cromwell doesn't detects the partial <call_root>/input.bam exists.; > - The job continues without relocalizing.; > ; > And when cromwell isn't killed:; > - Run SGE task with a large input.bam and copy localization; > - Cromwell starts copying to <call_root>/input.bam.tmp, ensuring to; > overwrite previous results; > - When copying finishes rename <call_root>/input.bam.tmp to; > <call_root>/input.bam; > - The job continues; > ; > NOTE: Most people do not like copying inputs anyway, so this hasn't been a; > major issue.; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1426, or mute the; > thread; > https://github.com/notifications/unsubscribe-auth/ABW4g1ZnxF4tyJPhaY4gsjrShn6qz9Vyks5qqrOxgaJpZM4J_DUj; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1426#issuecomment-247630452
https://github.com/broadinstitute/cromwell/pull/1427#issuecomment-247640697:34,Security,validat,validation,34,":+1: . So sad, i'm always seeking validation :'(. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1427/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1427#issuecomment-247640697
https://github.com/broadinstitute/cromwell/pull/1429#issuecomment-247718213:9,Availability,failure,failure,9,"The test failure looks like it might be real: `should parse all manner of well-formed auths *** FAILED *** (1 second, 53 milliseconds)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1429#issuecomment-247718213
https://github.com/broadinstitute/cromwell/pull/1429#issuecomment-247718213:4,Testability,test,test,4,"The test failure looks like it might be real: `should parse all manner of well-formed auths *** FAILED *** (1 second, 53 milliseconds)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1429#issuecomment-247718213
https://github.com/broadinstitute/cromwell/pull/1432#issuecomment-248368192:54,Modifiability,config,configurable,54,@cjllanwarne reverted allowSoftLinks flag and keeping configurable docker cmd.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1432#issuecomment-248368192
https://github.com/broadinstitute/cromwell/issues/1435#issuecomment-247774072:10,Modifiability,variab,variable,10,"""instance variable"" .... named ....?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1435#issuecomment-247774072
https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:84,Availability,ERROR,ERROR,84,"```; 2016-09-16 14:45:19,558 cromwell-system-akka.dispatchers.backend-dispatcher-86 ERROR - Google credentials are invalid: Connection reset; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-d86697f6-ca39-417b-b575-fc955c808983/WorkflowExecutionActor-d86697f6-ca39-417b-b575-fc955c808983/d86697f6-ca39-417b-b575-fc955c808983-EngineJobExecutionActor-DeliciousFileSpam.FileSpam:364:1/d86697f6-ca39-417b-b575-fc955c808983-BackendJobExecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719
https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1035,Performance,concurren,concurrent,1035,stem-akka.dispatchers.backend-dispatcher-86 ERROR - Google credentials are invalid: Connection reset; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-d86697f6-ca39-417b-b575-fc955c808983/WorkflowExecutionActor-d86697f6-ca39-417b-b575-fc955c808983/d86697f6-ca39-417b-b575-fc955c808983-EngineJobExecutionActor-DeliciousFileSpam.FileSpam:364:1/d86697f6-ca39-417b-b575-fc955c808983-BackendJobExecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719
https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1108,Performance,concurren,concurrent,1108,e invalid: Connection reset; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-d86697f6-ca39-417b-b575-fc955c808983/WorkflowExecutionActor-d86697f6-ca39-417b-b575-fc955c808983/d86697f6-ca39-417b-b575-fc955c808983-EngineJobExecutionActor-DeliciousFileSpam.FileSpam:364:1/d86697f6-ca39-417b-b575-fc955c808983-BackendJobExecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:30); at cromwell.backend.impl.jes.JesCallPaths.<init>(J,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719
https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1193,Performance,concurren,concurrent,1193,-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-d86697f6-ca39-417b-b575-fc955c808983/WorkflowExecutionActor-d86697f6-ca39-417b-b575-fc955c808983/d86697f6-ca39-417b-b575-fc955c808983-EngineJobExecutionActor-DeliciousFileSpam.FileSpam:364:1/d86697f6-ca39-417b-b575-fc955c808983-BackendJobExecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:30); at cromwell.backend.impl.jes.JesCallPaths.<init>(JesCallPaths.scala:16); at cromwell.backend.impl.jes.JesCallPaths$.apply(JesCallPaths.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719
https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1270,Performance,concurren,concurrent,1270,9-417b-b575-fc955c808983/WorkflowExecutionActor-d86697f6-ca39-417b-b575-fc955c808983/d86697f6-ca39-417b-b575-fc955c808983-EngineJobExecutionActor-DeliciousFileSpam.FileSpam:364:1/d86697f6-ca39-417b-b575-fc955c808983-BackendJobExecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:30); at cromwell.backend.impl.jes.JesCallPaths.<init>(JesCallPaths.scala:16); at cromwell.backend.impl.jes.JesCallPaths$.apply(JesCallPaths.scala:11); at cromwell.backend.impl.jes.JesWorkflowPaths.toJesCallPaths(JesWo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719
https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1485,Security,validat,validateCredentials,1485,xecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:30); at cromwell.backend.impl.jes.JesCallPaths.<init>(JesCallPaths.scala:16); at cromwell.backend.impl.jes.JesCallPaths$.apply(JesCallPaths.scala:11); at cromwell.backend.impl.jes.JesWorkflowPaths.toJesCallPaths(JesWorkflowPaths.scala:42); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.jesCallPaths$lzycompute(JesAsyncBackendJobExecutionActor.scala:108); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.jesCallPath,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719
https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1582,Security,validat,validateCredentials,1582,on during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:30); at cromwell.backend.impl.jes.JesCallPaths.<init>(JesCallPaths.scala:16); at cromwell.backend.impl.jes.JesCallPaths$.apply(JesCallPaths.scala:11); at cromwell.backend.impl.jes.JesWorkflowPaths.toJesCallPaths(JesWorkflowPaths.scala:42); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.jesCallPaths$lzycompute(JesAsyncBackendJobExecutionActor.scala:108); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.jesCallPaths(JesAsyncBackendJobExecutionActor.scala:108); at cromwell.backend.impl.jes.JesAsyncBackendJobExe,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719
https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-267834315:81,Availability,error,errorstorms,81,"This might be related to #1782 somehow, I believe I've seen these in some of the errorstorms which wind up w/ the symptoms described in #1782",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-267834315
https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248762101:198,Availability,down,downstream,198,"In the spirit of ""everything will be different in a month"" I'm less concerned about some of the weird stuff I pointed out above and I realized it'd be good to get this functionality in 0.21 as some downstream constituents would like this behavior",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248762101
https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902:54,Availability,error,error,54,@geoffjentry the travis build is failing because some error handling has changed and so 2 refresh token centaur tests are failing--they should go green once you're rebased onto develop.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902
https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902:146,Energy Efficiency,green,green,146,@geoffjentry the travis build is failing because some error handling has changed and so 2 refresh token centaur tests are failing--they should go green once you're rebased onto develop.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902
https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902:112,Testability,test,tests,112,@geoffjentry the travis build is failing because some error handling has changed and so 2 refresh token centaur tests are failing--they should go green once you're rebased onto develop.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:190,Integrability,wrap,wrapped,190,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:381,Performance,perform,performActionThenRespond,381,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:532,Safety,Abort,AbortJobCommand,532,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:552,Safety,abort,abort,552,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:578,Safety,Abort,AbortedResponse,578,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:298,Testability,Log,LoggingReceive,298,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:340,Usability,simpl,simpletons,340,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:431,Usability,simpl,simpletons,431,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-324471614:42,Testability,test,tests,42,@mcovarr still a problem with the restart tests?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-324471614
https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-324510908:14,Testability,test,testing,14,"This wasn't a testing problem, this was a problem with the production code. This issue probably still exists but I'm not 100% sure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-324510908
https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-325454037:88,Testability,test,tests,88,@Horneth was this issue covered by the fixes you made after you implemented the restart tests?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-325454037
https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-327920051:125,Security,hash,hash,125,@Horneth then it sounds like it's no longer the case that Cromwell doesn't check if a job is complete before calculating the hash when restarting. In that case I'll close it unless there are objections.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-327920051
https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254578204:105,Testability,test,tested,105,"Addressed by #1456, that _should_ have made things better, but technically not yet verified / regression tested",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254578204
https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826:190,Performance,load,loads,190,"Great, thanks. I originally saw this issue when running a 20000-wide Hello World scatter using mock JES. At a point when Cromwell temporarily seemed catatonic, I Control-backslashed and saw loads of engine dispatcher stack traces like the above. Mock JES is currently [broken](#1571) due to batching API changes but hopefully it will become great again soon and the #1456 changes can be validated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826
https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826:387,Security,validat,validated,387,"Great, thanks. I originally saw this issue when running a 20000-wide Hello World scatter using mock JES. At a point when Cromwell temporarily seemed catatonic, I Control-backslashed and saw loads of engine dispatcher stack traces like the above. Mock JES is currently [broken](#1571) due to batching API changes but hopefully it will become great again soon and the #1456 changes can be validated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826
https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826:95,Testability,mock,mock,95,"Great, thanks. I originally saw this issue when running a 20000-wide Hello World scatter using mock JES. At a point when Cromwell temporarily seemed catatonic, I Control-backslashed and saw loads of engine dispatcher stack traces like the above. Mock JES is currently [broken](#1571) due to batching API changes but hopefully it will become great again soon and the #1456 changes can be validated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826
https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826:246,Testability,Mock,Mock,246,"Great, thanks. I originally saw this issue when running a 20000-wide Hello World scatter using mock JES. At a point when Cromwell temporarily seemed catatonic, I Control-backslashed and saw loads of engine dispatcher stack traces like the above. Mock JES is currently [broken](#1571) due to batching API changes but hopefully it will become great again soon and the #1456 changes can be validated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826
https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254583482:132,Energy Efficiency,monitor,monitoring,132,"An addendum to this and #1456 is that a) I should formalize (at least a bit) what I was going a few weeks ago w/ spamming cromwell, monitoring, etc and b) we should go over it as a group such that everyone feels comfortable doing such things",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254583482
https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-267833739:27,Performance,scalab,scalability,27,@kcibul When making the Q3 scalability bucket I think that verification of what @kshakir said above should be part of it,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-267833739
https://github.com/broadinstitute/cromwell/issues/1446#issuecomment-248438503:143,Deployability,install,install,143,"Closing this; I [added a cromwell recipe to bioconda](https://github.com/bioconda/bioconda-recipes/pull/2348), so now it is possible to `conda install cromwell` (if you have the bioconda channel in your config). See here for more info:; https://bioconda.github.io/recipes/cromwell/README.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1446#issuecomment-248438503
https://github.com/broadinstitute/cromwell/issues/1446#issuecomment-248438503:203,Modifiability,config,config,203,"Closing this; I [added a cromwell recipe to bioconda](https://github.com/bioconda/bioconda-recipes/pull/2348), so now it is possible to `conda install cromwell` (if you have the bioconda channel in your config). See here for more info:; https://bioconda.github.io/recipes/cromwell/README.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1446#issuecomment-248438503
https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248118103:86,Performance,cache,cache,86,"@cjllanwarne @mcovarr you guys are the lucky reviewers! Making the changes to publish cache hit metadata that includes Workflow ID, call name and job index of the source cache hit call.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248118103
https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248118103:170,Performance,cache,cache,170,"@cjllanwarne @mcovarr you guys are the lucky reviewers! Making the changes to publish cache hit metadata that includes Workflow ID, call name and job index of the source cache hit call.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248118103
https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248127965:49,Safety,sanity check,sanity check,49,I'm (slowly) spinning up a cloud sql instance to sanity check that my branch can handle a gotc db migration in a single transaction.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248127965
https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248162039:47,Testability,test,test,47,"Merge at will. I've left a comment regarding a test value, but it's not a must fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248162039
https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-290808936:31,Testability,log,logs,31,I took a quick look at how the logs are currently copied. It looks like it would be fairly straightforward to have the logs copying periodically but I might be missing some nuance.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-290808936
https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-290808936:119,Testability,log,logs,119,I took a quick look at how the logs are currently copied. It looks like it would be fairly straightforward to have the logs copying periodically but I might be missing some nuance.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-290808936
https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-324662304:107,Testability,log,logs,107,@abaumann can you tell me a use case for when a user would want Cromwell to periodically copy the workflow logs?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-324662304
https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-327890924:184,Safety,Risk,Risk,184,"As a **workflow runner**, I want **periodically copy workflow logs**, so that I can **view intermediary results without waiting for the workflow to complete**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-327890924
https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-327890924:62,Testability,log,logs,62,"As a **workflow runner**, I want **periodically copy workflow logs**, so that I can **view intermediary results without waiting for the workflow to complete**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-327890924
https://github.com/broadinstitute/cromwell/issues/1457#issuecomment-248741884:95,Security,hash,hashCode,95,"The original title of this ticket turned out to be incorrect, the actual problem was needless `hashCode` computation when `Scope`s were put in `Set`s or used as `Map` keys. Thanks to @kshakir for pointing out that Scala should generate case class `equals` correctly and shouldn't have the problem I had claimed. 😄 . PR forthcoming for the `hashCode` issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1457#issuecomment-248741884
https://github.com/broadinstitute/cromwell/issues/1457#issuecomment-248741884:340,Security,hash,hashCode,340,"The original title of this ticket turned out to be incorrect, the actual problem was needless `hashCode` computation when `Scope`s were put in `Set`s or used as `Map` keys. Thanks to @kshakir for pointing out that Scala should generate case class `equals` correctly and shouldn't have the problem I had claimed. 😄 . PR forthcoming for the `hashCode` issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1457#issuecomment-248741884
https://github.com/broadinstitute/cromwell/issues/1461#issuecomment-248616220:41,Availability,down,down,41,You might have to lower the `range(100)` down. That was me testing my concurrent job limits...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461#issuecomment-248616220
https://github.com/broadinstitute/cromwell/issues/1461#issuecomment-248616220:70,Performance,concurren,concurrent,70,You might have to lower the `range(100)` down. That was me testing my concurrent job limits...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461#issuecomment-248616220
https://github.com/broadinstitute/cromwell/issues/1461#issuecomment-248616220:59,Testability,test,testing,59,You might have to lower the `range(100)` down. That was me testing my concurrent job limits...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461#issuecomment-248616220
https://github.com/broadinstitute/cromwell/pull/1464#issuecomment-248677461:94,Deployability,release,release,94,"@cjllanwarne -- great suggestion, but I want to make the least amount of possible changes pre-release. I can ticket it; @mcovarr other than re-starting the JES build, is there anything else require to get a thumb?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1464#issuecomment-248677461
https://github.com/broadinstitute/cromwell/pull/1468#issuecomment-248757551:39,Safety,avoid,avoid,39,"I concur-- more comments in general to avoid people ""fixing"" anything you've profiled here. After that 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1468/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1468#issuecomment-248757551
https://github.com/broadinstitute/cromwell/pull/1472#issuecomment-248970549:33,Deployability,release,release,33,:+1: glad you caught this before release! 😌 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1472/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1472#issuecomment-248970549
https://github.com/broadinstitute/cromwell/pull/1477#issuecomment-249003742:97,Modifiability,enhance,enhanced,97,I think it'd be good to right at the top make a big point about the number one 'feature' here is enhanced performance and scalability,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1477#issuecomment-249003742
https://github.com/broadinstitute/cromwell/pull/1477#issuecomment-249003742:106,Performance,perform,performance,106,I think it'd be good to right at the top make a big point about the number one 'feature' here is enhanced performance and scalability,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1477#issuecomment-249003742
https://github.com/broadinstitute/cromwell/pull/1477#issuecomment-249003742:122,Performance,scalab,scalability,122,I think it'd be good to right at the top make a big point about the number one 'feature' here is enhanced performance and scalability,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1477#issuecomment-249003742
https://github.com/broadinstitute/cromwell/issues/1479#issuecomment-249202328:59,Availability,avail,available,59,👍 . I was trying to do this with @sooheelee using the data available from the google genomics wdl runner and it was impossible to figure out.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1479#issuecomment-249202328
https://github.com/broadinstitute/cromwell/issues/1479#issuecomment-249202756:112,Availability,Error,Error,112,"@geoffjentry Yeah, I just had to run 14 `cat` commands before I finally found the (transient) issue... `docker: Error response from daemon: device or resource busy.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1479#issuecomment-249202756
https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249210875:65,Security,access,access,65,@Horneth @geoffjentry As per our discussion. Tell me if you need access to the VM...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249210875
https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220094:0,Deployability,Update,Update,0,"Update: Nothing is happening (according to `top`). But the workflow seems to be running... ```; {; ""status"": ""Running"",; ""id"": ""7b3cdd40-2c3c-4533-be62-06b7d9135546""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220094
https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220474:0,Deployability,Update,Update,0,Update: Still cannot get a timing diagram:; http://104.198.41.229:8080/api/workflows/v2/7b3cdd40-2c3c-4533-be62-06b7d9135546/timing. Just gives me a `)` in my browser window.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220474
https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220630:133,Deployability,release,release,133,what about the stats endpoint? . https://github.com/broadinstitute/cromwell#get-apiworkflowsversionstats. Note: If you're not on the release you probably don't have this endpoint,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220630
https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249237460:0,Deployability,Update,Update,0,"Update:. ```; $ curl http://localhost:8080/api/engine/v1/stats; {; ""workflows"": 0,; ""jobs"": 0; }; ```. Except, when using `top` I see several jobs running with the workflow ID: 7b3cdd40-2c3c-4533-be62-06b7d9135546. ```; java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --reference /root/case_gatk_acnv_workflow/**7b3cdd40-2c3c-4533**-b+; ```. Status gives me: . ```; {; ""status"": ""fail"",; ""message"": ""Failed lookup attempt for workflow ID 7b3cdd40-2c3c-4533-be62-06b7d9135546""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249237460
https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249237460:398,Integrability,message,message,398,"Update:. ```; $ curl http://localhost:8080/api/engine/v1/stats; {; ""workflows"": 0,; ""jobs"": 0; }; ```. Except, when using `top` I see several jobs running with the workflow ID: 7b3cdd40-2c3c-4533-be62-06b7d9135546. ```; java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --reference /root/case_gatk_acnv_workflow/**7b3cdd40-2c3c-4533**-b+; ```. Status gives me: . ```; {; ""status"": ""fail"",; ""message"": ""Failed lookup attempt for workflow ID 7b3cdd40-2c3c-4533-be62-06b7d9135546""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249237460
https://github.com/broadinstitute/cromwell/issues/1481#issuecomment-249233766:175,Modifiability,variab,variable,175,"Surprisingly, there is no way to do this directly in the docker compose. The solution was to change our sbt-docker usage to run cromwell with $CROMWELL_ARGS as an environment variable, which you can set in a docker compose. For example:. `app:; image: broadinstitute/cromwell:0.20-e56c9e8-SNAPSHOT; environment:; CROMWELL_ARGS: server; `",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1481#issuecomment-249233766
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:11398,Availability,down,downstream,11398,"fasta} \; --disable_all_read_filters ${disable_all_read_filters} --interval_set_rule UNION --interval_padding 0 \; --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \; --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; \; else touch ${entity_id}.coverage.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; }. #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Calculate coverage on Whole Genome Sequence using Spark.; # This task automatically creates a target output file.; task WholeGenomeCoverage {; String entity_id; File coverage_file ; File target_file; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; String gatk_jar; Boolean isWGS; Int wgsBinSize; Int mem. # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else cp ${coverage_file} ${entity_id}.coverage.tsv; cp ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:11496,Availability,down,downstream,11496,"fasta} \; --disable_all_read_filters ${disable_all_read_filters} --interval_set_rule UNION --interval_padding 0 \; --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \; --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; \; else touch ${entity_id}.coverage.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; }. #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Calculate coverage on Whole Genome Sequence using Spark.; # This task automatically creates a target output file.; task WholeGenomeCoverage {; String entity_id; File coverage_file ; File target_file; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; String gatk_jar; Boolean isWGS; Int wgsBinSize; Int mem. # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else cp ${coverage_file} ${entity_id}.coverage.tsv; cp ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:12341,Availability,down,downstream,12341,"eam tasks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else cp ${coverage_file} ${entity_id}.coverage.tsv; cp ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else cp ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent normaliz",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:12970,Availability,down,downstream,12970,"existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else cp ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent normalization (noise reduction) on the proportional coverage file.; task NormalizeSomaticReadCounts {; String entity_id; File coverage_file; File padded_target_file; File pon; String gatk_jar; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:666,Modifiability,variab,variable,666,"Tell me if you need me to post the metadata ...it is very big for github. WDL is below... ```; #; # Case sample workflow for a list of pairs of case-control samples. Includes GATK CNV and ACNV. Supports both WGS and WES samples. This was tested on a3c7368 commit of gatk-protected.; #; # Notes:; #; # - the input file(input_bam_list) must contain a list of tab separated values in the following format(one or more lines must be supplied):; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--first input; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--second input; # etc...; #; # - set isWGS variable to true or false to specify whether to run a WGS or WES workflow respectively; #; # - file names will use the entity ID specified, but inside the file, the bam SM tag will typically be used.; #; # - target file (which must be in tsv format) is only used with WES workflow, WGS workflow generates its own targets (so user can pass any string as an argument); #; # - the WGS PoN must be generated with WGS samples; # ; # - THIS SCRIPT SHOULD BE CONSIDERED OF ""BETA"" QUALITY; #; # - Example invocation:; # java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json; # - See case_gatk_acnv_workflow_template.json for a template json file to modify with your own parameters (please save; # your modified version with a different filename and do not commit to gatk-protected repo).; #; # - Some call inputs might seem out of place - consult with the comments in task definitions for details; #; #############. workflow case_gatk_acnv_workflow {; # Workflow input files; File target_file; File ref_fasta; File ref_fasta_dict; File ref_fasta_fai; File common_snp_list; File input_bam_list; Array[Array[String]] bam_list_array = read_tsv(input_bam_list); File PoN; String gatk_jar. # Input parameters of the PerformSegmentation tool; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_min",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:1888,Performance,Perform,PerformSegmentation,1888,"g as an argument); #; # - the WGS PoN must be generated with WGS samples; # ; # - THIS SCRIPT SHOULD BE CONSIDERED OF ""BETA"" QUALITY; #; # - Example invocation:; # java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json; # - See case_gatk_acnv_workflow_template.json for a template json file to modify with your own parameters (please save; # your modified version with a different filename and do not commit to gatk-protected repo).; #; # - Some call inputs might seem out of place - consult with the comments in task definitions for details; #; #############. workflow case_gatk_acnv_workflow {; # Workflow input files; File target_file; File ref_fasta; File ref_fasta_dict; File ref_fasta_fai; File common_snp_list; File input_bam_list; Array[Array[String]] bam_list_array = read_tsv(input_bam_list); File PoN; String gatk_jar. # Input parameters of the PerformSegmentation tool; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD. # Workflow output directories and options; String plots_dir; String call_cnloh_dir; Boolean disable_sequence_dictionary_validation; Boolean enable_gc_correction; Boolean isWGS; Int wgsBinSize. call PadTargets {; input:; target_file=target_file,; gatk_jar=gatk_jar,; isWGS=isWGS,; mem=1; }. scatter (row in bam_list_array) {. call CalculateTargetCoverage as TumorCalculateTargetCoverage {; input:; entity_id=row[0],; padded_target_file=PadTargets.padded_target_file,; input_bam=row[1],; bam_idx=row[2],; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; disable_sequence_dictionary_validation=disable_sequence_dictionary_validation,; isWGS=isWGS,; mem=2; } . call WholeGenomeCoverage as TumorWholeGenomeCoverage {; input:; entity_id=row[0],; target_file=PadTargets.padded_target_file,; input_b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:4085,Performance,Perform,PerformSegmentation,4085,"f_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; isWGS=isWGS,; wgsBinSize=wgsBinSize,; mem=4; }. call AnnotateTargets as TumorAnnotateTargets {; input:; entity_id=row[0],; gatk_jar=gatk_jar,; target_file=TumorWholeGenomeCoverage.gatk_target_file,; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; enable_gc_correction=enable_gc_correction,; mem=4; }. call CorrectGCBias as TumorCorrectGCBias {; input:; entity_id=row[0], ; gatk_jar=gatk_jar,; coverage_file=TumorWholeGenomeCoverage.gatk_coverage_file,; annotated_targets=TumorAnnotateTargets.annotated_targets,; enable_gc_correction=enable_gc_correction,; mem=4; }. call NormalizeSomaticReadCounts as TumorNormalizeSomaticReadCounts {; input:; entity_id=row[0], ; coverage_file=TumorCorrectGCBias.gatk_cnv_coverage_file_gcbias,; padded_target_file=TumorWholeGenomeCoverage.gatk_target_file,; pon=PoN,; gatk_jar=gatk_jar,; mem=2; }. call PerformSegmentation as TumorPerformSeg {; input:; entity_id=row[0],; gatk_jar=gatk_jar,; tn_file=TumorNormalizeSomaticReadCounts.tn_file,; seg_param_alpha=seg_param_alpha,; seg_param_nperm=seg_param_nperm,; seg_param_pmethod=seg_param_pmethod,; seg_param_minWidth=seg_param_minWidth,; seg_param_kmax=seg_param_kmax,; seg_param_nmin=seg_param_nmin,; seg_param_eta=seg_param_eta,; seg_param_trim=seg_param_trim,; seg_param_undoSplits=seg_param_undoSplits,; seg_param_undoPrune=seg_param_undoPrune,; seg_param_undoSD=seg_param_undoSD,; mem=2; }. call Caller as TumorCaller {; input:; entity_id=row[0],; gatk_jar=gatk_jar,; tn_file=TumorNormalizeSomaticReadCounts.tn_file,; seg_file=TumorPerformSeg.seg_file,; mem=2; }. call HetPulldown {; input:; entity_id_tumor=row[0],; entity_id_normal=row[3],; gatk_jar=gatk_jar,; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; tumor_bam=row[1],; tumor_bam_idx=row[2],; normal_bam=row[4],; normal_bam_idx=row[5],; common_snp_list=common_snp_list,; mem=4. }. call AllelicCNV {; input:; entity",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:7689,Performance,Perform,PerformSegmentation,7689,"a_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; isWGS=isWGS,; wgsBinSize=wgsBinSize,; mem=4; }. call AnnotateTargets as NormalAnnotateTargets{; input:; entity_id=row[3],; gatk_jar=gatk_jar,; target_file=NormalWholeGenomeCoverage.gatk_target_file,; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; enable_gc_correction=enable_gc_correction,; mem=4; }. call CorrectGCBias as NormalCorrectGCBias {; input:; entity_id=row[3],; gatk_jar=gatk_jar,; coverage_file=NormalWholeGenomeCoverage.gatk_coverage_file,; annotated_targets=NormalAnnotateTargets.annotated_targets,; enable_gc_correction=enable_gc_correction,; mem=4; }. call NormalizeSomaticReadCounts as NormalNormalizeSomaticReadCounts {; input:; entity_id=row[3],; coverage_file=NormalCorrectGCBias.gatk_cnv_coverage_file_gcbias,; padded_target_file=NormalWholeGenomeCoverage.gatk_target_file,; pon=PoN,; gatk_jar=gatk_jar,; mem=2; }. call PerformSegmentation as NormalPerformSeg {; input:; entity_id=row[3],; gatk_jar=gatk_jar,; tn_file=NormalNormalizeSomaticReadCounts.tn_file,; seg_param_alpha=seg_param_alpha,; seg_param_nperm=seg_param_nperm,; seg_param_pmethod=seg_param_pmethod,; seg_param_minWidth=seg_param_minWidth,; seg_param_kmax=seg_param_kmax,; seg_param_nmin=seg_param_nmin,; seg_param_eta=seg_param_eta,; seg_param_trim=seg_param_trim,; seg_param_undoSplits=seg_param_undoSplits,; seg_param_undoPrune=seg_param_undoPrune,; seg_param_undoSD=seg_param_undoSD,; mem=2; }. call Caller as NormalCaller {; input:; entity_id=row[3],; gatk_jar=gatk_jar,; tn_file=NormalNormalizeSomaticReadCounts.tn_file,; seg_file=NormalPerformSeg.seg_file,; mem=2; }; }; }. # Pad the target file. This was found to help sensitivity and specificity. This step should only be altered; # by advanced users. Note that by changing this, you need to have a PoN that also reflects the change.; task PadTargets {; File target_file; Int padding = 250; String gatk_jar; Boolean isWGS; Int mem. # Note that when isWGS is ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:13381,Performance,Perform,Perform,13381,"${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else cp ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent normalization (noise reduction) on the proportional coverage file.; task NormalizeSomaticReadCounts {; String entity_id; File coverage_file; File padded_target_file; File pon; String gatk_jar; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nm",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:14250,Performance,Perform,PerformSegmentation,14250,"ise reduction) on the proportional coverage file.; task NormalizeSomaticReadCounts {; String entity_id; File coverage_file; File padded_target_file; File pon; String gatk_jar; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:14631,Performance,Perform,PerformSegmentation,14631,"r} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} --output ${entity_id}.called --legacy false \; --help false --version false --verbosity INFO --QUIET false; }. output {; File called_file=""${entity_id}.called""; }; }. # Call heterozygous SNPs in the normal and then count the re",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:238,Testability,test,tested,238,"Tell me if you need me to post the metadata ...it is very big for github. WDL is below... ```; #; # Case sample workflow for a list of pairs of case-control samples. Includes GATK CNV and ACNV. Supports both WGS and WES samples. This was tested on a3c7368 commit of gatk-protected.; #; # Notes:; #; # - the input file(input_bam_list) must contain a list of tab separated values in the following format(one or more lines must be supplied):; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--first input; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--second input; # etc...; #; # - set isWGS variable to true or false to specify whether to run a WGS or WES workflow respectively; #; # - file names will use the entity ID specified, but inside the file, the bam SM tag will typically be used.; #; # - target file (which must be in tsv format) is only used with WES workflow, WGS workflow generates its own targets (so user can pass any string as an argument); #; # - the WGS PoN must be generated with WGS samples; # ; # - THIS SCRIPT SHOULD BE CONSIDERED OF ""BETA"" QUALITY; #; # - Example invocation:; # java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json; # - See case_gatk_acnv_workflow_template.json for a template json file to modify with your own parameters (please save; # your modified version with a different filename and do not commit to gatk-protected repo).; #; # - Some call inputs might seem out of place - consult with the comments in task definitions for details; #; #############. workflow case_gatk_acnv_workflow {; # Workflow input files; File target_file; File ref_fasta; File ref_fasta_dict; File ref_fasta_fai; File common_snp_list; File input_bam_list; Array[Array[String]] bam_list_array = read_tsv(input_bam_list); File PoN; String gatk_jar. # Input parameters of the PerformSegmentation tool; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_min",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:14942,Usability,undo,undoSplits,14942,"alized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} --output ${entity_id}.called --legacy false \; --help false --version false --verbosity INFO --QUIET false; }. output {; File called_file=""${entity_id}.called""; }; }. # Call heterozygous SNPs in the normal and then count the reads in the tumor for each called position.; # Entity IDs can be the same value; task HetPulldown {; String entity_id_tumor; String entity_id_normal; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File tumor_bam; File tumor_bam_idx; File",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:14982,Usability,undo,undoPrune,14982,"alized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} --output ${entity_id}.called --legacy false \; --help false --version false --verbosity INFO --QUIET false; }. output {; File called_file=""${entity_id}.called""; }; }. # Call heterozygous SNPs in the normal and then count the reads in the tumor for each called position.; # Entity IDs can be the same value; task HetPulldown {; String entity_id_tumor; String entity_id_normal; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File tumor_bam; File tumor_bam_idx; File",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:15017,Usability,undo,undoSD,15017,"alized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} --output ${entity_id}.called --legacy false \; --help false --version false --verbosity INFO --QUIET false; }. output {; File called_file=""${entity_id}.called""; }; }. # Call heterozygous SNPs in the normal and then count the reads in the tumor for each called position.; # Entity IDs can be the same value; task HetPulldown {; String entity_id_tumor; String entity_id_normal; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File tumor_bam; File tumor_bam_idx; File",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249863978:28,Performance,cache,cache,28,- throttling was on; - call cache hashing was done with file paths,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249863978
https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249863978:34,Security,hash,hashing,34,- throttling was on; - call cache hashing was done with file paths,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249863978
https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249866905:75,Performance,Cache,Cache,75,"To save future metadata spelunking: . ```; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6/execution/stdout"",; ""shardIndex"": 6,; ""outputs"": {; ""gatk_cnv_coverage_file_gcbias"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6/execution/SM-74NEG.gc_corrected_coverage.tsv""; },; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""coverage_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalWholeGenomeCoverage/shard-6/execution/SM-74NEG.coverage.tsv"",; ""enable_gc_correction"": true,; ""entity_id"": ""SM-74NEG"",; ""mem"": 4,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""annotated_targets"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalAnnotateTargets/shard-6/execution/SM-74NEG.annotated.tsv""; },; ""returnCode"": 0,; ""jobId"": ""12340"",; ""backend"": ""Local"",; ""end"": ""2016-09-26T19:52:35.224Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6"",; ""attempt"": 1,; ""executionEvents"": [{; ""startTime"": ""2016-09-26T19:49:19.556Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-26T19:49:19.556Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.556Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""201",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249866905
https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249866905:713,Performance,cache,cache,713,"To save future metadata spelunking: . ```; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6/execution/stdout"",; ""shardIndex"": 6,; ""outputs"": {; ""gatk_cnv_coverage_file_gcbias"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6/execution/SM-74NEG.gc_corrected_coverage.tsv""; },; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""coverage_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalWholeGenomeCoverage/shard-6/execution/SM-74NEG.coverage.tsv"",; ""enable_gc_correction"": true,; ""entity_id"": ""SM-74NEG"",; ""mem"": 4,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""annotated_targets"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalAnnotateTargets/shard-6/execution/SM-74NEG.annotated.tsv""; },; ""returnCode"": 0,; ""jobId"": ""12340"",; ""backend"": ""Local"",; ""end"": ""2016-09-26T19:52:35.224Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6"",; ""attempt"": 1,; ""executionEvents"": [{; ""startTime"": ""2016-09-26T19:49:19.556Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-26T19:49:19.556Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.556Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""201",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249866905
https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249866905:2707,Performance,Cache,Cache,2707,"esultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""coverage_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalWholeGenomeCoverage/shard-6/execution/SM-74NEG.coverage.tsv"",; ""enable_gc_correction"": true,; ""entity_id"": ""SM-74NEG"",; ""mem"": 4,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""annotated_targets"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalAnnotateTargets/shard-6/execution/SM-74NEG.annotated.tsv""; },; ""returnCode"": 0,; ""jobId"": ""12340"",; ""backend"": ""Local"",; ""end"": ""2016-09-26T19:52:35.224Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6"",; ""attempt"": 1,; ""executionEvents"": [{; ""startTime"": ""2016-09-26T19:49:19.556Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-26T19:49:19.556Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.556Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2016-09-26T19:49:19.557Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.557Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2016-09-26T19:49:19.559Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.559Z"",; ""description"": ""CheckingCallCache"",; ""endTime"": ""2016-09-26T19:49:19.564Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.564Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2016-09-26T19:49:31.957Z""; }, {; ""startTime"": ""2016-09-26T19:49:31.957Z"",; ""description"": ""UpdatingCallCache"",; ""endTime"": ""2016-09-26T19:49:31.966Z""; }, {; ""startTime"": ""2016-09-26T19:49:31.966Z"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2016-09-26T19:49:31.971Z""; }],; ""start"": ""2016-09-26T19:49:19.556Z""; }; ```. Side note, this affected Cache Hit and Cache Miss",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249866905
https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249866905:2721,Performance,Cache,Cache,2721,"esultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""coverage_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalWholeGenomeCoverage/shard-6/execution/SM-74NEG.coverage.tsv"",; ""enable_gc_correction"": true,; ""entity_id"": ""SM-74NEG"",; ""mem"": 4,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""annotated_targets"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalAnnotateTargets/shard-6/execution/SM-74NEG.annotated.tsv""; },; ""returnCode"": 0,; ""jobId"": ""12340"",; ""backend"": ""Local"",; ""end"": ""2016-09-26T19:52:35.224Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/70a6e380-1dd7-473b-a852-4bd54b22ecdf/call-NormalCorrectGCBias/shard-6"",; ""attempt"": 1,; ""executionEvents"": [{; ""startTime"": ""2016-09-26T19:49:19.556Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-26T19:49:19.556Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.556Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2016-09-26T19:49:19.557Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.557Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2016-09-26T19:49:19.559Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.559Z"",; ""description"": ""CheckingCallCache"",; ""endTime"": ""2016-09-26T19:49:19.564Z""; }, {; ""startTime"": ""2016-09-26T19:49:19.564Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2016-09-26T19:49:31.957Z""; }, {; ""startTime"": ""2016-09-26T19:49:31.957Z"",; ""description"": ""UpdatingCallCache"",; ""endTime"": ""2016-09-26T19:49:31.966Z""; }, {; ""startTime"": ""2016-09-26T19:49:31.966Z"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2016-09-26T19:49:31.971Z""; }],; ""start"": ""2016-09-26T19:49:19.556Z""; }; ```. Side note, this affected Cache Hit and Cache Miss",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249866905
https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-262541364:228,Security,access,access,228,"@LeeTL1220 -- I'm not sure what to do with this... have you seen it again? Seems like the timing diagram is fine, but that the underlying metadata was incorrect somehow... and I'm guessing we can't reproduce this and don't have access to the data any longer?. @cjllanwarne -- is there an actionable ticket from your digging?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-262541364
https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-262562130:458,Security,access,access,458,"At the time, I could replicate with 100 percent certainty. Have not tried; in a while. I'm out, otherwise, I'd do it now. On Nov 23, 2016 10:13 AM, ""kcibul"" notifications@github.com wrote:. > @LeeTL1220 https://github.com/LeeTL1220 -- I'm not sure what to do with; > this... have you seen it again? Seems like the timing diagram is fine, but; > that the underlying metadata was incorrect somehow... and I'm guessing we; > can't reproduce this and don't have access to the data any longer?; > ; > @cjllanwarne https://github.com/cjllanwarne -- is there an actionable; > ticket from your digging?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-262541364,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXkzqZCzlxr0CyuAuHkr2lcTvHXZjHks5rBFgNgaJpZM4KHqX4; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-262562130
https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-291571578:13,Integrability,message,message,13,"See my slack message. Unless you've fixed it recently, this is still an; issue. On Tue, Apr 4, 2017 at 12:48 PM, Kate Voss <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> have you had a chance to retry; > it? If it's no longer a problem, we'll close it.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-291561369>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_tbc5EfZFXbnN-7A0l7quaQjZgsks5rsnRBgaJpZM4KHqX4>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-291571578
https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-444077848:141,Safety,detect,detect,141,"Hi @katevoss I'm getting the same or very similar behaviour, running Cromwell 36. A job running on SGE finished at 18:55 but Cromwell didn't detect that it had finished until 9:50 the next morning. I could see the `rc` file in there with return code `0` while Cromwell still reported the job as `Running`. . This is a single, very low resource, 5 minute job, which runs right after a large (10k) scatter - don't know if that's really relevant though. It has happened twice in a row now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-444077848
https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-444462951:212,Testability,log,logs,212,"This is probably a different issue. My workflow is still showing as `Running` in the `Metadata` endpoint. ; I've retried now, and still get the same behaviour:; My SGE job finished ~17hs ago, and in the Cromwell logs I can see `Status change from Running to Done` for that task. However the metadata endpoint says `""executionStatus"": ""Running""`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-444462951
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984:504,Deployability,install,installation,504,"> Do we have a lot of users with UGER, external and internal?. I can only speak for internal: More and more ""Broad"" users are using Grid Engine, especially [UGER](https://intranet.broadinstitute.org/bits/service-catalog/compute-resources/high-performance-computing-uger), but not with cromwell, yet. DSP-methods have been big users of Grid Engine over the past few years, but mostly the older Sun Grid Engine, and with GATK-Queue. Meanwhile BITS is trying to get them to move over to a Univa Grid Engine installation BITS has called UGER. Unlike the older unlimited setup, UGER's setup has hard limits on the number of concurrent jobs that can be tracked by the Grid Engine scheduler, previously 100, now 1000. With the relaxing of the limit, plus the cromwell's `concurrent-job-limit` feature, this ticket a lower priority imho. Still, it ""would be nice"" if just like we submit in batches to JES, we also submitted in batches to other systems that support it including GridEngine/SLURM/PBS/etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984:674,Energy Efficiency,schedul,scheduler,674,"> Do we have a lot of users with UGER, external and internal?. I can only speak for internal: More and more ""Broad"" users are using Grid Engine, especially [UGER](https://intranet.broadinstitute.org/bits/service-catalog/compute-resources/high-performance-computing-uger), but not with cromwell, yet. DSP-methods have been big users of Grid Engine over the past few years, but mostly the older Sun Grid Engine, and with GATK-Queue. Meanwhile BITS is trying to get them to move over to a Univa Grid Engine installation BITS has called UGER. Unlike the older unlimited setup, UGER's setup has hard limits on the number of concurrent jobs that can be tracked by the Grid Engine scheduler, previously 100, now 1000. With the relaxing of the limit, plus the cromwell's `concurrent-job-limit` feature, this ticket a lower priority imho. Still, it ""would be nice"" if just like we submit in batches to JES, we also submitted in batches to other systems that support it including GridEngine/SLURM/PBS/etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984:243,Performance,perform,performance-computing-uger,243,"> Do we have a lot of users with UGER, external and internal?. I can only speak for internal: More and more ""Broad"" users are using Grid Engine, especially [UGER](https://intranet.broadinstitute.org/bits/service-catalog/compute-resources/high-performance-computing-uger), but not with cromwell, yet. DSP-methods have been big users of Grid Engine over the past few years, but mostly the older Sun Grid Engine, and with GATK-Queue. Meanwhile BITS is trying to get them to move over to a Univa Grid Engine installation BITS has called UGER. Unlike the older unlimited setup, UGER's setup has hard limits on the number of concurrent jobs that can be tracked by the Grid Engine scheduler, previously 100, now 1000. With the relaxing of the limit, plus the cromwell's `concurrent-job-limit` feature, this ticket a lower priority imho. Still, it ""would be nice"" if just like we submit in batches to JES, we also submitted in batches to other systems that support it including GridEngine/SLURM/PBS/etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984:424,Performance,Queue,Queue,424,"> Do we have a lot of users with UGER, external and internal?. I can only speak for internal: More and more ""Broad"" users are using Grid Engine, especially [UGER](https://intranet.broadinstitute.org/bits/service-catalog/compute-resources/high-performance-computing-uger), but not with cromwell, yet. DSP-methods have been big users of Grid Engine over the past few years, but mostly the older Sun Grid Engine, and with GATK-Queue. Meanwhile BITS is trying to get them to move over to a Univa Grid Engine installation BITS has called UGER. Unlike the older unlimited setup, UGER's setup has hard limits on the number of concurrent jobs that can be tracked by the Grid Engine scheduler, previously 100, now 1000. With the relaxing of the limit, plus the cromwell's `concurrent-job-limit` feature, this ticket a lower priority imho. Still, it ""would be nice"" if just like we submit in batches to JES, we also submitted in batches to other systems that support it including GridEngine/SLURM/PBS/etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984:619,Performance,concurren,concurrent,619,"> Do we have a lot of users with UGER, external and internal?. I can only speak for internal: More and more ""Broad"" users are using Grid Engine, especially [UGER](https://intranet.broadinstitute.org/bits/service-catalog/compute-resources/high-performance-computing-uger), but not with cromwell, yet. DSP-methods have been big users of Grid Engine over the past few years, but mostly the older Sun Grid Engine, and with GATK-Queue. Meanwhile BITS is trying to get them to move over to a Univa Grid Engine installation BITS has called UGER. Unlike the older unlimited setup, UGER's setup has hard limits on the number of concurrent jobs that can be tracked by the Grid Engine scheduler, previously 100, now 1000. With the relaxing of the limit, plus the cromwell's `concurrent-job-limit` feature, this ticket a lower priority imho. Still, it ""would be nice"" if just like we submit in batches to JES, we also submitted in batches to other systems that support it including GridEngine/SLURM/PBS/etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984:764,Performance,concurren,concurrent-job-limit,764,"> Do we have a lot of users with UGER, external and internal?. I can only speak for internal: More and more ""Broad"" users are using Grid Engine, especially [UGER](https://intranet.broadinstitute.org/bits/service-catalog/compute-resources/high-performance-computing-uger), but not with cromwell, yet. DSP-methods have been big users of Grid Engine over the past few years, but mostly the older Sun Grid Engine, and with GATK-Queue. Meanwhile BITS is trying to get them to move over to a Univa Grid Engine installation BITS has called UGER. Unlike the older unlimited setup, UGER's setup has hard limits on the number of concurrent jobs that can be tracked by the Grid Engine scheduler, previously 100, now 1000. With the relaxing of the limit, plus the cromwell's `concurrent-job-limit` feature, this ticket a lower priority imho. Still, it ""would be nice"" if just like we submit in batches to JES, we also submitted in batches to other systems that support it including GridEngine/SLURM/PBS/etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-371909917:92,Availability,avail,available,92,"No, not yet. A bunch of batching code was put into `JesPollingActor`, but that logic is not available in a generic way. If anyone with cycles does come across this ticket, that might be a good first place to start. The logic would need to be slightly different. ~JES~ PAPI allows batching _any_ jobs together, while a batch of GridEngine jobs have to use the same resource requirements, aka runtime attributes. Still partial-grouping-for-similar-jobs would still be an improvement over the current single-submit-per-job.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-371909917
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-371909917:79,Testability,log,logic,79,"No, not yet. A bunch of batching code was put into `JesPollingActor`, but that logic is not available in a generic way. If anyone with cycles does come across this ticket, that might be a good first place to start. The logic would need to be slightly different. ~JES~ PAPI allows batching _any_ jobs together, while a batch of GridEngine jobs have to use the same resource requirements, aka runtime attributes. Still partial-grouping-for-similar-jobs would still be an improvement over the current single-submit-per-job.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-371909917
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-371909917:219,Testability,log,logic,219,"No, not yet. A bunch of batching code was put into `JesPollingActor`, but that logic is not available in a generic way. If anyone with cycles does come across this ticket, that might be a good first place to start. The logic would need to be slightly different. ~JES~ PAPI allows batching _any_ jobs together, while a batch of GridEngine jobs have to use the same resource requirements, aka runtime attributes. Still partial-grouping-for-similar-jobs would still be an improvement over the current single-submit-per-job.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-371909917
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087:104,Deployability,pipeline,pipeline,104,"Hello, does anyone tried to implement a Job Array with LSF scheduler within Cromwell?. I am adapting my pipeline (that uses job array and the LSF scheduler) written in bash for Cromwell. To be sure that every task runs smoothly I have used only 1 sample and everything seems to work fine. Now I would like to create a job array with multiple samples, e.g. given a file with a list of unique ID (as I usually do in bash with LSF scheduler). Can anyone share his/her own experience?. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087:59,Energy Efficiency,schedul,scheduler,59,"Hello, does anyone tried to implement a Job Array with LSF scheduler within Cromwell?. I am adapting my pipeline (that uses job array and the LSF scheduler) written in bash for Cromwell. To be sure that every task runs smoothly I have used only 1 sample and everything seems to work fine. Now I would like to create a job array with multiple samples, e.g. given a file with a list of unique ID (as I usually do in bash with LSF scheduler). Can anyone share his/her own experience?. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087:92,Energy Efficiency,adapt,adapting,92,"Hello, does anyone tried to implement a Job Array with LSF scheduler within Cromwell?. I am adapting my pipeline (that uses job array and the LSF scheduler) written in bash for Cromwell. To be sure that every task runs smoothly I have used only 1 sample and everything seems to work fine. Now I would like to create a job array with multiple samples, e.g. given a file with a list of unique ID (as I usually do in bash with LSF scheduler). Can anyone share his/her own experience?. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087:146,Energy Efficiency,schedul,scheduler,146,"Hello, does anyone tried to implement a Job Array with LSF scheduler within Cromwell?. I am adapting my pipeline (that uses job array and the LSF scheduler) written in bash for Cromwell. To be sure that every task runs smoothly I have used only 1 sample and everything seems to work fine. Now I would like to create a job array with multiple samples, e.g. given a file with a list of unique ID (as I usually do in bash with LSF scheduler). Can anyone share his/her own experience?. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087:428,Energy Efficiency,schedul,scheduler,428,"Hello, does anyone tried to implement a Job Array with LSF scheduler within Cromwell?. I am adapting my pipeline (that uses job array and the LSF scheduler) written in bash for Cromwell. To be sure that every task runs smoothly I have used only 1 sample and everything seems to work fine. Now I would like to create a job array with multiple samples, e.g. given a file with a list of unique ID (as I usually do in bash with LSF scheduler). Can anyone share his/her own experience?. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087:92,Modifiability,adapt,adapting,92,"Hello, does anyone tried to implement a Job Array with LSF scheduler within Cromwell?. I am adapting my pipeline (that uses job array and the LSF scheduler) written in bash for Cromwell. To be sure that every task runs smoothly I have used only 1 sample and everything seems to work fine. Now I would like to create a job array with multiple samples, e.g. given a file with a list of unique ID (as I usually do in bash with LSF scheduler). Can anyone share his/her own experience?. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513919982:238,Testability,test,test,238,"@geoffjentry thanks for the quick reply! I am using now scatter and it works pretty well, meaning that multiple samples are running in parallel for the same task.. My question is: how many samples the scatter can run simultaneously? As a test am doing with 2 samples and they both run at the same time.. but am wondering if there is a limit (e.g. 100 or 200 etc..). This will help me to understand the time of the run of my entire workflow. Thanks in advance for any reply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513919982
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513922676:200,Modifiability,config,config,200,"@tAndreani in terms of cromwell there's not a limit per se, although your underlying backend might get grouchy at you if you wind up submitting too many jobs. You could set the `concurrent-job-limit` config field to help with this if you do wind up with issues",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513922676
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513922676:178,Performance,concurren,concurrent-job-limit,178,"@tAndreani in terms of cromwell there's not a limit per se, although your underlying backend might get grouchy at you if you wind up submitting too many jobs. You could set the `concurrent-job-limit` config field to help with this if you do wind up with issues",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513922676
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-516913367:406,Availability,error,errors,406,"A quick question: if I run in local my workflow and I parallelize with several sample using scatter, Cromwell, takes all the CPU that we have in our machine. I then used this option in every task in order to take only 6 CPU and let some memory also for my colleagues:. ```; runtime {; docker_user: ""ngs""; cpu: 6; }; ```. But this does not work, and all the CPU are taken at the same time. And I have these errors:. The first is this one: ; `[warn] Local [f8d35e0f]: Key/s [cpu] is/are not supported by backend. Unsupported attributes will not be part of job executions.`. The second this one: ; `[warn] BackgroundConfigAsyncJobExecutionActor [7e5755bcscMeth.mapping:0:1]: Unrecognized runtime attribute keys: cpu`. Is there any explanation for this? These errors do not cause any interruption though.; Thanks in advance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-516913367
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-516913367:756,Availability,error,errors,756,"A quick question: if I run in local my workflow and I parallelize with several sample using scatter, Cromwell, takes all the CPU that we have in our machine. I then used this option in every task in order to take only 6 CPU and let some memory also for my colleagues:. ```; runtime {; docker_user: ""ngs""; cpu: 6; }; ```. But this does not work, and all the CPU are taken at the same time. And I have these errors:. The first is this one: ; `[warn] Local [f8d35e0f]: Key/s [cpu] is/are not supported by backend. Unsupported attributes will not be part of job executions.`. The second this one: ; `[warn] BackgroundConfigAsyncJobExecutionActor [7e5755bcscMeth.mapping:0:1]: Unrecognized runtime attribute keys: cpu`. Is there any explanation for this? These errors do not cause any interruption though.; Thanks in advance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-516913367
https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-517051507:360,Performance,concurren,concurrent,360,"@tAndreani `cpu` isn't a supported `runtime` value in the local backend (really, very few of them are). With the local backend Cromwell won't limit the resource usage of any job. We view the local backend more as a developmental/experimentation tool than something to be used in a real world scenario, so there's less attention paid to that sort of thing. The concurrent job limit could be used here as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-517051507
https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-325469974:553,Testability,log,logs,553,"A server mode CLI would be able to interact with Cromwell's REST endpoints, which the current one can't do. It would probably not even be part of the Cromwell codebase per se and could be done in any language. We're maybe overloading the term CLI a little bit but what I'm thinking of is something like ; ```bash; $ cromwell-cli --url=localhost:8000 --run ""my_workflow.wdl""; submitted 9d658883-2abe-484f-83fe-06731a768057; $ cromwell-cli --url=localhost:8000 --status 9d658883-2abe-484f-83fe-06731a768057; Running; $ cromwell-cli --url=localhost:8000 --logs 9d658883-2abe-484f-83fe-06731a768057; stdout: ...; stderr: ...; backendLogs: ... etc..; ```. Could even be more interactive than that like . ```bash; $ cromwell-cli console; --- Welcome to Cromwell ! ---; > run ""my_workflow.wdl""; submitted 9d658883-2abe-484f-83fe-06731a768057; > status 9d658883-2abe-484f-83fe-06731a768057; Running; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-325469974
https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-325470216:104,Usability,simpl,simplify,104,"Agree with @Horneth in that my interpretation was that this ticket was looking for an outside script to simplify running things, not cromwellian behavior itself",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-325470216
https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-328192845:118,Integrability,interface,interface,118,@kcibul @geoffjentry I actually have a python cli for running Workflows (and a single task) against the cromwell rest interface which I can open source when I get a chance. The cli lets you run one or more workflow. Its missing some parameters but you guys can feel free to expand it. Ive run 1000's of workflows with this little tool,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-328192845
https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-328205809:242,Safety,Risk,Risk,242,"As a **developer using Cromwell**, I want **to run workflows from the CLI that connect directly to the Cromwell REST APIs**, so that I can **easily interact with the APIs (does that sound right? @geoffjentry @Horneth )**.; - Effort: **?**; - Risk: **?**; - Business value: **?**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-328205809
https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291572106:173,Availability,failure,failure,173,"I still see this, somewhat. It might be due to the long time on cromwell; final overhead. In other words, my job finishes, but the overhead takes so; long that an unrelated failure prevents the write to the call-cache; database. On Tue, Apr 4, 2017 at 12:48 PM, Kate Voss <notifications@github.com> wrote:. > If there are no more problems, we'll close this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291561557>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk0dV9BaGvlpzXleUmgZ3s6-BsN6Lks5rsnRngaJpZM4KJB9H>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291572106
https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291572106:212,Performance,cache,cache,212,"I still see this, somewhat. It might be due to the long time on cromwell; final overhead. In other words, my job finishes, but the overhead takes so; long that an unrelated failure prevents the write to the call-cache; database. On Tue, Apr 4, 2017 at 12:48 PM, Kate Voss <notifications@github.com> wrote:. > If there are no more problems, we'll close this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291561557>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk0dV9BaGvlpzXleUmgZ3s6-BsN6Lks5rsnRngaJpZM4KJB9H>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291572106
https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-325444341:101,Deployability,update,update,101,"@LeeTL1220 could you confirm that you are still unable to kill Cromwell with Ctl-C? If so, then I'll update this issue to add support for it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-325444341
https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426:101,Availability,down,down,101,"As a **workflow runner**, I want **use Command+C to abort running jobs**, so that I can **fully shut down Cromwell using a standard command shortcut**.; - Effort: **?** @geoffjentry ; - Risk: **?**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426
https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426:52,Safety,abort,abort,52,"As a **workflow runner**, I want **use Command+C to abort running jobs**, so that I can **fully shut down Cromwell using a standard command shortcut**.; - Effort: **?** @geoffjentry ; - Risk: **?**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426
https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426:186,Safety,Risk,Risk,186,"As a **workflow runner**, I want **use Command+C to abort running jobs**, so that I can **fully shut down Cromwell using a standard command shortcut**.; - Effort: **?** @geoffjentry ; - Risk: **?**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426
https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328217375:187,Availability,down,down,187,I do this all the time and never have a problem. It's possible that there's some pathological state things can be in which causes problem but we'd need a reproducible example to track it down and we don't have one. I vote to close this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328217375
https://github.com/broadinstitute/cromwell/issues/1497#issuecomment-250271058:37,Performance,latency,latency,37,Desired behavior is to decrease this latency.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1497#issuecomment-250271058
https://github.com/broadinstitute/cromwell/issues/1497#issuecomment-250459311:83,Deployability,release,release,83,"Sure, I forgot about the the other issue. And this was not in a branch, but in the release.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1497#issuecomment-250459311
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:354,Availability,robust,robust,354,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:591,Availability,alive,alive,591,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:496,Deployability,release,release,496,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:598,Deployability,configurat,configuration,598,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:428,Energy Efficiency,schedul,scheduler,428,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:725,Energy Efficiency,schedul,scheduler,725,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:598,Modifiability,config,configuration,598,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:324,Safety,detect,detect,324,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:345,Availability,failure,failures-based-on-check-alive,345,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:508,Availability,alive,alive,508,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:525,Availability,failure,failure,525,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:601,Availability,alive,alive,601,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:988,Availability,alive,alive,988,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:1052,Availability,alive,alive,1052,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:454,Modifiability,config,configure,454,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:236,Safety,detect,detects,236,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:333,Safety,detect,detect-task-failures-based-on-check-alive,333,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932:160,Availability,alive,alive,160,"As a **user running workflows on an HPC cluster**, I want **Cromwell to periodically check that my jobs are still running**, so that I can **know when jobs are alive versus when they reach the runtime limits and are killed by the backend**.; - Workaround: **Yes**; - from @delocalizer ; > The hacky non-async solution I have been using...was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to [poll the scheduler itself](https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166); - Effort: **Small**; - Risk: **Small**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932:452,Energy Efficiency,schedul,scheduler,452,"As a **user running workflows on an HPC cluster**, I want **Cromwell to periodically check that my jobs are still running**, so that I can **know when jobs are alive versus when they reach the runtime limits and are killed by the backend**.; - Workaround: **Yes**; - from @delocalizer ; > The hacky non-async solution I have been using...was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to [poll the scheduler itself](https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166); - Effort: **Small**; - Risk: **Small**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932:625,Safety,Risk,Risk,625,"As a **user running workflows on an HPC cluster**, I want **Cromwell to periodically check that my jobs are still running**, so that I can **know when jobs are alive versus when they reach the runtime limits and are killed by the backend**.; - Workaround: **Yes**; - from @delocalizer ; > The hacky non-async solution I have been using...was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to [poll the scheduler itself](https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166); - Effort: **Small**; - Risk: **Small**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:298,Availability,alive,alive,298,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:388,Availability,ping,ping,388,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:1263,Availability,ping,ping,1263,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:1334,Availability,alive,alive,1334,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:1392,Availability,down,down,1392,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:267,Energy Efficiency,schedul,scheduler,267,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:395,Energy Efficiency,schedul,scheduler,395,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:1416,Energy Efficiency,schedul,scheduler,1416,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:140,Testability,test,tests,140,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:940,Testability,test,test,940,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:1047,Testability,test,tests,1047,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:1128,Testability,test,tests,1128,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:1149,Testability,test,test,1149,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:369,Availability,alive,alive,369,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:426,Availability,alive,alive,426,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:466,Availability,alive,alive,466,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:527,Availability,alive,alive,527,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:101,Energy Efficiency,schedul,schedulers,101,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:135,Energy Efficiency,schedul,scheduler,135,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:608,Testability,test,test,608,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:13,Usability,simpl,simple,13,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:660,Availability,error,error,660,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:889,Availability,ping,pinged,889,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:1047,Availability,error,error,1047,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:1230,Availability,reliab,reliably,1230,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:1177,Deployability,pipeline,pipelines,1177,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:1212,Deployability,pipeline,pipelines,1212,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:933,Energy Efficiency,schedul,scheduler,933,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:809,Modifiability,config,configurable,809,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:591,Performance,queue,queue,591,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360128581:125,Availability,avail,available,125,"@delocalizer Any chance you still have your ""hacky non-async"" piece of code still? The original link you posted is no longer available, and it might be useful for @caross73. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360128581
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:787,Availability,alive,alive-or-dead,787,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:846,Availability,alive,alive,846,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:248,Deployability,configurat,configuration,248,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:717,Energy Efficiency,schedul,scheduler,717,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:769,Energy Efficiency,schedul,scheduler,769,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:817,Energy Efficiency,schedul,scheduler,817,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:248,Modifiability,config,configuration,248,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:853,Safety,abort,aborts,853,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:518,Usability,simpl,simple,518,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-371303358:159,Availability,down,down,159,"Just to comment that this feature would be really useful for AWS/Cfncluster based work, since cfncluster has a known issue where compute nodes could be scaled down even with jobs still running, leaving no rc file and no way for cromwell to know that the job is already dead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-371303358
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549:92,Availability,alive,alive,92,"A fix for this issue would be much appreciated! It is particularly frustrating as the check-alive config parameter sounds like exactly the test I want cromwell to run, but it is actually for different usecase.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549:98,Modifiability,config,config,98,"A fix for this issue would be much appreciated! It is particularly frustrating as the check-alive config parameter sounds like exactly the test I want cromwell to run, but it is actually for different usecase.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549:139,Testability,test,test,139,"A fix for this issue would be much appreciated! It is particularly frustrating as the check-alive config parameter sounds like exactly the test I want cromwell to run, but it is actually for different usecase.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380675072:131,Modifiability,config,config,131,I thought I would be able to get a similar effect by using ; 'run-in-background = true'; and (for SGE anyway) 'qsub -sync y' in my config. Can anyone explain why that did not work?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380675072
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715:64,Availability,alive,alive,64,"@EvanTheB I think I can answer that for you... much like `check-alive`, the `run-in-background` config point is only relevant for aborts and restarts; cromwell identifies what it needs to kill or restart based on the PID instead of the scheduler job id. The only way that cromwell knows whether a job is done or not is by checking for the existence of the `rc` file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715:236,Energy Efficiency,schedul,scheduler,236,"@EvanTheB I think I can answer that for you... much like `check-alive`, the `run-in-background` config point is only relevant for aborts and restarts; cromwell identifies what it needs to kill or restart based on the PID instead of the scheduler job id. The only way that cromwell knows whether a job is done or not is by checking for the existence of the `rc` file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715:96,Modifiability,config,config,96,"@EvanTheB I think I can answer that for you... much like `check-alive`, the `run-in-background` config point is only relevant for aborts and restarts; cromwell identifies what it needs to kill or restart based on the PID instead of the scheduler job id. The only way that cromwell knows whether a job is done or not is by checking for the existence of the `rc` file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715:130,Safety,abort,aborts,130,"@EvanTheB I think I can answer that for you... much like `check-alive`, the `run-in-background` config point is only relevant for aborts and restarts; cromwell identifies what it needs to kill or restart based on the PID instead of the scheduler job id. The only way that cromwell knows whether a job is done or not is by checking for the existence of the `rc` file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380991110:261,Performance,concurren,concurrent,261,"Yes, but as pointed out in the summary above https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929, `isAlive` is a much more expensive check than looking at the filesystem and calling it at the same frequency won't scale to thousands of concurrent jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380991110
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-406950091:44,Usability,simpl,simple,44,FYI Our current workaround is running [this simple script](https://gist.github.com/delocalizer/6b4d97158044e1331f3c4393c9e05586) once an hour as a cron job. The details may differ slightly for SGE/SLURM/etc backends.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-406950091
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161:144,Availability,alive,alive,144,I just ran into the problem of an SGE node crash that we didn't find out until we wondered why certain jobs took so long to run. Having the `is-alive` command and check run at a configurable poll interval would be really usefull for me. Having a poll interval of 0 by default (i.e. turned off) and the value in hours/days could leave it configurable without overloading the queque masters. But as Uncle Ben said: With great power...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161:424,Energy Efficiency,power,power,424,I just ran into the problem of an SGE node crash that we didn't find out until we wondered why certain jobs took so long to run. Having the `is-alive` command and check run at a configurable poll interval would be really usefull for me. Having a poll interval of 0 by default (i.e. turned off) and the value in hours/days could leave it configurable without overloading the queque masters. But as Uncle Ben said: With great power...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161:178,Modifiability,config,configurable,178,I just ran into the problem of an SGE node crash that we didn't find out until we wondered why certain jobs took so long to run. Having the `is-alive` command and check run at a configurable poll interval would be really usefull for me. Having a poll interval of 0 by default (i.e. turned off) and the value in hours/days could leave it configurable without overloading the queque masters. But as Uncle Ben said: With great power...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161:337,Modifiability,config,configurable,337,I just ran into the problem of an SGE node crash that we didn't find out until we wondered why certain jobs took so long to run. Having the `is-alive` command and check run at a configurable poll interval would be really usefull for me. Having a poll interval of 0 by default (i.e. turned off) and the value in hours/days could leave it configurable without overloading the queque masters. But as Uncle Ben said: With great power...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-475176014:7,Energy Efficiency,adapt,adapted,7,"I have adapted @delocalizer's script for an LSF backend: https://github.com/wtsi-hgi/olly-maersk/blob/master/zombie-killer.sh It's currently untested, but it shouldn't be far off from what's required.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-475176014
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-475176014:7,Modifiability,adapt,adapted,7,"I have adapted @delocalizer's script for an LSF backend: https://github.com/wtsi-hgi/olly-maersk/blob/master/zombie-killer.sh It's currently untested, but it shouldn't be far off from what's required.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-475176014
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:34,Availability,alive,alive,34,"What actually happens when `check-alive` is called?. Cromwell was accidentally terminated, while Cromwell was terminated the job finished (and an RC file with status 0 was created). When I restart Cromwell, it checks all the jobs successfully and then for the task that was running, Cromwell does the following:. 1. `Restarting alignsortedbam.samtools`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:586,Availability,alive,alive,586,"What actually happens when `check-alive` is called?. Cromwell was accidentally terminated, while Cromwell was terminated the job finished (and an RC file with status 0 was created). When I restart Cromwell, it checks all the jobs successfully and then for the task that was running, Cromwell does the following:. 1. `Restarting alignsortedbam.samtools`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:1141,Availability,alive,alive,1141,"ll was accidentally terminated, while Cromwell was terminated the job finished (and an RC file with status 0 was created). When I restart Cromwell, it checks all the jobs successfully and then for the task that was running, Cromwell does the following:. 1. `Restarting alignsortedbam.samtools`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it per",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:1266,Availability,alive,alive,1266," the task that was running, Cromwell does the following:. 1. `Restarting alignsortedbam.samtools`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it performed exactly the same process~~ (although still relevant, see my edit below - needed 30 minutes). ---. **Edit**: _I left the Cromwell server running for 30 minutes and it just randomly started th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:1511,Availability,error,error,1511,"`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it performed exactly the same process~~ (although still relevant, see my edit below - needed 30 minutes). ---. **Edit**: _I left the Cromwell server running for 30 minutes and it just randomly started the next job. I don't know if this is related or not, but obviously needed to update my comment._",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:2332,Deployability,update,update,2332,"`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it performed exactly the same process~~ (although still relevant, see my edit below - needed 30 minutes). ---. **Edit**: _I left the Cromwell server running for 30 minutes and it just randomly started the next job. I don't know if this is related or not, but obviously needed to update my comment._",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:1110,Energy Efficiency,schedul,scheduler,1110,"ll was accidentally terminated, while Cromwell was terminated the job finished (and an RC file with status 0 was created). When I restart Cromwell, it checks all the jobs successfully and then for the task that was running, Cromwell does the following:. 1. `Restarting alignsortedbam.samtools`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it per",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:1393,Modifiability,config,config,1393,"`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it performed exactly the same process~~ (although still relevant, see my edit below - needed 30 minutes). ---. **Edit**: _I left the Cromwell server running for 30 minutes and it just randomly started the next job. I don't know if this is related or not, but obviously needed to update my comment._",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:2056,Performance,perform,performed,2056,"`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it performed exactly the same process~~ (although still relevant, see my edit below - needed 30 minutes). ---. **Edit**: _I left the Cromwell server running for 30 minutes and it just randomly started the next job. I don't know if this is related or not, but obviously needed to update my comment._",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:985,Testability,test,tests,985,"ll was accidentally terminated, while Cromwell was terminated the job finished (and an RC file with status 0 was created). When I restart Cromwell, it checks all the jobs successfully and then for the task that was running, Cromwell does the following:. 1. `Restarting alignsortedbam.samtools`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it per",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736
https://github.com/broadinstitute/cromwell/pull/1500#issuecomment-250574493:123,Testability,log,logic,123,@geoffjentry We could certainly move the token dispenser into a per-BE model (and then only have one tokenPool to make the logic simpler. Hooray!). Also FWIW I don't believe the EJEA is a hairball. It's just getting large enough to merit decomposing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1500#issuecomment-250574493
https://github.com/broadinstitute/cromwell/pull/1500#issuecomment-250574493:129,Usability,simpl,simpler,129,@geoffjentry We could certainly move the token dispenser into a per-BE model (and then only have one tokenPool to make the logic simpler. Hooray!). Also FWIW I don't believe the EJEA is a hairball. It's just getting large enough to merit decomposing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1500#issuecomment-250574493
https://github.com/broadinstitute/cromwell/issues/1501#issuecomment-250503406:107,Deployability,pipeline,pipeline,107,"I think you're right, in order to allow for private IPs, the noAddress field needs to be added both in the pipeline and the run resources.; It's possible that the `pipelineArgs`resources don't contain what they're supposed to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501#issuecomment-250503406
https://github.com/broadinstitute/cromwell/issues/1501#issuecomment-250503406:164,Deployability,pipeline,pipelineArgs,164,"I think you're right, in order to allow for private IPs, the noAddress field needs to be added both in the pipeline and the run resources.; It's possible that the `pipelineArgs`resources don't contain what they're supposed to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501#issuecomment-250503406
https://github.com/broadinstitute/cromwell/issues/1504#issuecomment-287462701:24,Safety,Abort,Aborts,24,This is included in the Aborts design doc: https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit . Closing,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504#issuecomment-287462701
https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250539095:196,Testability,test,tests,196,"@kcibul This might deserve a 0.21_hotfix ? Currently the run time resources are empty (except for the noAddress flag), so they override some (apparently not all) of the attributes. I did a couple tests and it overrides preemptibility but not zone for example...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250539095
https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250545327:63,Deployability,install,install,63,"Hey @Horneth -- would you mind replacing the command line 7 of install.sh to say ""sbt assembly""? Just another sneaky fix for Jose!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250545327
https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250560524:34,Testability,test,tests,34,@ruchim so I assume they want the tests to run when running assembly ? Also is this file only used by gotc ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250560524
https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-251378603:45,Testability,test,tests,45,"@Horneth I _think_ they dont' care about the tests, and I think we took the tests out of sbt assembly? ...Hmm I think the script was built for GOTC but I'll check with the GAWB team.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-251378603
https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-251378603:76,Testability,test,tests,76,"@Horneth I _think_ they dont' care about the tests, and I think we took the tests out of sbt assembly? ...Hmm I think the script was built for GOTC but I'll check with the GAWB team.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-251378603
https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-251389554:109,Testability,test,tests,109,"Oh right I just realize I forgot the ""don't"" in my previous commentm I meant ""I assume they _don't_ want the tests to run"" -_- sounds good I just wanted to make sure no one is going to be surprised if we change this script.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-251389554
https://github.com/broadinstitute/cromwell/issues/1510#issuecomment-251685046:79,Performance,cache,cache,79,"This is particularly useful since we might have lots of entries for the ""same"" cache result, so we might be able to find another one that works next time we hit the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1510#issuecomment-251685046
https://github.com/broadinstitute/cromwell/issues/1510#issuecomment-251685046:165,Performance,cache,cache,165,"This is particularly useful since we might have lots of entries for the ""same"" cache result, so we might be able to find another one that works next time we hit the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1510#issuecomment-251685046
https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342:123,Integrability,rout,route,123,"**TL;DR Discussed in person with @ruchim. Going to 👍 , and perhaps dev choice a PR to change the syntax, using a secondary route that looks for `workflowInputs[]`.**. This current PR is very swagger spec friendly, using a fixed 2-based list of additional inputs:; - `workflowInputs`; - `workflowInputs_2`; - `workflowInputs_3`; - `workflowInputs_4`; - `workflowInputs_5`. Ideally we could use a PHP compatible syntax on a secondary spray route:; - `workflowInputs[]`; - `workflowInputs[]`; - `workflowInputs[]`; - etc. This array, using a [custom](https://groups.google.com/d/msg/spray-user/5kSZ87OnfkE/I_A_OcaIticJ) spray marshaller could be programmatically converted into an variable length `workflowInputs: Seq[String]`. Passing the sequence into the business logic would also allow storing the separated inputs in the metadata. For now the five inputs are merged into a single value in the web service and stored in the database as a merged clob. At this second I do not know if swagger would allow multiple form data elements with the same name. I'm assuming curl, HTTPie, and jvm clients such as spray-client would, as PHP supports the above syntax. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1511/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342
https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342:438,Integrability,rout,route,438,"**TL;DR Discussed in person with @ruchim. Going to 👍 , and perhaps dev choice a PR to change the syntax, using a secondary route that looks for `workflowInputs[]`.**. This current PR is very swagger spec friendly, using a fixed 2-based list of additional inputs:; - `workflowInputs`; - `workflowInputs_2`; - `workflowInputs_3`; - `workflowInputs_4`; - `workflowInputs_5`. Ideally we could use a PHP compatible syntax on a secondary spray route:; - `workflowInputs[]`; - `workflowInputs[]`; - `workflowInputs[]`; - etc. This array, using a [custom](https://groups.google.com/d/msg/spray-user/5kSZ87OnfkE/I_A_OcaIticJ) spray marshaller could be programmatically converted into an variable length `workflowInputs: Seq[String]`. Passing the sequence into the business logic would also allow storing the separated inputs in the metadata. For now the five inputs are merged into a single value in the web service and stored in the database as a merged clob. At this second I do not know if swagger would allow multiple form data elements with the same name. I'm assuming curl, HTTPie, and jvm clients such as spray-client would, as PHP supports the above syntax. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1511/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342
https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342:678,Modifiability,variab,variable,678,"**TL;DR Discussed in person with @ruchim. Going to 👍 , and perhaps dev choice a PR to change the syntax, using a secondary route that looks for `workflowInputs[]`.**. This current PR is very swagger spec friendly, using a fixed 2-based list of additional inputs:; - `workflowInputs`; - `workflowInputs_2`; - `workflowInputs_3`; - `workflowInputs_4`; - `workflowInputs_5`. Ideally we could use a PHP compatible syntax on a secondary spray route:; - `workflowInputs[]`; - `workflowInputs[]`; - `workflowInputs[]`; - etc. This array, using a [custom](https://groups.google.com/d/msg/spray-user/5kSZ87OnfkE/I_A_OcaIticJ) spray marshaller could be programmatically converted into an variable length `workflowInputs: Seq[String]`. Passing the sequence into the business logic would also allow storing the separated inputs in the metadata. For now the five inputs are merged into a single value in the web service and stored in the database as a merged clob. At this second I do not know if swagger would allow multiple form data elements with the same name. I'm assuming curl, HTTPie, and jvm clients such as spray-client would, as PHP supports the above syntax. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1511/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342
https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342:764,Testability,log,logic,764,"**TL;DR Discussed in person with @ruchim. Going to 👍 , and perhaps dev choice a PR to change the syntax, using a secondary route that looks for `workflowInputs[]`.**. This current PR is very swagger spec friendly, using a fixed 2-based list of additional inputs:; - `workflowInputs`; - `workflowInputs_2`; - `workflowInputs_3`; - `workflowInputs_4`; - `workflowInputs_5`. Ideally we could use a PHP compatible syntax on a secondary spray route:; - `workflowInputs[]`; - `workflowInputs[]`; - `workflowInputs[]`; - etc. This array, using a [custom](https://groups.google.com/d/msg/spray-user/5kSZ87OnfkE/I_A_OcaIticJ) spray marshaller could be programmatically converted into an variable length `workflowInputs: Seq[String]`. Passing the sequence into the business logic would also allow storing the separated inputs in the metadata. For now the five inputs are merged into a single value in the web service and stored in the database as a merged clob. At this second I do not know if swagger would allow multiple form data elements with the same name. I'm assuming curl, HTTPie, and jvm clients such as spray-client would, as PHP supports the above syntax. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1511/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342
https://github.com/broadinstitute/cromwell/issues/1512#issuecomment-251564865:112,Modifiability,config,config,112,"This looks like a bug to me too - happening because in SGE backend the stdout used by cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs for job submission (where the SGE job id is written) is the same as the stdout specified to the SGE job itself in qsub. The other sfs implementation cromwell.backend.sfs.BackgroundAsyncJobExecutionActor specifies a different stdout and stderr in makeProcessRunner, suffixed with ""background"", so it doesn't have the same problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1512#issuecomment-251564865
https://github.com/broadinstitute/cromwell/issues/1512#issuecomment-251564865:119,Modifiability,Config,ConfigAsyncJobExecutionActor,119,"This looks like a bug to me too - happening because in SGE backend the stdout used by cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs for job submission (where the SGE job id is written) is the same as the stdout specified to the SGE job itself in qsub. The other sfs implementation cromwell.backend.sfs.BackgroundAsyncJobExecutionActor specifies a different stdout and stderr in makeProcessRunner, suffixed with ""background"", so it doesn't have the same problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1512#issuecomment-251564865
https://github.com/broadinstitute/cromwell/issues/1513#issuecomment-263652829:113,Deployability,update,update,113,Should be fixed once https://github.com/broadinstitute/wdl4s/pull/52 is merged and Cromwell brings in that WDL4S update,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1513#issuecomment-263652829
https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962:145,Availability,down,downl,145,@delocalizer Yeah we changed this a while back to help w/ submission performance under load. Those synchronous checks were really bogging things downl.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962
https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962:69,Performance,perform,performance,69,@delocalizer Yeah we changed this a while back to help w/ submission performance under load. Those synchronous checks were really bogging things downl.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962
https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962:87,Performance,load,load,87,@delocalizer Yeah we changed this a while back to help w/ submission performance under load. Those synchronous checks were really bogging things downl.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-253939899:155,Testability,test,test,155,"I messed with this for a while but unfortunately my sbt fu is too weak. 😢 . The fastest way I found to reproduce the problem was . ```; sbt ""project core"" test; ```. All the possible modifiers in `sbt foo:test` appear to reproduce the problem with the exception of `alltests`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-253939899
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-253939899:205,Testability,test,test,205,"I messed with this for a while but unfortunately my sbt fu is too weak. 😢 . The fastest way I found to reproduce the problem was . ```; sbt ""project core"" test; ```. All the possible modifiers in `sbt foo:test` appear to reproduce the problem with the exception of `alltests`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-253939899
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:39,Integrability,depend,dependencies,39,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:91,Testability,test,test,91,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:127,Testability,test,test,127,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:168,Testability,test,test,168,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:201,Testability,test,test,201,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:221,Testability,test,test,221,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:241,Testability,test,test,241,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:269,Testability,test,test,269,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:288,Testability,test,test,288,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:293,Testability,test,test,293,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:318,Testability,test,test,318,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:347,Testability,test,test,347,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:379,Testability,test,test,379,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344
https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566854:197,Deployability,update,updated,197,"TL;DR 👍 post rebase and minor comments. ToL 1: I dislike that one can't `Paths.get(Paths.get(""gs://bucket/path"").toString)`, and one must insert a `.toUri` for this to work. Hopefully this will be updated over in GCS NIO in the future and we can convert all the `.toUri.toString` back to `.toString`. ToL 2: Already discussed with Thibault. I concur that I don't like the proxy classes for long term. I'm picturing a custom cromwell wrapper that externally retries `Path.copy(): Unit`. Right now the proxy `.copy()` themselves are retrying internally while blocking the `Unit`. This custom wrapper should have a different behavior. Possibly a method returning a scala `Future`, even better Akka actors using messages and supervisors!. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1519/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566854
https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566854:433,Integrability,wrap,wrapper,433,"TL;DR 👍 post rebase and minor comments. ToL 1: I dislike that one can't `Paths.get(Paths.get(""gs://bucket/path"").toString)`, and one must insert a `.toUri` for this to work. Hopefully this will be updated over in GCS NIO in the future and we can convert all the `.toUri.toString` back to `.toString`. ToL 2: Already discussed with Thibault. I concur that I don't like the proxy classes for long term. I'm picturing a custom cromwell wrapper that externally retries `Path.copy(): Unit`. Right now the proxy `.copy()` themselves are retrying internally while blocking the `Unit`. This custom wrapper should have a different behavior. Possibly a method returning a scala `Future`, even better Akka actors using messages and supervisors!. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1519/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566854
https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566854:590,Integrability,wrap,wrapper,590,"TL;DR 👍 post rebase and minor comments. ToL 1: I dislike that one can't `Paths.get(Paths.get(""gs://bucket/path"").toString)`, and one must insert a `.toUri` for this to work. Hopefully this will be updated over in GCS NIO in the future and we can convert all the `.toUri.toString` back to `.toString`. ToL 2: Already discussed with Thibault. I concur that I don't like the proxy classes for long term. I'm picturing a custom cromwell wrapper that externally retries `Path.copy(): Unit`. Right now the proxy `.copy()` themselves are retrying internally while blocking the `Unit`. This custom wrapper should have a different behavior. Possibly a method returning a scala `Future`, even better Akka actors using messages and supervisors!. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1519/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566854
https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566854:708,Integrability,message,messages,708,"TL;DR 👍 post rebase and minor comments. ToL 1: I dislike that one can't `Paths.get(Paths.get(""gs://bucket/path"").toString)`, and one must insert a `.toUri` for this to work. Hopefully this will be updated over in GCS NIO in the future and we can convert all the `.toUri.toString` back to `.toString`. ToL 2: Already discussed with Thibault. I concur that I don't like the proxy classes for long term. I'm picturing a custom cromwell wrapper that externally retries `Path.copy(): Unit`. Right now the proxy `.copy()` themselves are retrying internally while blocking the `Unit`. This custom wrapper should have a different behavior. Possibly a method returning a scala `Future`, even better Akka actors using messages and supervisors!. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1519/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566854
https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566974:60,Testability,log,logic,60,Or even a stream ;) at least I believe you can handle retry logic in streams,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566974
https://github.com/broadinstitute/cromwell/pull/1520#issuecomment-252114005:79,Modifiability,refactor,refactoring,79,"For now 👍 assuming you switch the file extension. ToL: This code may need some refactoring love (some functionality could be in the base SFS backend, some should migrate to the Config backend). I'm fine leaving it until somehow we need to touch it. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1520/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1520#issuecomment-252114005
https://github.com/broadinstitute/cromwell/pull/1520#issuecomment-252114005:177,Modifiability,Config,Config,177,"For now 👍 assuming you switch the file extension. ToL: This code may need some refactoring love (some functionality could be in the base SFS backend, some should migrate to the Config backend). I'm fine leaving it until somehow we need to touch it. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1520/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1520#issuecomment-252114005
https://github.com/broadinstitute/cromwell/issues/1521#issuecomment-325027517:62,Safety,Abort,Aborts,62,@cjllanwarne what's the WEA? Sounds like another one for the [Aborts google doc](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1521#issuecomment-325027517
https://github.com/broadinstitute/cromwell/issues/1534#issuecomment-259547764:59,Testability,mock,mock-jes,59,Step 1 - a generic 20k wide scatter w/o call caching using mock-jes. Scored 💯 / 💯,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1534#issuecomment-259547764
https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531:1244,Availability,avail,available,1244,"**Status update**. tl;dr If one had to try to run the single sample pipeline on AWS, use a single i3.16xlarge spot instance, SUSE ECS optimized Linux, user data something like [this](https://github.com/broadinstitute/aws-backend-private/blob/master/scripts/suse-monster.sh), and otherwise follow the [lengthy script](https://docs.google.com/document/d/1qwY0QBo04WIAvsACbvtIshfxbs1OJRvNp93oclxCRk8/edit). But there’s still very little chance (like < 2%) that the workflow will succeed with full size BAMs. **SEGVs**. Amazon Linux ECS optimized (currently 64-bit amzn-ami-2016.09.g-amazon-ecs-optimized - ami-275ffe31) produces SEGVs with a consistency that makes it rare for workflows to survive to the MarkDuplicates step. No known workaround. Action: Don’t use Amazon Linux. **EFS**. EFS is currently several times slower for most tasks and orders of magnitude slower for MarkDuplicates. No known workaround. Action: Don’t use EFS, use a single-machine cluster. **NVMe drive disconnection**. SUSE Linux has a bug where the NVMe drives on i3 instance types (my preferred instance type with lots of fast instance storage) are randomly disconnected at boot time. Action: Live with it, harden the user data script to roll with whatever drives are available. Post in the forums. Nick reported this is a known problem and will be fixed upstream in SUSE eventually, but Amazon doesn’t control this AMI. **Random Docker CannotStartContainerErrors**. SUSE Linux consistently gets further in the workflow than Amazon Linux, but also consistently exhibits CannotStartContainerErrors deeper into the workflow. Action: Post in the forums. Although SUSE is more stable than Amazon Linux, there’s still not enough stability to run the workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531
https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531:9,Deployability,update,update,9,"**Status update**. tl;dr If one had to try to run the single sample pipeline on AWS, use a single i3.16xlarge spot instance, SUSE ECS optimized Linux, user data something like [this](https://github.com/broadinstitute/aws-backend-private/blob/master/scripts/suse-monster.sh), and otherwise follow the [lengthy script](https://docs.google.com/document/d/1qwY0QBo04WIAvsACbvtIshfxbs1OJRvNp93oclxCRk8/edit). But there’s still very little chance (like < 2%) that the workflow will succeed with full size BAMs. **SEGVs**. Amazon Linux ECS optimized (currently 64-bit amzn-ami-2016.09.g-amazon-ecs-optimized - ami-275ffe31) produces SEGVs with a consistency that makes it rare for workflows to survive to the MarkDuplicates step. No known workaround. Action: Don’t use Amazon Linux. **EFS**. EFS is currently several times slower for most tasks and orders of magnitude slower for MarkDuplicates. No known workaround. Action: Don’t use EFS, use a single-machine cluster. **NVMe drive disconnection**. SUSE Linux has a bug where the NVMe drives on i3 instance types (my preferred instance type with lots of fast instance storage) are randomly disconnected at boot time. Action: Live with it, harden the user data script to roll with whatever drives are available. Post in the forums. Nick reported this is a known problem and will be fixed upstream in SUSE eventually, but Amazon doesn’t control this AMI. **Random Docker CannotStartContainerErrors**. SUSE Linux consistently gets further in the workflow than Amazon Linux, but also consistently exhibits CannotStartContainerErrors deeper into the workflow. Action: Post in the forums. Although SUSE is more stable than Amazon Linux, there’s still not enough stability to run the workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531
https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531:68,Deployability,pipeline,pipeline,68,"**Status update**. tl;dr If one had to try to run the single sample pipeline on AWS, use a single i3.16xlarge spot instance, SUSE ECS optimized Linux, user data something like [this](https://github.com/broadinstitute/aws-backend-private/blob/master/scripts/suse-monster.sh), and otherwise follow the [lengthy script](https://docs.google.com/document/d/1qwY0QBo04WIAvsACbvtIshfxbs1OJRvNp93oclxCRk8/edit). But there’s still very little chance (like < 2%) that the workflow will succeed with full size BAMs. **SEGVs**. Amazon Linux ECS optimized (currently 64-bit amzn-ami-2016.09.g-amazon-ecs-optimized - ami-275ffe31) produces SEGVs with a consistency that makes it rare for workflows to survive to the MarkDuplicates step. No known workaround. Action: Don’t use Amazon Linux. **EFS**. EFS is currently several times slower for most tasks and orders of magnitude slower for MarkDuplicates. No known workaround. Action: Don’t use EFS, use a single-machine cluster. **NVMe drive disconnection**. SUSE Linux has a bug where the NVMe drives on i3 instance types (my preferred instance type with lots of fast instance storage) are randomly disconnected at boot time. Action: Live with it, harden the user data script to roll with whatever drives are available. Post in the forums. Nick reported this is a known problem and will be fixed upstream in SUSE eventually, but Amazon doesn’t control this AMI. **Random Docker CannotStartContainerErrors**. SUSE Linux consistently gets further in the workflow than Amazon Linux, but also consistently exhibits CannotStartContainerErrors deeper into the workflow. Action: Post in the forums. Although SUSE is more stable than Amazon Linux, there’s still not enough stability to run the workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531
https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531:134,Performance,optimiz,optimized,134,"**Status update**. tl;dr If one had to try to run the single sample pipeline on AWS, use a single i3.16xlarge spot instance, SUSE ECS optimized Linux, user data something like [this](https://github.com/broadinstitute/aws-backend-private/blob/master/scripts/suse-monster.sh), and otherwise follow the [lengthy script](https://docs.google.com/document/d/1qwY0QBo04WIAvsACbvtIshfxbs1OJRvNp93oclxCRk8/edit). But there’s still very little chance (like < 2%) that the workflow will succeed with full size BAMs. **SEGVs**. Amazon Linux ECS optimized (currently 64-bit amzn-ami-2016.09.g-amazon-ecs-optimized - ami-275ffe31) produces SEGVs with a consistency that makes it rare for workflows to survive to the MarkDuplicates step. No known workaround. Action: Don’t use Amazon Linux. **EFS**. EFS is currently several times slower for most tasks and orders of magnitude slower for MarkDuplicates. No known workaround. Action: Don’t use EFS, use a single-machine cluster. **NVMe drive disconnection**. SUSE Linux has a bug where the NVMe drives on i3 instance types (my preferred instance type with lots of fast instance storage) are randomly disconnected at boot time. Action: Live with it, harden the user data script to roll with whatever drives are available. Post in the forums. Nick reported this is a known problem and will be fixed upstream in SUSE eventually, but Amazon doesn’t control this AMI. **Random Docker CannotStartContainerErrors**. SUSE Linux consistently gets further in the workflow than Amazon Linux, but also consistently exhibits CannotStartContainerErrors deeper into the workflow. Action: Post in the forums. Although SUSE is more stable than Amazon Linux, there’s still not enough stability to run the workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531
https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531:533,Performance,optimiz,optimized,533,"**Status update**. tl;dr If one had to try to run the single sample pipeline on AWS, use a single i3.16xlarge spot instance, SUSE ECS optimized Linux, user data something like [this](https://github.com/broadinstitute/aws-backend-private/blob/master/scripts/suse-monster.sh), and otherwise follow the [lengthy script](https://docs.google.com/document/d/1qwY0QBo04WIAvsACbvtIshfxbs1OJRvNp93oclxCRk8/edit). But there’s still very little chance (like < 2%) that the workflow will succeed with full size BAMs. **SEGVs**. Amazon Linux ECS optimized (currently 64-bit amzn-ami-2016.09.g-amazon-ecs-optimized - ami-275ffe31) produces SEGVs with a consistency that makes it rare for workflows to survive to the MarkDuplicates step. No known workaround. Action: Don’t use Amazon Linux. **EFS**. EFS is currently several times slower for most tasks and orders of magnitude slower for MarkDuplicates. No known workaround. Action: Don’t use EFS, use a single-machine cluster. **NVMe drive disconnection**. SUSE Linux has a bug where the NVMe drives on i3 instance types (my preferred instance type with lots of fast instance storage) are randomly disconnected at boot time. Action: Live with it, harden the user data script to roll with whatever drives are available. Post in the forums. Nick reported this is a known problem and will be fixed upstream in SUSE eventually, but Amazon doesn’t control this AMI. **Random Docker CannotStartContainerErrors**. SUSE Linux consistently gets further in the workflow than Amazon Linux, but also consistently exhibits CannotStartContainerErrors deeper into the workflow. Action: Post in the forums. Although SUSE is more stable than Amazon Linux, there’s still not enough stability to run the workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531
https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531:591,Performance,optimiz,optimized,591,"**Status update**. tl;dr If one had to try to run the single sample pipeline on AWS, use a single i3.16xlarge spot instance, SUSE ECS optimized Linux, user data something like [this](https://github.com/broadinstitute/aws-backend-private/blob/master/scripts/suse-monster.sh), and otherwise follow the [lengthy script](https://docs.google.com/document/d/1qwY0QBo04WIAvsACbvtIshfxbs1OJRvNp93oclxCRk8/edit). But there’s still very little chance (like < 2%) that the workflow will succeed with full size BAMs. **SEGVs**. Amazon Linux ECS optimized (currently 64-bit amzn-ami-2016.09.g-amazon-ecs-optimized - ami-275ffe31) produces SEGVs with a consistency that makes it rare for workflows to survive to the MarkDuplicates step. No known workaround. Action: Don’t use Amazon Linux. **EFS**. EFS is currently several times slower for most tasks and orders of magnitude slower for MarkDuplicates. No known workaround. Action: Don’t use EFS, use a single-machine cluster. **NVMe drive disconnection**. SUSE Linux has a bug where the NVMe drives on i3 instance types (my preferred instance type with lots of fast instance storage) are randomly disconnected at boot time. Action: Live with it, harden the user data script to roll with whatever drives are available. Post in the forums. Nick reported this is a known problem and will be fixed upstream in SUSE eventually, but Amazon doesn’t control this AMI. **Random Docker CannotStartContainerErrors**. SUSE Linux consistently gets further in the workflow than Amazon Linux, but also consistently exhibits CannotStartContainerErrors deeper into the workflow. Action: Post in the forums. Although SUSE is more stable than Amazon Linux, there’s still not enough stability to run the workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531
https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-313258027:53,Security,access,accessing,53,"EFS shines only if it's multi-TB, otherwise directly accessing S3 is much faster (and there's no need to first copy the data from S3 to a filesystem when it can be streamed).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-313258027
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252407502:23,Modifiability,config,configs,23,Curious why ficus over configs (or something else),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252407502
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252418216:58,Modifiability,config,config,58,I chose ficus since its `as` API returns `A`s rather than config's `configs.Result[A]`s which would have been a lot more disruptive. I didn't look at other alternatives.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252418216
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252418216:68,Modifiability,config,configs,68,I chose ficus since its `as` API returns `A`s rather than config's `configs.Result[A]`s which would have been a lot more disruptive. I didn't look at other alternatives.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252418216
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:36,Deployability,rolling,rolling,36,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:90,Deployability,rolling,rolling,90,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:302,Integrability,depend,depending,302,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:193,Modifiability,config,configs,193,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:504,Usability,clear,clear,504,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037
https://github.com/broadinstitute/cromwell/issues/1555#issuecomment-253861510:102,Deployability,release,released,102,"Hey there, thanks for the bug report! I believe this is actually fixed in our later versions (we just released v22) - could you confirm whether upgrading resolves this issue?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1555#issuecomment-253861510
https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253000963:222,Testability,test,test,222,"Hi Alex, thanks for this report and the investigation you've already done! Out of interest, would you be able or willing to submit the change which made this work for you as a concrete PR? That way we could try it out and test it (and try to compare and investigate why the original fails in your specific situation)?. Thanks so much!; Chris",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253000963
https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034:16,Modifiability,config,config,16,"I used a custom config file:. ```; akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; logging-filter = ""akka.event.slf4j.Slf4jLoggingFilter""; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. backend {; providers {; Local {; config {; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""; }; }; }; }; ```. I'm happy to submit a PR to `core/src/main/resources/reference.conf` if that's helpful, but it would basically just replace the `submit-docker` line with the one above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034
https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034:293,Modifiability,config,config,293,"I used a custom config file:. ```; akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; logging-filter = ""akka.event.slf4j.Slf4jLoggingFilter""; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. backend {; providers {; Local {; config {; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""; }; }; }; }; ```. I'm happy to submit a PR to `core/src/main/resources/reference.conf` if that's helpful, but it would basically just replace the `submit-docker` line with the one above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034
https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034:177,Safety,timeout,timeout,177,"I used a custom config file:. ```; akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; logging-filter = ""akka.event.slf4j.Slf4jLoggingFilter""; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. backend {; providers {; Local {; config {; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""; }; }; }; }; ```. I'm happy to submit a PR to `core/src/main/resources/reference.conf` if that's helpful, but it would basically just replace the `submit-docker` line with the one above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034
https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034:213,Safety,timeout,timeout,213,"I used a custom config file:. ```; akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; logging-filter = ""akka.event.slf4j.Slf4jLoggingFilter""; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. backend {; providers {; Local {; config {; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""; }; }; }; }; ```. I'm happy to submit a PR to `core/src/main/resources/reference.conf` if that's helpful, but it would basically just replace the `submit-docker` line with the one above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034
https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034:239,Safety,timeout,timeout,239,"I used a custom config file:. ```; akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; logging-filter = ""akka.event.slf4j.Slf4jLoggingFilter""; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. backend {; providers {; Local {; config {; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""; }; }; }; }; ```. I'm happy to submit a PR to `core/src/main/resources/reference.conf` if that's helpful, but it would basically just replace the `submit-docker` line with the one above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034
https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034:43,Testability,log,loggers,43,"I used a custom config file:. ```; akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; logging-filter = ""akka.event.slf4j.Slf4jLoggingFilter""; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. backend {; providers {; Local {; config {; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""; }; }; }; }; ```. I'm happy to submit a PR to `core/src/main/resources/reference.conf` if that's helpful, but it would basically just replace the `submit-docker` line with the one above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034
https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034:87,Testability,log,logging-filter,87,"I used a custom config file:. ```; akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; logging-filter = ""akka.event.slf4j.Slf4jLoggingFilter""; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. backend {; providers {; Local {; config {; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""; }; }; }; }; ```. I'm happy to submit a PR to `core/src/main/resources/reference.conf` if that's helpful, but it would basically just replace the `submit-docker` line with the one above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034
https://github.com/broadinstitute/cromwell/pull/1559#issuecomment-253327465:16,Testability,log,logic,16,"Not 100% of the logic, but as a PR/step in the right direction: LGTM 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1559/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1559#issuecomment-253327465
https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-326636978:56,Safety,risk,risks,56,@geoffjentry what do you think the effort would be? Any risks?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-326636978
https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327723019:165,Performance,queue,queue,165,"Hi,; Kate brought me here by this thread in the [forum](https://gatkforums.broadinstitute.org/wdl/discussion/10296/prioritize-workflows-which-are-allready-in-server-queue#latest) . It mostly covers the features @kcibul already requested. In addition, I would like to have an API command which forces a workflow directly to start by sending an actual running workflow to sleep/ pause. Maybe this could go hand in hand with call-caching for the sleeping workflow?. Greetings Selonka / EADG",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327723019
https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327723019:377,Usability,pause,pause,377,"Hi,; Kate brought me here by this thread in the [forum](https://gatkforums.broadinstitute.org/wdl/discussion/10296/prioritize-workflows-which-are-allready-in-server-queue#latest) . It mostly covers the features @kcibul already requested. In addition, I would like to have an API command which forces a workflow directly to start by sending an actual running workflow to sleep/ pause. Maybe this could go hand in hand with call-caching for the sleeping workflow?. Greetings Selonka / EADG",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327723019
https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327938812:446,Energy Efficiency,green,green,446,"@katevoss @geoffjentry @kcibul @dshiga . The HCA has no current need for prioritization of workflows, but has a strong need for the ability to submit jobs without starting them (in a queue) and then start them later on. Our entire infrastructure design relies on this feature existing. This ticket is framed by Kristian above as though that functionality already exists, but from my understanding it does not? . I believe it is @ktibbett and the green team / gp production who really need the prioritization feature, so I'll tag her here to add in their use cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327938812
https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327938812:183,Performance,queue,queue,183,"@katevoss @geoffjentry @kcibul @dshiga . The HCA has no current need for prioritization of workflows, but has a strong need for the ability to submit jobs without starting them (in a queue) and then start them later on. Our entire infrastructure design relies on this feature existing. This ticket is framed by Kristian above as though that functionality already exists, but from my understanding it does not? . I believe it is @ktibbett and the green team / gp production who really need the prioritization feature, so I'll tag her here to add in their use cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327938812
https://github.com/broadinstitute/cromwell/issues/1570#issuecomment-253837416:119,Integrability,message,messages,119,@geoffjentry I agree. I think we could easily replace the `promise.complete`s in `AwsSdkAsyncHandler` with appropriate messages to appropriate actors (NB we probably don't _need_ the `AwsSdkAsyncHandler` wrapper class at all),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1570#issuecomment-253837416
https://github.com/broadinstitute/cromwell/issues/1570#issuecomment-253837416:204,Integrability,wrap,wrapper,204,@geoffjentry I agree. I think we could easily replace the `promise.complete`s in `AwsSdkAsyncHandler` with appropriate messages to appropriate actors (NB we probably don't _need_ the `AwsSdkAsyncHandler` wrapper class at all),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1570#issuecomment-253837416
https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745:95,Deployability,deploy,deployed,95,"All fixed -- see https://github.com/broadinstitute/mock-jes/pull/1. Since this only works when deployed to App Engine, I've deployed it to our ""mock production"" there and ran 50 workflows... only the /batch endpoint is invoked and everything seems to be working properly",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745
https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745:124,Deployability,deploy,deployed,124,"All fixed -- see https://github.com/broadinstitute/mock-jes/pull/1. Since this only works when deployed to App Engine, I've deployed it to our ""mock production"" there and ran 50 workflows... only the /batch endpoint is invoked and everything seems to be working properly",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745
https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745:51,Testability,mock,mock-jes,51,"All fixed -- see https://github.com/broadinstitute/mock-jes/pull/1. Since this only works when deployed to App Engine, I've deployed it to our ""mock production"" there and ran 50 workflows... only the /batch endpoint is invoked and everything seems to be working properly",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745
https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745:144,Testability,mock,mock,144,"All fixed -- see https://github.com/broadinstitute/mock-jes/pull/1. Since this only works when deployed to App Engine, I've deployed it to our ""mock production"" there and ran 50 workflows... only the /batch endpoint is invoked and everything seems to be working properly",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745
https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257119375:14,Usability,clear,clear,14,"@kcibul To be clear you mean that you only exercised the /batch endpoint, right? There's no weird reliance on that (I can't imagine how that even would be the case otherwise)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257119375
https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257119779:26,Testability,mock,mock-jes,26,"No I ran Cromwell against mock-jes and successfully ran (now) a few thousand workflows, so it hit both the submit as well as batch status endpoints. . Cromwell no longer calls the singleton status endpoint, just the batch, so I can't tell if the singleton status still works (although no code there changed so I'm confident it's fine)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257119779
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328202942:194,Safety,Risk,Risk,194,"As a **workflow runner**, I want **to be able to reference Cromwell's workflow ID in a WDL**, so that I can **programmatically query for metadata about that workflow**.; - Effort: **Small** ; - Risk: **Small** ; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328202942
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328204343:442,Modifiability,variab,variables,442,"@katevoss I previously asked about this as well. Our use case had more to do with tracking individual costs associated with Jobs initiated within a Task, but not managed by cromwell at all (Ie add labels to a google api call from within a wdl task). The response I received back was that it would be considered, but it creates a NonDeterministic task that will be different each time you call it with the same parameters. I wonder if certain variables should be labelled as `volatile` and will always invalidate a chache hit?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328204343
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328292551:86,Modifiability,variab,variable,86,"Another option would be a cromwell speicifc thing. With cromwell, you could reserve a variable at the level of the workflow called `String cromwell_workflow_id`. When cromwell is resolving inputs, it basically would set this to the value of the current cromwell id. We would not need to change syntax, just document what vairable names are reserved for cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328292551
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328295266:16,Usability,clear,clear,16,@patmagee to be clear i'm not *too* worried about that implementation statement I made. It's just that I think this particular concept is trending in that direction so I'd like us to be careful.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328295266
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:330,Availability,avail,available,330,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:79,Deployability,configurat,configuration,79,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:79,Modifiability,config,configuration,79,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:164,Modifiability,variab,variable,164,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:320,Modifiability,variab,variables,320,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:420,Modifiability,config,configs,420,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:97,Security,access,access,97,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825:337,Deployability,pipeline,pipeline,337,@katevoss ; I would like to add our use-case for the ability to access an ID of a workflow/subworkflow. Our users want the output of the Cromwell to be copied to their local locations and they complain that they cannot read the directory structure - I agree with them as they are data scientists and they want something useful after the pipeline finishes. As we provide the service we are responsible to provide them with something. An idea was to use [croo](https://github.com/ENCODE-DCC/croo) to achieve this. It is really useful solution but it requires manual intervention and knowledge of the pipeline IDs etc. Thus I though I could split the workflow into root and two sub-workflows: `do-the-job` and `copy-files`. However to achieve this the `copy-files` would need to have an access to the `do-the-job` sub-workflow ID or at least the root workflow ID to query for the metadata. I agree it is not deterministic and it should not be. Such a task cannot be cached too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825:598,Deployability,pipeline,pipeline,598,@katevoss ; I would like to add our use-case for the ability to access an ID of a workflow/subworkflow. Our users want the output of the Cromwell to be copied to their local locations and they complain that they cannot read the directory structure - I agree with them as they are data scientists and they want something useful after the pipeline finishes. As we provide the service we are responsible to provide them with something. An idea was to use [croo](https://github.com/ENCODE-DCC/croo) to achieve this. It is really useful solution but it requires manual intervention and knowledge of the pipeline IDs etc. Thus I though I could split the workflow into root and two sub-workflows: `do-the-job` and `copy-files`. However to achieve this the `copy-files` would need to have an access to the `do-the-job` sub-workflow ID or at least the root workflow ID to query for the metadata. I agree it is not deterministic and it should not be. Such a task cannot be cached too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825:963,Performance,cache,cached,963,@katevoss ; I would like to add our use-case for the ability to access an ID of a workflow/subworkflow. Our users want the output of the Cromwell to be copied to their local locations and they complain that they cannot read the directory structure - I agree with them as they are data scientists and they want something useful after the pipeline finishes. As we provide the service we are responsible to provide them with something. An idea was to use [croo](https://github.com/ENCODE-DCC/croo) to achieve this. It is really useful solution but it requires manual intervention and knowledge of the pipeline IDs etc. Thus I though I could split the workflow into root and two sub-workflows: `do-the-job` and `copy-files`. However to achieve this the `copy-files` would need to have an access to the `do-the-job` sub-workflow ID or at least the root workflow ID to query for the metadata. I agree it is not deterministic and it should not be. Such a task cannot be cached too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825:64,Security,access,access,64,@katevoss ; I would like to add our use-case for the ability to access an ID of a workflow/subworkflow. Our users want the output of the Cromwell to be copied to their local locations and they complain that they cannot read the directory structure - I agree with them as they are data scientists and they want something useful after the pipeline finishes. As we provide the service we are responsible to provide them with something. An idea was to use [croo](https://github.com/ENCODE-DCC/croo) to achieve this. It is really useful solution but it requires manual intervention and knowledge of the pipeline IDs etc. Thus I though I could split the workflow into root and two sub-workflows: `do-the-job` and `copy-files`. However to achieve this the `copy-files` would need to have an access to the `do-the-job` sub-workflow ID or at least the root workflow ID to query for the metadata. I agree it is not deterministic and it should not be. Such a task cannot be cached too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825:784,Security,access,access,784,@katevoss ; I would like to add our use-case for the ability to access an ID of a workflow/subworkflow. Our users want the output of the Cromwell to be copied to their local locations and they complain that they cannot read the directory structure - I agree with them as they are data scientists and they want something useful after the pipeline finishes. As we provide the service we are responsible to provide them with something. An idea was to use [croo](https://github.com/ENCODE-DCC/croo) to achieve this. It is really useful solution but it requires manual intervention and knowledge of the pipeline IDs etc. Thus I though I could split the workflow into root and two sub-workflows: `do-the-job` and `copy-files`. However to achieve this the `copy-files` would need to have an access to the `do-the-job` sub-workflow ID or at least the root workflow ID to query for the metadata. I agree it is not deterministic and it should not be. Such a task cannot be cached too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537826459:310,Modifiability,variab,variable,310,"So, for anybody is interested how I do it now. I just parse path to any file produced by the workflow and I extract first Cromwell workflow ID in the path as a root ID. That is a bit of a hack IMHO because it requires the knowledge of the directory structure but it works. I did not put it as an environmental variable because I was afraid cluster space spoilage. EDIT: As I mentioned in previous entry the cash needs to be invalidated for the particular task for this to work and until #1695 is not done (or some other solution is reached) this solution in principle excludes restart.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537826459
https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328204540:123,Safety,avoid,avoid,123,"As a **workflow runner using the CLI**, I want **an option to send the output JSON into a separate file**, so that I can **avoid digging through the whole metadata for the information I need**.; - Effort: **Small** ; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328204540
https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328204540:219,Safety,Risk,Risk,219,"As a **workflow runner using the CLI**, I want **an option to send the output JSON into a separate file**, so that I can **avoid digging through the whole metadata for the information I need**.; - Effort: **Small** ; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328204540
https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328279623:134,Integrability,interface,interface,134,"hands on keyboard effort is relatively low. the real work is defining how to specify it, although now that we have the snazzy new CLI interface it's probably pretty simple to do so",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328279623
https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328279623:165,Usability,simpl,simple,165,"hands on keyboard effort is relatively low. the real work is defining how to specify it, although now that we have the snazzy new CLI interface it's probably pretty simple to do so",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328279623
https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-849722760:268,Testability,log,log,268,"Yes if you are writing the ouputs json to the screen there should be an option to save this outputs json to file. Only requires adding 1 extra command line options such as -j --json-outputs [file] and then you write to screen and to file, i.e. not the entire cromwell log, just the outputs json. We are talking about this outputs:; ```; [2021-05-27 04:21:25,83] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""pindel.cwl.unfiltered_vcf"": {; ""format"": null,; ""location"":; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-849722760
https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253886201:234,Availability,down,downstream,234,That's probably going to bork a lot more stuff than just our tests. I can see why that change was put in but I don't think we can just blindly accept that behavior w/o running it past @kcibul - it's likely to cause a lot of commotion downstream.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253886201
https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253886201:61,Testability,test,tests,61,That's probably going to bork a lot more stuff than just our tests. I can see why that change was put in but I don't think we can just blindly accept that behavior w/o running it past @kcibul - it's likely to cause a lot of commotion downstream.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253886201
https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253887045:125,Safety,avoid,avoid,125,"I agree we should talk about it - although in order to enable sub-workflows, I personally think it's a good thing as it will avoid confusion (both on user and cromwell side) on what is being called.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253887045
https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-289824728:97,Testability,test,tests,97,Nothing is broken per se. It's just good old tech debt where we have duplicated WDLs in the unit tests that could be unified. It is still there AFAIK.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-289824728
https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-325483348:31,Testability,test,tests,31,@Horneth does it make the unit tests a mess? Or is it just ugliness that makes us cringe slightly?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-325483348
https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-325485279:54,Testability,test,test,54,Probably more the latter even though it does make the test code a bit messier.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-325485279
https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-328204130:87,Safety,avoid,avoid,87,"As a **Cromwell developer**, I want **each unit test to appear once**, so that I can **avoid duplicate (and messy) tests**.; - Effort: **TBD**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-328204130
https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-328204130:146,Safety,Risk,Risk,146,"As a **Cromwell developer**, I want **each unit test to appear once**, so that I can **avoid duplicate (and messy) tests**.; - Effort: **TBD**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-328204130
https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-328204130:48,Testability,test,test,48,"As a **Cromwell developer**, I want **each unit test to appear once**, so that I can **avoid duplicate (and messy) tests**.; - Effort: **TBD**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-328204130
https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-328204130:115,Testability,test,tests,115,"As a **Cromwell developer**, I want **each unit test to appear once**, so that I can **avoid duplicate (and messy) tests**.; - Effort: **TBD**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-328204130
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-267833782:82,Performance,scalab,scalability,82,@mcovarr @kcibul Can we refine this a bit? This sounds like it could involve the `scalability` label,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-267833782
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269833846:242,Availability,error,error,242,I explored `rewriteBatchedStatements` as per https://github.com/slick/slick/issues/1272 as I thought this might be the magic we're supposed to ask @dvoet about. It didn't seem to have an effect but there could be some combination of operator error and our slick code confounding this. I did note that Rawls is only using this in their test `reference.conf` so perhaps this isn't what he was talking about. I'll also note one of the last comments in that issue states that it munges the return count. I didn't look but I wouldn't be surprised if we have code checking the # of inserted entries.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269833846
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269833846:12,Modifiability,rewrite,rewriteBatchedStatements,12,I explored `rewriteBatchedStatements` as per https://github.com/slick/slick/issues/1272 as I thought this might be the magic we're supposed to ask @dvoet about. It didn't seem to have an effect but there could be some combination of operator error and our slick code confounding this. I did note that Rawls is only using this in their test `reference.conf` so perhaps this isn't what he was talking about. I'll also note one of the last comments in that issue states that it munges the return count. I didn't look but I wouldn't be surprised if we have code checking the # of inserted entries.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269833846
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269833846:335,Testability,test,test,335,I explored `rewriteBatchedStatements` as per https://github.com/slick/slick/issues/1272 as I thought this might be the magic we're supposed to ask @dvoet about. It didn't seem to have an effect but there could be some combination of operator error and our slick code confounding this. I did note that Rawls is only using this in their test `reference.conf` so perhaps this isn't what he was talking about. I'll also note one of the last comments in that issue states that it munges the return count. I didn't look but I wouldn't be surprised if we have code checking the # of inserted entries.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269833846
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704:26,Modifiability,rewrite,rewriteBatchedStatements,26,rawls production does use rewriteBatchedStatements=true (see https://github.com/broadinstitute/firecloud-develop/blob/prod/configs/rawls/rawls.conf.ctmpl). You need to use it in conjunction with ```tableQuery ++= collection``` instead of ```tableQuery += item```. How are you measuring the effect? I have noticed that jProfiler will still count the inserts individually. I think the slick logs show the right thing. But this is some magic that happens inside the JDBC layer so it is hard to tell what is actually happening. I would suggest running with a local mysql with the general query log on http://dev.mysql.com/doc/refman/5.7/en/query-log.html.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704:123,Modifiability,config,configs,123,rawls production does use rewriteBatchedStatements=true (see https://github.com/broadinstitute/firecloud-develop/blob/prod/configs/rawls/rawls.conf.ctmpl). You need to use it in conjunction with ```tableQuery ++= collection``` instead of ```tableQuery += item```. How are you measuring the effect? I have noticed that jProfiler will still count the inserts individually. I think the slick logs show the right thing. But this is some magic that happens inside the JDBC layer so it is hard to tell what is actually happening. I would suggest running with a local mysql with the general query log on http://dev.mysql.com/doc/refman/5.7/en/query-log.html.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704:389,Testability,log,logs,389,rawls production does use rewriteBatchedStatements=true (see https://github.com/broadinstitute/firecloud-develop/blob/prod/configs/rawls/rawls.conf.ctmpl). You need to use it in conjunction with ```tableQuery ++= collection``` instead of ```tableQuery += item```. How are you measuring the effect? I have noticed that jProfiler will still count the inserts individually. I think the slick logs show the right thing. But this is some magic that happens inside the JDBC layer so it is hard to tell what is actually happening. I would suggest running with a local mysql with the general query log on http://dev.mysql.com/doc/refman/5.7/en/query-log.html.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704:590,Testability,log,log,590,rawls production does use rewriteBatchedStatements=true (see https://github.com/broadinstitute/firecloud-develop/blob/prod/configs/rawls/rawls.conf.ctmpl). You need to use it in conjunction with ```tableQuery ++= collection``` instead of ```tableQuery += item```. How are you measuring the effect? I have noticed that jProfiler will still count the inserts individually. I think the slick logs show the right thing. But this is some magic that happens inside the JDBC layer so it is hard to tell what is actually happening. I would suggest running with a local mysql with the general query log on http://dev.mysql.com/doc/refman/5.7/en/query-log.html.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704:642,Testability,log,log,642,rawls production does use rewriteBatchedStatements=true (see https://github.com/broadinstitute/firecloud-develop/blob/prod/configs/rawls/rawls.conf.ctmpl). You need to use it in conjunction with ```tableQuery ++= collection``` instead of ```tableQuery += item```. How are you measuring the effect? I have noticed that jProfiler will still count the inserts individually. I think the slick logs show the right thing. But this is some magic that happens inside the JDBC layer so it is hard to tell what is actually happening. I would suggest running with a local mysql with the general query log on http://dev.mysql.com/doc/refman/5.7/en/query-log.html.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846704
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846991:662,Availability,reliab,reliably,662,"@dvoet I wondered if you all were overriding that elsewhere. Have you run into issues w/ the loss of number of inserted elements or do you all just not care about that ever?. The function in question (don't have it in front of me) was using `++=` but that's why I was wondering if perhaps there's something else about our slick code which counteracts this. My slick-fu is likely not strong enough, I'll probably need to rely on bigger guns next week. My guess is that this is the culprit. I was looking at the general query log (I've only been using mysql, not cloudsql) and all I saw were individual inserts, never a batch insert. I can't get jprofiler to work reliably running against a JVM on GCS vms (at least not from home) so wasn't even looking at that :). Another thought is that something upstream is actually calling our slick code per-item instead of per-collection but I don't think that's the case. It's at least something I can double check easily.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846991
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846991:524,Testability,log,log,524,"@dvoet I wondered if you all were overriding that elsewhere. Have you run into issues w/ the loss of number of inserted elements or do you all just not care about that ever?. The function in question (don't have it in front of me) was using `++=` but that's why I was wondering if perhaps there's something else about our slick code which counteracts this. My slick-fu is likely not strong enough, I'll probably need to rely on bigger guns next week. My guess is that this is the culprit. I was looking at the general query log (I've only been using mysql, not cloudsql) and all I saw were individual inserts, never a batch insert. I can't get jprofiler to work reliably running against a JVM on GCS vms (at least not from home) so wasn't even looking at that :). Another thought is that something upstream is actually calling our slick code per-item instead of per-collection but I don't think that's the case. It's at least something I can double check easily.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846991
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269848880:18,Testability,log,log,18,Compare the mysql log with the slick log. If slick thinks it is doing batches it will be the log. Rawls is not doing anything else special.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269848880
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269848880:37,Testability,log,log,37,Compare the mysql log with the slick log. If slick thinks it is doing batches it will be the log. Rawls is not doing anything else special.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269848880
https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269848880:93,Testability,log,log,93,Compare the mysql log with the slick log. If slick thinks it is doing batches it will be the log. Rawls is not doing anything else special.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269848880
https://github.com/broadinstitute/cromwell/pull/1586#issuecomment-254273543:28,Performance,load,loaded,28,"To explain, ""1.0"" is a very loaded term in my mind. People have already been ignoring the `0.` colloquially anyways, I'd rather just drop it and move on with life",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1586#issuecomment-254273543
https://github.com/broadinstitute/cromwell/issues/1587#issuecomment-254313101:67,Performance,cache,cached,67,"Yes this looks right - we also want to iterate over every possible cached file, just try on each until we hopefully find a hit we have permissions on",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1587#issuecomment-254313101
https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254399325:5,Deployability,update,update,5,"Will update wdl4s to a published version once https://github.com/broadinstitute/wdl4s/pull/38 is reviewed, merged, and auto-published. https://github.com/broadinstitute/lenthall/pull/22 needs to be reviewed also, but the artifact update isn't required as lenthall's cats dependencies are listed as Provided.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254399325
https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254399325:230,Deployability,update,update,230,"Will update wdl4s to a published version once https://github.com/broadinstitute/wdl4s/pull/38 is reviewed, merged, and auto-published. https://github.com/broadinstitute/lenthall/pull/22 needs to be reviewed also, but the artifact update isn't required as lenthall's cats dependencies are listed as Provided.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254399325
https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254399325:271,Integrability,depend,dependencies,271,"Will update wdl4s to a published version once https://github.com/broadinstitute/wdl4s/pull/38 is reviewed, merged, and auto-published. https://github.com/broadinstitute/lenthall/pull/22 needs to be reviewed also, but the artifact update isn't required as lenthall's cats dependencies are listed as Provided.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254399325
https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254530925:19,Deployability,update,update,19,"Even so, I'd still update the Lenthall dep if it's updated (so we can easily see it's a SNAPSHOT version)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254530925
https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254530925:51,Deployability,update,updated,51,"Even so, I'd still update the Lenthall dep if it's updated (so we can easily see it's a SNAPSHOT version)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254530925
https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254657707:80,Integrability,depend,dependency,80,"Lenthall doesn't have any other code changes for cromwell at the moment, as the dependency is currently listed at 0.19 stable. I don't like updating lenthall _to_ a snapshot unless needed, as from what I can tell sbt is [broken](http://stackoverflow.com/questions/37225775/idea-sbt-unable-to-reparse-warning) on snapshot resolving at the moment, with a recommendation of basically ""[just don't use snapshot dependencies with sbt](https://github.com/sbt/sbt/issues/2687#issuecomment-236586241)"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254657707
https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254657707:407,Integrability,depend,dependencies,407,"Lenthall doesn't have any other code changes for cromwell at the moment, as the dependency is currently listed at 0.19 stable. I don't like updating lenthall _to_ a snapshot unless needed, as from what I can tell sbt is [broken](http://stackoverflow.com/questions/37225775/idea-sbt-unable-to-reparse-warning) on snapshot resolving at the moment, with a recommendation of basically ""[just don't use snapshot dependencies with sbt](https://github.com/sbt/sbt/issues/2687#issuecomment-236586241)"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254657707
https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178:173,Availability,down,down,173,"I agree with Lee here. I think we could do much better at getting people rolling rather than pointing them at a mega-file inside of our source tree. For example -- slimming down what a user needs to have in their conf file, and also providing template conf files for common configurations (SGE, JES, Local, etc). This issue needs more refinement before being ready for development",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178
https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178:73,Deployability,rolling,rolling,73,"I agree with Lee here. I think we could do much better at getting people rolling rather than pointing them at a mega-file inside of our source tree. For example -- slimming down what a user needs to have in their conf file, and also providing template conf files for common configurations (SGE, JES, Local, etc). This issue needs more refinement before being ready for development",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178
https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178:274,Deployability,configurat,configurations,274,"I agree with Lee here. I think we could do much better at getting people rolling rather than pointing them at a mega-file inside of our source tree. For example -- slimming down what a user needs to have in their conf file, and also providing template conf files for common configurations (SGE, JES, Local, etc). This issue needs more refinement before being ready for development",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178
https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178:274,Modifiability,config,configurations,274,"I agree with Lee here. I think we could do much better at getting people rolling rather than pointing them at a mega-file inside of our source tree. For example -- slimming down what a user needs to have in their conf file, and also providing template conf files for common configurations (SGE, JES, Local, etc). This issue needs more refinement before being ready for development",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178
https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255498081:12,Availability,down,down,12,"re slimming down conf, one doesn't need a large conf file. While many folks seem to go the route of copying in the entire reference.conf and making mods, I only ever include the exact bits that I'm tweaking and my conf files are pretty tight. That doesn't address the other issue. If only we had a tech writer joining our ranks soon .... ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255498081
https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255498081:91,Integrability,rout,route,91,"re slimming down conf, one doesn't need a large conf file. While many folks seem to go the route of copying in the entire reference.conf and making mods, I only ever include the exact bits that I'm tweaking and my conf files are pretty tight. That doesn't address the other issue. If only we had a tech writer joining our ranks soon .... ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255498081
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992:417,Security,password,password,417,"I feel uneasy recommending an unencrypted connection to a database, especially when the MySQL team went out of their way to start warning about this issue. That said, the simplest copypasta you can use to remove that warning is to pass the specified parameter in your database url:. In this stanza, change the url from:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. To:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. Or if there are already other params already on your url, append using ""&"" instead of ""?"":. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992:584,Security,password,password,584,"I feel uneasy recommending an unencrypted connection to a database, especially when the MySQL team went out of their way to start warning about this issue. That said, the simplest copypasta you can use to remove that warning is to pass the specified parameter in your database url:. In this stanza, change the url from:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. To:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. Or if there are already other params already on your url, append using ""&"" instead of ""?"":. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992:850,Security,password,password,850,"I feel uneasy recommending an unencrypted connection to a database, especially when the MySQL team went out of their way to start warning about this issue. That said, the simplest copypasta you can use to remove that warning is to pass the specified parameter in your database url:. In this stanza, change the url from:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. To:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. Or if there are already other params already on your url, append using ""&"" instead of ""?"":. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992:171,Usability,simpl,simplest,171,"I feel uneasy recommending an unencrypted connection to a database, especially when the MySQL team went out of their way to start warning about this issue. That said, the simplest copypasta you can use to remove that warning is to pass the specified parameter in your database url:. In this stanza, change the url from:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. To:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. Or if there are already other params already on your url, append using ""&"" instead of ""?"":. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171:573,Security,password,password,573,"This is good for casual users, thanks @kshakir. On Tue, Oct 18, 2016 at 10:25 AM, kshakir notifications@github.com wrote:. > I feel uneasy recommending an unencrypted connection to a database,; > especially when the MySQL team went out of their way to start warning about; > this issue.; > ; > That said, the simplest copypasta you can use to remove that warning is to; > pass the specified parameter in your database url:; > ; > In this stanza, change the url from:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > To:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > Or if there are already other params already on your url, append using ""&""; > instead of ""?"":; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk53aShDWYxlKJyOGeGW4XyTntKdFks5q1NbxgaJpZM4KZ1eV; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171:754,Security,password,password,754,"This is good for casual users, thanks @kshakir. On Tue, Oct 18, 2016 at 10:25 AM, kshakir notifications@github.com wrote:. > I feel uneasy recommending an unencrypted connection to a database,; > especially when the MySQL team went out of their way to start warning about; > this issue.; > ; > That said, the simplest copypasta you can use to remove that warning is to; > pass the specified parameter in your database url:; > ; > In this stanza, change the url from:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > To:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > Or if there are already other params already on your url, append using ""&""; > instead of ""?"":; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk53aShDWYxlKJyOGeGW4XyTntKdFks5q1NbxgaJpZM4KZ1eV; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171:1037,Security,password,password,1037,"This is good for casual users, thanks @kshakir. On Tue, Oct 18, 2016 at 10:25 AM, kshakir notifications@github.com wrote:. > I feel uneasy recommending an unencrypted connection to a database,; > especially when the MySQL team went out of their way to start warning about; > this issue.; > ; > That said, the simplest copypasta you can use to remove that warning is to; > pass the specified parameter in your database url:; > ; > In this stanza, change the url from:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > To:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > Or if there are already other params already on your url, append using ""&""; > instead of ""?"":; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk53aShDWYxlKJyOGeGW4XyTntKdFks5q1NbxgaJpZM4KZ1eV; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171:309,Usability,simpl,simplest,309,"This is good for casual users, thanks @kshakir. On Tue, Oct 18, 2016 at 10:25 AM, kshakir notifications@github.com wrote:. > I feel uneasy recommending an unencrypted connection to a database,; > especially when the MySQL team went out of their way to start warning about; > this issue.; > ; > That said, the simplest copypasta you can use to remove that warning is to; > pass the specified parameter in your database url:; > ; > In this stanza, change the url from:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > To:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > Or if there are already other params already on your url, append using ""&""; > instead of ""?"":; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk53aShDWYxlKJyOGeGW4XyTntKdFks5q1NbxgaJpZM4KZ1eV; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-255756516:312,Integrability,message,message,312,"If you want to alert general users of this solution, I can publish a FAQ on the support forum. If you'd rather it be limited to developer-minded users (i.e. the ones likely to scour the Cromwell repo), then merely having this ticket in your repository will be good. If you don't want to encourage it, then add a message to this thread about why you would discourage this--though what has already been said above may be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-255756516
https://github.com/broadinstitute/cromwell/pull/1592#issuecomment-254606765:0,Deployability,Update,Update,0,"Update README, including documenting what the default behavior is if unspecified.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1592#issuecomment-254606765
https://github.com/broadinstitute/cromwell/pull/1592#issuecomment-254609707:32,Integrability,message,messages,32,😡 apparently thumbs in approval messages don't count so here's another :+1:. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1592/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1592#issuecomment-254609707
https://github.com/broadinstitute/cromwell/issues/1593#issuecomment-254806519:168,Performance,Queue,Queue,168,"hanging = ""job will run forever unless terminated"". Wrote that previous comment too quickly, I guess. Simply requesting that cromwell stdout prints a status similar to Queue -- this makes debugging a lot easier:. ```; .....snip....; INFO  15:50:08,653 QGraph - 0 Pend, 1 Run, 0 Fail, 1375 Done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1593#issuecomment-254806519
https://github.com/broadinstitute/cromwell/issues/1593#issuecomment-254806519:102,Usability,Simpl,Simply,102,"hanging = ""job will run forever unless terminated"". Wrote that previous comment too quickly, I guess. Simply requesting that cromwell stdout prints a status similar to Queue -- this makes debugging a lot easier:. ```; .....snip....; INFO  15:50:08,653 QGraph - 0 Pend, 1 Run, 0 Fail, 1375 Done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1593#issuecomment-254806519
https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-254813125:122,Integrability,depend,dependent,122,"In previous versions of cromwell, I did not have this issue using the same inputs (WDL, local_application.conf, json, and dependent files referenced by the json).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-254813125
https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255101545:492,Safety,Abort,Aborting,492,"The timestamps are wrong... This was hanging for > 12 hours. Also, I cannot ctl-C I have to `kill`. ```; [2016-10-20 00:09:40,74] [info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; [2016-10-20 00:09:41,04] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; [2016-10-20 00:09:41,05] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows... ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255101545
https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255103703:106,Performance,cache,cache,106,"@cjllanwarne Not sure if this is related (tell me if I should file another issue), but all jobs should be cache hits. Yet, I can see that it is running jobs. This may be due to the ""path"" hashing strategy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255103703
https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255103703:188,Security,hash,hashing,188,"@cjllanwarne Not sure if this is related (tell me if I should file another issue), but all jobs should be cache hits. Yet, I can see that it is running jobs. This may be due to the ""path"" hashing strategy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255103703
https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255117503:65,Safety,abort,aborted,65,"I speculate that this issue is caused by trying to bring in half-aborted previous jobs. Fixing that in Single Workflow mode should alleviate this ticket although the underlying cause (""abort"" leaves cromwell in an inconsistent state) still needs addressing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255117503
https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255117503:185,Safety,abort,abort,185,"I speculate that this issue is caused by trying to bring in half-aborted previous jobs. Fixing that in Single Workflow mode should alleviate this ticket although the underlying cause (""abort"" leaves cromwell in an inconsistent state) still needs addressing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255117503
https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325474160:19,Modifiability,config,config,19,"@mcovarr what is a config key, and when would a user interact with it? ; @LeeTL1220 is this something you encounter often?; @geoffjentry do you still think it would be really tough?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325474160
https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325490308:62,Availability,down,down,62,"My view of difficulty has only increased. You could narrow it down to a subset of things - . - Nothing in the backend; - Nothing in the service registry; - In the future, nothing involving filesystems. And it'd be easy to do. But most of the stuff people will be fiddling with are in those blocks. One could set up maybe some service (not necessarily ServiceRegistry service)where things which care about config register themselves and then process htings that way but that seems like a giant pain in the ass for not enough gain.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325490308
https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325490308:405,Modifiability,config,config,405,"My view of difficulty has only increased. You could narrow it down to a subset of things - . - Nothing in the backend; - Nothing in the service registry; - In the future, nothing involving filesystems. And it'd be easy to do. But most of the stuff people will be fiddling with are in those blocks. One could set up maybe some service (not necessarily ServiceRegistry service)where things which care about config register themselves and then process htings that way but that seems like a giant pain in the ass for not enough gain.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325490308
https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325842825:78,Modifiability,config,config,78,"This was a bigger deal in earlier version changes of cromwell. I.e. when; the config files were changing a lot. On Tue, Aug 29, 2017 at 5:42 PM, mcovarr <notifications@github.com> wrote:. > I'm fine with closing this but that might make @LeeTL1220; > <https://github.com/leetl1220> cry.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325813025>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk0OKSOMcQL7wP_sK2qChxr85O6dIks5sdIXLgaJpZM4KbN4n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325842825
https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-254901155:67,Safety,abort,abort,67,"In fact, I now have more running jobs than I had when I issued the abort signal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-254901155
https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043:202,Deployability,release,releases,202,"This happens because the EJEAs are not aborted directly. Because they're waiting for tokens the cycle looks like this:; - WEA is waiting for all EJEAs to finish; - All running BJEAs get aborted; - This releases tokens so the next group of EJEAs can begin; - They immediately get aborted; - this releases tokens so the next group of EJEAs can begin; - Ad nauseam, until all queued EJEAs start and get aborted",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043
https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043:295,Deployability,release,releases,295,"This happens because the EJEAs are not aborted directly. Because they're waiting for tokens the cycle looks like this:; - WEA is waiting for all EJEAs to finish; - All running BJEAs get aborted; - This releases tokens so the next group of EJEAs can begin; - They immediately get aborted; - this releases tokens so the next group of EJEAs can begin; - Ad nauseam, until all queued EJEAs start and get aborted",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043
https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043:373,Performance,queue,queued,373,"This happens because the EJEAs are not aborted directly. Because they're waiting for tokens the cycle looks like this:; - WEA is waiting for all EJEAs to finish; - All running BJEAs get aborted; - This releases tokens so the next group of EJEAs can begin; - They immediately get aborted; - this releases tokens so the next group of EJEAs can begin; - Ad nauseam, until all queued EJEAs start and get aborted",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043
https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043:39,Safety,abort,aborted,39,"This happens because the EJEAs are not aborted directly. Because they're waiting for tokens the cycle looks like this:; - WEA is waiting for all EJEAs to finish; - All running BJEAs get aborted; - This releases tokens so the next group of EJEAs can begin; - They immediately get aborted; - this releases tokens so the next group of EJEAs can begin; - Ad nauseam, until all queued EJEAs start and get aborted",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043
https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043:186,Safety,abort,aborted,186,"This happens because the EJEAs are not aborted directly. Because they're waiting for tokens the cycle looks like this:; - WEA is waiting for all EJEAs to finish; - All running BJEAs get aborted; - This releases tokens so the next group of EJEAs can begin; - They immediately get aborted; - this releases tokens so the next group of EJEAs can begin; - Ad nauseam, until all queued EJEAs start and get aborted",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043
https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043:279,Safety,abort,aborted,279,"This happens because the EJEAs are not aborted directly. Because they're waiting for tokens the cycle looks like this:; - WEA is waiting for all EJEAs to finish; - All running BJEAs get aborted; - This releases tokens so the next group of EJEAs can begin; - They immediately get aborted; - this releases tokens so the next group of EJEAs can begin; - Ad nauseam, until all queued EJEAs start and get aborted",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043
https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043:400,Safety,abort,aborted,400,"This happens because the EJEAs are not aborted directly. Because they're waiting for tokens the cycle looks like this:; - WEA is waiting for all EJEAs to finish; - All running BJEAs get aborted; - This releases tokens so the next group of EJEAs can begin; - They immediately get aborted; - this releases tokens so the next group of EJEAs can begin; - Ad nauseam, until all queued EJEAs start and get aborted",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043
https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-325444026:71,Safety,abort,abort,71,@LeeTL1220 is this related to #1495 in that the Ctl-C did not properly abort? If so then I'd like to track the effort there and I'll close this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-325444026
https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-342586443:128,Integrability,message,message,128,"Fixed by https://github.com/broadinstitute/cromwell/pull/2808, the WEA doesn't bypass EJEAs anymore. When they receive an abort message they'll die if they don't have a BJEA.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-342586443
https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-342586443:122,Safety,abort,abort,122,"Fixed by https://github.com/broadinstitute/cromwell/pull/2808, the WEA doesn't bypass EJEAs anymore. When they receive an abort message they'll die if they don't have a BJEA.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-342586443
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-254944647:15,Modifiability,rewrite,rewrite,15,Should we just rewrite the hash / equals methods in the Scope trait ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-254944647
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-254944647:27,Security,hash,hash,27,Should we just rewrite the hash / equals methods in the Scope trait ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-254944647
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-255131528:168,Security,hash,hashCode,168,"It's currently cheap to `==` `Scope`s since matches satisfy instance equality and misses fail quickly, so filtering `List`s of `Scope`s is not an issue. It's the deep `hashCode` computation to determine map buckets that's the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-255131528
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277882112:25,Security,hash,hashCode,25,"@mcovarr: ""It's the deep hashCode computation to determine map buckets that's the problem."" how did you determine this? jprofiler?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277882112
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430:13,Security,hash,hashCode,13,"At the time `hashCode` was all over the Ctrl-\ thread dumps. After seeing all the expensive computation in `hashCode` and considering that keys should have been comparable via instance equality, it became clear that was the problem. It sounds like that's not what you're seeing now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430:108,Security,hash,hashCode,108,"At the time `hashCode` was all over the Ctrl-\ thread dumps. After seeing all the expensive computation in `hashCode` and considering that keys should have been comparable via instance equality, it became clear that was the problem. It sounds like that's not what you're seeing now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430:205,Usability,clear,clear,205,"At the time `hashCode` was all over the Ctrl-\ thread dumps. After seeing all the expensive computation in `hashCode` and considering that keys should have been comparable via instance equality, it became clear that was the problem. It sounds like that's not what you're seeing now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277886109:297,Safety,avoid,avoid,297,"@mcovarr I don't *think* so but as I mentioned earlier I wasn't completely buying what jprofiler was selling and need to take a closer look. Also our comparisons shouldn't be hash code based anyways. One thing I did turn up was elsewhere in that file you had been converting some sets to lists to avoid hashes coming out of filters and such, that might be a thing here as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277886109
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277886109:175,Security,hash,hash,175,"@mcovarr I don't *think* so but as I mentioned earlier I wasn't completely buying what jprofiler was selling and need to take a closer look. Also our comparisons shouldn't be hash code based anyways. One thing I did turn up was elsewhere in that file you had been converting some sets to lists to avoid hashes coming out of filters and such, that might be a thing here as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277886109
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277886109:303,Security,hash,hashes,303,"@mcovarr I don't *think* so but as I mentioned earlier I wasn't completely buying what jprofiler was selling and need to take a closer look. Also our comparisons shouldn't be hash code based anyways. One thing I did turn up was elsewhere in that file you had been converting some sets to lists to avoid hashes coming out of filters and such, that might be a thing here as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277886109
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-295373255:98,Energy Efficiency,schedul,scheduled,98,"The 10K call caching run revealed that we still spend a lot of time doing in this area, even with scheduled time based calls to `runnableCalls`. ![screen shot 2017-04-19 at 2 09 51 pm](https://cloud.githubusercontent.com/assets/2978948/25194952/1bd7ee2e-250a-11e7-9eff-cef3b3f78c4f.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-295373255
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-295373442:23,Energy Efficiency,reduce,reduce,23,I'm looking at ways to reduce that,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-295373442
https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-255139898:137,Usability,simpl,simple,137,"is overloading the right thing to do here?. i think so, but as this is a DSL it's worth focus grouping to make ure it is still viewed as simple",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-255139898
https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270197593:672,Availability,error,error,672,"The most frequent use case I can imagine would be to iterate over all items in an array inside a `for` loop of some kind. However, I was under the impression that `for` loops weren't being implemented in WDL?. I agree with @vdauwera's suggestion on naming, provided that finding the aggregate file size of an array of files is something we want to do. If not, then extra naming distinctions would be confusing. (I can see a user trying to use `size()` rather than `array_size()`, particularly if they have previously written in a language that uses `size()` to check the length of an array. Personally, I often write things as I think they might be, and if there isn't an error thrown (i.e. `function size() does not exist`) and the behavior is not as I expect (returning a file size rather than an array length), I will spend a while trying to troubleshoot the error somewhere else, as I assume `size()` worked the way I wanted it to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270197593
https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270197593:862,Availability,error,error,862,"The most frequent use case I can imagine would be to iterate over all items in an array inside a `for` loop of some kind. However, I was under the impression that `for` loops weren't being implemented in WDL?. I agree with @vdauwera's suggestion on naming, provided that finding the aggregate file size of an array of files is something we want to do. If not, then extra naming distinctions would be confusing. (I can see a user trying to use `size()` rather than `array_size()`, particularly if they have previously written in a language that uses `size()` to check the length of an array. Personally, I often write things as I think they might be, and if there isn't an error thrown (i.e. `function size() does not exist`) and the behavior is not as I expect (returning a file size rather than an array length), I will spend a while trying to troubleshoot the error somewhere else, as I assume `size()` worked the way I wanted it to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270197593
https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270205111:273,Availability,down,down,273,"@knoblett I would imagine you can combine a `size()` with a `range()` to scatter over the elements in a list (I think). I don't mind the overloading but would `length()` and `size()` be simpler here? I'm not really keen on stuff like `array_size` because then I usually go down the road of what other `X_size` exist, what do they do, and why are they special but I might just not be normal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270205111
https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270205111:186,Usability,simpl,simpler,186,"@knoblett I would imagine you can combine a `size()` with a `range()` to scatter over the elements in a list (I think). I don't mind the overloading but would `length()` and `size()` be simpler here? I'm not really keen on stuff like `array_size` because then I usually go down the road of what other `X_size` exist, what do they do, and why are they special but I might just not be normal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270205111
https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270675432:168,Safety,sanity check,sanity check,168,"I find that it's always worth looking those up in any language you're not using regularly, because they're not used consistently from one to the other. And including a sanity check/test in whatever code you write, to make sure it's returning what you expect :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270675432
https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270675432:181,Testability,test,test,181,"I find that it's always worth looking those up in any language you're not using regularly, because they're not used consistently from one to the other. And including a sanity check/test in whatever code you write, to make sure it's returning what you expect :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270675432
https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270723595:68,Usability,feedback,feedback,68,"@knoblett That's exactly the reason we try to spread a wide net for feedback on these syntactical choices, I feel you all have the pulse of that sort of user a lot better than we do. So keep on doing what you're doing :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270723595
https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-255404918:305,Deployability,release,released,305,"@geoffjentry -- to put this in your head. Let's not jump to it being an engine feature. But I think the point here is a good one. . I think this, and other use cases, might be best met by a set of ""built-in"" tasks that cromwell could come with. For example, through the use of imports and having cromwell released with a equivalent of ""genomics-stdlib"" we could provide genomics specific manipulations as tasks. This solves the problem of the user having to write them themselves. Then through the use of smart multi-backend support we could also have some of these stdlib tasks run on the same machine as cromwell. This requires a bunch of advances to the engine, but I think it's where we can provide a lot of value to the users. The first step in this could be having Kate & Crew (along with our help) publish that ""gatk-stdlib.wdl"" that performs these functions and people could import. Then if that is successful we could see how we would best provide that sort of support in a batteries included fashion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-255404918
https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-255404918:841,Performance,perform,performs,841,"@geoffjentry -- to put this in your head. Let's not jump to it being an engine feature. But I think the point here is a good one. . I think this, and other use cases, might be best met by a set of ""built-in"" tasks that cromwell could come with. For example, through the use of imports and having cromwell released with a equivalent of ""genomics-stdlib"" we could provide genomics specific manipulations as tasks. This solves the problem of the user having to write them themselves. Then through the use of smart multi-backend support we could also have some of these stdlib tasks run on the same machine as cromwell. This requires a bunch of advances to the engine, but I think it's where we can provide a lot of value to the users. The first step in this could be having Kate & Crew (along with our help) publish that ""gatk-stdlib.wdl"" that performs these functions and people could import. Then if that is successful we could see how we would best provide that sort of support in a batteries included fashion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-255404918
https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-496543757:119,Integrability,depend,depending,119,"I recently went through a scenario where there were two ways of combining a long list of intervals into 50 scatters -- depending on whether the actual intervals were contiguous or not--though its uncommon for them to be contiguous. My point is -- there are going to be times where it wont be obvious how to combine intervals ---I feel it may be best to publish an explicit task, and let users customize",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-496543757
https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255241948:8,Deployability,update,update,8,"Can you update the issue with an example of what you want to do, using the regex?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255241948
https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255440746:55,Availability,echo,echo,55,"E.g. (I think). ```; task foo {; File bar; command { ; echo ""contents of ${filename(bar)}:""; cat ${bar}; }; ```. Would generate the script:. ```; echo ""contents of myFile.txt:""; cat /home/chrisl/superexcitingfiles/myFile.txt; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255440746
https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255440746:146,Availability,echo,echo,146,"E.g. (I think). ```; task foo {; File bar; command { ; echo ""contents of ${filename(bar)}:""; cat ${bar}; }; ```. Would generate the script:. ```; echo ""contents of myFile.txt:""; cat /home/chrisl/superexcitingfiles/myFile.txt; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255440746
https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255594077:49,Usability,simpl,simple,49,"Yes, this comes from a user trying to run a very simple wdl, but because of the way localization / docker / path transformation works in cromwell something like that doesn't work:. ```; task foo {; File myinputfile # let's say this is /Users/me/myfile.txt. command {; dosomething -using ${myinputfile} > ${myinputfile}.samples; // evaluates to dosomething -using /Users/me/cromwell_executions/.../inputs/Users/me/myfile.txt > /Users/me/cromwell_executions/.../inputs/Users/me/myfile.txt.samples; }. output {; File out = ${myinputfile}.samples # this doesn't work because myinputfile resolves to /Users/me/myfile.txt.samples; }; }; ```. Now that I write this I realize that maybe the real issue is actually fixing the above behaviour ?. Regardless, what the user really wanted to do here is write the output of his command to a file having the same name as his input file + "".samples"", which it turns out is a lot more difficult than it should.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255594077
https://github.com/broadinstitute/cromwell/pull/1609#issuecomment-256070879:17,Deployability,update,updated,17,OK will keep you updated. Looping in @abaumann as well.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1609#issuecomment-256070879
https://github.com/broadinstitute/cromwell/pull/1609#issuecomment-256959982:23,Testability,test,testing,23,Dev and Prod migration testing went well. It will be a while before we do the real Prod migration.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1609#issuecomment-256959982
https://github.com/broadinstitute/cromwell/issues/1612#issuecomment-255765188:10,Integrability,message,message,10,"The token message is a safety-catch in the token distributer. If the job actor exits in an unexpected way without returning its execution token, the token distributer will spot that and reclaim the token anyway. . So the underlying issue here is that the job actor is crashing or exiting inappropriately. EDIT: Here, ""job actor"" == `EJEA`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612#issuecomment-255765188
https://github.com/broadinstitute/cromwell/issues/1612#issuecomment-255765188:23,Safety,safe,safety-catch,23,"The token message is a safety-catch in the token distributer. If the job actor exits in an unexpected way without returning its execution token, the token distributer will spot that and reclaim the token anyway. . So the underlying issue here is that the job actor is crashing or exiting inappropriately. EDIT: Here, ""job actor"" == `EJEA`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612#issuecomment-255765188
https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-256142741:113,Availability,failure,failures,113,"Yes, @jgentry is correct. Something that I can just cut and paste easily; to check intermediate results or debug failures. The metadata is a bit; much... On Mon, Oct 24, 2016 at 2:24 PM, Jeff Gentry notifications@github.com; wrote:. > @cjllanwarne https://github.com/cjllanwarne You can also specify a; > command line option to run which is a path to output the workflow; > metadata. I _have_ heard a request (there's probably an issue somewhere); > to allow it to be just the outputs instead of the full metadata firehose.; > ; > I suspect @LeeTL1220 https://github.com/LeeTL1220 might be looking for; > something more on the fly, for the eager person who wants to check stuff; > out prior to workflow completion; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-255823594,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk_9uFGV2wpj8ojO7kpkq2KBRqWMVks5q3PfLgaJpZM4Kezeu; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-256142741
https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-325478495:333,Safety,Risk,Risk,333,"Another issue for the [Log spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). As a **user on the CLI and single workflow mode**, I want **to easily see the output location**, so that I can **check the status of my jobs while they are still completing**.; - Effort: **TBD** @geoffjentry ; - Risk: **TBD** @geoffjentry ; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-325478495
https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-325478495:23,Testability,Log,Log,23,"Another issue for the [Log spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). As a **user on the CLI and single workflow mode**, I want **to easily see the output location**, so that I can **check the status of my jobs while they are still completing**.; - Effort: **TBD** @geoffjentry ; - Risk: **TBD** @geoffjentry ; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-325478495
https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-516557883:25,Modifiability,enhance,enhanced,25,"We believe this has been enhanced to some extent since this was filed. Further, with the advent of miniwdl there are better outlets for the behavior the single workflow runner provides. Closing for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-516557883
https://github.com/broadinstitute/cromwell/issues/1615#issuecomment-255774602:63,Availability,failure,failures,63,looks like a missed `NonEmptyList` opportunity for `RunnerData#failures`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615#issuecomment-255774602
https://github.com/broadinstitute/cromwell/pull/1616#issuecomment-256419183:58,Integrability,message,message,58,Safe to merge - the only centaur fails are due to changed message syntax in centaur and unrelated to this change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1616#issuecomment-256419183
https://github.com/broadinstitute/cromwell/pull/1616#issuecomment-256419183:0,Safety,Safe,Safe,0,Safe to merge - the only centaur fails are due to changed message syntax in centaur and unrelated to this change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1616#issuecomment-256419183
https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256067052:0,Availability,Ping,Pinging,0,Pinging @kcibul for priority,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256067052
https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256125203:336,Availability,Ping,Pinging,336,"This is being discussed with faces. First with Hussein and his PO to see; if this is a priority for FireCloud. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Tue, Oct 25, 2016 at 11:21 AM, Chris Llanwarne notifications@github.com; wrote:. > Pinging @kcibul https://github.com/kcibul for priority; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256067052,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4g1wBXEQL8kkTuRTeebKXBoJ8xfOpks5q3h56gaJpZM4KfN-O; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256125203
https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505:219,Performance,queue,queue,219,"I believe to do this properly, it's the specific backend that should grab the docker image hash when a task is actually run (as opposed to the engine which can evaluate it when it sends it to the backend... which could queue it for any length of time). When checking for a call cache hit... we should first check everything else that's cheap before getting this hash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505
https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505:278,Performance,cache,cache,278,"I believe to do this properly, it's the specific backend that should grab the docker image hash when a task is actually run (as opposed to the engine which can evaluate it when it sends it to the backend... which could queue it for any length of time). When checking for a call cache hit... we should first check everything else that's cheap before getting this hash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505
https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505:91,Security,hash,hash,91,"I believe to do this properly, it's the specific backend that should grab the docker image hash when a task is actually run (as opposed to the engine which can evaluate it when it sends it to the backend... which could queue it for any length of time). When checking for a call cache hit... we should first check everything else that's cheap before getting this hash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505
https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505:362,Security,hash,hash,362,"I believe to do this properly, it's the specific backend that should grab the docker image hash when a task is actually run (as opposed to the engine which can evaluate it when it sends it to the backend... which could queue it for any length of time). When checking for a call cache hit... we should first check everything else that's cheap before getting this hash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505
https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-286154588:38,Security,hash,hashing,38,"@kshakir is this a dupe of the Docker hashing work that is already completed? If so, I'll close it out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-286154588
https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377:98,Integrability,Inject,Inject,98,"FYI: Places we allow expressions:. | Position | Notes |; | --- | --- |; | Workflow declarations | Inject a task with matching dependencies. Use ""Local"" backend... But what if Local is not configured (e.g. FC) |; | Task declarations | Use the same backend. Inject a preceeding task (maybe evaluate everything at once?) |; | Task outputs | Use the same backend We'd need to insert a new task afterwards and rewire following tasks. We'd also need to back-fill results once they're known |; | Subexpressions | In the same context as the main expression |; | Scatter signatures | Actually this one doesn't work right now (sad!) but we could probably treat them just like workflow declarations |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377
https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377:126,Integrability,depend,dependencies,126,"FYI: Places we allow expressions:. | Position | Notes |; | --- | --- |; | Workflow declarations | Inject a task with matching dependencies. Use ""Local"" backend... But what if Local is not configured (e.g. FC) |; | Task declarations | Use the same backend. Inject a preceeding task (maybe evaluate everything at once?) |; | Task outputs | Use the same backend We'd need to insert a new task afterwards and rewire following tasks. We'd also need to back-fill results once they're known |; | Subexpressions | In the same context as the main expression |; | Scatter signatures | Actually this one doesn't work right now (sad!) but we could probably treat them just like workflow declarations |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377
https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377:256,Integrability,Inject,Inject,256,"FYI: Places we allow expressions:. | Position | Notes |; | --- | --- |; | Workflow declarations | Inject a task with matching dependencies. Use ""Local"" backend... But what if Local is not configured (e.g. FC) |; | Task declarations | Use the same backend. Inject a preceeding task (maybe evaluate everything at once?) |; | Task outputs | Use the same backend We'd need to insert a new task afterwards and rewire following tasks. We'd also need to back-fill results once they're known |; | Subexpressions | In the same context as the main expression |; | Scatter signatures | Actually this one doesn't work right now (sad!) but we could probably treat them just like workflow declarations |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377
https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377:188,Modifiability,config,configured,188,"FYI: Places we allow expressions:. | Position | Notes |; | --- | --- |; | Workflow declarations | Inject a task with matching dependencies. Use ""Local"" backend... But what if Local is not configured (e.g. FC) |; | Task declarations | Use the same backend. Inject a preceeding task (maybe evaluate everything at once?) |; | Task outputs | Use the same backend We'd need to insert a new task afterwards and rewire following tasks. We'd also need to back-fill results once they're known |; | Subexpressions | In the same context as the main expression |; | Scatter signatures | Actually this one doesn't work right now (sad!) but we could probably treat them just like workflow declarations |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377
https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377:98,Security,Inject,Inject,98,"FYI: Places we allow expressions:. | Position | Notes |; | --- | --- |; | Workflow declarations | Inject a task with matching dependencies. Use ""Local"" backend... But what if Local is not configured (e.g. FC) |; | Task declarations | Use the same backend. Inject a preceeding task (maybe evaluate everything at once?) |; | Task outputs | Use the same backend We'd need to insert a new task afterwards and rewire following tasks. We'd also need to back-fill results once they're known |; | Subexpressions | In the same context as the main expression |; | Scatter signatures | Actually this one doesn't work right now (sad!) but we could probably treat them just like workflow declarations |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377
https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377:256,Security,Inject,Inject,256,"FYI: Places we allow expressions:. | Position | Notes |; | --- | --- |; | Workflow declarations | Inject a task with matching dependencies. Use ""Local"" backend... But what if Local is not configured (e.g. FC) |; | Task declarations | Use the same backend. Inject a preceeding task (maybe evaluate everything at once?) |; | Task outputs | Use the same backend We'd need to insert a new task afterwards and rewire following tasks. We'd also need to back-fill results once they're known |; | Subexpressions | In the same context as the main expression |; | Scatter signatures | Actually this one doesn't work right now (sad!) but we could probably treat them just like workflow declarations |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377
https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256199538:87,Usability,clear,clearly,87,"I should add that I was using expression a little too liberally, I was also referring (clearly, based on my examples) to what we call engine functions",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256199538
https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-313468808:62,Integrability,depend,depending,62,"Leave this as a placeholder for now - it might not make sense depending on how the WOM stuff shakes out, but it might make a ton of sense.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-313468808
https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325476582:22,Testability,Log,Logging,22,I'll add this to the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit) to make sure we document the logging options.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325476582
https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325476582:151,Testability,log,logging,151,I'll add this to the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit) to make sure we document the logging options.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325476582
https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325477087:201,Safety,Risk,Risk,201,"As a **user of all types**, I want **to read documentation about how to access logs**, so that I can **debug my issues, whether they are within the workflow or outside of it**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325477087
https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325477087:72,Security,access,access,72,"As a **user of all types**, I want **to read documentation about how to access logs**, so that I can **debug my issues, whether they are within the workflow or outside of it**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325477087
https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325477087:79,Testability,log,logs,79,"As a **user of all types**, I want **to read documentation about how to access logs**, so that I can **debug my issues, whether they are within the workflow or outside of it**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325477087
https://github.com/broadinstitute/cromwell/issues/1623#issuecomment-267834071:44,Performance,scalab,scalability,44,@kcibul I'm advocating that this earns the `scalability` label but leaving it up to you,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1623#issuecomment-267834071
https://github.com/broadinstitute/cromwell/issues/1623#issuecomment-325658884:58,Security,hash,hashing,58,"@kcibul with the 20k genomes complete, would you say slow hashing is still a problem?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1623#issuecomment-325658884
https://github.com/broadinstitute/cromwell/issues/1625#issuecomment-325468218:88,Safety,Abort,Abort,88,"Most of these issues are complete so I am going to add the remaining one, #637, to the [Abort Spec](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1625#issuecomment-325468218
https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537:100,Performance,cache,cached,100,"Not super urgent, but yes I agree. Ideally we would have access to each of the hashes that get hash cached so we could compare them later to see if things changed (i.e. file hashes, other input hashes, docker image hashes, etc)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537
https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537:57,Security,access,access,57,"Not super urgent, but yes I agree. Ideally we would have access to each of the hashes that get hash cached so we could compare them later to see if things changed (i.e. file hashes, other input hashes, docker image hashes, etc)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537
https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537:79,Security,hash,hashes,79,"Not super urgent, but yes I agree. Ideally we would have access to each of the hashes that get hash cached so we could compare them later to see if things changed (i.e. file hashes, other input hashes, docker image hashes, etc)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537
https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537:95,Security,hash,hash,95,"Not super urgent, but yes I agree. Ideally we would have access to each of the hashes that get hash cached so we could compare them later to see if things changed (i.e. file hashes, other input hashes, docker image hashes, etc)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537
https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537:174,Security,hash,hashes,174,"Not super urgent, but yes I agree. Ideally we would have access to each of the hashes that get hash cached so we could compare them later to see if things changed (i.e. file hashes, other input hashes, docker image hashes, etc)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537
https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537:194,Security,hash,hashes,194,"Not super urgent, but yes I agree. Ideally we would have access to each of the hashes that get hash cached so we could compare them later to see if things changed (i.e. file hashes, other input hashes, docker image hashes, etc)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537
https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537:215,Security,hash,hashes,215,"Not super urgent, but yes I agree. Ideally we would have access to each of the hashes that get hash cached so we could compare them later to see if things changed (i.e. file hashes, other input hashes, docker image hashes, etc)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537
https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256488583:74,Performance,cache,cached,74,"The use case in Cromwell is the same as FireCloud - Cromwell now will use cached data if it exists, and not if it doesn't, but you can't tell why it wasn't in cache when you expected it to be. People could do forensics themselves, but if we had this stored and accessible you could quickly see ""the docker image changed"" or ""the file changed"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256488583
https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256488583:159,Performance,cache,cache,159,"The use case in Cromwell is the same as FireCloud - Cromwell now will use cached data if it exists, and not if it doesn't, but you can't tell why it wasn't in cache when you expected it to be. People could do forensics themselves, but if we had this stored and accessible you could quickly see ""the docker image changed"" or ""the file changed"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256488583
https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256488583:261,Security,access,accessible,261,"The use case in Cromwell is the same as FireCloud - Cromwell now will use cached data if it exists, and not if it doesn't, but you can't tell why it wasn't in cache when you expected it to be. People could do forensics themselves, but if we had this stored and accessible you could quickly see ""the docker image changed"" or ""the file changed"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256488583
https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-332236660:67,Security,hash,hash,67,"I believe it does now cover it since you can see the difference in hash per input and trace that back to a change in the contents of a file (however you might not be able to tell difference between different file path to same input vs same file path with different file contents, but you can tell it's a different file path from the inputs that were used, so I believe you have enough info to make this call). Unless @helgridly can think of something this isn't covering",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-332236660
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256639456:57,Availability,error,error,57,This one is different from previous JES issues that were error 500. So this issue might be a dupe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256639456
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:863,Availability,error,error,863,"Here is another one (only appeared once), but the workflow keeps going... I am not even sure if I need to quit... ```; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:13:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:13:1] Status change from - to Initializing; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:9:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:9:1] Status change from - to Initializing; [2016-10-27 13:47:19,27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChann",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:1030,Availability,error,errors,1030,"s going... I am not even sure if I need to quit... ```; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:13:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:13:1] Status change from - to Initializing; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:9:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:9:1] Status change from - to Initializing; [2016-10-27 13:47:19,27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:1088,Availability,Error,Error,1088,"s going... I am not even sure if I need to quit... ```; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:13:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:13:1] Status change from - to Initializing; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:9:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:9:1] Status change from - to Initializing; [2016-10-27 13:47:19,27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:1151,Availability,Error,Error,1151,"s going... I am not even sure if I need to quit... ```; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:13:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:13:1] Status change from - to Initializing; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:9:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:9:1] Status change from - to Initializing; [2016-10-27 13:47:19,27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:1068,Integrability,message,message,1068,"s going... I am not even sure if I need to quit... ```; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:13:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:13:1] Status change from - to Initializing; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:9:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:9:1] Status change from - to Initializing; [2016-10-27 13:47:19,27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:1131,Integrability,message,message,1131,"s going... I am not even sure if I need to quit... ```; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:13:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:13:1] Status change from - to Initializing; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:9:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:9:1] Status change from - to Initializing; [2016-10-27 13:47:19,27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:2081,Performance,concurren,concurrent,2081,"27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:2303,Performance,concurren,concurrent,2303,"27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:2376,Performance,concurren,concurrent,2376,"27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:2461,Performance,concurren,concurrent,2461,"27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:2538,Performance,concurren,concurrent,2538,"27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:16,Availability,error,error,16,"Here is another error that a user will have trouble determining if the whole workflow failed:. ```; [2016-10-28 14:37:09,70] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:37:09,70] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:2:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:134,Availability,failure,failures,134,"Here is another error that a user will have trouble determining if the whole workflow failed:. ```; [2016-10-28 14:37:09,70] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:37:09,70] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:2:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:490,Availability,error,errors,490,"Here is another error that a user will have trouble determining if the whole workflow failed:. ```; [2016-10-28 14:37:09,70] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:37:09,70] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:2:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:1744,Availability,error,error,1744,"nd.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:4582,Availability,error,error,4582,"ctor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-28 14:37:35,25] [error] The JES polling actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$a#551868791] unexpectedly terminated while conducting 5 polls. Making a new one...; [2016-10-28 14:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(M",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:10889,Availability,failure,failures,10889,"t$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:11235,Availability,error,errors,11235,"2); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:13808,Availability,failure,failures,13808,"s: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.recei",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:14154,Availability,error,errors,14154,"Results:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:186,Integrability,message,message,186,"Here is another error that a user will have trouble determining if the whole workflow failed:. ```; [2016-10-28 14:37:09,70] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:37:09,70] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:2:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:528,Integrability,message,message,528,"Here is another error that a user will have trouble determining if the whole workflow failed:. ```; [2016-10-28 14:37:09,70] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:37:09,70] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:2:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:627,Integrability,message,message,627,"Here is another error that a user will have trouble determining if the whole workflow failed:. ```; [2016-10-28 14:37:09,70] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:37:09,70] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:2:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2756,Integrability,protocol,protocol,2756,d out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receiv,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2849,Integrability,protocol,protocol,2849,ead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scal,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:3016,Integrability,protocol,protocol,3016,SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:10941,Integrability,message,message,10941,"t$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:11273,Integrability,message,message,11273,"2); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:11372,Integrability,message,message,11372,"dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:13860,Integrability,message,message,13860,"s: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.recei",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:14192,Integrability,message,message,14192,"Results:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:14291,Integrability,message,message,14291,"] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:1406,Performance,concurren,concurrent,1406,"ion, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:1479,Performance,concurren,concurrent,1479,"04,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:1564,Performance,concurren,concurrent,1564,"tion could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStre",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:1641,Performance,concurren,concurrent,1641,"e expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHT",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:4244,Performance,concurren,concurrent,4244,"te(NetHttpRequest.java:94); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-28 14:37:35,25] [error] The JES polling actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$a#551868791] unexpectedly terminated while conducting 5 polls. Making a new one...; [2016-10-28 14:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.Par",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:4317,Performance,concurren,concurrent,4317,"cute(HttpRequest.java:972); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-28 14:37:35,25] [error] The JES polling actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$a#551868791] unexpectedly terminated while conducting 5 polls. Making a new one...; [2016-10-28 14:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:4402,Performance,concurren,concurrent,4402,"ecute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-28 14:37:35,25] [error] The JES polling actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$a#551868791] unexpectedly terminated while conducting 5 polls. Making a new one...; [2016-10-28 14:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:4479,Performance,concurren,concurrent,4479,"ollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-28 14:37:35,25] [error] The JES polling actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$a#551868791] unexpectedly terminated while conducting 5 polls. Making a new one...; [2016-10-28 14:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:5770,Performance,concurren,concurrent,5770,"4:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:5843,Performance,concurren,concurrent,5843,"lowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:5928,Performance,concurren,concurrent,5928,"bExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mail",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:6005,Performance,concurren,concurrent,6005,"aught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:6959,Performance,concurren,concurrent,6959,"Worker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(Act",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:7032,Performance,concurren,concurrent,7032,"erThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:7117,Performance,concurren,concurrent,7117,"BackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:10:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:7194,Performance,concurren,concurrent,7194,"aught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:8152,Performance,concurren,concurrent,8152,"er(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(Act",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:8225,Performance,concurren,concurrent,8225,"read.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:8310,Performance,concurren,concurrent,8310,"endJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:8387,Performance,concurren,concurrent,8387,"aught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:9345,Performance,concurren,concurrent,9345,"er(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invok",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:9418,Performance,concurren,concurrent,9418,"read.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:9503,Performance,concurren,concurrent,9503,"endJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:9580,Performance,concurren,concurrent,9580,"aught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(For",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:10543,Performance,concurren,concurrent,10543,"rkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pol",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:10616,Performance,concurren,concurrent,10616,"run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:10701,Performance,concurren,concurrent,10701,"bExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:10778,Performance,concurren,concurrent,10778,"aught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:12151,Performance,concurren,concurrent,12151,"ion, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:17,54] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.AllelicCNV:3:1] Status change from Running to Success; [2016-10-28 14:38:17,67] [info] WorkflowExecutionActor-a3dd8163-de37-4467-a227-5364959a8940 [a3dd8163]: Starting calls: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acn",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:12224,Performance,concurren,concurrent,12224,"04,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:17,54] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.AllelicCNV:3:1] Status change from Running to Success; [2016-10-28 14:38:17,67] [info] WorkflowExecutionActor-a3dd8163-de37-4467-a227-5364959a8940 [a3dd8163]: Starting calls: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI6",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:12309,Performance,concurren,concurrent,12309,"tion could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:17,54] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.AllelicCNV:3:1] Status change from Running to Success; [2016-10-28 14:38:17,67] [info] WorkflowExecutionActor-a3dd8163-de37-4467-a227-5364959a8940 [a3dd8163]: Starting calls: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:12386,Performance,concurren,concurrent,12386,"e expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:17,54] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.AllelicCNV:3:1] Status change from Running to Success; [2016-10-28 14:38:17,67] [info] WorkflowExecutionActor-a3dd8163-de37-4467-a227-5364959a8940 [a3dd8163]: Starting calls: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:15070,Performance,concurren,concurrent,15070,"aller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:15143,Performance,concurren,concurrent,15143,"aller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:15228,Performance,concurren,concurrent,15228,"aller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:15305,Performance,concurren,concurrent,15305,"aller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2079,Security,secur,security,2079,"AsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2144,Security,secur,security,2144,"receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2204,Security,secur,security,2204,".invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2274,Security,secur,security,2274,"Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); at",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2348,Security,secur,security,2348,"ka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291572219:77,Availability,error,errors,77,"Yes. Not as bad, but still pretty hard to debug. A fair number of; transient errors when I use JES. On Tue, Apr 4, 2017 at 1:03 PM, Kate Voss <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> does this still happen?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291566093>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk-_InNBhbZ4MPXcOdseOkaYuW2N2ks5rsnf_gaJpZM4KiVST>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291572219
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291589871:62,Availability,error,errors,62,"@LeeTL1220 we have made number of improvements with transient errors in JES, Cromwell (in version 26) will now retry transient errors automatically.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291589871
https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291589871:127,Availability,error,errors,127,"@LeeTL1220 we have made number of improvements with transient errors in JES, Cromwell (in version 26) will now retry transient errors automatically.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291589871
https://github.com/broadinstitute/cromwell/issues/1632#issuecomment-291567534:178,Modifiability,config,configurable,178,"@adamstruck this is actually intended behavior, when you include `Boolean long = false` it is hardcoding the value of `long` as `false`. You are correct that to make this into a configurable value that you should make the Boolean optional.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1632#issuecomment-291567534
https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-325659139:71,Testability,test,testing,71,"@kcibul when or why would a Cromwell user want to know what Centaur is testing? Is this a Cromwell user who is going direct to Cromwell, or via some other portal?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-325659139
https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-325661432:80,Testability,test,test,80,"@katevoss Good examples would be GOTC or Firecloud, where they expend effort to test things before incorporating a new version into their system. It often happens that we discover that and tell them ""we already test X"", but they have no way of knowing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-325661432
https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-325661432:211,Testability,test,test,211,"@katevoss Good examples would be GOTC or Firecloud, where they expend effort to test things before incorporating a new version into their system. It often happens that we discover that and tell them ""we already test X"", but they have no way of knowing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-325661432
https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326419082:62,Testability,test,tests,62,"yep -- that's where this came from, basically documenting our tests (in a very lightweight manner). Might not also be bad for Cromwell internally rather than trying to guess what a certain WDL tests although the names are quite clever ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326419082
https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326419082:193,Testability,test,tests,193,"yep -- that's where this came from, basically documenting our tests (in a very lightweight manner). Might not also be bad for Cromwell internally rather than trying to guess what a certain WDL tests although the names are quite clever ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326419082
https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660:111,Deployability,release,release,111,"As a **workbench QA**, I want **to know what is being tested**, so that I **don't under- or over-test for each release**. ; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660
https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660:148,Safety,Risk,Risk,148,"As a **workbench QA**, I want **to know what is being tested**, so that I **don't under- or over-test for each release**. ; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660
https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660:54,Testability,test,tested,54,"As a **workbench QA**, I want **to know what is being tested**, so that I **don't under- or over-test for each release**. ; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660
https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660:97,Testability,test,test,97,"As a **workbench QA**, I want **to know what is being tested**, so that I **don't under- or over-test for each release**. ; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660
https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-327532339:0,Safety,risk,risk,0,risk is negligible ; effort is probably higher than it should be but still shouldn't be too much,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-327532339
https://github.com/broadinstitute/cromwell/issues/1637#issuecomment-325668920:36,Safety,abort,aborts,36,"@LeeTL1220 this is a dual problem: [aborts need work](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit), and [logs need work](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). I'm going to link this issue there and close it out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637#issuecomment-325668920
https://github.com/broadinstitute/cromwell/issues/1637#issuecomment-325668920:146,Testability,log,logs,146,"@LeeTL1220 this is a dual problem: [aborts need work](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit), and [logs need work](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). I'm going to link this issue there and close it out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637#issuecomment-325668920
https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257681928:88,Performance,perform,perform,88,Instead of creating a future in the actor could you create a separate actor which would perform the blocking operation and pass it back?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257681928
https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257712585:329,Performance,scalab,scalability,329,"@geoffjentry yeah but assuming it's on the same EC that doesn't actually solve anything. . It'd be much better to have... (wait for it....)... a work pulling system that limits how many of these are going on at once. Now admittedly that's an easier transition if you already have the actors in place but unless you want the full scalability fix here, I suggest we address that as part of the scalability sprint (and I'd be very happy to do it then!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257712585
https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257712585:392,Performance,scalab,scalability,392,"@geoffjentry yeah but assuming it's on the same EC that doesn't actually solve anything. . It'd be much better to have... (wait for it....)... a work pulling system that limits how many of these are going on at once. Now admittedly that's an easier transition if you already have the actors in place but unless you want the full scalability fix here, I suggest we address that as part of the scalability sprint (and I'd be very happy to do it then!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257712585
https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257737080:279,Performance,scalab,scalability,279,"nm, now that i actually look at it i see it's due to those blasted Futures in the signatures. I don't think I agree with just throwing up hands and saying ""here's a potential explosion, let's just wait until we work on scaling"" as this causes the very real possibility of making scalability much worse. On an 8 core system (which is what I think FC uses) it wouldn't take very many of these to wreak massive havoc. Also I don't see how it could be more than a small amount of work to go all the way. Tagging @kcibul in case he wants to overrule me from a product perspective but the number of times we've wound up in trouble when we just handwave situations like this makes me uneasy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1640#issuecomment-257737080
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959:65,Deployability,pipeline,pipeline,65,"@ktibbett is this still a feature that would help the production pipeline? ; @geoffjentry aside from the risk of duplicate naming for output and logs, are there any other risks involved? What would be the effort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959:105,Safety,risk,risk,105,"@ktibbett is this still a feature that would help the production pipeline? ; @geoffjentry aside from the risk of duplicate naming for output and logs, are there any other risks involved? What would be the effort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959:171,Safety,risk,risks,171,"@ktibbett is this still a feature that would help the production pipeline? ; @geoffjentry aside from the risk of duplicate naming for output and logs, are there any other risks involved? What would be the effort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959:145,Testability,log,logs,145,"@ktibbett is this still a feature that would help the production pipeline? ; @geoffjentry aside from the risk of duplicate naming for output and logs, are there any other risks involved? What would be the effort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325773239:66,Performance,load,loaded,66,"@katevoss As long as we're happy providing users a bi-directional loaded gun, it's easy to do. The only difficulty here is coming up with a scheme that prevents people from overwriting their files, but KT doesn't care about that here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325773239
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325781805:17,Performance,load,loaded,17,"A bi-directional loaded gun sounds dangerous, what other ways can users shoot themselves?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325781805
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-326068415:18,Deployability,pipeline,pipeline,18,"As a **production pipeline runner**, I want **to write all output files in one directory (rather than hierarchical)**, so that I can **(@ktibbett why is this helpful?)**.; - Effort: **Small**; - Risk: **Medium**; - if files have the same name they could be overwritten; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-326068415
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-326068415:195,Safety,Risk,Risk,195,"As a **production pipeline runner**, I want **to write all output files in one directory (rather than hierarchical)**, so that I can **(@ktibbett why is this helpful?)**.; - Effort: **Small**; - Risk: **Medium**; - if files have the same name they could be overwritten; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-326068415
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355:636,Safety,risk,risks,636,">The issue is that she's asking to have everything dumped into one location. In many workflows I have a task like this:; ```. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; And I apply this task to all final outputs of my workflows because my colleagues do not want to go into super-nested folder structure with many debugging files (like logs and so on), they just want to get the final results! Having a flat-copy feature will save me from copy-pasting this task everywhere =) . Regarding overwrite risks, I think they are exaggerated:; 1) Usually, it takes you multiple runs until you get everything working, however, after that you switch to another dataset and point other members of the team to the folder they should go to pick the results from you. As I understand the copying of the workflow output happens only when the workflow succeeded.; 2) The final output folder is assigned in the options. That means that for another run you can simply change it.; 3) It is possible to put rewriting only if last file is newer than previous.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355:474,Testability,log,logs,474,">The issue is that she's asking to have everything dumped into one location. In many workflows I have a task like this:; ```. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; And I apply this task to all final outputs of my workflows because my colleagues do not want to go into super-nested folder structure with many debugging files (like logs and so on), they just want to get the final results! Having a flat-copy feature will save me from copy-pasting this task everywhere =) . Regarding overwrite risks, I think they are exaggerated:; 1) Usually, it takes you multiple runs until you get everything working, however, after that you switch to another dataset and point other members of the team to the folder they should go to pick the results from you. As I understand the copying of the workflow output happens only when the workflow succeeded.; 2) The final output folder is assigned in the options. That means that for another run you can simply change it.; 3) It is possible to put rewriting only if last file is newer than previous.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355:1081,Usability,simpl,simply,1081,">The issue is that she's asking to have everything dumped into one location. In many workflows I have a task like this:; ```. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; And I apply this task to all final outputs of my workflows because my colleagues do not want to go into super-nested folder structure with many debugging files (like logs and so on), they just want to get the final results! Having a flat-copy feature will save me from copy-pasting this task everywhere =) . Regarding overwrite risks, I think they are exaggerated:; 1) Usually, it takes you multiple runs until you get everything working, however, after that you switch to another dataset and point other members of the team to the folder they should go to pick the results from you. As I understand the copying of the workflow output happens only when the workflow succeeded.; 2) The final output folder is assigned in the options. That means that for another run you can simply change it.; 3) It is possible to put rewriting only if last file is newer than previous.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-447385933:150,Testability,log,logs,150,I too encountered this need. The use case is to automate the handoff of the final results to another team that does not need the full set of workflow logs and intermediates.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-447385933
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-451557132:160,Availability,down,downstream,160,"We're currently evaluating Cromwell for use automating some fairly large and complicated workflows, and this feature would definitely make automated handoff to downstream users easier. @katevoss, to complete your user story:. As a production pipeline runner, I want to write all output files in one directory (rather than hierarchical), so that I can more easily and automatically locate and pass on those files to the next stage/team in the pipeline (who may not be running Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-451557132
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-451557132:242,Deployability,pipeline,pipeline,242,"We're currently evaluating Cromwell for use automating some fairly large and complicated workflows, and this feature would definitely make automated handoff to downstream users easier. @katevoss, to complete your user story:. As a production pipeline runner, I want to write all output files in one directory (rather than hierarchical), so that I can more easily and automatically locate and pass on those files to the next stage/team in the pipeline (who may not be running Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-451557132
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-451557132:442,Deployability,pipeline,pipeline,442,"We're currently evaluating Cromwell for use automating some fairly large and complicated workflows, and this feature would definitely make automated handoff to downstream users easier. @katevoss, to complete your user story:. As a production pipeline runner, I want to write all output files in one directory (rather than hierarchical), so that I can more easily and automatically locate and pass on those files to the next stage/team in the pipeline (who may not be running Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-451557132
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466146002:82,Testability,test,test,82,"Yeah, an exception would be a good idea in this case. It should be fairly easy to test for, and if the exception also indicated the locations of the files in execution space so they can be retrieved, that would be helpful. In general, good user practices would be to choose a separate output directories for separate data, and to provide meaningful and unique filenames. Note that there is a case, when re-running a workflow that failed, for intentionally overwriting the results of a previous run. It would be good to allow users to choose to do so, but probably not by default.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466146002
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:272,Availability,failure,failure,272,"For our use cases, I’d say put responsibility on the pipeline developer. If there’s a collision, which file “wins” may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, we’d like to distinguish “pipeline failure” from “output failure” in an automated way. So if it’s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:294,Availability,failure,failure,294,"For our use cases, I’d say put responsibility on the pipeline developer. If there’s a collision, which file “wins” may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, we’d like to distinguish “pipeline failure” from “output failure” in an automated way. So if it’s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:351,Availability,Failure,Failure,351,"For our use cases, I’d say put responsibility on the pipeline developer. If there’s a collision, which file “wins” may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, we’d like to distinguish “pipeline failure” from “output failure” in an automated way. So if it’s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:386,Availability,error,error,386,"For our use cases, I’d say put responsibility on the pipeline developer. If there’s a collision, which file “wins” may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, we’d like to distinguish “pipeline failure” from “output failure” in an automated way. So if it’s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:425,Availability,failure,failure,425,"For our use cases, I’d say put responsibility on the pipeline developer. If there’s a collision, which file “wins” may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, we’d like to distinguish “pipeline failure” from “output failure” in an automated way. So if it’s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:461,Availability,error,error,461,"For our use cases, I’d say put responsibility on the pipeline developer. If there’s a collision, which file “wins” may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, we’d like to distinguish “pipeline failure” from “output failure” in an automated way. So if it’s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:53,Deployability,pipeline,pipeline,53,"For our use cases, I’d say put responsibility on the pipeline developer. If there’s a collision, which file “wins” may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, we’d like to distinguish “pipeline failure” from “output failure” in an automated way. So if it’s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:263,Deployability,pipeline,pipeline,263,"For our use cases, I’d say put responsibility on the pipeline developer. If there’s a collision, which file “wins” may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, we’d like to distinguish “pipeline failure” from “output failure” in an automated way. So if it’s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:467,Integrability,message,message,467,"For our use cases, I’d say put responsibility on the pipeline developer. If there’s a collision, which file “wins” may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, we’d like to distinguish “pipeline failure” from “output failure” in an automated way. So if it’s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705:49,Deployability,pipeline,pipelines,49,"We run into the same problem with our production pipelines (i.e. not having a clean output directory structure unless we do some magic). So this would be a huge improvement to us.; @ruchim Throwing an exception and marking the job as failed would be the best. As this makes sure the pipeline developers can easily spot when there are file collisions. Cromwell exiting with an exit code other than 0 after that will also make it easier. Another argument for having a flattened output structure: Continuous integration. We want to know if our pipeline produces certain files and these files have a certain content. However with the current output folder structure it is impossible to know where files are going to end up, so it is hard to test them.; We circumvent this by setting an ""outputDir"" variable in our workflows and tasks which is absolute, so the files are copied to one place. But this is less than ideal. We should not have to hack in WDL as a replacement for functionality in the execution engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705:283,Deployability,pipeline,pipeline,283,"We run into the same problem with our production pipelines (i.e. not having a clean output directory structure unless we do some magic). So this would be a huge improvement to us.; @ruchim Throwing an exception and marking the job as failed would be the best. As this makes sure the pipeline developers can easily spot when there are file collisions. Cromwell exiting with an exit code other than 0 after that will also make it easier. Another argument for having a flattened output structure: Continuous integration. We want to know if our pipeline produces certain files and these files have a certain content. However with the current output folder structure it is impossible to know where files are going to end up, so it is hard to test them.; We circumvent this by setting an ""outputDir"" variable in our workflows and tasks which is absolute, so the files are copied to one place. But this is less than ideal. We should not have to hack in WDL as a replacement for functionality in the execution engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705:494,Deployability,Continuous,Continuous,494,"We run into the same problem with our production pipelines (i.e. not having a clean output directory structure unless we do some magic). So this would be a huge improvement to us.; @ruchim Throwing an exception and marking the job as failed would be the best. As this makes sure the pipeline developers can easily spot when there are file collisions. Cromwell exiting with an exit code other than 0 after that will also make it easier. Another argument for having a flattened output structure: Continuous integration. We want to know if our pipeline produces certain files and these files have a certain content. However with the current output folder structure it is impossible to know where files are going to end up, so it is hard to test them.; We circumvent this by setting an ""outputDir"" variable in our workflows and tasks which is absolute, so the files are copied to one place. But this is less than ideal. We should not have to hack in WDL as a replacement for functionality in the execution engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705:505,Deployability,integrat,integration,505,"We run into the same problem with our production pipelines (i.e. not having a clean output directory structure unless we do some magic). So this would be a huge improvement to us.; @ruchim Throwing an exception and marking the job as failed would be the best. As this makes sure the pipeline developers can easily spot when there are file collisions. Cromwell exiting with an exit code other than 0 after that will also make it easier. Another argument for having a flattened output structure: Continuous integration. We want to know if our pipeline produces certain files and these files have a certain content. However with the current output folder structure it is impossible to know where files are going to end up, so it is hard to test them.; We circumvent this by setting an ""outputDir"" variable in our workflows and tasks which is absolute, so the files are copied to one place. But this is less than ideal. We should not have to hack in WDL as a replacement for functionality in the execution engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705:541,Deployability,pipeline,pipeline,541,"We run into the same problem with our production pipelines (i.e. not having a clean output directory structure unless we do some magic). So this would be a huge improvement to us.; @ruchim Throwing an exception and marking the job as failed would be the best. As this makes sure the pipeline developers can easily spot when there are file collisions. Cromwell exiting with an exit code other than 0 after that will also make it easier. Another argument for having a flattened output structure: Continuous integration. We want to know if our pipeline produces certain files and these files have a certain content. However with the current output folder structure it is impossible to know where files are going to end up, so it is hard to test them.; We circumvent this by setting an ""outputDir"" variable in our workflows and tasks which is absolute, so the files are copied to one place. But this is less than ideal. We should not have to hack in WDL as a replacement for functionality in the execution engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705:505,Integrability,integrat,integration,505,"We run into the same problem with our production pipelines (i.e. not having a clean output directory structure unless we do some magic). So this would be a huge improvement to us.; @ruchim Throwing an exception and marking the job as failed would be the best. As this makes sure the pipeline developers can easily spot when there are file collisions. Cromwell exiting with an exit code other than 0 after that will also make it easier. Another argument for having a flattened output structure: Continuous integration. We want to know if our pipeline produces certain files and these files have a certain content. However with the current output folder structure it is impossible to know where files are going to end up, so it is hard to test them.; We circumvent this by setting an ""outputDir"" variable in our workflows and tasks which is absolute, so the files are copied to one place. But this is less than ideal. We should not have to hack in WDL as a replacement for functionality in the execution engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705:794,Modifiability,variab,variable,794,"We run into the same problem with our production pipelines (i.e. not having a clean output directory structure unless we do some magic). So this would be a huge improvement to us.; @ruchim Throwing an exception and marking the job as failed would be the best. As this makes sure the pipeline developers can easily spot when there are file collisions. Cromwell exiting with an exit code other than 0 after that will also make it easier. Another argument for having a flattened output structure: Continuous integration. We want to know if our pipeline produces certain files and these files have a certain content. However with the current output folder structure it is impossible to know where files are going to end up, so it is hard to test them.; We circumvent this by setting an ""outputDir"" variable in our workflows and tasks which is absolute, so the files are copied to one place. But this is less than ideal. We should not have to hack in WDL as a replacement for functionality in the execution engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705:737,Testability,test,test,737,"We run into the same problem with our production pipelines (i.e. not having a clean output directory structure unless we do some magic). So this would be a huge improvement to us.; @ruchim Throwing an exception and marking the job as failed would be the best. As this makes sure the pipeline developers can easily spot when there are file collisions. Cromwell exiting with an exit code other than 0 after that will also make it easier. Another argument for having a flattened output structure: Continuous integration. We want to know if our pipeline produces certain files and these files have a certain content. However with the current output folder structure it is impossible to know where files are going to end up, so it is hard to test them.; We circumvent this by setting an ""outputDir"" variable in our workflows and tasks which is absolute, so the files are copied to one place. But this is less than ideal. We should not have to hack in WDL as a replacement for functionality in the execution engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705
https://github.com/broadinstitute/cromwell/issues/1642#issuecomment-325751623:100,Deployability,pipeline,pipeline,100,"@ktibbett I finally have time to follow up on this, can you explain more about why you want the WDL pipeline to copy workflow outputs into two locations?; How often do you face this problem? Is it a weekly/daily/monthly thing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1642#issuecomment-325751623
https://github.com/broadinstitute/cromwell/issues/1642#issuecomment-345035327:227,Safety,risk,risk,227,"""This"" is the original topic, copying outputs to different locations. It could be a way to ""flatten"" directories. However, as you noted on the [other issue](https://github.com/broadinstitute/cromwell/issues/1641), there is the risk of overwriting results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1642#issuecomment-345035327
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895:111,Modifiability,config,config,111,"What authentication mode are you running in (default credentials, service account or refresh token)? Does your config make use of private dockerhub credentials. I'm wondering why it's writing an authorization at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895:5,Security,authenticat,authentication,5,"What authentication mode are you running in (default credentials, service account or refresh token)? Does your config make use of private dockerhub credentials. I'm wondering why it's writing an authorization at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895:195,Security,authoriz,authorization,195,"What authentication mode are you running in (default credentials, service account or refresh token)? Does your config make use of private dockerhub credentials. I'm wondering why it's writing an authorization at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410:331,Modifiability,config,config,331,"- Yes, does use private docker hub credentials.; - I took defaults otherwise. This was working fine yesterday. Did I need to re-authenticate?. On Wed, Nov 2, 2016 at 9:55 AM, kcibul notifications@github.com wrote:. > What authentication mode are you running in (default credentials, service; > account or refresh token)? Does your config make use of private dockerhub; > credentials; > ; > I'm wondering why it's writing an authorization at all.; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk2yTbJKsspghDzPz2y5Tp7jKKmnNks5q6JY_gaJpZM4KnP3t; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410:128,Security,authenticat,authenticate,128,"- Yes, does use private docker hub credentials.; - I took defaults otherwise. This was working fine yesterday. Did I need to re-authenticate?. On Wed, Nov 2, 2016 at 9:55 AM, kcibul notifications@github.com wrote:. > What authentication mode are you running in (default credentials, service; > account or refresh token)? Does your config make use of private dockerhub; > credentials; > ; > I'm wondering why it's writing an authorization at all.; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk2yTbJKsspghDzPz2y5Tp7jKKmnNks5q6JY_gaJpZM4KnP3t; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410:222,Security,authenticat,authentication,222,"- Yes, does use private docker hub credentials.; - I took defaults otherwise. This was working fine yesterday. Did I need to re-authenticate?. On Wed, Nov 2, 2016 at 9:55 AM, kcibul notifications@github.com wrote:. > What authentication mode are you running in (default credentials, service; > account or refresh token)? Does your config make use of private dockerhub; > credentials; > ; > I'm wondering why it's writing an authorization at all.; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk2yTbJKsspghDzPz2y5Tp7jKKmnNks5q6JY_gaJpZM4KnP3t; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410:424,Security,authoriz,authorization,424,"- Yes, does use private docker hub credentials.; - I took defaults otherwise. This was working fine yesterday. Did I need to re-authenticate?. On Wed, Nov 2, 2016 at 9:55 AM, kcibul notifications@github.com wrote:. > What authentication mode are you running in (default credentials, service; > account or refresh token)? Does your config make use of private dockerhub; > credentials; > ; > I'm wondering why it's writing an authorization at all.; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk2yTbJKsspghDzPz2y5Tp7jKKmnNks5q6JY_gaJpZM4KnP3t; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487:20,Deployability,update,updates,20,"@LeeTL1220 Any more updates on this ticket?. In general we were wondering if a `gcloud logout` and then `gcloud login` helped. If this is no longer an issue, mind closing this one, and open another in the future with current wdl / details / version-info?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487:87,Testability,log,logout,87,"@LeeTL1220 Any more updates on this ticket?. In general we were wondering if a `gcloud logout` and then `gcloud login` helped. If this is no longer an issue, mind closing this one, and open another in the future with current wdl / details / version-info?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487:112,Testability,log,login,112,"@LeeTL1220 Any more updates on this ticket?. In general we were wondering if a `gcloud logout` and then `gcloud login` helped. If this is no longer an issue, mind closing this one, and open another in the future with current wdl / details / version-info?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282735570:233,Deployability,update,updates,233,"I have not seen it in a while. And I think your suggestion would work.; Feel free to close this ticket. On Sun, Feb 26, 2017 at 1:35 PM, kshakir <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Any more updates on this ticket?; >; > In general we were wondering if a gcloud logout and then gcloud login; > helped.; >; > If this is no longer an issue, mind closing this one, and open another in; > the future with current wdl / details / version-info?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk8Bmz3FbZVh7tyQAJ63aCZyquTaoks5rgcYPgaJpZM4KnP3t>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282735570
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282735570:304,Testability,log,logout,304,"I have not seen it in a while. And I think your suggestion would work.; Feel free to close this ticket. On Sun, Feb 26, 2017 at 1:35 PM, kshakir <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Any more updates on this ticket?; >; > In general we were wondering if a gcloud logout and then gcloud login; > helped.; >; > If this is no longer an issue, mind closing this one, and open another in; > the future with current wdl / details / version-info?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk8Bmz3FbZVh7tyQAJ63aCZyquTaoks5rgcYPgaJpZM4KnP3t>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282735570
https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282735570:327,Testability,log,login,327,"I have not seen it in a while. And I think your suggestion would work.; Feel free to close this ticket. On Sun, Feb 26, 2017 at 1:35 PM, kshakir <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Any more updates on this ticket?; >; > In general we were wondering if a gcloud logout and then gcloud login; > helped.; >; > If this is no longer an issue, mind closing this one, and open another in; > the future with current wdl / details / version-info?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk8Bmz3FbZVh7tyQAJ63aCZyquTaoks5rgcYPgaJpZM4KnP3t>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282735570
https://github.com/broadinstitute/cromwell/issues/1645#issuecomment-259756546:35,Safety,abort,abort-jobs-on-terminate,35,"I believe that this was a case of `abort-jobs-on-terminate` not being set. The default has changed in #1664, so one shouldn't have to opt in to this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1645#issuecomment-259756546
https://github.com/broadinstitute/cromwell/issues/1646#issuecomment-261067332:34,Availability,error,error,34,"@kshakir As you said, this was an error in the input file. I successfully completed a run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646#issuecomment-261067332
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307:117,Availability,error,error,117,"@LeeTL1220 ; - Is this .21, .22 or develop?; - Did you capture the 'CMD \' thread dump from the JVM?; - Was there an error message from Cromwell before it got stuck?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307:123,Integrability,message,message,123,"@LeeTL1220 ; - Is this .21, .22 or develop?; - Did you capture the 'CMD \' thread dump from the JVM?; - Was there an error message from Cromwell before it got stuck?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258435488:363,Availability,error,error,363,"Must have forgotten this info... 1) develop (cromwell-23-79f6e12-SNAPSHOT.jar); 2) No, cannot believe I forgot again...; 3) No. On Fri, Nov 4, 2016 at 9:43 AM, Chris Llanwarne notifications@github.com; wrote:. > @LeeTL1220 https://github.com/LeeTL1220; > - Is this .21, .22 or develop?; > - Did you capture the 'CMD \' thread dump from the JVM?; > - Was there an error message from Cromwell before it got stuck?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk5vBVr6UFscUsg3sazpo1H9pyVgMks5q6zaXgaJpZM4Ko1_r; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258435488
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258435488:369,Integrability,message,message,369,"Must have forgotten this info... 1) develop (cromwell-23-79f6e12-SNAPSHOT.jar); 2) No, cannot believe I forgot again...; 3) No. On Fri, Nov 4, 2016 at 9:43 AM, Chris Llanwarne notifications@github.com; wrote:. > @LeeTL1220 https://github.com/LeeTL1220; > - Is this .21, .22 or develop?; > - Did you capture the 'CMD \' thread dump from the JVM?; > - Was there an error message from Cromwell before it got stuck?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk5vBVr6UFscUsg3sazpo1H9pyVgMks5q6zaXgaJpZM4Ko1_r; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258435488
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:31,Availability,error,error,31,"Trivially happened again, same error... Output of `Ctl-\`:. ```; ""shutdownHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5291,Energy Efficiency,Schedul,ScheduledThreadPoolExecutor,5291,"dArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fd",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5341,Energy Efficiency,Schedul,ScheduledThreadPoolExecutor,5341,"k.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5405,Energy Efficiency,Schedul,ScheduledThreadPoolExecutor,5405," at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 dae",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5455,Energy Efficiency,Schedul,ScheduledThreadPoolExecutor,5455,"utor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5973,Energy Efficiency,monitor,monitor,5973,"it for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d1000 nid=0x9f8 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread10"" #15 daemon prio=9 os_prio=0 tid=0x00007fdbcc2cf000 nid=0x9f7 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread9"" #14 daemon prio=9 os_prio=0 tid=0x00007fdbcc2cd000 nid=0x9f6 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE; ""C1 CompilerThread8"" #13 daemon prio=9 os_prio=0 tid=0x00007fdbcc2cb000 nid=0x9f5 waitin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:8615,Energy Efficiency,monitor,monitor,8615,"tion [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread3"" #8 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b8800 nid=0x9f0 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread2"" #7 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b6800 nid=0x9ef waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread1"" #6 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b4800 nid=0x9ee waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b1800 nid=0x9ec waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b0000 nid=0x9ea waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007fdbcc27d800 nid=0x9e8 in Object.wait() [0x00007fdb8d2d9000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b4175f0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007fdbcc279000 nid=0x9e7 in Object.wait() [0x00007fdb8d3da000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference.tryHandlePending(Reference.java:191); - locked <0x000000015b4177a8> (a java.lang.ref.Reference$Lock); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153). ""main"" #1 prio=5 os_prio=0 tid=0x00007fdbcc00a000 nid=0x9d7 in Object.wait() [0x00007fdbd452c000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Thread.join(Threa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:9096,Energy Efficiency,monitor,monitor,9096,"x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b1800 nid=0x9ec waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b0000 nid=0x9ea waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007fdbcc27d800 nid=0x9e8 in Object.wait() [0x00007fdb8d2d9000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b4175f0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007fdbcc279000 nid=0x9e7 in Object.wait() [0x00007fdb8d3da000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference.tryHandlePending(Reference.java:191); - locked <0x000000015b4177a8> (a java.lang.ref.Reference$Lock); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153). ""main"" #1 prio=5 os_prio=0 tid=0x00007fdbcc00a000 nid=0x9d7 in Object.wait() [0x00007fdbd452c000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Thread.join(Thread.java:1245); - locked <0x000000015d89d070> (a scala.sys.ShutdownHookThread$$anon$1); at java.lang.Thread.join(Thread.java:1319); at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106); at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46); at java.lang.Shutdown.runHooks(Shutdown.java:123); at java.lang.Shutdown.sequence(Shutdown.java:167); at java.lang.Shutdown.exit(Shutdown.java:212); - locked <0x000000015b6815a8> (a java.la",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:9530,Energy Efficiency,monitor,monitor,9530,"=0x9e8 in Object.wait() [0x00007fdb8d2d9000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b4175f0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007fdbcc279000 nid=0x9e7 in Object.wait() [0x00007fdb8d3da000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference.tryHandlePending(Reference.java:191); - locked <0x000000015b4177a8> (a java.lang.ref.Reference$Lock); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153). ""main"" #1 prio=5 os_prio=0 tid=0x00007fdbcc00a000 nid=0x9d7 in Object.wait() [0x00007fdbd452c000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Thread.join(Thread.java:1245); - locked <0x000000015d89d070> (a scala.sys.ShutdownHookThread$$anon$1); at java.lang.Thread.join(Thread.java:1319); at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106); at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46); at java.lang.Shutdown.runHooks(Shutdown.java:123); at java.lang.Shutdown.sequence(Shutdown.java:167); at java.lang.Shutdown.exit(Shutdown.java:212); - locked <0x000000015b6815a8> (a java.lang.Class for java.lang.Shutdown); at java.lang.Runtime.exit(Runtime.java:109); at java.lang.System.exit(System.java:971); at scala.sys.package$.exit(package.scala:40); at cromwell.Main$.waitAndExit(Main.scala:92); at cromwell.Main$.runWorkflow(Main.scala:77); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:25); at cromwell.Main$delayedInit$body.apply(Main.scala:15); at scala.Function0$class.apply$mcV$sp(Function0.scal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:722,Performance,concurren,concurrent,722,"Trivially happened again, same error... Output of `Ctl-\`:. ```; ""shutdownHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:797,Performance,concurren,concurrent,797,"Trivially happened again, same error... Output of `Ctl-\`:. ```; ""shutdownHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:867,Performance,concurren,concurrent,867,"Trivially happened again, same error... Output of `Ctl-\`:. ```; ""shutdownHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:985,Performance,concurren,concurrent,985,"Trivially happened again, same error... Output of `Ctl-\`:. ```; ""shutdownHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1065,Performance,concurren,concurrent,1065,"downHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1147,Performance,concurren,concurrent,1147,"tion [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(Thre",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1231,Performance,concurren,concurrent,1231,".lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1599,Performance,concurren,concurrent,1599,"e: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1674,Performance,concurren,concurrent,1674,"wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1744,Performance,concurren,concurrent,1744,"euedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1862,Performance,concurren,concurrent,1862,"concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1942,Performance,concurren,concurrent,1942,"Synchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2024,Performance,concurren,concurrent,2024,"ockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(Thre",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2108,Performance,concurren,concurrent,2108,"Executor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""Fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2476,Performance,concurren,concurrent,2476,"e: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2551,Performance,concurren,concurrent,2551,"wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:197",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2621,Performance,concurren,concurrent,2621,"euedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWork",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2739,Performance,concurren,concurrent,2739,"concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007f",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2819,Performance,concurren,concurrent,2819,"Synchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2901,Performance,concurren,concurrent,2901,"ockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2985,Performance,concurren,concurrent,2985,"Executor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.Lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3377,Performance,concurren,concurrent,3377," sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3421,Performance,concurren,concurrent,3421,"ing to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3493,Performance,concurren,concurrent,3493,"actQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.j",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3570,Performance,concurren,concurrent,3570,"port.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3897,Performance,concurren,concurrent,3897,"oncurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (pa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3972,Performance,concurren,concurrent,3972,"va.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:4042,Performance,concurren,concurrent,4042,"va:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:4441,Performance,concurren,concurrent,4441,"oinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQue",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:4523,Performance,concurren,concurrent,4523,"nWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:4607,Performance,concurren,concurrent,4607,"un(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5007,Performance,concurren,concurrent,5007,"pport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Nativ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5082,Performance,concurren,concurrent,5082,"onditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5157,Performance,concurren,concurrent,5157,"anagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at co",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5280,Performance,concurren,concurrent,5280,"e.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon pr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5394,Performance,concurren,concurrent,5394,"rayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. """,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5507,Performance,concurren,concurrent,5507," java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d1000 nid=0x9f8 waiting on condition [0x0000000",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5589,Performance,concurren,concurrent,5589,"at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d1000 nid=0x9f8 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread10"" #15 daemon pri",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5673,Performance,concurren,concurrent,5673,"at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d1000 nid=0x9f8 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread10"" #15 daemon prio=9 os_prio=0 tid=0x00007fdbcc2cf000 nid=0x9f7 waiting on condition [0x0000000000000",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:638,Safety,Unsafe,Unsafe,638,"Trivially happened again, same error... Output of `Ctl-\`:. ```; ""shutdownHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1515,Safety,Unsafe,Unsafe,1515,"fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2392,Safety,Unsafe,Unsafe,2392,"fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3297,Safety,Unsafe,Unsafe,3297,"g on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterrup",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3813,Safety,Unsafe,Unsafe,3813,"il.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:4923,Safety,Unsafe,Unsafe,4923,"edSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; jav",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259207344:93,Deployability,configurat,configuration,93,"I might have missed it -- can you put together the files so we can; reproduce this (cromwell configuration, WDL and json)? We might need some; permissions, but I want to run this sort of thing continually. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Tue, Nov 8, 2016 at 12:32 PM, Lee Lichtenstein notifications@github.com; wrote:. > N=4/4 on my workflow...; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259203566,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4gwHvsAAXHshNNM4GFWqWhx1Cvrsgks5q8LI_gaJpZM4Ko1_r; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259207344
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259207344:93,Modifiability,config,configuration,93,"I might have missed it -- can you put together the files so we can; reproduce this (cromwell configuration, WDL and json)? We might need some; permissions, but I want to run this sort of thing continually. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Tue, Nov 8, 2016 at 12:32 PM, Lee Lichtenstein notifications@github.com; wrote:. > N=4/4 on my workflow...; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259203566,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4gwHvsAAXHshNNM4GFWqWhx1Cvrsgks5q8LI_gaJpZM4Ko1_r; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259207344
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116:306,Availability,error,errors,306,"@LeeTL1220 I'm still on this, not @ruchim. A couple days ago you asked what I changed to get the workflow to run for success and exit cleanly. I was having problems even getting the workflow to reach success with the given hash `cromwell-23-79f6e12-SNAPSHOT.jar`, as these outputs with spaces were causing errors:. ```; File purity_series_small_amp = ""purity_plots/purity_series_small_Small Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_small_Small Deletions.png""; Array[File] purity_files = glob(""purity_plots/*""); ```. I've been investigating whether this failure is a problem with `glob()`s, or just the `File`s with spaces. . FYI swapping out the three elements for another set of files ""random"" files allowed the workflow to succeed, and cromwell exited cleanly in single workflow mode. ```; # hacked paths to allow downstream calls to still run; File purity_series_small_amp = ""purity_plots/purity_series_Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_Deletions.png""; Array[File] purity_files = glob(""purity_plots/purity_series_*.png""); ```. If there's another hash besides `79f6e12` that causes cromwell to run with spaces, _finish successfully_, and then, let me know. **TL;DR I can either get `79f6e12` with the workflow to succeed & exit, or fail-- but not succeed and lock up.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116:592,Availability,failure,failure,592,"@LeeTL1220 I'm still on this, not @ruchim. A couple days ago you asked what I changed to get the workflow to run for success and exit cleanly. I was having problems even getting the workflow to reach success with the given hash `cromwell-23-79f6e12-SNAPSHOT.jar`, as these outputs with spaces were causing errors:. ```; File purity_series_small_amp = ""purity_plots/purity_series_small_Small Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_small_Small Deletions.png""; Array[File] purity_files = glob(""purity_plots/*""); ```. I've been investigating whether this failure is a problem with `glob()`s, or just the `File`s with spaces. . FYI swapping out the three elements for another set of files ""random"" files allowed the workflow to succeed, and cromwell exited cleanly in single workflow mode. ```; # hacked paths to allow downstream calls to still run; File purity_series_small_amp = ""purity_plots/purity_series_Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_Deletions.png""; Array[File] purity_files = glob(""purity_plots/purity_series_*.png""); ```. If there's another hash besides `79f6e12` that causes cromwell to run with spaces, _finish successfully_, and then, let me know. **TL;DR I can either get `79f6e12` with the workflow to succeed & exit, or fail-- but not succeed and lock up.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116:855,Availability,down,downstream,855,"@LeeTL1220 I'm still on this, not @ruchim. A couple days ago you asked what I changed to get the workflow to run for success and exit cleanly. I was having problems even getting the workflow to reach success with the given hash `cromwell-23-79f6e12-SNAPSHOT.jar`, as these outputs with spaces were causing errors:. ```; File purity_series_small_amp = ""purity_plots/purity_series_small_Small Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_small_Small Deletions.png""; Array[File] purity_files = glob(""purity_plots/*""); ```. I've been investigating whether this failure is a problem with `glob()`s, or just the `File`s with spaces. . FYI swapping out the three elements for another set of files ""random"" files allowed the workflow to succeed, and cromwell exited cleanly in single workflow mode. ```; # hacked paths to allow downstream calls to still run; File purity_series_small_amp = ""purity_plots/purity_series_Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_Deletions.png""; Array[File] purity_files = glob(""purity_plots/purity_series_*.png""); ```. If there's another hash besides `79f6e12` that causes cromwell to run with spaces, _finish successfully_, and then, let me know. **TL;DR I can either get `79f6e12` with the workflow to succeed & exit, or fail-- but not succeed and lock up.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116:223,Security,hash,hash,223,"@LeeTL1220 I'm still on this, not @ruchim. A couple days ago you asked what I changed to get the workflow to run for success and exit cleanly. I was having problems even getting the workflow to reach success with the given hash `cromwell-23-79f6e12-SNAPSHOT.jar`, as these outputs with spaces were causing errors:. ```; File purity_series_small_amp = ""purity_plots/purity_series_small_Small Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_small_Small Deletions.png""; Array[File] purity_files = glob(""purity_plots/*""); ```. I've been investigating whether this failure is a problem with `glob()`s, or just the `File`s with spaces. . FYI swapping out the three elements for another set of files ""random"" files allowed the workflow to succeed, and cromwell exited cleanly in single workflow mode. ```; # hacked paths to allow downstream calls to still run; File purity_series_small_amp = ""purity_plots/purity_series_Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_Deletions.png""; Array[File] purity_files = glob(""purity_plots/purity_series_*.png""); ```. If there's another hash besides `79f6e12` that causes cromwell to run with spaces, _finish successfully_, and then, let me know. **TL;DR I can either get `79f6e12` with the workflow to succeed & exit, or fail-- but not succeed and lock up.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116:1134,Security,hash,hash,1134,"@LeeTL1220 I'm still on this, not @ruchim. A couple days ago you asked what I changed to get the workflow to run for success and exit cleanly. I was having problems even getting the workflow to reach success with the given hash `cromwell-23-79f6e12-SNAPSHOT.jar`, as these outputs with spaces were causing errors:. ```; File purity_series_small_amp = ""purity_plots/purity_series_small_Small Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_small_Small Deletions.png""; Array[File] purity_files = glob(""purity_plots/*""); ```. I've been investigating whether this failure is a problem with `glob()`s, or just the `File`s with spaces. . FYI swapping out the three elements for another set of files ""random"" files allowed the workflow to succeed, and cromwell exited cleanly in single workflow mode. ```; # hacked paths to allow downstream calls to still run; File purity_series_small_amp = ""purity_plots/purity_series_Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_Deletions.png""; Array[File] purity_files = glob(""purity_plots/purity_series_*.png""); ```. If there's another hash besides `79f6e12` that causes cromwell to run with spaces, _finish successfully_, and then, let me know. **TL;DR I can either get `79f6e12` with the workflow to succeed & exit, or fail-- but not succeed and lock up.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:532,Availability,ERROR,ERROR,532,"I upgraded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:2,Deployability,upgrade,upgraded,2,"I upgraded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:14,Deployability,release,release,14,"I upgraded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:3710,Performance,concurren,concurrent,3710,t scala.Option.foreach(Option.scala:257); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:303); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:288); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:154); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:154); at akka.actor.FSM$class.processEvent(FSM.scala:668); at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:154); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:154); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.WorkflowActor.aroundReceive(WorkflowActor.scala:154); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:3783,Performance,concurren,concurrent,3783,t scala.Option.foreach(Option.scala:257); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:303); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:288); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:154); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:154); at akka.actor.FSM$class.processEvent(FSM.scala:668); at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:154); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:154); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.WorkflowActor.aroundReceive(WorkflowActor.scala:154); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:3868,Performance,concurren,concurrent,3868,t scala.Option.foreach(Option.scala:257); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:303); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:288); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:154); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:154); at akka.actor.FSM$class.processEvent(FSM.scala:668); at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:154); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:154); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.WorkflowActor.aroundReceive(WorkflowActor.scala:154); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:3945,Performance,concurren,concurrent,3945,t scala.Option.foreach(Option.scala:257); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:303); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:288); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:154); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:154); at akka.actor.FSM$class.processEvent(FSM.scala:668); at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:154); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:154); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.WorkflowActor.aroundReceive(WorkflowActor.scala:154); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:362,Testability,log,log,362,"I upgraded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:517,Testability,log,logs,517,"I upgraded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:804,Testability,log,logs,804,"I upgraded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:855,Testability,log,log,855,"I upgraded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:948,Testability,log,logs,948,"I upgraded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:999,Testability,log,log,999,"raded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Workflo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:1505,Testability,log,logging,1505,orkflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:303); at scala.Option.foreach(Option.scala:257); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:303); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:288); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:1603,Testability,log,logging,1603,ispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:303); at scala.Option.foreach(Option.scala:257); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:303); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:288); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:1744,Testability,log,logging,1744,3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:303); at scala.Option.foreach(Option.scala:257); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:303); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:288); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:154); at akka.actor.FSM$class.ap,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:2958,Testability,Log,LoggingFSM,2958,un$applyOrElse$1.apply(WorkflowActor.scala:303); at scala.Option.foreach(Option.scala:257); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:303); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:288); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:154); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:154); at akka.actor.FSM$class.processEvent(FSM.scala:668); at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:154); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:154); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.WorkflowActor.aroundReceive(WorkflowActor.scala:154); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:3029,Testability,Log,LoggingFSM,3029,t scala.Option.foreach(Option.scala:257); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:303); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:288); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:154); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:154); at akka.actor.FSM$class.processEvent(FSM.scala:668); at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:154); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:154); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.WorkflowActor.aroundReceive(WorkflowActor.scala:154); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265754582:351,Performance,concurren,concurrent,351,"Addendum...; ``Ctl-\``. I see a lot of the following, but not much else that stands out.; ```; ""pool-1-thread-11"" #77 prio=5 os_prio=0 tid=0x00007fe0fc083800 nid=0x6ea8 waiting on condition [0x00007fe1ae391000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b793b68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265754582
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265754582:267,Safety,Unsafe,Unsafe,267,"Addendum...; ``Ctl-\``. I see a lot of the following, but not much else that stands out.; ```; ""pool-1-thread-11"" #77 prio=5 os_prio=0 tid=0x00007fe0fc083800 nid=0x6ea8 waiting on condition [0x00007fe1ae391000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b793b68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265754582
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265755507:41,Usability,simpl,simply,41,@geoffjentry Any ideas? Looks like it is simply not exiting.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265755507
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265761144:88,Testability,log,logs,88,"@LeeTL1220 are you using the workflow options ""final_workflow_log_dir"" to copy workflow logs?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265761144
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588:180,Testability,log,log,180,"I don't think so:. ```; lichtens@lichtens-big:~/test_eval$ egrep final jes_application.conf; lichtens@lichtens-big:~/test_eval$; ```; ```; lichtens@lichtens-big:~/test_eval$ egrep log jes_application.conf; number-of-workflow-log-copy-workers = 10; # Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs""; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: true; lichtens@lichtens-big:~/test_eval$; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588:225,Testability,log,log-copy-workers,225,"I don't think so:. ```; lichtens@lichtens-big:~/test_eval$ egrep final jes_application.conf; lichtens@lichtens-big:~/test_eval$; ```; ```; lichtens@lichtens-big:~/test_eval$ egrep log jes_application.conf; number-of-workflow-log-copy-workers = 10; # Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs""; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: true; lichtens@lichtens-big:~/test_eval$; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588:288,Testability,log,logs,288,"I don't think so:. ```; lichtens@lichtens-big:~/test_eval$ egrep final jes_application.conf; lichtens@lichtens-big:~/test_eval$; ```; ```; lichtens@lichtens-big:~/test_eval$ egrep log jes_application.conf; number-of-workflow-log-copy-workers = 10; # Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs""; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: true; lichtens@lichtens-big:~/test_eval$; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588:303,Testability,log,log-dir,303,"I don't think so:. ```; lichtens@lichtens-big:~/test_eval$ egrep final jes_application.conf; lichtens@lichtens-big:~/test_eval$; ```; ```; lichtens@lichtens-big:~/test_eval$ egrep log jes_application.conf; number-of-workflow-log-copy-workers = 10; # Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs""; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: true; lichtens@lichtens-big:~/test_eval$; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588:331,Testability,log,logs,331,"I don't think so:. ```; lichtens@lichtens-big:~/test_eval$ egrep final jes_application.conf; lichtens@lichtens-big:~/test_eval$; ```; ```; lichtens@lichtens-big:~/test_eval$ egrep log jes_application.conf; number-of-workflow-log-copy-workers = 10; # Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs""; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: true; lichtens@lichtens-big:~/test_eval$; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588:364,Testability,log,logs,364,"I don't think so:. ```; lichtens@lichtens-big:~/test_eval$ egrep final jes_application.conf; lichtens@lichtens-big:~/test_eval$; ```; ```; lichtens@lichtens-big:~/test_eval$ egrep log jes_application.conf; number-of-workflow-log-copy-workers = 10; # Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs""; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: true; lichtens@lichtens-big:~/test_eval$; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588:409,Testability,log,log-temporary,409,"I don't think so:. ```; lichtens@lichtens-big:~/test_eval$ egrep final jes_application.conf; lichtens@lichtens-big:~/test_eval$; ```; ```; lichtens@lichtens-big:~/test_eval$ egrep log jes_application.conf; number-of-workflow-log-copy-workers = 10; # Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs""; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: true; lichtens@lichtens-big:~/test_eval$; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265764588
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3531,Deployability,configurat,configuration,3531,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3260,Integrability,Message,Message,3260,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3531,Modifiability,config,configuration,3531,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3315,Safety,Abort,AbortAllWorkflowsCommand,3315,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:83,Testability,log,log,83,"Joyous day!. So issue #1754 regards poor cromwell behavior when a user attempts to log to a non-writeable directory. I fixed the log directory and got the following:. ```; lichtens@lichtens-big:~/test_eval$ bash run_crsp_validation_jes.sh; ....snip...; ""crsp_validation_workflow_clinical_run_sensitivity_precision_small_sens_file"": [""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-clinical_run_sensitivity_precision/shard-0/TCGA-05-4390-01A-02D-1752-01.sens_prec.small_segs.tsv"", ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-clinical_run_sensitivity_precision/shard-1/TCGA-55-8615-01A-11D-2389-01.sens_prec.small_segs.tsv"", ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-clinical_run_sensitivity_precision/shard-2/TCGA-DU-A5TY-01A-11D-A288-01.sens_prec.small_segs.tsv"", ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-clinical_run_sensitivity_precision/shard-3/TCGA-FG-A4MU-01B-11D-A288-01.sens_prec.small_segs.tsv"", ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-clinical_run_sensitivity_precision/shard-4/TCGA-HT-A5RC-01A-11D-A288-01.sens_prec.small_segs.tsv""],; ""crsp_validation_workflow_run_plot_reproducibility_reproducibility_plot"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-run_plot_reproducibility/reproducibility_plots/reproducibility_Reproducibility.png"",; ""crsp_validation_workflow_specificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specifici",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:129,Testability,log,log,129,"Joyous day!. So issue #1754 regards poor cromwell behavior when a user attempts to log to a non-writeable directory. I fixed the log directory and got the following:. ```; lichtens@lichtens-big:~/test_eval$ bash run_crsp_validation_jes.sh; ....snip...; ""crsp_validation_workflow_clinical_run_sensitivity_precision_small_sens_file"": [""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-clinical_run_sensitivity_precision/shard-0/TCGA-05-4390-01A-02D-1752-01.sens_prec.small_segs.tsv"", ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-clinical_run_sensitivity_precision/shard-1/TCGA-55-8615-01A-11D-2389-01.sens_prec.small_segs.tsv"", ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-clinical_run_sensitivity_precision/shard-2/TCGA-DU-A5TY-01A-11D-A288-01.sens_prec.small_segs.tsv"", ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-clinical_run_sensitivity_precision/shard-3/TCGA-FG-A4MU-01B-11D-A288-01.sens_prec.small_segs.tsv"", ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-clinical_run_sensitivity_precision/shard-4/TCGA-HT-A5RC-01A-11D-A288-01.sens_prec.small_segs.tsv""],; ""crsp_validation_workflow_run_plot_reproducibility_reproducibility_plot"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-run_plot_reproducibility/reproducibility_plots/reproducibility_Reproducibility.png"",; ""crsp_validation_workflow_specificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specifici",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3488,Testability,log,logging,3488,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3560,Testability,log,log-dead-letters,3560,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3588,Testability,log,log-dead-letters-during-shutdown,3588,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266028389:57,Usability,clear,clear,57,That's awesome!! It's great that the actual issue is now clear too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266028389
https://github.com/broadinstitute/cromwell/pull/1650#issuecomment-258478289:119,Modifiability,refactor,refactor,119,"On top of the original fix in #1647, the scalaz->cats in #1648, there is an additional commit to review for develop to refactor `SymbolTableMigration` to use trait `BatchedTaskChange`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1650#issuecomment-258478289
https://github.com/broadinstitute/cromwell/pull/1652#issuecomment-259157871:17,Deployability,update,update,17,"@mcovarr Need to update the readme and changelog, thanks for the reminder :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1652#issuecomment-259157871
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538:347,Deployability,configurat,configuration,347,"IMO if we put any security recommendations in our README we should be very careful to disclaimer it. **Heavily**. Maybe with something along the lines of:. ```; Warning! ; - Only YOU are responsible for your own security! ; - Cromwell is NOT a security appliance! ; - What follows are ideas and starting points and not necessarily a secure system configuration in your situation""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538:347,Modifiability,config,configuration,347,"IMO if we put any security recommendations in our README we should be very careful to disclaimer it. **Heavily**. Maybe with something along the lines of:. ```; Warning! ; - Only YOU are responsible for your own security! ; - Cromwell is NOT a security appliance! ; - What follows are ideas and starting points and not necessarily a secure system configuration in your situation""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538:18,Security,secur,security,18,"IMO if we put any security recommendations in our README we should be very careful to disclaimer it. **Heavily**. Maybe with something along the lines of:. ```; Warning! ; - Only YOU are responsible for your own security! ; - Cromwell is NOT a security appliance! ; - What follows are ideas and starting points and not necessarily a secure system configuration in your situation""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538:212,Security,secur,security,212,"IMO if we put any security recommendations in our README we should be very careful to disclaimer it. **Heavily**. Maybe with something along the lines of:. ```; Warning! ; - Only YOU are responsible for your own security! ; - Cromwell is NOT a security appliance! ; - What follows are ideas and starting points and not necessarily a secure system configuration in your situation""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538:244,Security,secur,security,244,"IMO if we put any security recommendations in our README we should be very careful to disclaimer it. **Heavily**. Maybe with something along the lines of:. ```; Warning! ; - Only YOU are responsible for your own security! ; - Cromwell is NOT a security appliance! ; - What follows are ideas and starting points and not necessarily a secure system configuration in your situation""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538:333,Security,secur,secure,333,"IMO if we put any security recommendations in our README we should be very careful to disclaimer it. **Heavily**. Maybe with something along the lines of:. ```; Warning! ; - Only YOU are responsible for your own security! ; - Cromwell is NOT a security appliance! ; - What follows are ideas and starting points and not necessarily a secure system configuration in your situation""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259256319:140,Security,secur,security,140,"I'm with @cjllanwarne in favouring some form of disclaimer on this, wanting as little as you to be or feel actually responsible for others' security!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259256319
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:956,Deployability,update,update,956,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:78,Security,hash,hashed,78,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:527,Security,secur,security,527,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:641,Security,secur,security,641,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:801,Security,secur,security,801,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:252,Usability,clear,clear,252,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259488332:70,Security,Secur,SecurityRecommendations,70,Discussed name of the file with @katevoss and we're going to go with `SecurityRecommendations.md`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259488332
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259814290:42,Availability,down,downside,42,"This is great, thanks @delocalizer !. The downside of you doing those followups is that now I don't have the open PR staring me in the face reminding me I have to do something ;) I'll try to put in some content re Firecloud in the near future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259814290
https://github.com/broadinstitute/cromwell/issues/1654#issuecomment-259459453:147,Integrability,wrap,wrapped,147,"Hi @kcibul it's about consolidating all of our various devops-y things into a more unified manner. Ideally we'd like to have the Push To DockerHub wrapped into our Jenkins. @hjfbynara can explain more, but from a strategic point, unifying how we move our Dockers around is a good thing, particularly for security and accountability. You can all meet but we're trying to make our DevOps work more unified. Please find me for this meeting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1654#issuecomment-259459453
https://github.com/broadinstitute/cromwell/issues/1654#issuecomment-259459453:304,Security,secur,security,304,"Hi @kcibul it's about consolidating all of our various devops-y things into a more unified manner. Ideally we'd like to have the Push To DockerHub wrapped into our Jenkins. @hjfbynara can explain more, but from a strategic point, unifying how we move our Dockers around is a good thing, particularly for security and accountability. You can all meet but we're trying to make our DevOps work more unified. Please find me for this meeting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1654#issuecomment-259459453
https://github.com/broadinstitute/cromwell/pull/1661#issuecomment-259710497:14,Availability,error,erroring,14,"`sbt doc` was erroring out, preventing `sbt publish` from running and pushing artifacts to artifactory. Now `sbt doc` is fixed and continuously tested.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1661#issuecomment-259710497
https://github.com/broadinstitute/cromwell/pull/1661#issuecomment-259710497:131,Deployability,continuous,continuously,131,"`sbt doc` was erroring out, preventing `sbt publish` from running and pushing artifacts to artifactory. Now `sbt doc` is fixed and continuously tested.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1661#issuecomment-259710497
https://github.com/broadinstitute/cromwell/pull/1661#issuecomment-259710497:144,Testability,test,tested,144,"`sbt doc` was erroring out, preventing `sbt publish` from running and pushing artifacts to artifactory. Now `sbt doc` is fixed and continuously tested.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1661#issuecomment-259710497
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260480237:8,Testability,mock,mock-jes,8,What is mock-jes? Just curious.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260480237
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636:209,Performance,load,load,209,"@seandavi Ah, it's something @kcibul whipped up which runs on google app engine which presents the JES API for trivial tasks - we've been using it to be able to run things which test the cromwell engine under load w/o having need to run up a large bill (or run into quota issues!) on JES. I've been starting to use it heavily and have run into some weird issues, such as this ticket and those unexpected actor death notifications from the other issue. I've been theorizing that they're due to responses from appengine which we don't see in JES but i need to verify that - and clearly that's not the case w/ the unexpected actor death ones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636:178,Testability,test,test,178,"@seandavi Ah, it's something @kcibul whipped up which runs on google app engine which presents the JES API for trivial tasks - we've been using it to be able to run things which test the cromwell engine under load w/o having need to run up a large bill (or run into quota issues!) on JES. I've been starting to use it heavily and have run into some weird issues, such as this ticket and those unexpected actor death notifications from the other issue. I've been theorizing that they're due to responses from appengine which we don't see in JES but i need to verify that - and clearly that's not the case w/ the unexpected actor death ones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636:576,Usability,clear,clearly,576,"@seandavi Ah, it's something @kcibul whipped up which runs on google app engine which presents the JES API for trivial tasks - we've been using it to be able to run things which test the cromwell engine under load w/o having need to run up a large bill (or run into quota issues!) on JES. I've been starting to use it heavily and have run into some weird issues, such as this ticket and those unexpected actor death notifications from the other issue. I've been theorizing that they're due to responses from appengine which we don't see in JES but i need to verify that - and clearly that's not the case w/ the unexpected actor death ones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225:237,Availability,down,down,237,"Forgot to update this. I'm fairly certain of two things:; - This is an artifact of using mock jes; - There's a subtle bug somewhere. I've not been able to replicate this. I'm still not sure _what_ happened exactly nor why but I will jot down what I saw in case this comes up again. There were 2 jobs out of the 20k scatters which found their way back into the engine with FailedRetryable errors. In the code there are only 2 places where those are created and both involve preemption. On the engine side at the moment there's a direct assumption when this happens that the job was indeed preempted. I'm not certain how exactly this led to wacky behavior (and admittedly don't completely remember the details) but it appeared that the original ""preempted"" jobs did in fact complete and the preempted jobs never ran. In the DB the errors were CromwellFatalErrors w/ 500 messages in them. This should be impossible considering that when these FailedRetyrable things are created they're stuffed with a preempted error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225:388,Availability,error,errors,388,"Forgot to update this. I'm fairly certain of two things:; - This is an artifact of using mock jes; - There's a subtle bug somewhere. I've not been able to replicate this. I'm still not sure _what_ happened exactly nor why but I will jot down what I saw in case this comes up again. There were 2 jobs out of the 20k scatters which found their way back into the engine with FailedRetryable errors. In the code there are only 2 places where those are created and both involve preemption. On the engine side at the moment there's a direct assumption when this happens that the job was indeed preempted. I'm not certain how exactly this led to wacky behavior (and admittedly don't completely remember the details) but it appeared that the original ""preempted"" jobs did in fact complete and the preempted jobs never ran. In the DB the errors were CromwellFatalErrors w/ 500 messages in them. This should be impossible considering that when these FailedRetyrable things are created they're stuffed with a preempted error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225:829,Availability,error,errors,829,"Forgot to update this. I'm fairly certain of two things:; - This is an artifact of using mock jes; - There's a subtle bug somewhere. I've not been able to replicate this. I'm still not sure _what_ happened exactly nor why but I will jot down what I saw in case this comes up again. There were 2 jobs out of the 20k scatters which found their way back into the engine with FailedRetryable errors. In the code there are only 2 places where those are created and both involve preemption. On the engine side at the moment there's a direct assumption when this happens that the job was indeed preempted. I'm not certain how exactly this led to wacky behavior (and admittedly don't completely remember the details) but it appeared that the original ""preempted"" jobs did in fact complete and the preempted jobs never ran. In the DB the errors were CromwellFatalErrors w/ 500 messages in them. This should be impossible considering that when these FailedRetyrable things are created they're stuffed with a preempted error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225:1008,Availability,error,error,1008,"Forgot to update this. I'm fairly certain of two things:; - This is an artifact of using mock jes; - There's a subtle bug somewhere. I've not been able to replicate this. I'm still not sure _what_ happened exactly nor why but I will jot down what I saw in case this comes up again. There were 2 jobs out of the 20k scatters which found their way back into the engine with FailedRetryable errors. In the code there are only 2 places where those are created and both involve preemption. On the engine side at the moment there's a direct assumption when this happens that the job was indeed preempted. I'm not certain how exactly this led to wacky behavior (and admittedly don't completely remember the details) but it appeared that the original ""preempted"" jobs did in fact complete and the preempted jobs never ran. In the DB the errors were CromwellFatalErrors w/ 500 messages in them. This should be impossible considering that when these FailedRetyrable things are created they're stuffed with a preempted error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225:10,Deployability,update,update,10,"Forgot to update this. I'm fairly certain of two things:; - This is an artifact of using mock jes; - There's a subtle bug somewhere. I've not been able to replicate this. I'm still not sure _what_ happened exactly nor why but I will jot down what I saw in case this comes up again. There were 2 jobs out of the 20k scatters which found their way back into the engine with FailedRetryable errors. In the code there are only 2 places where those are created and both involve preemption. On the engine side at the moment there's a direct assumption when this happens that the job was indeed preempted. I'm not certain how exactly this led to wacky behavior (and admittedly don't completely remember the details) but it appeared that the original ""preempted"" jobs did in fact complete and the preempted jobs never ran. In the DB the errors were CromwellFatalErrors w/ 500 messages in them. This should be impossible considering that when these FailedRetyrable things are created they're stuffed with a preempted error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225:868,Integrability,message,messages,868,"Forgot to update this. I'm fairly certain of two things:; - This is an artifact of using mock jes; - There's a subtle bug somewhere. I've not been able to replicate this. I'm still not sure _what_ happened exactly nor why but I will jot down what I saw in case this comes up again. There were 2 jobs out of the 20k scatters which found their way back into the engine with FailedRetryable errors. In the code there are only 2 places where those are created and both involve preemption. On the engine side at the moment there's a direct assumption when this happens that the job was indeed preempted. I'm not certain how exactly this led to wacky behavior (and admittedly don't completely remember the details) but it appeared that the original ""preempted"" jobs did in fact complete and the preempted jobs never ran. In the DB the errors were CromwellFatalErrors w/ 500 messages in them. This should be impossible considering that when these FailedRetyrable things are created they're stuffed with a preempted error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225:89,Testability,mock,mock,89,"Forgot to update this. I'm fairly certain of two things:; - This is an artifact of using mock jes; - There's a subtle bug somewhere. I've not been able to replicate this. I'm still not sure _what_ happened exactly nor why but I will jot down what I saw in case this comes up again. There were 2 jobs out of the 20k scatters which found their way back into the engine with FailedRetryable errors. In the code there are only 2 places where those are created and both involve preemption. On the engine side at the moment there's a direct assumption when this happens that the job was indeed preempted. I'm not certain how exactly this led to wacky behavior (and admittedly don't completely remember the details) but it appeared that the original ""preempted"" jobs did in fact complete and the preempted jobs never ran. In the DB the errors were CromwellFatalErrors w/ 500 messages in them. This should be impossible considering that when these FailedRetyrable things are created they're stuffed with a preempted error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225
https://github.com/broadinstitute/cromwell/issues/1663#issuecomment-259755456:25,Integrability,depend,depends,25,"Yes. How it gets handled depends on what version you're using. Up until recently (I believe 22) would see the rate limit notification and just retry it. The latest version changes how we poll JES in that it is sending bulk requests over. The rate at which that happens is designed to stay below the rate limits, although if something were to still happen it'd just go back to the previous behavior and retry it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1663#issuecomment-259755456
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260097586:188,Testability,log,logging,188,"I'm pretty sure what you're seeing is ""let it crash"" (a design principal for actor based systems) in action and isn't an issue. I was just thinking a few hours ago that I should make that logging less frightening. It's on my todo list to verify that my first statement is actually true. Now I don't need to make an issue for it :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260097586
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260120082:298,Availability,alive,alive,298,"I was able to track partway back on the Google side of things. In short, Cromwell reported jobs in the ""Running"" state. These were associated with Google Genomics API operations, also listed as ""Running"". However, the instances that were supposed to be associated with those GG operations were not alive. I noticed in my billing statement that there were credits to offset compute instances, so I suspect that something happened at Google that was unexpected. I'll dig a bit further. . In the meantime, it would definitely help to have a way to differentiate exceptions that are ""expected"" from those that are not. In this case, I assumed that a restart of a service after an exception was probably OK, but it was associated with ""dead"" jobs that now appear to have been on the google side of things and not with Cromwell directly. I'll follow up with whatever else I can learn.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260120082
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260120082:872,Usability,learn,learn,872,"I was able to track partway back on the Google side of things. In short, Cromwell reported jobs in the ""Running"" state. These were associated with Google Genomics API operations, also listed as ""Running"". However, the instances that were supposed to be associated with those GG operations were not alive. I noticed in my billing statement that there were credits to offset compute instances, so I suspect that something happened at Google that was unexpected. I'll dig a bit further. . In the meantime, it would definitely help to have a way to differentiate exceptions that are ""expected"" from those that are not. In this case, I assumed that a restart of a service after an exception was probably OK, but it was associated with ""dead"" jobs that now appear to have been on the google side of things and not with Cromwell directly. I'll follow up with whatever else I can learn.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260120082
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:1000,Availability,mask,masking,1000,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:255,Performance,latency,latency,255,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:316,Safety,detect,detecting,316,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:1051,Testability,log,logging,1051,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:1332,Testability,log,logging,1332,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:1393,Testability,log,log,1393,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:51,Usability,clear,clear,51,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260142269:172,Deployability,Pipeline,Pipeline,172,"I dropped a [question into the Google Genomics google group](https://groups.google.com/forum/#!topic/google-genomics-discuss/OL18jPoPvPE) about the ""stuck"" Google Genomics Pipeline operations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260142269
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027:901,Availability,down,down,901,"Hi Jeff,. You've built this great Actor system, but it needs to be actors on both ends. The round-robin pool of actors is not an actor system anymore if you cannot pass a Promise/Future/ActorRef to the other side, even if the API/channel capacity is limited. The status response should happen in less than a second, not an hour. We both know that we can have [millions of Actors in Akka/Scala](http://doc.akka.io/docs/akka/snapshot/general/actor-systems.html#What_you_should_not_concern_yourself_with), and the throughput on the Google network is huge. Thus no API limits should be prohibitive. I suggested using Pub/Sub API here:. https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152. And if that's not an option, you can implement the whole Google Genomics Pipeline API super-easy as an ephemeral GCE instance, which really is just becomes a promise/future. I even broke it down in a [post here](https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50), specifically in the paragraph that starts with **_So you might ask what exactly is an Operation resource_**:. https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50. You know my philosophy, always build it yourself to bypass any limitations :). `p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027:784,Deployability,Pipeline,Pipeline,784,"Hi Jeff,. You've built this great Actor system, but it needs to be actors on both ends. The round-robin pool of actors is not an actor system anymore if you cannot pass a Promise/Future/ActorRef to the other side, even if the API/channel capacity is limited. The status response should happen in less than a second, not an hour. We both know that we can have [millions of Actors in Akka/Scala](http://doc.akka.io/docs/akka/snapshot/general/actor-systems.html#What_you_should_not_concern_yourself_with), and the throughput on the Google network is huge. Thus no API limits should be prohibitive. I suggested using Pub/Sub API here:. https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152. And if that's not an option, you can implement the whole Google Genomics Pipeline API super-easy as an ephemeral GCE instance, which really is just becomes a promise/future. I even broke it down in a [post here](https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50), specifically in the paragraph that starts with **_So you might ask what exactly is an Operation resource_**:. https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50. You know my philosophy, always build it yourself to bypass any limitations :). `p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027:511,Performance,throughput,throughput,511,"Hi Jeff,. You've built this great Actor system, but it needs to be actors on both ends. The round-robin pool of actors is not an actor system anymore if you cannot pass a Promise/Future/ActorRef to the other side, even if the API/channel capacity is limited. The status response should happen in less than a second, not an hour. We both know that we can have [millions of Actors in Akka/Scala](http://doc.akka.io/docs/akka/snapshot/general/actor-systems.html#What_you_should_not_concern_yourself_with), and the throughput on the Google network is huge. Thus no API limits should be prohibitive. I suggested using Pub/Sub API here:. https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152. And if that's not an option, you can implement the whole Google Genomics Pipeline API super-easy as an ephemeral GCE instance, which really is just becomes a promise/future. I even broke it down in a [post here](https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50), specifically in the paragraph that starts with **_So you might ask what exactly is an Operation resource_**:. https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50. You know my philosophy, always build it yourself to bypass any limitations :). `p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260216282:127,Deployability,Pipeline,Pipelines,127,"Thanks for the comment Paul -- we've already started those discussions with the Google folks. We want this built deep into the Pipelines API as true notifications via pubsub which seems a reasonable feature. It's possible to do what you say yourself (using a GCE node(s) to query and then post) but with 100,000s of operations in flight it's much more efficient to be notified than poll... but I'm sure you agree with that ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260216282
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260216282:352,Energy Efficiency,efficient,efficient,352,"Thanks for the comment Paul -- we've already started those discussions with the Google folks. We want this built deep into the Pipelines API as true notifications via pubsub which seems a reasonable feature. It's possible to do what you say yourself (using a GCE node(s) to query and then post) but with 100,000s of operations in flight it's much more efficient to be notified than poll... but I'm sure you agree with that ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260216282
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:1167,Availability,down,down,1167,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:419,Deployability,Pipeline,Pipeline,419,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:653,Deployability,update,updated,653,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:744,Deployability,Pipeline,Pipeline,744,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:1025,Deployability,Pipeline,Pipeline,1025,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:1038,Modifiability,evolve,evolves,1038,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:341,Performance,bottleneck,bottleneck,341,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:1073,Performance,scalab,scalability,1073,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:308,Testability,log,logging,308,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:832,Testability,log,log,832,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260527835:169,Safety,timeout,timeout,169,@seandavi Thanks. One could say though that we're not adhering to one of the tenets of distributed computing which is to make sure everything you do has some reasonable timeout where you give up and assume its dead :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260527835
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260528439:219,Availability,error,error,219,"There is a second issue that came up after Google fixed the first issue that is oddly ironic given your comment. If they hit API quotas, it appears that they were immediately failing the operations, leading to some odd error messages passed through to Cromwell (and then to me). In a sense, they were failing too quickly. Tough balancing act, it seems.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260528439
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260528439:225,Integrability,message,messages,225,"There is a second issue that came up after Google fixed the first issue that is oddly ironic given your comment. If they hit API quotas, it appears that they were immediately failing the operations, leading to some odd error messages passed through to Cromwell (and then to me). In a sense, they were failing too quickly. Tough balancing act, it seems.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260528439
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:761,Deployability,pipeline,pipelines,761,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:799,Deployability,Pipeline,Pipeline,799,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:930,Deployability,pipeline,pipelines,930,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:1086,Deployability,pipeline,pipelines,1086,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:443,Integrability,interface,interface,443,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:1180,Integrability,depend,dependent,1180,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:611,Performance,perform,performing,611,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:1264,Performance,scalab,scalable,1264,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:1248,Safety,predict,predictable,1248,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260644297:43,Availability,failure,failures,43,"@seandavi What are you seeing on the quota failures? We should be robust to that and putting things back in the to-retry pool, while our quotas are jacked pretty high we run into this as well for our larger stuff, so that's something we've needed to work around ourselves.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260644297
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260644297:66,Availability,robust,robust,66,"@seandavi What are you seeing on the quota failures? We should be robust to that and putting things back in the to-retry pool, while our quotas are jacked pretty high we run into this as well for our larger stuff, so that's something we've needed to work around ourselves.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260644297
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132:32,Availability,failure,failures,32,"Thanks, @geoffjentry. The quota failures were on the JES side, I think, and were reported back to Cromwell; I'm not sure how you would be able to detect a failure of one type versus another. Probably good to follow the conversation [here](https://groups.google.com/forum/#!topic/google-genomics-discuss/OL18jPoPvPE), as it sounds like Google is still sorting out a few things.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132:155,Availability,failure,failure,155,"Thanks, @geoffjentry. The quota failures were on the JES side, I think, and were reported back to Cromwell; I'm not sure how you would be able to detect a failure of one type versus another. Probably good to follow the conversation [here](https://groups.google.com/forum/#!topic/google-genomics-discuss/OL18jPoPvPE), as it sounds like Google is still sorting out a few things.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132:146,Safety,detect,detect,146,"Thanks, @geoffjentry. The quota failures were on the JES side, I think, and were reported back to Cromwell; I'm not sure how you would be able to detect a failure of one type versus another. Probably good to follow the conversation [here](https://groups.google.com/forum/#!topic/google-genomics-discuss/OL18jPoPvPE), as it sounds like Google is still sorting out a few things.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645161:306,Performance,scalab,scalability,306,@pgrosu I can assure you that nobody would like to see cromwell have the ability to scale to infinity and beyond on any arbitrary backend more than I :) OTOH I need to empower people to at least crawl prior to building out the running capability. Could we put together something which would provide better scalability for our purposes? Probably. However they have people who know this stuff a lot better than I do and we've got many other things on the todo list so for now the best course of action appears to be working w/ them to help parallelize our efforts.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645161
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342:65,Integrability,message,message,65,"@seandavi Gotcha, this might be another case of ""frightening log message"" if you were detecting it via cromwell, although IIRC we squelched those specifically as they were too spammy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342:86,Safety,detect,detecting,86,"@seandavi Gotcha, this might be another case of ""frightening log message"" if you were detecting it via cromwell, although IIRC we squelched those specifically as they were too spammy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342:61,Testability,log,log,61,"@seandavi Gotcha, this might be another case of ""frightening log message"" if you were detecting it via cromwell, although IIRC we squelched those specifically as they were too spammy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645830:30,Availability,failure,failures,30,"@geoffjentry, these were real failures. The jobs were marked by JES as failures, so Cromwell (correctly) did not attempt to retry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645830
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645830:71,Availability,failure,failures,71,"@geoffjentry, these were real failures. The jobs were marked by JES as failures, so Cromwell (correctly) did not attempt to retry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645830
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260646477:115,Availability,failure,failures,115,"@seandavi Yeah, was just reading the thread. That's something different than what I was picturing when I saw quota failures. I'll look into it a bit.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260646477
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260719088:750,Usability,simpl,simplify,750,"@geoffjentry I agree with crawling before running, and have definitely seen how busy you are through the evolution of Cromwell over the years, which will always be appreciated. I do not disagree that the Google folks understand how to parallelize their products/services, but that does not mean they are the only ones to understand the concepts and how to actually implement it. I agree it is not an overnight thing, as it took me many years to put the pieces together. I think by opening these discussions with the larger Community here, you might be surprised how many more people can synergistically help you and your team out. Vetting design decisions from the todo list before they approach an implementation stage, can sometimes streamline and simplify multiple requests from the list. I understand that Cromwell is a mix of features to satisfy sometimes users' legacy preferences, but if an implemented backend of Cromwell has the capability to scale in such a way as to become a paradigm shift in the way one does analysis, I am sure users would welcome that. Again I do not disagree with crawling, though I feel the Community would welcome the opportunity to help pave paths with the transition to running :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260719088
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:128,Availability,fault,fault-tolerant,128,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:1001,Availability,fault,fault-tolerant,1001,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:161,Deployability,pipeline,pipelines,161,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:470,Deployability,Continuous,Continuous,470,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:481,Deployability,Pipeline,Pipelines,481,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:580,Deployability,Pipeline,Pipelines,580,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:895,Deployability,integrat,integration,895,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:937,Deployability,pipeline,pipelined,937,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:778,Energy Efficiency,efficient,efficient-whole-exome-data-processing-using-workflows-on-the-cloud,778,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:895,Integrability,integrat,integration,895,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:143,Performance,scalab,scalable,143,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:765,Performance,scalab,scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud,765,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:879,Performance,throughput,throughput,879,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:1098,Performance,scalab,scalability,1098,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261786552:422,Modifiability,flexible,flexible,422,"@pgrosu Thanks for reminding me about that ticket - there's definitely a bug somewhere but I think it's an artifact of using mock-jes and I was never able to replicate it so it's hard to say what was going on. What I saw in the database _should_ be impossible to have achieved, joy ;). I'm definitely aware of this sort of stuff, if our goal was pure scale you'd see a pretty different design to things. The goal is to be flexible enough to respond to scale demands as they increase. To date the folks that set our priorities for us have consistently set the bar for scale to be just enough to manage what we need to handle internally - as you note this means there's always more to squeeze out. . At the moment the plan is to loop back to scaling in the next quarter, but we're still talking about what we project for Broad's sequencing production (which really means ""how big of a joint genotyping run does daniel macarthur want to run this time?"") and a projection of firecloud usage for a relatively near-mid term horizon.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261786552
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261786552:125,Testability,mock,mock-jes,125,"@pgrosu Thanks for reminding me about that ticket - there's definitely a bug somewhere but I think it's an artifact of using mock-jes and I was never able to replicate it so it's hard to say what was going on. What I saw in the database _should_ be impossible to have achieved, joy ;). I'm definitely aware of this sort of stuff, if our goal was pure scale you'd see a pretty different design to things. The goal is to be flexible enough to respond to scale demands as they increase. To date the folks that set our priorities for us have consistently set the bar for scale to be just enough to manage what we need to handle internally - as you note this means there's always more to squeeze out. . At the moment the plan is to loop back to scaling in the next quarter, but we're still talking about what we project for Broad's sequencing production (which really means ""how big of a joint genotyping run does daniel macarthur want to run this time?"") and a projection of firecloud usage for a relatively near-mid term horizon.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261786552
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:55,Availability,error,errors,55,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:116,Availability,failure,failure-recovery,116,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:741,Deployability,deploy,deployment,741,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:229,Energy Efficiency,monitor,monitoring,229,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:823,Performance,scalab,scalability,823,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:881,Performance,perform,performance,881,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:124,Safety,recover,recovery,124,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:225,Testability,log,log,225,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262136207:191,Integrability,message,message,191,"Given that the critical portion of this issue was related to a google bug that has been resolved, I'm happy to close the issue. The original report was about the ""unexpected actor death"" log message, but that turned out to be a red herring relative to the underlying google bug. @geoffjentry, I'll leave it up to you as to whether leaving this issue open serves a purpose to you, but my original problem is solved, I believe.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262136207
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262136207:187,Testability,log,log,187,"Given that the critical portion of this issue was related to a google bug that has been resolved, I'm happy to close the issue. The original report was about the ""unexpected actor death"" log message, but that turned out to be a red herring relative to the underlying google bug. @geoffjentry, I'll leave it up to you as to whether leaving this issue open serves a purpose to you, but my original problem is solved, I believe.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262136207
https://github.com/broadinstitute/cromwell/issues/1666#issuecomment-260670462:239,Modifiability,flexible,flexible,239,"@seandavi To throw out another possibility would you be just as (or even more) happy w/ user defined labeling? We've discussed both and personally I've been leaning towards the latter as it seems like it could cover the former and is more flexible. Either way it might be a bit (or not, one never knows) - the reason this doesn't currently exist is our one primary internal use case for batch submit is firecloud and they're already managing logical collections of workflows using their own data model, so one batch submit might represent multiple logical collections.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1666#issuecomment-260670462
https://github.com/broadinstitute/cromwell/issues/1666#issuecomment-260670462:442,Testability,log,logical,442,"@seandavi To throw out another possibility would you be just as (or even more) happy w/ user defined labeling? We've discussed both and personally I've been leaning towards the latter as it seems like it could cover the former and is more flexible. Either way it might be a bit (or not, one never knows) - the reason this doesn't currently exist is our one primary internal use case for batch submit is firecloud and they're already managing logical collections of workflows using their own data model, so one batch submit might represent multiple logical collections.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1666#issuecomment-260670462
https://github.com/broadinstitute/cromwell/issues/1666#issuecomment-260670462:548,Testability,log,logical,548,"@seandavi To throw out another possibility would you be just as (or even more) happy w/ user defined labeling? We've discussed both and personally I've been leaning towards the latter as it seems like it could cover the former and is more flexible. Either way it might be a bit (or not, one never knows) - the reason this doesn't currently exist is our one primary internal use case for batch submit is firecloud and they're already managing logical collections of workflows using their own data model, so one batch submit might represent multiple logical collections.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1666#issuecomment-260670462
https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-260833983:49,Deployability,patch,patch,49,"Once this review is complete, I'll back port the patch to the actual hotfix branches `0.21_hotfix` and `0.22_hotfix`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-260833983
https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-260833983:69,Deployability,hotfix,hotfix,69,"Once this review is complete, I'll back port the patch to the actual hotfix branches `0.21_hotfix` and `0.22_hotfix`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-260833983
https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-261033017:239,Deployability,release,released,239,"They'll look like the two tags currently in https://hub.docker.com/r/broadinstitute/cromwell/tags/ that are:; - `23-0c25192`; - `23`. Note to future viewers: I'll be deleting the git hashed tag, but 23 should re-appear when 23 is actually released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-261033017
https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-261033017:183,Security,hash,hashed,183,"They'll look like the two tags currently in https://hub.docker.com/r/broadinstitute/cromwell/tags/ that are:; - `23-0c25192`; - `23`. Note to future viewers: I'll be deleting the git hashed tag, but 23 should re-appear when 23 is actually released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-261033017
https://github.com/broadinstitute/cromwell/issues/1669#issuecomment-261998434:35,Integrability,message,message,35,"Did you see that ""token reclaimed"" message or not?. If not, we might also want to add some deathwatching around the JABJEA so that at least we don't run forever if it crashes",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1669#issuecomment-261998434
https://github.com/broadinstitute/cromwell/issues/1669#issuecomment-325686560:53,Usability,clear,clear,53,IMHO since the instructions to reproduce this are so clear it might be more appropriate for us to check this rather than putting that on @kcibul. It might even be something for the current bug rotation person to look at... 🙂,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1669#issuecomment-325686560
https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-326330274:27,Performance,cache,cache,27,"I see, so in order to call cache on an old workflow Cromwell has to dig into the database. . - Effort: **?**; - Risk: **Medium**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-326330274
https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-326330274:112,Safety,Risk,Risk,112,"I see, so in order to call cache on an old workflow Cromwell has to dig into the database. . - Effort: **?**; - Risk: **Medium**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-326330274
https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-327930116:111,Performance,cache,cache-hit,111,"As a **workflow runner**, I want **to selectively invalidate a workflow so that Cromwell does not use it for a cache-hit**, so that I can **not use bad or old workflow results in my new workflows**.; - Effort: **Medium**; - Risk: **Medium**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-327930116
https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-327930116:224,Safety,Risk,Risk,224,"As a **workflow runner**, I want **to selectively invalidate a workflow so that Cromwell does not use it for a cache-hit**, so that I can **not use bad or old workflow results in my new workflows**.; - Effort: **Medium**; - Risk: **Medium**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-327930116
https://github.com/broadinstitute/cromwell/issues/1674#issuecomment-325661331:77,Integrability,message,message,77,"@cjllanwarne So if a runtime attribute is not supported, there is no warning message? What happens instead?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1674#issuecomment-325661331
https://github.com/broadinstitute/cromwell/issues/1677#issuecomment-304958057:58,Deployability,update,update,58,"This is related to Custom labels (being able to apply and update them at different times in the workflow) and custom retry strategies, #1847 & #2253",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1677#issuecomment-304958057
https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-329023606:598,Integrability,message,message,598,"@katevoss iirc we can't do anything with it directly (as i said, pubsub itself has been around for ages), but would need papi to be smart about it. I think there's a ticket, and sine you said you saw it in the tracker that's likely true. also note that what @Horneth said is true. i've resisted this request for quite some time because i don't like the idea of wiring in email sending into cromwell. I wouldn't be opposed to adding a service to the service registry for workflow notifications where we provide two out of the box implementations - one which is just a Noop and one which will push a message to a pubsub topic and then users can do whatever they want with it. Similarly if a user *really* wants to have email notifications they can write their own implementation and plug it in.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-329023606
https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290:479,Availability,error,error,479,"@geoffjentry Is there a mechanism for pub/sub when running Cromwell in Server mode? We're moving to running Cromwell more as a service and less interaction-y, but we're hoping for a way that we can find out about job status changes without writing a wrapper and polling the API every 5 seconds or so. / moderately related. Is there a way Cromwell can pub/sub for certain issues. If a Slurm job fails, I was hoping Cromwell could be notified that this has happened and relate the error back up the chain. Best I can come up with is submitting an `afternotok:jobid` dependent slurm job to write a non-zero rc file to where it's supposed to be.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290
https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290:250,Integrability,wrap,wrapper,250,"@geoffjentry Is there a mechanism for pub/sub when running Cromwell in Server mode? We're moving to running Cromwell more as a service and less interaction-y, but we're hoping for a way that we can find out about job status changes without writing a wrapper and polling the API every 5 seconds or so. / moderately related. Is there a way Cromwell can pub/sub for certain issues. If a Slurm job fails, I was hoping Cromwell could be notified that this has happened and relate the error back up the chain. Best I can come up with is submitting an `afternotok:jobid` dependent slurm job to write a non-zero rc file to where it's supposed to be.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290
https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290:564,Integrability,depend,dependent,564,"@geoffjentry Is there a mechanism for pub/sub when running Cromwell in Server mode? We're moving to running Cromwell more as a service and less interaction-y, but we're hoping for a way that we can find out about job status changes without writing a wrapper and polling the API every 5 seconds or so. / moderately related. Is there a way Cromwell can pub/sub for certain issues. If a Slurm job fails, I was hoping Cromwell could be notified that this has happened and relate the error back up the chain. Best I can come up with is submitting an `afternotok:jobid` dependent slurm job to write a non-zero rc file to where it's supposed to be.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290
https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483902912:437,Integrability,depend,dependency,437,"For the SFS backend (as I understand it), Cromwell looks for the presence of an `rc` file to determine whether the job has succeeded or failed. However, Slurm may terminate the job if it runs overtime or over memory and Cromwell will hang indefinitely. I was hoping there might be a way that Slurm could notify Cromwell that the job had failed. . Only suggestion I've got at the moment is submitting a secondary job with a `afternotok:` dependency, and if the original job fails, the secondary job will write a non-zero `rc` file into the execution directory so Cromwell knows that something has failed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483902912
https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-1508872775:315,Deployability,deploy,deploy,315,"Yeah, that Terra email notification is awesome. For methods developers, we rely on our own Cromwell servers, which will benefit from this requested feature.; In addition, in Cromwell-as-an-app (Azure, AoU workbench, etc), this feature will be handy as well. BTW, if there's some DSP-provided product/service we can deploy together with our Cromwell server (similarly on Azure and AoU), that'd be great too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-1508872775
https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-1508942159:47,Security,expose,expose,47,"It's possible that CaaA will both (1) directly expose the Cromwell API and (2) allow hooking in to Terra services such as email. . I filed tickets for this (requires Broad login): [CaaA direct access](https://broadworkbench.atlassian.net/browse/WM-1909), [CaaA email notifs](https://broadworkbench.atlassian.net/browse/WM-1910). Since both of those stories refer to future work in not-Cromwell, I'm going to close this issue for now and we can continue the conversation there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-1508942159
https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-1508942159:193,Security,access,access,193,"It's possible that CaaA will both (1) directly expose the Cromwell API and (2) allow hooking in to Terra services such as email. . I filed tickets for this (requires Broad login): [CaaA direct access](https://broadworkbench.atlassian.net/browse/WM-1909), [CaaA email notifs](https://broadworkbench.atlassian.net/browse/WM-1910). Since both of those stories refer to future work in not-Cromwell, I'm going to close this issue for now and we can continue the conversation there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-1508942159
https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-1508942159:172,Testability,log,login,172,"It's possible that CaaA will both (1) directly expose the Cromwell API and (2) allow hooking in to Terra services such as email. . I filed tickets for this (requires Broad login): [CaaA direct access](https://broadworkbench.atlassian.net/browse/WM-1909), [CaaA email notifs](https://broadworkbench.atlassian.net/browse/WM-1910). Since both of those stories refer to future work in not-Cromwell, I'm going to close this issue for now and we can continue the conversation there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-1508942159
https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-262358206:137,Security,access,accessing,137,"@geoffjentry , I misspoke. We are, I believe, using the REST API for all Cromwell info and expect to continue to do that. We will not be accessing the DB directly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-262358206
https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-326331514:127,Safety,risk,risk,127,"@geoffjentry sounds like the workflow notes would be another endpoint, is that right? If so, what would you say the effort and risk is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-326331514
https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-326660895:359,Availability,down,down,359,"low and low, other than ""it's another thing to maintain, document, and such"". one thing i'll throw out there, one could do this if there maximum desired note size was smaller than the maximum label size. At the moment that number is 63 characters. . Note (pardon the pun) that just increasing that limit comes with other headaches but we can wait until we go down that path before discussing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-326660895
https://github.com/broadinstitute/cromwell/pull/1682#issuecomment-262129570:64,Availability,repair,repaired,64,👍 after comments are addressed and the _non-centaur_ builds are repaired. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1682/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682#issuecomment-262129570
https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953:217,Availability,error,error,217,"As a **user running workflows and setting up configs**, I want **Cromwell to (fail nicely?) when I reference a pluggable backend class that's not on the classpath**, so that I can **(still run my workflow? get a nice error message?)**. @geoffjentry ; - Effort: **Small?**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953
https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953:223,Integrability,message,message,223,"As a **user running workflows and setting up configs**, I want **Cromwell to (fail nicely?) when I reference a pluggable backend class that's not on the classpath**, so that I can **(still run my workflow? get a nice error message?)**. @geoffjentry ; - Effort: **Small?**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953
https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953:45,Modifiability,config,configs,45,"As a **user running workflows and setting up configs**, I want **Cromwell to (fail nicely?) when I reference a pluggable backend class that's not on the classpath**, so that I can **(still run my workflow? get a nice error message?)**. @geoffjentry ; - Effort: **Small?**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953
https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953:275,Safety,Risk,Risk,275,"As a **user running workflows and setting up configs**, I want **Cromwell to (fail nicely?) when I reference a pluggable backend class that's not on the classpath**, so that I can **(still run my workflow? get a nice error message?)**. @geoffjentry ; - Effort: **Small?**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953
https://github.com/broadinstitute/cromwell/issues/1684#issuecomment-326406438:124,Security,authenticat,authentication,124,The impact is on all workflows not just sub. The issue is that in a multi-backend world supporting different filesystems or authentication mechanisms it is likely that the current implementation of workflow outputs copying would break.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1684#issuecomment-326406438
https://github.com/broadinstitute/cromwell/pull/1686#issuecomment-261366667:166,Usability,simpl,simpler,166,@cjllanwarne re retry: i went back and forth on that topic and managed to come up with an argument I felt was valid the other way. This definition ended up being far simpler so I chose the short path.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1686#issuecomment-261366667
https://github.com/broadinstitute/cromwell/issues/1689#issuecomment-261650310:193,Availability,echo,echo,193,"As far as I know it's never been tested so I told @takutosato that it probably wasn't working yet, but a quick test with something like. ```; String a = ""hello"". task t {; String i; command {; echo ${i}; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t {input: i = a }; }; ```. proved me wrong so apparently yes it's live !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1689#issuecomment-261650310
https://github.com/broadinstitute/cromwell/issues/1689#issuecomment-261650310:33,Testability,test,tested,33,"As far as I know it's never been tested so I told @takutosato that it probably wasn't working yet, but a quick test with something like. ```; String a = ""hello"". task t {; String i; command {; echo ${i}; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t {input: i = a }; }; ```. proved me wrong so apparently yes it's live !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1689#issuecomment-261650310
https://github.com/broadinstitute/cromwell/issues/1689#issuecomment-261650310:111,Testability,test,test,111,"As far as I know it's never been tested so I told @takutosato that it probably wasn't working yet, but a quick test with something like. ```; String a = ""hello"". task t {; String i; command {; echo ${i}; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t {input: i = a }; }; ```. proved me wrong so apparently yes it's live !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1689#issuecomment-261650310
https://github.com/broadinstitute/cromwell/issues/1690#issuecomment-261690558:75,Deployability,release,release,75,"@ruchim Just fyi, the workflows ran fine with preemptions occuring in 0.21 release",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1690#issuecomment-261690558
https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546:219,Availability,error,error,219,"I found two logback.xml files in cromwell. They were the exact same, but my PR only updates the one under engine. Is because I did not update the other one as well - the reason that travis is failing with the following error?. [error] 1 error was encountered during merge; …java.lang.RuntimeException: deduplicate: different file contents found in the following:; logback.xml; logback.xml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546
https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546:228,Availability,error,error,228,"I found two logback.xml files in cromwell. They were the exact same, but my PR only updates the one under engine. Is because I did not update the other one as well - the reason that travis is failing with the following error?. [error] 1 error was encountered during merge; …java.lang.RuntimeException: deduplicate: different file contents found in the following:; logback.xml; logback.xml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546
https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546:237,Availability,error,error,237,"I found two logback.xml files in cromwell. They were the exact same, but my PR only updates the one under engine. Is because I did not update the other one as well - the reason that travis is failing with the following error?. [error] 1 error was encountered during merge; …java.lang.RuntimeException: deduplicate: different file contents found in the following:; logback.xml; logback.xml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546
https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546:84,Deployability,update,updates,84,"I found two logback.xml files in cromwell. They were the exact same, but my PR only updates the one under engine. Is because I did not update the other one as well - the reason that travis is failing with the following error?. [error] 1 error was encountered during merge; …java.lang.RuntimeException: deduplicate: different file contents found in the following:; logback.xml; logback.xml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546
https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546:135,Deployability,update,update,135,"I found two logback.xml files in cromwell. They were the exact same, but my PR only updates the one under engine. Is because I did not update the other one as well - the reason that travis is failing with the following error?. [error] 1 error was encountered during merge; …java.lang.RuntimeException: deduplicate: different file contents found in the following:; logback.xml; logback.xml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546
https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546:12,Testability,log,logback,12,"I found two logback.xml files in cromwell. They were the exact same, but my PR only updates the one under engine. Is because I did not update the other one as well - the reason that travis is failing with the following error?. [error] 1 error was encountered during merge; …java.lang.RuntimeException: deduplicate: different file contents found in the following:; logback.xml; logback.xml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546
https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546:364,Testability,log,logback,364,"I found two logback.xml files in cromwell. They were the exact same, but my PR only updates the one under engine. Is because I did not update the other one as well - the reason that travis is failing with the following error?. [error] 1 error was encountered during merge; …java.lang.RuntimeException: deduplicate: different file contents found in the following:; logback.xml; logback.xml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546
https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546:377,Testability,log,logback,377,"I found two logback.xml files in cromwell. They were the exact same, but my PR only updates the one under engine. Is because I did not update the other one as well - the reason that travis is failing with the following error?. [error] 1 error was encountered during merge; …java.lang.RuntimeException: deduplicate: different file contents found in the following:; logback.xml; logback.xml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546
https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261995390:43,Testability,log,logs,43,"Actually, micro concern: does our workflow-logs copier need updating to reflect more than 1 Cromwell log file? (I don't know who might know the answer to this)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261995390
https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261995390:101,Testability,log,log,101,"Actually, micro concern: does our workflow-logs copier need updating to reflect more than 1 Cromwell log file? (I don't know who might know the answer to this)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261995390
https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-262028968:27,Testability,log,logback,27,Workflows create their own logback appender programmatically so they shouldn't be affected by this ? @kshakir should know I think.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-262028968
https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-262056362:87,Testability,log,logback,87,"ToL: In a future life, I would like to see if we can fix the issue requiring duplicate logback.xml files. For now, LGTM 👍",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-262056362
https://github.com/broadinstitute/cromwell/pull/1693#issuecomment-261994351:139,Usability,simpl,simple,139,"LGTM. Some sort of comment in the docs about when/why I'd want to use this might be nice. My thinking is, if I just want to ""try it out as simple as possible"" I probably don't want to be scared off by all these options?. Anyway, that's just a ToL so 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1693/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1693#issuecomment-261994351
https://github.com/broadinstitute/cromwell/issues/1694#issuecomment-262027405:42,Availability,avail,available,42,"It would be nice to have this information available from the command line too, i.e. a -version flag that dumps the version/hash and exits.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1694#issuecomment-262027405
https://github.com/broadinstitute/cromwell/issues/1694#issuecomment-262027405:123,Security,hash,hash,123,"It would be nice to have this information available from the command line too, i.e. a -version flag that dumps the version/hash and exits.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1694#issuecomment-262027405
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023:78,Performance,cache,cache,78,"As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **?**. @LeeTL1220 to help with this. @geoffjentry thoughts on the following?; - Effort: **TBD**; - Risk: **TBD**; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023:96,Performance,cache,cache,96,"As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **?**. @LeeTL1220 to help with this. @geoffjentry thoughts on the following?; - Effort: **TBD**; - Risk: **TBD**; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023:221,Safety,Risk,Risk,221,"As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **?**. @LeeTL1220 to help with this. @geoffjentry thoughts on the following?; - Effort: **TBD**; - Risk: **TBD**; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327550937:122,Safety,risk,risk,122,With the add-on that this should *not* find its way into runtime attributes .... This should be relatively low effort and risk. The risk would be that whenever we don't get something call caching related correct that causes a lot of angst,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327550937
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327550937:132,Safety,risk,risk,132,With the add-on that this should *not* find its way into runtime attributes .... This should be relatively low effort and risk. The risk would be that whenever we don't get something call caching related correct that causes a lot of angst,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327550937
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:169,Performance,cache,cache,169,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:187,Performance,cache,cache,187,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:240,Performance,cache,cache,240,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:215,Safety,avoid,avoid,215,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:282,Safety,Risk,Risk,282,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:446,Usability,clear,clearly,446,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823:718,Energy Efficiency,adapt,adapt,718,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823:718,Modifiability,adapt,adapt,718,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823:162,Performance,optimiz,optimizations,162,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-1190685987:14,Availability,mainten,maintenance,14,draft-2 is in maintenance only mode and does not receive new features.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-1190685987
https://github.com/broadinstitute/cromwell/issues/1702#issuecomment-331019025:30,Deployability,release,release,30,"@katevoss I've re-tested with release 29 and this works as expected now — and may have been working for some time, I haven't revisited this for a while. Thanks to all for the fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702#issuecomment-331019025
https://github.com/broadinstitute/cromwell/issues/1702#issuecomment-331019025:18,Testability,test,tested,18,"@katevoss I've re-tested with release 29 and this works as expected now — and may have been working for some time, I haven't revisited this for a while. Thanks to all for the fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702#issuecomment-331019025
https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263436694:175,Modifiability,extend,extend,175,"Sorry that I missed this over the Thanksgiving break. This is awesome. Personally I would favour a `JsObject` containing `left` and `right` members. I agree that this doesn't extend well to 3,4,5-tuples, etc, but I suspect for anything above 2 we'd want some sort of generic Tuple type which could have it's own rules for objectifying. In that case I could imagine a Tuple which happened to be length 2 might objectify differently from a Pair.... And IMO that'd be fine. But I'm not the only one with an opinion, I'm sure! @kcibul, @geoffjentry?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263436694
https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263437160:178,Testability,test,test,178,Two other comments ; - Could you add a ticket to allow the same style of JSON to be used for Pair inputs (whatever we decide here re `JsArray`s vs `JsObject`s; - Could you add a test case?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263437160
https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263444662:342,Usability,simpl,simply,342,"@cjllanwarne @delocalizer I don't have a strong opinion either way although I think left/right looks nicer. I'm not concerned about expandability into tuples. Chris what you said is pretty much exactly what we laid out in the ""but what if we want tuples?"" discussion when talking about Pairs. My take on that is if that happens then the pair simply becomes semantic sugar over a tuple2 at which point we can map left/right to whatever representation a tuple2 uses.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263444662
https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263474909:23,Testability,test,test,23,@cjllanwarne I added a test - is that the kind of thing you were after?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263474909
https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263932322:28,Testability,test,tests,28,"Reminder, while some of the tests should work, the entire current Travis test suite will never pass due to #1717.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263932322
https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263932322:73,Testability,test,test,73,"Reminder, while some of the tests should work, the entire current Travis test suite will never pass due to #1717.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263932322
https://github.com/broadinstitute/cromwell/issues/1705#issuecomment-267833828:115,Usability,learn,learning,115,"Closing this for now due to the combo of the ""I don't know if it works like that"" i noted as well as the more I am learning the less I think it works like that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1705#issuecomment-267833828
https://github.com/broadinstitute/cromwell/pull/1709#issuecomment-263679390:113,Deployability,update,updates,113,This'll be brought in by https://github.com/broadinstitute/cromwell/pull/1676 and required some (minor) Cromwell updates. Closing this PR.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1709#issuecomment-263679390
https://github.com/broadinstitute/cromwell/pull/1714#issuecomment-263888360:12,Energy Efficiency,green,green,12,"two thumbs, green tests, and it's holding up workbench... so merging!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1714#issuecomment-263888360
https://github.com/broadinstitute/cromwell/pull/1714#issuecomment-263888360:18,Testability,test,tests,18,"two thumbs, green tests, and it's holding up workbench... so merging!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1714#issuecomment-263888360
https://github.com/broadinstitute/cromwell/pull/1718#issuecomment-264297263:0,Deployability,Update,Updated,0,Updated for code review,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1718#issuecomment-264297263
https://github.com/broadinstitute/cromwell/issues/1725#issuecomment-267833975:38,Availability,robust,robustness,38,"@kcibul - I was thinking of adding a ""robustness"" (or something like that) label for tickets like this and #1762 as these aren't really about scaling but definitely could impact the health of a cromwell server. Figured I'd leave it up to you as to a) if that's the right thing and b) the exact nomenclature if so",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1725#issuecomment-267833975
https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302:161,Deployability,release,release,161,"@geoffjentry sounds like this will be necessary in a CaaS world, do you agree?. As a **FC/GOTC developer**, I want **Cromwell to test with Cloud SQL after every release**, so that I can **avoid critical (? @helgridly a bug in Cloud SQL would be critical, right?) regressions and issues in production**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302
https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302:188,Safety,avoid,avoid,188,"@geoffjentry sounds like this will be necessary in a CaaS world, do you agree?. As a **FC/GOTC developer**, I want **Cromwell to test with Cloud SQL after every release**, so that I can **avoid critical (? @helgridly a bug in Cloud SQL would be critical, right?) regressions and issues in production**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302
https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302:328,Safety,Risk,Risk,328,"@geoffjentry sounds like this will be necessary in a CaaS world, do you agree?. As a **FC/GOTC developer**, I want **Cromwell to test with Cloud SQL after every release**, so that I can **avoid critical (? @helgridly a bug in Cloud SQL would be critical, right?) regressions and issues in production**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302
https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302:129,Testability,test,test,129,"@geoffjentry sounds like this will be necessary in a CaaS world, do you agree?. As a **FC/GOTC developer**, I want **Cromwell to test with Cloud SQL after every release**, so that I can **avoid critical (? @helgridly a bug in Cloud SQL would be critical, right?) regressions and issues in production**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302
https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328538475:99,Deployability,release,release,99,"Depending on the issue, a cloud SQL issue could cause anything from a failed migration and delayed release to data loss and restore from backup. (The first is extremely more likely.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328538475
https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328538475:0,Integrability,Depend,Depending,0,"Depending on the issue, a cloud SQL issue could cause anything from a failed migration and delayed release to data loss and restore from backup. (The first is extremely more likely.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328538475
https://github.com/broadinstitute/cromwell/issues/1736#issuecomment-264643571:112,Usability,simpl,simplest,112,Yes it was. There was internal debate on how to handle version numbering in general and dropping the 0. Was the simplest path to our original intention (plus matched how they were colloquially referred to),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1736#issuecomment-264643571
https://github.com/broadinstitute/cromwell/issues/1736#issuecomment-264643631:30,Usability,clear,clearing,30,"@geoffjentry Cool, thanks for clearing that up. I'll merge 23 to Homebrew now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1736#issuecomment-264643631
https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264853068:80,Deployability,update,update,80,Is there an equivalent for JES runtime attributes validation that could need an update as well ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264853068
https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264853068:50,Security,validat,validation,50,Is there an equivalent for JES runtime attributes validation that could need an update as well ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264853068
https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049:93,Deployability,update,update,93,"@Horneth . > Is there an equivalent for JES runtime attributes validation that could need an update as well ?. Not that I can think of. JES's runtime attributes are [hardcoded](https://github.com/broadinstitute/cromwell/blob/23/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L19-L28) into the scala code. Meanwhile, these are the declarations of runtime attributes for the Config based backend. Via the config, one can specify the list of valid runtime attributes for _your_ backend, PBS, LSF, SGE, etc. See #1737 for an example of where this was broken.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049
https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049:416,Modifiability,Config,Config,416,"@Horneth . > Is there an equivalent for JES runtime attributes validation that could need an update as well ?. Not that I can think of. JES's runtime attributes are [hardcoded](https://github.com/broadinstitute/cromwell/blob/23/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L19-L28) into the scala code. Meanwhile, these are the declarations of runtime attributes for the Config based backend. Via the config, one can specify the list of valid runtime attributes for _your_ backend, PBS, LSF, SGE, etc. See #1737 for an example of where this was broken.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049
https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049:446,Modifiability,config,config,446,"@Horneth . > Is there an equivalent for JES runtime attributes validation that could need an update as well ?. Not that I can think of. JES's runtime attributes are [hardcoded](https://github.com/broadinstitute/cromwell/blob/23/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L19-L28) into the scala code. Meanwhile, these are the declarations of runtime attributes for the Config based backend. Via the config, one can specify the list of valid runtime attributes for _your_ backend, PBS, LSF, SGE, etc. See #1737 for an example of where this was broken.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049
https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049:63,Security,validat,validation,63,"@Horneth . > Is there an equivalent for JES runtime attributes validation that could need an update as well ?. Not that I can think of. JES's runtime attributes are [hardcoded](https://github.com/broadinstitute/cromwell/blob/23/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L19-L28) into the scala code. Meanwhile, these are the declarations of runtime attributes for the Config based backend. Via the config, one can specify the list of valid runtime attributes for _your_ backend, PBS, LSF, SGE, etc. See #1737 for an example of where this was broken.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049
https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-264953339:139,Safety,timeout,timeout,139,Similarly I passed in `wdlInputs` instead of `workflowInputs` and got a giant stack trace on the cromwell side but no response (other than timeout) on the client side,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-264953339
https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-326409216:28,Security,validat,validation,28,@mcovarr is there any input validation now? Or are malformed inputs still poorly handled?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-326409216
https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-327936538:9,Availability,failure,failure,9,"Explicit failure means success:. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Unexpected character 'r' at input index 0 (line 1, position 1), expected JSON Value:\nruhroh\n^\n""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-327936538
https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-327936538:72,Availability,Error,Error,72,"Explicit failure means success:. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Unexpected character 'r' at input index 0 (line 1, position 1), expected JSON Value:\nruhroh\n^\n""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-327936538
https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-327936538:61,Integrability,message,message,61,"Explicit failure means success:. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Unexpected character 'r' at input index 0 (line 1, position 1), expected JSON Value:\nruhroh\n^\n""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-327936538
https://github.com/broadinstitute/cromwell/issues/1747#issuecomment-326420760:86,Energy Efficiency,monitor,monitoring,86,@geoffjentry Did you do anything related to dispatcher tooling for your recent Health monitoring pre-gull PR?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1747#issuecomment-326420760
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:42,Availability,ERROR,ERROR,42,And here is the exception message:. ```; [ERROR] [12/06/2016 13:58:30.046] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] Unable to create actor for ActorRef Actor[akka://; cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorPro,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:712,Availability,Fault,FaultHandling,712,And here is the exception message:. ```; [ERROR] [12/06/2016 13:58:30.046] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] Unable to create actor for ActorRef Actor[akka://; cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorPro,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:760,Availability,Fault,FaultHandling,760,And here is the exception message:. ```; [ERROR] [12/06/2016 13:58:30.046] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] Unable to create actor for ActorRef Actor[akka://; cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorPro,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:794,Availability,Fault,FaultHandling,794,And here is the exception message:. ```; [ERROR] [12/06/2016 13:58:30.046] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] Unable to create actor for ActorRef Actor[akka://; cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorPro,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2548,Deployability,configurat,configuration,2548,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:26,Integrability,message,message,26,And here is the exception message:. ```; [ERROR] [12/06/2016 13:58:30.046] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] Unable to create actor for ActorRef Actor[akka://; cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorPro,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2513,Modifiability,config,config,2513,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2520,Modifiability,Config,ConfigException,2520,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2548,Modifiability,config,configuration,2548,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2608,Modifiability,config,config,2608,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2687,Modifiability,config,config,2687,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2760,Modifiability,config,config,2760,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2836,Modifiability,config,config,2836,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2906,Modifiability,config,config,2906,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2976,Modifiability,config,config,2976,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:3051,Modifiability,config,config,3051,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:3126,Modifiability,config,config,3126,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:1175,Performance,concurren,concurrent,1175, Unable to create actor for ActorRef Actor[akka://; cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:1248,Performance,concurren,concurrent,1248,SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:1333,Performance,concurren,concurrent,1333,Exception: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$cl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:1410,Performance,concurren,concurrent,1410,ser/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKey,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2620,Usability,Simpl,SimpleConfig,2620,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2647,Usability,Simpl,SimpleConfig,2647,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2699,Usability,Simpl,SimpleConfig,2699,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2720,Usability,Simpl,SimpleConfig,2720,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2772,Usability,Simpl,SimpleConfig,2772,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2796,Usability,Simpl,SimpleConfig,2796,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2848,Usability,Simpl,SimpleConfig,2848,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2866,Usability,Simpl,SimpleConfig,2866,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2918,Usability,Simpl,SimpleConfig,2918,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2936,Usability,Simpl,SimpleConfig,2936,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2988,Usability,Simpl,SimpleConfig,2988,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:3011,Usability,Simpl,SimpleConfig,3011,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:3063,Usability,Simpl,SimpleConfig,3063,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:3086,Usability,Simpl,SimpleConfig,3086,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:3138,Usability,Simpl,SimpleConfig,3138,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:3161,Usability,Simpl,SimpleConfig,3161,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974
https://github.com/broadinstitute/cromwell/pull/1749#issuecomment-265312580:171,Modifiability,config,config,171,@kshakir That list is used to provide a warning in the log if you specify a key that isn't used. I'd prefer to either do away w/ the warning or find some way to force the config keys we use and this list to be the same but I figured my fellow cromwellians would balk at the former and I couldn't think of a non-annoying way to do the latter.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1749#issuecomment-265312580
https://github.com/broadinstitute/cromwell/pull/1749#issuecomment-265312580:55,Testability,log,log,55,@kshakir That list is used to provide a warning in the log if you specify a key that isn't used. I'd prefer to either do away w/ the warning or find some way to force the config keys we use and this list to be the same but I figured my fellow cromwellians would balk at the former and I couldn't think of a non-annoying way to do the latter.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1749#issuecomment-265312580
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190:267,Deployability,configurat,configuration,267,I came here to say what I apparently said a few months ago already :). We don't have access to a SLURM cluster so anything we put together would be a guess on our part. I know folks have been able to pretty easily get LSF & PBS working based on our example of an SGE configuration so my assumption is that it's not hard but I have no way of knowing. If someone were to get it working and submit docs we'd happily accept them but we have no way of handling that ourselves.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190:267,Modifiability,config,configuration,267,I came here to say what I apparently said a few months ago already :). We don't have access to a SLURM cluster so anything we put together would be a guess on our part. I know folks have been able to pretty easily get LSF & PBS working based on our example of an SGE configuration so my assumption is that it's not hard but I have no way of knowing. If someone were to get it working and submit docs we'd happily accept them but we have no way of handling that ourselves.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190:85,Security,access,access,85,I came here to say what I apparently said a few months ago already :). We don't have access to a SLURM cluster so anything we put together would be a guess on our part. I know folks have been able to pretty easily get LSF & PBS working based on our example of an SGE configuration so my assumption is that it's not hard but I have no way of knowing. If someone were to get it working and submit docs we'd happily accept them but we have no way of handling that ourselves.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-291606740:89,Deployability,configurat,configuration,89,@geoffjentry I am currently debugging a workflow on SLURM and can provide a beta backend configuration. Should I submit this as a merge request to `core/src/main/resources/reference.conf`?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-291606740
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-291606740:89,Modifiability,config,configuration,89,@geoffjentry I am currently debugging a workflow on SLURM and can provide a beta backend configuration. Should I submit this as a merge request to `core/src/main/resources/reference.conf`?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-291606740
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328208741:93,Usability,feedback,feedback,93,"Of the folks who have expressed in SLURM support, has anyone had a chance to try it out? Any feedback (or recommended documentation) would be great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328208741
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328209522:86,Modifiability,config,configs,86,We know that both @MatthewMah and Intel were able to use SLURM (albeit with different configs),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328209522
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:713,Availability,avail,available,713,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:106,Deployability,configurat,configuration,106,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:786,Deployability,pipeline,pipeline,786,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:373,Energy Efficiency,schedul,scheduler,373,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:536,Energy Efficiency,schedul,scheduler,536,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:106,Modifiability,config,configuration,106,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:605,Modifiability,rewrite,rewrite,605,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:314,Performance,concurren,concurrent-job-limit,314,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:518,Performance,bottleneck,bottleneck,518,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:548,Performance,concurren,concurrent-job-limit,548,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:734,Performance,concurren,concurrent-job-limit,734,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401
https://github.com/broadinstitute/cromwell/issues/1751#issuecomment-326410556:110,Availability,avail,available,110,"@kshakir Per https://github.com/broadinstitute/cromwell/pull/2547, I believe that concurrent-job-limit is now available on all backends, so this issue is null and void. Correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751#issuecomment-326410556
https://github.com/broadinstitute/cromwell/issues/1751#issuecomment-326410556:82,Performance,concurren,concurrent-job-limit,82,"@kshakir Per https://github.com/broadinstitute/cromwell/pull/2547, I believe that concurrent-job-limit is now available on all backends, so this issue is null and void. Correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751#issuecomment-326410556
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265537710:92,Deployability,release,release,92,"Is this a question, bug, or feature request? As far as I know this does already work (as of release 23). Maybe it missed the changelog though?. Check out the wdlDependencies field of the batch endpoint.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265537710
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:890,Availability,failure,failures,890,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:392,Integrability,depend,dependency,392,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:612,Integrability,depend,dependency,612,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:632,Integrability,depend,dependency,632,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:672,Integrability,depend,dependency,672,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:906,Integrability,message,message,906,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:962,Performance,load,load,962,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:328,Testability,test,test,328,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:355,Testability,test,test,355,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:559,Testability,test,test,559,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:655,Testability,test,test,655,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532
https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-268377951:53,Deployability,update,updated,53,this is fixed and the README appears to have been be updated too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-268377951
https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265341832:59,Deployability,release,release,59,@kcibul This is important. I was testing to see if the new release fixed the hanging issue previously reported #1649,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265341832
https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265341832:33,Testability,test,testing,33,@kcibul This is important. I was testing to see if the new release fixed the hanging issue previously reported #1649,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265341832
https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265532682:20,Deployability,update,updated,20,"Yup... just started updated workflow where spaces are removed. On Wed, Dec 7, 2016 at 12:53 PM, Thib <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> I'm taking a look at this, in; > the meantime if you can remove the space from the file (maybe in your; > command line) it should work around it; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265520297>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk3re5Xwc7pha3xBCkVSb5IRuqdkHks5rFvK0gaJpZM4LGJFu>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265532682
https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265560268:308,Modifiability,extend,extend,308,"I used this command for the task `run_plot_purity_series`, and got a run of the workflow to exit with succeeded for me. Ran with `cromwell-24-b3e07d2-SNAP.jar`. This new command changes the file names to get rid of spaces:. ```; command {; python <<CODE; files = ""${sep="","" amp_sens_prec}"".split("",""); files.extend(""${sep="","" del_sens_prec}"".split("","")); with open(""sens_prec_aggregate.txt"", ""w"") as fp:; 	 fp.write('\n'.join(files)); CODE; wc -l sens_prec_aggregate.txt. python <<CODE; files = ""${sep="","" small_sens}"".split("",""); with open(""small_sens_aggregate.txt"", ""w"") as fp:; 	 fp.write('\n'.join(files)); CODE; wc -l small_sens_aggregate.txt. run_plot_purity_series sens_prec_aggregate.txt small_sens_aggregate.txt /root/eval-gatk-protected/sample_purity_table.tsv purity_plots/. # Rename files to get rid of spaces; mv 'purity_plots/purity_series_small_Small Amplifications.png' 'purity_plots/purity_series_small_Small_Amplifications.png'; mv 'purity_plots/purity_series_small_Small Deletions.png' 'purity_plots/purity_series_small_Small_Deletions.png'; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265560268
https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265561480:451,Modifiability,extend,extend,451,"I just modified it in my code and WDL. No big deal. On Wed, Dec 7, 2016 at 3:13 PM, kshakir <notifications@github.com> wrote:. > I used this command for the task run_plot_purity_series, and got a run of; > the workflow to exit with succeeded for me. Ran with; > cromwell-24-b3e07d2-SNAP.jar.; >; > This new command changes the file names to get rid of spaces:; >; > command {; > python <<CODE; > files = ""${sep="","" amp_sens_prec}"".split("",""); > files.extend(""${sep="","" del_sens_prec}"".split("","")); > with open(""sens_prec_aggregate.txt"", ""w"") as fp:; > 	 fp.write('\n'.join(files)); > CODE; > wc -l sens_prec_aggregate.txt; >; > python <<CODE; > files = ""${sep="","" small_sens}"".split("",""); > with open(""small_sens_aggregate.txt"", ""w"") as fp:; > 	 fp.write('\n'.join(files)); > CODE; > wc -l small_sens_aggregate.txt; >; > run_plot_purity_series sens_prec_aggregate.txt small_sens_aggregate.txt /root/eval-gatk-protected/sample_purity_table.tsv purity_plots/; >; > # Rename files to get rid of spaces; > mv 'purity_plots/purity_series_small_Small Amplifications.png' 'purity_plots/purity_series_small_Small_Amplifications.png'; > mv 'purity_plots/purity_series_small_Small Deletions.png' 'purity_plots/purity_series_small_Small_Deletions.png'; > }; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265560268>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7v2Tv0V0uA75r0QmSydOvXRrWnvks5rFxNxgaJpZM4LGJFu>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265561480
https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-288147368:81,Testability,test,test,81,@LeeTL1220 can I close this ? Cromwell does handle spaces properly now we have a test for it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-288147368
https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-288170237:201,Testability,test,test,201,"Go for it. On Tue, Mar 21, 2017 at 1:03 PM, Thib <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> can I close this ? Cromwell; > does handle spaces properly now we have a test for it.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-288147368>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2ujJRvV-jVLyHPbgWuepDL8hELBks5roAL1gaJpZM4LGJFu>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-288170237
https://github.com/broadinstitute/cromwell/pull/1757#issuecomment-265764232:95,Energy Efficiency,green,green,95,"We have an issue with the centaurJes tests running for external users. . Assuming the tests go green for https://github.com/broadinstitute/cromwell/pull/1760, I think this one should be considered green too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1757#issuecomment-265764232
https://github.com/broadinstitute/cromwell/pull/1757#issuecomment-265764232:197,Energy Efficiency,green,green,197,"We have an issue with the centaurJes tests running for external users. . Assuming the tests go green for https://github.com/broadinstitute/cromwell/pull/1760, I think this one should be considered green too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1757#issuecomment-265764232
https://github.com/broadinstitute/cromwell/pull/1757#issuecomment-265764232:37,Testability,test,tests,37,"We have an issue with the centaurJes tests running for external users. . Assuming the tests go green for https://github.com/broadinstitute/cromwell/pull/1760, I think this one should be considered green too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1757#issuecomment-265764232
https://github.com/broadinstitute/cromwell/pull/1757#issuecomment-265764232:86,Testability,test,tests,86,"We have an issue with the centaurJes tests running for external users. . Assuming the tests go green for https://github.com/broadinstitute/cromwell/pull/1760, I think this one should be considered green too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1757#issuecomment-265764232
https://github.com/broadinstitute/cromwell/issues/1761#issuecomment-265824956:66,Testability,log,log,66,"Hrm, the most likely thing here then is that I misinterpreted the log and did something stupid :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1761#issuecomment-265824956
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266805146:152,Deployability,pipeline,pipeline,152,This seems like an excellent idea -- do you have a suggestion for how big this should be? I'd vote for several-x the size of anything the joint calling pipeline does (which might be our biggest user of this?),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266805146
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447:16,Energy Efficiency,green,greens,16,"I talked to the greens a little bit today and I'm thinking the way to go about this is per-function limits. `read_bool()` - 5 chars (""false""); `read_int()` - 19 chars (if I did my sleuthing & counting correct for MAX_INT); `read_float()` - 50 chars (made up, but maxint plus a lot of decimal places); `read_string()` - 128K (because it's 2x what CWL gives you ;) ). Where it gets tricky are the larger objects. I'm less certain on what to do here:. `read_lines()` - I'm thinking this should be the same as `read_string()`; `read_json()` - Same? I think?; `read_[tsv|map|object]()` - No idea but from talking to the greens there are certainly reasonable use cases which would be much larger than the above. It sounded like if we capped this at 1MB it'd easily cover everything they did. From a workbench safety-from-users perspective this seems like the class of functions most likely to be abused as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447:615,Energy Efficiency,green,greens,615,"I talked to the greens a little bit today and I'm thinking the way to go about this is per-function limits. `read_bool()` - 5 chars (""false""); `read_int()` - 19 chars (if I did my sleuthing & counting correct for MAX_INT); `read_float()` - 50 chars (made up, but maxint plus a lot of decimal places); `read_string()` - 128K (because it's 2x what CWL gives you ;) ). Where it gets tricky are the larger objects. I'm less certain on what to do here:. `read_lines()` - I'm thinking this should be the same as `read_string()`; `read_json()` - Same? I think?; `read_[tsv|map|object]()` - No idea but from talking to the greens there are certainly reasonable use cases which would be much larger than the above. It sounded like if we capped this at 1MB it'd easily cover everything they did. From a workbench safety-from-users perspective this seems like the class of functions most likely to be abused as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447:803,Safety,safe,safety-from-users,803,"I talked to the greens a little bit today and I'm thinking the way to go about this is per-function limits. `read_bool()` - 5 chars (""false""); `read_int()` - 19 chars (if I did my sleuthing & counting correct for MAX_INT); `read_float()` - 50 chars (made up, but maxint plus a lot of decimal places); `read_string()` - 128K (because it's 2x what CWL gives you ;) ). Where it gets tricky are the larger objects. I'm less certain on what to do here:. `read_lines()` - I'm thinking this should be the same as `read_string()`; `read_json()` - Same? I think?; `read_[tsv|map|object]()` - No idea but from talking to the greens there are certainly reasonable use cases which would be much larger than the above. It sounded like if we capped this at 1MB it'd easily cover everything they did. From a workbench safety-from-users perspective this seems like the class of functions most likely to be abused as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268774441:100,Modifiability,config,config,100,"Curious about the ""should be in the WDL spec too"" part. IMO this would be most obviously a Cromwell config file option",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268774441
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268785327:107,Modifiability,config,configurable,107,"Disagree, the point of a wdl is to provide the same result. This could radically alter a result of it were configurable, and that's exactly the reason it is specified in cwl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268785327
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268799397:488,Availability,down,down,488,"@cjllanwarne note that what I just said in #1777 could be used against my disagreement although I think handling all the different bifurcations here would be trickier than the one over there. At the end of the day the number one priority is that a WDL should always produce the exact same result regardless of where it is run (possibly excepting docker, and that's part of what rubs me the wrong way about runtime attrs). The number two priority should be that a user can't possibly take down a server w/ a rogue wdl. . After that anything can be tunable in terms of how a server achieves the first one whilst not allowing the second one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268799397
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861:89,Availability,Error,Error,89,One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861:171,Testability,log,log,171,One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861:117,Usability,feedback,feedback,117,One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364:0,Availability,Error,Error,0,"Error... not reading the whole file probably will not produce the right behavior in the pipeline being run. > On Apr 15, 2017, at 11:41 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364:282,Availability,Error,Error,282,"Error... not reading the whole file probably will not produce the right behavior in the pipeline being run. > On Apr 15, 2017, at 11:41 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364:88,Deployability,pipeline,pipeline,88,"Error... not reading the whole file probably will not produce the right behavior in the pipeline being run. > On Apr 15, 2017, at 11:41 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364:364,Testability,log,log,364,"Error... not reading the whole file probably will not produce the right behavior in the pipeline being run. > On Apr 15, 2017, at 11:41 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364:310,Usability,feedback,feedback,310,"Error... not reading the whole file probably will not produce the right behavior in the pipeline being run. > On Apr 15, 2017, at 11:41 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294311917:213,Availability,Error,Error,213,Although now that I look the CWL analogy doesn't hold up. They have a max-64K chunk attached to their File object for similar purposes as this (input into expressions) but not to enable further typing like we do. Error it is.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294311917
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294354889:199,Availability,down,downloading,199,"Ideally size would be checked prior to doing anything. . For instance: If there's a 1T file sitting on GCS and we're trying to `read_bool()` on it, it'd be nice to know that we shouldn't even bother downloading the file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294354889
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185:39,Availability,error,error,39,"Given that this is being considered an error (I'd prefer ""Failure"") rather than truncation, I'm going to try to push one more time the idea that this should be a configuration option rather than some hard-coded magic number in WDL (which will inevitable need to be raised at some point, and then will different WDL versions have different `read_string` limits even when run on the same Cromwell?). If we were truncating then the worry about different results would be correct, end of story. But what we could be saying now is ""Failure. Your WDL engine isn't able to process this workflow because the file is too large. Try increasing resources or restructuring your WDL""... not really any different from ""Failure. You didn't have enough memory to run that scatter so wide..."", or indeed any other resource constraint. This is just me TOL of course... but it does seem a lot more elegant to me than having a special case for file sizes written in the WDL spec but every other resource limit being implicit and left up to the engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185:58,Availability,Failure,Failure,58,"Given that this is being considered an error (I'd prefer ""Failure"") rather than truncation, I'm going to try to push one more time the idea that this should be a configuration option rather than some hard-coded magic number in WDL (which will inevitable need to be raised at some point, and then will different WDL versions have different `read_string` limits even when run on the same Cromwell?). If we were truncating then the worry about different results would be correct, end of story. But what we could be saying now is ""Failure. Your WDL engine isn't able to process this workflow because the file is too large. Try increasing resources or restructuring your WDL""... not really any different from ""Failure. You didn't have enough memory to run that scatter so wide..."", or indeed any other resource constraint. This is just me TOL of course... but it does seem a lot more elegant to me than having a special case for file sizes written in the WDL spec but every other resource limit being implicit and left up to the engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185:527,Availability,Failure,Failure,527,"Given that this is being considered an error (I'd prefer ""Failure"") rather than truncation, I'm going to try to push one more time the idea that this should be a configuration option rather than some hard-coded magic number in WDL (which will inevitable need to be raised at some point, and then will different WDL versions have different `read_string` limits even when run on the same Cromwell?). If we were truncating then the worry about different results would be correct, end of story. But what we could be saying now is ""Failure. Your WDL engine isn't able to process this workflow because the file is too large. Try increasing resources or restructuring your WDL""... not really any different from ""Failure. You didn't have enough memory to run that scatter so wide..."", or indeed any other resource constraint. This is just me TOL of course... but it does seem a lot more elegant to me than having a special case for file sizes written in the WDL spec but every other resource limit being implicit and left up to the engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185:705,Availability,Failure,Failure,705,"Given that this is being considered an error (I'd prefer ""Failure"") rather than truncation, I'm going to try to push one more time the idea that this should be a configuration option rather than some hard-coded magic number in WDL (which will inevitable need to be raised at some point, and then will different WDL versions have different `read_string` limits even when run on the same Cromwell?). If we were truncating then the worry about different results would be correct, end of story. But what we could be saying now is ""Failure. Your WDL engine isn't able to process this workflow because the file is too large. Try increasing resources or restructuring your WDL""... not really any different from ""Failure. You didn't have enough memory to run that scatter so wide..."", or indeed any other resource constraint. This is just me TOL of course... but it does seem a lot more elegant to me than having a special case for file sizes written in the WDL spec but every other resource limit being implicit and left up to the engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185:162,Deployability,configurat,configuration,162,"Given that this is being considered an error (I'd prefer ""Failure"") rather than truncation, I'm going to try to push one more time the idea that this should be a configuration option rather than some hard-coded magic number in WDL (which will inevitable need to be raised at some point, and then will different WDL versions have different `read_string` limits even when run on the same Cromwell?). If we were truncating then the worry about different results would be correct, end of story. But what we could be saying now is ""Failure. Your WDL engine isn't able to process this workflow because the file is too large. Try increasing resources or restructuring your WDL""... not really any different from ""Failure. You didn't have enough memory to run that scatter so wide..."", or indeed any other resource constraint. This is just me TOL of course... but it does seem a lot more elegant to me than having a special case for file sizes written in the WDL spec but every other resource limit being implicit and left up to the engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185:162,Modifiability,config,configuration,162,"Given that this is being considered an error (I'd prefer ""Failure"") rather than truncation, I'm going to try to push one more time the idea that this should be a configuration option rather than some hard-coded magic number in WDL (which will inevitable need to be raised at some point, and then will different WDL versions have different `read_string` limits even when run on the same Cromwell?). If we were truncating then the worry about different results would be correct, end of story. But what we could be saying now is ""Failure. Your WDL engine isn't able to process this workflow because the file is too large. Try increasing resources or restructuring your WDL""... not really any different from ""Failure. You didn't have enough memory to run that scatter so wide..."", or indeed any other resource constraint. This is just me TOL of course... but it does seem a lot more elegant to me than having a special case for file sizes written in the WDL spec but every other resource limit being implicit and left up to the engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294810629:51,Modifiability,config,configurable,51,I think I agree with Chris re: the limit should be configurable,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294810629
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294850347:52,Modifiability,portab,portable,52,"Geraldine, how does that align with workflows being portable? If a user takes advantage of that one Cromwell, and then it dies in dnanexus because they have a different setting (or implementation!). What about having something in the spec about a minimum that must be supported? Therefore if users move above that they know their workflow might not be compliant with other servers. We can still have the knob of course in cromwell. > On Apr 18, 2017, at 9:42 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > As long as it is pitched as a cromwell feature and not part of WDL, I agree.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294850347
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294851492:100,Modifiability,Portab,Portability,100,"Fwiw while CWL has maximums TES has minimum required like what you suggest. I see both sides on it. Portability is helpful but if someone has a workflow which only succeeds because their server has juiced limits and they do t realize that, it isn't super helpful",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294851492
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637:147,Availability,error,error,147,"I would support spec-mandated minimums, supplemented by knobs in Cromwell. . For the user experience, a key thing you can do is write really clear error messages. Ie don't make it die with just ""File was too big""; add a note in there about where to get more info/what can be done to get past this. @katevoss can help with this; she has strong feelings about microcopy as I'm sure you know by now ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637:153,Integrability,message,messages,153,"I would support spec-mandated minimums, supplemented by knobs in Cromwell. . For the user experience, a key thing you can do is write really clear error messages. Ie don't make it die with just ""File was too big""; add a note in there about where to get more info/what can be done to get past this. @katevoss can help with this; she has strong feelings about microcopy as I'm sure you know by now ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637:85,Usability,user experience,user experience,85,"I would support spec-mandated minimums, supplemented by knobs in Cromwell. . For the user experience, a key thing you can do is write really clear error messages. Ie don't make it die with just ""File was too big""; add a note in there about where to get more info/what can be done to get past this. @katevoss can help with this; she has strong feelings about microcopy as I'm sure you know by now ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637:141,Usability,clear,clear,141,"I would support spec-mandated minimums, supplemented by knobs in Cromwell. . For the user experience, a key thing you can do is write really clear error messages. Ie don't make it die with just ""File was too big""; add a note in there about where to get more info/what can be done to get past this. @katevoss can help with this; she has strong feelings about microcopy as I'm sure you know by now ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:264,Availability,Error,Error,264,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:73,Deployability,configurat,configuration,73,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:270,Integrability,message,messages,270,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:73,Modifiability,config,configuration,73,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:113,Performance,tune,tune,113,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:294,Usability,clear,clear,294,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266790899:7,Security,hash,hash,7,commit hash has our cromwell was running on - 192ea6025613df967d60e9e975693144035379d7,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266790899
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085:51,Availability,down,down,51,"@kcibul Ok - that was bothering me so I tracked it down. Turns out that I was misremembering how it was working in the first place in terms of what we retry. We'll retry perpetually for any HTTP error we receive except for a 404 using the exponential backoff up to maxPollingInterval. . How does this related to preemption? Well, it shouldn't. Preemption isn't an HTTP error it's an actual operation failure. When we receive an operation failure we process that and among other things look to see if it had a preemption error code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085:195,Availability,error,error,195,"@kcibul Ok - that was bothering me so I tracked it down. Turns out that I was misremembering how it was working in the first place in terms of what we retry. We'll retry perpetually for any HTTP error we receive except for a 404 using the exponential backoff up to maxPollingInterval. . How does this related to preemption? Well, it shouldn't. Preemption isn't an HTTP error it's an actual operation failure. When we receive an operation failure we process that and among other things look to see if it had a preemption error code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085:369,Availability,error,error,369,"@kcibul Ok - that was bothering me so I tracked it down. Turns out that I was misremembering how it was working in the first place in terms of what we retry. We'll retry perpetually for any HTTP error we receive except for a 404 using the exponential backoff up to maxPollingInterval. . How does this related to preemption? Well, it shouldn't. Preemption isn't an HTTP error it's an actual operation failure. When we receive an operation failure we process that and among other things look to see if it had a preemption error code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085:400,Availability,failure,failure,400,"@kcibul Ok - that was bothering me so I tracked it down. Turns out that I was misremembering how it was working in the first place in terms of what we retry. We'll retry perpetually for any HTTP error we receive except for a 404 using the exponential backoff up to maxPollingInterval. . How does this related to preemption? Well, it shouldn't. Preemption isn't an HTTP error it's an actual operation failure. When we receive an operation failure we process that and among other things look to see if it had a preemption error code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085:438,Availability,failure,failure,438,"@kcibul Ok - that was bothering me so I tracked it down. Turns out that I was misremembering how it was working in the first place in terms of what we retry. We'll retry perpetually for any HTTP error we receive except for a 404 using the exponential backoff up to maxPollingInterval. . How does this related to preemption? Well, it shouldn't. Preemption isn't an HTTP error it's an actual operation failure. When we receive an operation failure we process that and among other things look to see if it had a preemption error code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085:520,Availability,error,error,520,"@kcibul Ok - that was bothering me so I tracked it down. Turns out that I was misremembering how it was working in the first place in terms of what we retry. We'll retry perpetually for any HTTP error we receive except for a 404 using the exponential backoff up to maxPollingInterval. . How does this related to preemption? Well, it shouldn't. Preemption isn't an HTTP error it's an actual operation failure. When we receive an operation failure we process that and among other things look to see if it had a preemption error code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271635305:70,Availability,error,errors,70,They upped our QPS on our projects so we can't actually reproduce 429 errors in our projects anymore with any tests that we currently have(we were able to do it before with our 50 workflow test). You would have to write some wdl that scatters wide and send in a number of them at the same time to cromwell in some non-upped QPS project.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271635305
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271635305:110,Testability,test,tests,110,They upped our QPS on our projects so we can't actually reproduce 429 errors in our projects anymore with any tests that we currently have(we were able to do it before with our 50 workflow test). You would have to write some wdl that scatters wide and send in a number of them at the same time to cromwell in some non-upped QPS project.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271635305
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271635305:189,Testability,test,test,189,They upped our QPS on our projects so we can't actually reproduce 429 errors in our projects anymore with any tests that we currently have(we were able to do it before with our 50 workflow test). You would have to write some wdl that scatters wide and send in a number of them at the same time to cromwell in some non-upped QPS project.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271635305
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:389,Availability,failure,failure,389,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:553,Availability,error,errors,553,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:1744,Availability,ERROR,ERROR,1744,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:2060,Availability,error,errors,2060,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:591,Integrability,message,message,591,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:816,Integrability,message,message,816,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:2098,Integrability,message,message,2098,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:2323,Integrability,message,message,2323,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:697,Modifiability,sandbox,sandbox,697,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:922,Modifiability,sandbox,sandbox,922,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:2204,Modifiability,sandbox,sandbox,2204,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:2429,Modifiability,sandbox,sandbox,2429,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:20,Testability,log,logs,20,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:697,Testability,sandbox,sandbox,697,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:922,Testability,sandbox,sandbox,922,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:2204,Testability,sandbox,sandbox,2204,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:2429,Testability,sandbox,sandbox,2429,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490
https://github.com/broadinstitute/cromwell/issues/1765#issuecomment-266057047:38,Availability,down,down,38,@delocalizer Thanks for tracking this down ! I'm working on a fix. From what you describe and what I've found out so far it looks like it would not work either even if the value is supplied. Have you hit that case as well ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765#issuecomment-266057047
https://github.com/broadinstitute/cromwell/issues/1765#issuecomment-266867284:32,Testability,test,test,32,And we should definitely have a test for this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765#issuecomment-266867284
https://github.com/broadinstitute/cromwell/issues/1766#issuecomment-278688246:76,Testability,test,tests,76,"Centaur WDLs pass, including call caching, excluding local backend specific tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1766#issuecomment-278688246
https://github.com/broadinstitute/cromwell/pull/1771#issuecomment-266563405:20,Testability,test,test,20,"Mind copy-pasting a test into `CromwellApiServiceSpec`, especially for the new `IllegalArgumentException` behavior? Otherwise LGTM 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1771/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1771#issuecomment-266563405
https://github.com/broadinstitute/cromwell/pull/1772#issuecomment-266470980:39,Deployability,patch,patched,39,"`ServiceRegistryActorSpec` needs to be patched also, but otherwise LGTM 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1772/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1772#issuecomment-266470980
https://github.com/broadinstitute/cromwell/issues/1773#issuecomment-320524669:38,Testability,test,test,38,@LeeTL1220 Do you have a reproducible test case? Otherwise we'll have to close this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1773#issuecomment-320524669
https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885:76,Availability,ERROR,ERROR,76,"So I tested that this will now correctly fail a workflow, and indeed:; ```; ERROR - WorkflowManagerActor Workflow f587f57a-2897-4450-a4e1-410442f2460c failed (during ExecutingWorkflowState): Variable 'xs' not found; wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'xs' not found; ```. However as noted, this is a Cromwell runtime catch. It'd be much nicer as a WDL4S validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885
https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885:191,Modifiability,Variab,Variable,191,"So I tested that this will now correctly fail a workflow, and indeed:; ```; ERROR - WorkflowManagerActor Workflow f587f57a-2897-4450-a4e1-410442f2460c failed (during ExecutingWorkflowState): Variable 'xs' not found; wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'xs' not found; ```. However as noted, this is a Cromwell runtime catch. It'd be much nicer as a WDL4S validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885
https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885:232,Modifiability,Variab,VariableNotFoundException,232,"So I tested that this will now correctly fail a workflow, and indeed:; ```; ERROR - WorkflowManagerActor Workflow f587f57a-2897-4450-a4e1-410442f2460c failed (during ExecutingWorkflowState): Variable 'xs' not found; wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'xs' not found; ```. However as noted, this is a Cromwell runtime catch. It'd be much nicer as a WDL4S validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885
https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885:267,Modifiability,Variab,Variable,267,"So I tested that this will now correctly fail a workflow, and indeed:; ```; ERROR - WorkflowManagerActor Workflow f587f57a-2897-4450-a4e1-410442f2460c failed (during ExecutingWorkflowState): Variable 'xs' not found; wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'xs' not found; ```. However as noted, this is a Cromwell runtime catch. It'd be much nicer as a WDL4S validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885
https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885:379,Security,validat,validation,379,"So I tested that this will now correctly fail a workflow, and indeed:; ```; ERROR - WorkflowManagerActor Workflow f587f57a-2897-4450-a4e1-410442f2460c failed (during ExecutingWorkflowState): Variable 'xs' not found; wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'xs' not found; ```. However as noted, this is a Cromwell runtime catch. It'd be much nicer as a WDL4S validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885
https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885:5,Testability,test,tested,5,"So I tested that this will now correctly fail a workflow, and indeed:; ```; ERROR - WorkflowManagerActor Workflow f587f57a-2897-4450-a4e1-410442f2460c failed (during ExecutingWorkflowState): Variable 'xs' not found; wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'xs' not found; ```. However as noted, this is a Cromwell runtime catch. It'd be much nicer as a WDL4S validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885
https://github.com/broadinstitute/cromwell/issues/1775#issuecomment-326421477:189,Safety,Abort,Aborts,189,"@ndbolliger I'm guessing this is no longer a problem since it hasn't gotten any attention in 9 months...sorry about that! Closing it out unless anyone objects, we have a [Spec Document for Aborts](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit) now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775#issuecomment-326421477
https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-266547452:152,Testability,test,test,152,"I'll just throw out there the scala `par` collection again even, if I know @geoffjentry doesn't like it :p, I think it could be a cheap way to at least test if parallelization does improve speed significantly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-266547452
https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-266563690:137,Deployability,rolling,rolling,137,"@Horneth True, I can set that up quick to see how it goes. . If it does go well, this might end up being a good simple use case to start rolling w/ streams: https://groups.google.com/forum/#!topic/akka-user/v01YeU6Zb-o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-266563690
https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-266563690:112,Usability,simpl,simple,112,"@Horneth True, I can set that up quick to see how it goes. . If it does go well, this might end up being a good simple use case to start rolling w/ streams: https://groups.google.com/forum/#!topic/akka-user/v01YeU6Zb-o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-266563690
https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-295902760:21,Testability,test,test,21,Was there a specific test case for this? I'm not sure how big is reasonably expected to work.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-295902760
https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-295908844:62,Usability,simpl,simple,62,"Based on when I was likely using either 20k, 40k or 200k wide simple scatters and just watching jprofiler and such",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-295908844
https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-295922229:144,Performance,perform,performant,144,"On develop Cromwell running locally with local MySQL, this WDL takes about 30 seconds to write 400,050 metadata rows. Does this need to be more performant than that? Perhaps the problem before was a lack of batched DB writes?. ```; task IntToIntArray {; Int number. command <<<; python <<CODE; for i in range(${number}):; print(i); CODE; >>>. runtime {; docker: ""python@sha256:bda277274d53484e4026f96379205760a424061933f91816a6d66784c5e8afdf""; memory: ""1 GB""; preemptible: 2; }. output {; Array[Int] array = read_lines(stdout()); }; }. workflow wide {; call IntToIntArray { input: number = 200000 }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-295922229
https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066:395,Availability,down,down,395,"@cjllanwarne I'm going to take this the complete opposite direction but I'll give you all the credit as it was what you said that led me here. Specifically it started with your statement "" a WDL author can decide when they want their globs to be exploded ..."" my reaction was ""hell no!"" as the point here is system sanctity and not user desire. If the point is to not allow a large glob to take down a cromwell server then we should not allow users to be the ones deciding what's getting processed in a more/less efficient manner. To bend an old gem of wisdom a bit beyond its original meaning, ""never trust the client"". That got me thinking that I think all of this (including my original post) is coming at this all wrong. To the WDL user they should only ever have to think in terms of `File` and `Array[File]`, but that should imply no specific implementation under the hood. I think some of this goes to how tightly coupled WDL is to implementation in Cromwell and how that fact tends to guide our thinking in certain directions. If instead we *always* treated `Array[File]` as a FOFN behind the scenes in a completely invisible to the user manner we'd be able to keep the simplistic sugar of WDL but go even beyond glob situations when it comes to memory savings. That way both `File` and `Array[File]` are internally managed as just a single file path. For obvious reasons (including your last statement and others that we both made) that wouldn't be a tiny change but I don't think it'd be monumental either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066
https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066:513,Energy Efficiency,efficient,efficient,513,"@cjllanwarne I'm going to take this the complete opposite direction but I'll give you all the credit as it was what you said that led me here. Specifically it started with your statement "" a WDL author can decide when they want their globs to be exploded ..."" my reaction was ""hell no!"" as the point here is system sanctity and not user desire. If the point is to not allow a large glob to take down a cromwell server then we should not allow users to be the ones deciding what's getting processed in a more/less efficient manner. To bend an old gem of wisdom a bit beyond its original meaning, ""never trust the client"". That got me thinking that I think all of this (including my original post) is coming at this all wrong. To the WDL user they should only ever have to think in terms of `File` and `Array[File]`, but that should imply no specific implementation under the hood. I think some of this goes to how tightly coupled WDL is to implementation in Cromwell and how that fact tends to guide our thinking in certain directions. If instead we *always* treated `Array[File]` as a FOFN behind the scenes in a completely invisible to the user manner we'd be able to keep the simplistic sugar of WDL but go even beyond glob situations when it comes to memory savings. That way both `File` and `Array[File]` are internally managed as just a single file path. For obvious reasons (including your last statement and others that we both made) that wouldn't be a tiny change but I don't think it'd be monumental either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066
https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066:993,Usability,guid,guide,993,"@cjllanwarne I'm going to take this the complete opposite direction but I'll give you all the credit as it was what you said that led me here. Specifically it started with your statement "" a WDL author can decide when they want their globs to be exploded ..."" my reaction was ""hell no!"" as the point here is system sanctity and not user desire. If the point is to not allow a large glob to take down a cromwell server then we should not allow users to be the ones deciding what's getting processed in a more/less efficient manner. To bend an old gem of wisdom a bit beyond its original meaning, ""never trust the client"". That got me thinking that I think all of this (including my original post) is coming at this all wrong. To the WDL user they should only ever have to think in terms of `File` and `Array[File]`, but that should imply no specific implementation under the hood. I think some of this goes to how tightly coupled WDL is to implementation in Cromwell and how that fact tends to guide our thinking in certain directions. If instead we *always* treated `Array[File]` as a FOFN behind the scenes in a completely invisible to the user manner we'd be able to keep the simplistic sugar of WDL but go even beyond glob situations when it comes to memory savings. That way both `File` and `Array[File]` are internally managed as just a single file path. For obvious reasons (including your last statement and others that we both made) that wouldn't be a tiny change but I don't think it'd be monumental either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066
https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066:1178,Usability,simpl,simplistic,1178,"@cjllanwarne I'm going to take this the complete opposite direction but I'll give you all the credit as it was what you said that led me here. Specifically it started with your statement "" a WDL author can decide when they want their globs to be exploded ..."" my reaction was ""hell no!"" as the point here is system sanctity and not user desire. If the point is to not allow a large glob to take down a cromwell server then we should not allow users to be the ones deciding what's getting processed in a more/less efficient manner. To bend an old gem of wisdom a bit beyond its original meaning, ""never trust the client"". That got me thinking that I think all of this (including my original post) is coming at this all wrong. To the WDL user they should only ever have to think in terms of `File` and `Array[File]`, but that should imply no specific implementation under the hood. I think some of this goes to how tightly coupled WDL is to implementation in Cromwell and how that fact tends to guide our thinking in certain directions. If instead we *always* treated `Array[File]` as a FOFN behind the scenes in a completely invisible to the user manner we'd be able to keep the simplistic sugar of WDL but go even beyond glob situations when it comes to memory savings. That way both `File` and `Array[File]` are internally managed as just a single file path. For obvious reasons (including your last statement and others that we both made) that wouldn't be a tiny change but I don't think it'd be monumental either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066
https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-328528593:369,Availability,down,down,369,"The problem turns out to be broader than globs as joint genotyping pointed out. The way we handle large arrays, particularly large file arrays chews up a massive amount of ram- in the case of a single large workflow it can be impossible to run. In a multi tenant case it can be death by a thousand cuts. . This will absolutely need to be improved before we get too far down the caas journey",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-328528593
https://github.com/broadinstitute/cromwell/pull/1779#issuecomment-267111826:279,Modifiability,enhance,enhanced,279,Some small questions but otherwise looks good to me. I'm assuming [this](https://github.com/broadinstitute/cromwell/blob/e59dd128945ade2abc1ca6c001e76b0128afc5b8/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L406) is going to be enhanced. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1779/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1779#issuecomment-267111826
https://github.com/broadinstitute/cromwell/pull/1780#issuecomment-267122947:109,Deployability,release,release,109,I'd double check what I say with @kshakir as he always corrects me but I'd vote for putting this in `src/bin/release`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1780#issuecomment-267122947
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:21,Availability,error,error,21,"Same thing with this error: ; ```; could not download return code file, retrying; com.google.cloud.storage.StorageException: 500 Internal Server Error; Backend Error; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.read(DefaultStorageRpc.java:482); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:85); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); 	at java.nio.file.Files.read(Files.java:3105); 	at java.nio.file.Files.readAllBytes(Files.java:3158); 	at better.files.File.loadBytes(File.scala:163); 	at better.files.File.byteArray(File.scala:166); 	at better.files.File.contentAsString(File.scala:206); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$back",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:45,Availability,down,download,45,"Same thing with this error: ; ```; could not download return code file, retrying; com.google.cloud.storage.StorageException: 500 Internal Server Error; Backend Error; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.read(DefaultStorageRpc.java:482); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:85); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); 	at java.nio.file.Files.read(Files.java:3105); 	at java.nio.file.Files.readAllBytes(Files.java:3158); 	at better.files.File.loadBytes(File.scala:163); 	at better.files.File.byteArray(File.scala:166); 	at better.files.File.contentAsString(File.scala:206); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$back",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:145,Availability,Error,Error,145,"Same thing with this error: ; ```; could not download return code file, retrying; com.google.cloud.storage.StorageException: 500 Internal Server Error; Backend Error; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.read(DefaultStorageRpc.java:482); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:85); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); 	at java.nio.file.Files.read(Files.java:3105); 	at java.nio.file.Files.readAllBytes(Files.java:3158); 	at better.files.File.loadBytes(File.scala:163); 	at better.files.File.byteArray(File.scala:166); 	at better.files.File.contentAsString(File.scala:206); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$back",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:160,Availability,Error,Error,160,"Same thing with this error: ; ```; could not download return code file, retrying; com.google.cloud.storage.StorageException: 500 Internal Server Error; Backend Error; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.read(DefaultStorageRpc.java:482); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:85); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); 	at java.nio.file.Files.read(Files.java:3105); 	at java.nio.file.Files.readAllBytes(Files.java:3158); 	at better.files.File.loadBytes(File.scala:163); 	at better.files.File.byteArray(File.scala:166); 	at better.files.File.contentAsString(File.scala:206); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$back",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:3156,Availability,Error,Error,3156,syncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:3171,Availability,Error,Error,3171,syncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:1146,Performance,load,loadBytes,1146,ackend Error; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.read(DefaultStorageRpc.java:482); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:85); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); 	at java.nio.file.Files.read(Files.java:3105); 	at java.nio.file.Files.readAllBytes(Files.java:3158); 	at better.files.File.loadBytes(File.scala:163); 	at better.files.File.byteArray(File.scala:166); 	at better.files.File.contentAsString(File.scala:206); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBack,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:2404,Performance,concurren,concurrent,2404,returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:2495,Performance,concurren,concurrent,2495,d.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoog,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:2743,Performance,concurren,concurrent,2743,impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.Ht,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:2817,Performance,concurren,concurrent,2817,ncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.google,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:2903,Performance,concurren,concurrent,2903,ctor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:2981,Performance,concurren,concurrent,2981,r.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at com.google.api.client.googleapis.services.AbstractGoogleClientR,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762
https://github.com/broadinstitute/cromwell/issues/1785#issuecomment-267361485:75,Modifiability,Variab,Variable,75,Sample output:; ```; Caused by: lenthall.exception.AggregatedException: :; Variable is resolved to scope subworkflow.subwf.is but cannot be evaluated.; 	at lenthall.util.TryUtil$.sequenceIterable(TryUtil.scala:26); 	at lenthall.util.TryUtil$.sequence(TryUtil.scala:33); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:364); 	... 22 more; 	Suppressed: wdl4s.exception.VariableLookupException: Variable is resolved to scope subworkflow.subwf.is but cannot be evaluated.; 		at wdl4s.Scope$$anonfun$5.apply(Scope.scala:255); 		at wdl4s.Scope$$anonfun$5.apply(Scope.scala:246); 		at scala.Option.map(Option.scala:146); 		at wdl4s.Scope$class.lookup$1(Scope.scala:246); 		at wdl4s.Scope$$anonfun$lookupFunction$1.apply(Scope.scala:263); 		at wdl4s.Scope$$anonfun$lookupFunction$1.apply(Scope.scala:263); 		at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); 		at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1785#issuecomment-267361485
https://github.com/broadinstitute/cromwell/issues/1785#issuecomment-267361485:509,Modifiability,Variab,VariableLookupException,509,Sample output:; ```; Caused by: lenthall.exception.AggregatedException: :; Variable is resolved to scope subworkflow.subwf.is but cannot be evaluated.; 	at lenthall.util.TryUtil$.sequenceIterable(TryUtil.scala:26); 	at lenthall.util.TryUtil$.sequence(TryUtil.scala:33); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:364); 	... 22 more; 	Suppressed: wdl4s.exception.VariableLookupException: Variable is resolved to scope subworkflow.subwf.is but cannot be evaluated.; 		at wdl4s.Scope$$anonfun$5.apply(Scope.scala:255); 		at wdl4s.Scope$$anonfun$5.apply(Scope.scala:246); 		at scala.Option.map(Option.scala:146); 		at wdl4s.Scope$class.lookup$1(Scope.scala:246); 		at wdl4s.Scope$$anonfun$lookupFunction$1.apply(Scope.scala:263); 		at wdl4s.Scope$$anonfun$lookupFunction$1.apply(Scope.scala:263); 		at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); 		at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1785#issuecomment-267361485
https://github.com/broadinstitute/cromwell/issues/1785#issuecomment-267361485:534,Modifiability,Variab,Variable,534,Sample output:; ```; Caused by: lenthall.exception.AggregatedException: :; Variable is resolved to scope subworkflow.subwf.is but cannot be evaluated.; 	at lenthall.util.TryUtil$.sequenceIterable(TryUtil.scala:26); 	at lenthall.util.TryUtil$.sequence(TryUtil.scala:33); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:364); 	... 22 more; 	Suppressed: wdl4s.exception.VariableLookupException: Variable is resolved to scope subworkflow.subwf.is but cannot be evaluated.; 		at wdl4s.Scope$$anonfun$5.apply(Scope.scala:255); 		at wdl4s.Scope$$anonfun$5.apply(Scope.scala:246); 		at scala.Option.map(Option.scala:146); 		at wdl4s.Scope$class.lookup$1(Scope.scala:246); 		at wdl4s.Scope$$anonfun$lookupFunction$1.apply(Scope.scala:263); 		at wdl4s.Scope$$anonfun$lookupFunction$1.apply(Scope.scala:263); 		at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); 		at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1785#issuecomment-267361485
https://github.com/broadinstitute/cromwell/issues/1791#issuecomment-326420190:110,Availability,error,errors,110,"@kshakir Can you tell me a bit more about this ticket? Is it still happening, or is Travis not having network errors anymore?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1791#issuecomment-326420190
https://github.com/broadinstitute/cromwell/issues/1793#issuecomment-328529130:7,Deployability,upgrade,upgraded,7,"When I upgraded to Akka http I found and verified that note from @kshakir that out paging ""exists"" but is massively broken . Generally it is viewed as good practice to provide paging for either all or all potentially large results, this is definitely the latter and obviously the former :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1793#issuecomment-328529130
https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267638887:105,Safety,timeout,timeout,105,"How ""eventually"" consistent should I be thinking on metadata, etc.? Seconds, minutes, hours, longer? The timeout at batch submission is likely a client-side thing, so we can probably ignore that issue for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267638887
https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267738860:354,Modifiability,config,configurable,354,"for query I'd expect for it to typically be measured in seconds, not minutes and certainly not longer than that, it's just that we don't make any guarantee that the data is in there before we move on. For the metadata endpoint there's a summarization process which happens (to make the response time faster) that sweeps around every few minutes (rate is configurable) and calculates the current state of the world",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267738860
https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267738860:286,Performance,response time,response time,286,"for query I'd expect for it to typically be measured in seconds, not minutes and certainly not longer than that, it's just that we don't make any guarantee that the data is in there before we move on. For the metadata endpoint there's a summarization process which happens (to make the response time faster) that sweeps around every few minutes (rate is configurable) and calculates the current state of the world",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267738860
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739604:104,Deployability,configurat,configuration,104,"That's not currently possible, but IMO it's a good idea. . @kcibul what do you think about having a JES configuration setting for the default zone(s) for jobs? Currently it's just hardwired to us-central1-b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739604
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739604:104,Modifiability,config,configuration,104,"That's not currently possible, but IMO it's a good idea. . @kcibul what do you think about having a JES configuration setting for the default zone(s) for jobs? Currently it's just hardwired to us-central1-b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739604
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739748:442,Deployability,configurat,configuration,442,"Yes -- this is an excellent idea! Let's make it so. -------------------------------; Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Fri, Dec 16, 2016 at 10:18 PM, Jeff Gentry <notifications@github.com>; wrote:. > That's not currently possible, but IMO it's a good idea.; >; > @kcibul <https://github.com/kcibul> what do you think about having a JES; > configuration setting for the default zone(s) for jobs? Currently it's just; > hardwired to us-central1-b; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739604>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g0FHRjUSCxRbTRcCYsMvaZwaa1mZks5rI1SAgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739748
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739748:442,Modifiability,config,configuration,442,"Yes -- this is an excellent idea! Let's make it so. -------------------------------; Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Fri, Dec 16, 2016 at 10:18 PM, Jeff Gentry <notifications@github.com>; wrote:. > That's not currently possible, but IMO it's a good idea.; >; > @kcibul <https://github.com/kcibul> what do you think about having a JES; > configuration setting for the default zone(s) for jobs? Currently it's just; > hardwired to us-central1-b; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739604>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g0FHRjUSCxRbTRcCYsMvaZwaa1mZks5rI1SAgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739748
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836:33,Safety,avoid,avoid,33,@pgrosu IIRC we use `central` to avoid some of their other large customers in other zones. However note that we *are* one of their large customers so choosing the same zone as us might not be the best plan for success in avoiding preemptions :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836:221,Safety,avoid,avoiding,221,@pgrosu IIRC we use `central` to avoid some of their other large customers in other zones. However note that we *are* one of their large customers so choosing the same zone as us might not be the best plan for success in avoiding preemptions :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:150,Deployability,configurat,configuration,150,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:150,Modifiability,config,configuration,150,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:122,Safety,predict,predictions,122,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:345,Safety,avoid,avoid,345,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:542,Safety,avoid,avoiding,542,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:105,Usability,learn,learning,105,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:193,Usability,Simpl,Simple,193,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267840564:36,Safety,safe,safe,36,The shoulders of giants is always a safe place :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267840564
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365:208,Modifiability,config,config,208,"Hey @geoffjentry this is a great change to make, didn't make sense to have a non-changeable hardcoded default!. ToL: What struck me in this PR was that it took changing of 11 files to wire a default from the config file to where it is used. I struggled with this same thing when adding in the alternate compute service account. Felt like a lot of boilerplate. As a future refactoring, it would be nice if this was simplified overall",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365:372,Modifiability,refactor,refactoring,372,"Hey @geoffjentry this is a great change to make, didn't make sense to have a non-changeable hardcoded default!. ToL: What struck me in this PR was that it took changing of 11 files to wire a default from the config file to where it is used. I struggled with this same thing when adding in the alternate compute service account. Felt like a lot of boilerplate. As a future refactoring, it would be nice if this was simplified overall",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365:414,Usability,simpl,simplified,414,"Hey @geoffjentry this is a great change to make, didn't make sense to have a non-changeable hardcoded default!. ToL: What struck me in this PR was that it took changing of 11 files to wire a default from the config file to where it is used. I struggled with this same thing when adding in the alternate compute service account. Felt like a lot of boilerplate. As a future refactoring, it would be nice if this was simplified overall",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669:70,Availability,down,down,70,@kcibul next time I won't update tests or docs to keep the file count down ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669:26,Deployability,update,update,26,@kcibul next time I won't update tests or docs to keep the file count down ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669:33,Testability,test,tests,33,@kcibul next time I won't update tests or docs to keep the file count down ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773:1047,Modifiability,refactor,refactoring,1047,"When rebasing and resolving conflicts this PR will need to be rewired, starting with:. ```scala; object JesRuntimeAttributes {; val runtimeAttributesBuilder: StandardValidatedRuntimeAttributesBuilder = …. private val zonesValidation: RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(ZoneDefaultValue)). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes): JesRuntimeAttributes = …; ```. converted to:. ```scala; object JesRuntimeAttributes {; def runtimeAttributesBuilder(jesConfiguration: JesConfiguration): StandardValidatedRuntimeAttributesBuilder = . private def zonesValidation(defaultZones: NonEmptyList[String]): RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(defaultZones.toList.mkString("",""))). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes,; jesConfiguration: JesConfiguration): JesRuntimeAttributes = …; ```. This should be fine for plumbing from the main code, as each caller has a jesConfiguration. The test code may need some refactoring and/or mocks-- or the JRA object can add overloads, that in addition to receiving jesConfigurations, can take a nel of default zones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773:349,Security,validat,validatedRuntimeAttributes,349,"When rebasing and resolving conflicts this PR will need to be rewired, starting with:. ```scala; object JesRuntimeAttributes {; val runtimeAttributesBuilder: StandardValidatedRuntimeAttributesBuilder = …. private val zonesValidation: RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(ZoneDefaultValue)). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes): JesRuntimeAttributes = …; ```. converted to:. ```scala; object JesRuntimeAttributes {; def runtimeAttributesBuilder(jesConfiguration: JesConfiguration): StandardValidatedRuntimeAttributesBuilder = . private def zonesValidation(defaultZones: NonEmptyList[String]): RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(defaultZones.toList.mkString("",""))). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes,; jesConfiguration: JesConfiguration): JesRuntimeAttributes = …; ```. This should be fine for plumbing from the main code, as each caller has a jesConfiguration. The test code may need some refactoring and/or mocks-- or the JRA object can add overloads, that in addition to receiving jesConfigurations, can take a nel of default zones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773:377,Security,Validat,ValidatedRuntimeAttributes,377,"When rebasing and resolving conflicts this PR will need to be rewired, starting with:. ```scala; object JesRuntimeAttributes {; val runtimeAttributesBuilder: StandardValidatedRuntimeAttributesBuilder = …. private val zonesValidation: RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(ZoneDefaultValue)). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes): JesRuntimeAttributes = …; ```. converted to:. ```scala; object JesRuntimeAttributes {; def runtimeAttributesBuilder(jesConfiguration: JesConfiguration): StandardValidatedRuntimeAttributesBuilder = . private def zonesValidation(defaultZones: NonEmptyList[String]): RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(defaultZones.toList.mkString("",""))). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes,; jesConfiguration: JesConfiguration): JesRuntimeAttributes = …; ```. This should be fine for plumbing from the main code, as each caller has a jesConfiguration. The test code may need some refactoring and/or mocks-- or the JRA object can add overloads, that in addition to receiving jesConfigurations, can take a nel of default zones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773:802,Security,validat,validatedRuntimeAttributes,802,"When rebasing and resolving conflicts this PR will need to be rewired, starting with:. ```scala; object JesRuntimeAttributes {; val runtimeAttributesBuilder: StandardValidatedRuntimeAttributesBuilder = …. private val zonesValidation: RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(ZoneDefaultValue)). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes): JesRuntimeAttributes = …; ```. converted to:. ```scala; object JesRuntimeAttributes {; def runtimeAttributesBuilder(jesConfiguration: JesConfiguration): StandardValidatedRuntimeAttributesBuilder = . private def zonesValidation(defaultZones: NonEmptyList[String]): RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(defaultZones.toList.mkString("",""))). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes,; jesConfiguration: JesConfiguration): JesRuntimeAttributes = …; ```. This should be fine for plumbing from the main code, as each caller has a jesConfiguration. The test code may need some refactoring and/or mocks-- or the JRA object can add overloads, that in addition to receiving jesConfigurations, can take a nel of default zones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773:830,Security,Validat,ValidatedRuntimeAttributes,830,"When rebasing and resolving conflicts this PR will need to be rewired, starting with:. ```scala; object JesRuntimeAttributes {; val runtimeAttributesBuilder: StandardValidatedRuntimeAttributesBuilder = …. private val zonesValidation: RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(ZoneDefaultValue)). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes): JesRuntimeAttributes = …; ```. converted to:. ```scala; object JesRuntimeAttributes {; def runtimeAttributesBuilder(jesConfiguration: JesConfiguration): StandardValidatedRuntimeAttributesBuilder = . private def zonesValidation(defaultZones: NonEmptyList[String]): RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(defaultZones.toList.mkString("",""))). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes,; jesConfiguration: JesConfiguration): JesRuntimeAttributes = …; ```. This should be fine for plumbing from the main code, as each caller has a jesConfiguration. The test code may need some refactoring and/or mocks-- or the JRA object can add overloads, that in addition to receiving jesConfigurations, can take a nel of default zones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773:1023,Testability,test,test,1023,"When rebasing and resolving conflicts this PR will need to be rewired, starting with:. ```scala; object JesRuntimeAttributes {; val runtimeAttributesBuilder: StandardValidatedRuntimeAttributesBuilder = …. private val zonesValidation: RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(ZoneDefaultValue)). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes): JesRuntimeAttributes = …; ```. converted to:. ```scala; object JesRuntimeAttributes {; def runtimeAttributesBuilder(jesConfiguration: JesConfiguration): StandardValidatedRuntimeAttributesBuilder = . private def zonesValidation(defaultZones: NonEmptyList[String]): RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(defaultZones.toList.mkString("",""))). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes,; jesConfiguration: JesConfiguration): JesRuntimeAttributes = …; ```. This should be fine for plumbing from the main code, as each caller has a jesConfiguration. The test code may need some refactoring and/or mocks-- or the JRA object can add overloads, that in addition to receiving jesConfigurations, can take a nel of default zones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773:1066,Testability,mock,mocks,1066,"When rebasing and resolving conflicts this PR will need to be rewired, starting with:. ```scala; object JesRuntimeAttributes {; val runtimeAttributesBuilder: StandardValidatedRuntimeAttributesBuilder = …. private val zonesValidation: RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(ZoneDefaultValue)). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes): JesRuntimeAttributes = …; ```. converted to:. ```scala; object JesRuntimeAttributes {; def runtimeAttributesBuilder(jesConfiguration: JesConfiguration): StandardValidatedRuntimeAttributesBuilder = . private def zonesValidation(defaultZones: NonEmptyList[String]): RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(defaultZones.toList.mkString("",""))). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes,; jesConfiguration: JesConfiguration): JesRuntimeAttributes = …; ```. This should be fine for plumbing from the main code, as each caller has a jesConfiguration. The test code may need some refactoring and/or mocks-- or the JRA object can add overloads, that in addition to receiving jesConfigurations, can take a nel of default zones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773
https://github.com/broadinstitute/cromwell/issues/1799#issuecomment-268196742:15,Availability,error,error,15,"the old pebkac error, missed '@' in the curl command line :blush:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1799#issuecomment-268196742
https://github.com/broadinstitute/cromwell/pull/1800#issuecomment-268567045:36,Testability,test,test--maybe,36,Seems like a good scenario to add a test--maybe in the SharedFileSystemSpec? Otherwise looks good! 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1800/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1800#issuecomment-268567045
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268553111:44,Testability,test,test,44,Something seems off in the globbing centaur test metadata but otherwise 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1801/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268553111
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268798342:17,Availability,failure,failures,17,Some of the test failures seem very genuine... 😨,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268798342
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268798342:12,Testability,test,test,12,Some of the test failures seem very genuine... 😨,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268798342
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737:4743,Performance,concurren,concurrent,4743,flowExecutionActor.scala:353); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737:4817,Performance,concurren,concurrent,4817,flowExecutionActor.scala:353); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737:4903,Performance,concurren,concurrent,4903,flowExecutionActor.scala:353); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737:4981,Performance,concurren,concurrent,4981,flowExecutionActor.scala:353); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737:41,Testability,log,logs,41,I'm seeing a lot of this in the Cromwell logs:; ```; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: Could not find suitable filesystem among Default to parse gs://cloud-cromwell-dev/cromwell_execution/globbingBehavior/0f14036c-402e-4368-986b-6fb8baf512d1/call-writeFiles/glob-7bb0c33ac658a900e2c3804726fc1d2a/hello.txt.; 	at cromwell.core.path.PathFactory$$anonfun$buildPath$5.apply(PathFactory.scala:49); 	at cromwell.core.path.PathFactory$$anonfun$buildPath$5.apply(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:47); 	at cromwell.core.path.PathFactory$class.buildPath(PathFactory.scala:31); 	at cromwell.engine.WdlFunctions.buildPath(WdlFunctions.scala:10); 	at cromwell.backend.wdl.ReadLikeFunctions$class.readFile(ReadLikeFunctions.scala:31); 	at cromwell.engine.WdlFunctions.readFile(WdlFunctions.scala:10); 	at wdl4s.expression.WdlStandardLibraryFunctions$class.fileContentsToString(WdlStandardLibraryFunctions.scala:12); 	at cromwell.engine.WdlFunctions.fileContentsToString(WdlFunctions.scala:10); 	at cromwell.backend.wdl.ReadLikeFunctions$$anonfun$readContentsFromSingleFileParameter$1.apply(ReadLikeFunctions.scala:22); 	at cromwell.backend.wdl.ReadLikeFunctions$$anonfun$readContentsFromSingleFileParameter$1.apply(ReadLikeFunctions.scala:21); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at cromwell.backend.wdl.ReadLikeFunctions$class.readContentsFromSingleFileParameter(ReadLikeFunctions.scala:21); 	at cromwell.backend.wdl.ReadLikeFunctions$class.read_string(ReadLikeFunctions.scala:62); 	at cromwell.engine.WdlFunctions.read_string(WdlFunctions.scala:10); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delegating,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737:3897,Testability,Log,LoggingFSM,3897,rkflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:353); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkj,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737:3977,Testability,Log,LoggingFSM,3977,orkflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:353); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:326); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:304); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268829737
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268931134:43,Deployability,update,update,43,"When we get back, please also make sure to update the documentation as well. This is a breaking change (which is essential for consistency) but we should be super clear about that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268931134
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268931134:163,Usability,clear,clear,163,"When we get back, please also make sure to update the documentation as well. This is a breaking change (which is essential for consistency) but we should be super clear about that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268931134
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-270474842:116,Deployability,release,release,116,:+1: I particularly like @cjllanwarne's ADT suggestions but I'd prefer to do that in a separate PR and not gate the release on that. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1801/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-270474842
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295561066:119,Availability,error,errors,119,"@ruchim Were you able to get to the bottom of the ""Could not find suitable filesystem among Default to parse gs://..."" errors you mention in the review thread? I'm hitting the same error when running a WDL on Google's wdl_runner, but the same WDL with the same inputs runs fine on the DSDE Methods Cromwell server (both C24 and C25). In my case the error occurs when using read_lines() on a gs:// filepath that points to a text file (list of file paths that I'd rather not include in the json for practical reasons). . Also tagging friendly bug rotator @cjllanwarne",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295561066
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295561066:181,Availability,error,error,181,"@ruchim Were you able to get to the bottom of the ""Could not find suitable filesystem among Default to parse gs://..."" errors you mention in the review thread? I'm hitting the same error when running a WDL on Google's wdl_runner, but the same WDL with the same inputs runs fine on the DSDE Methods Cromwell server (both C24 and C25). In my case the error occurs when using read_lines() on a gs:// filepath that points to a text file (list of file paths that I'd rather not include in the json for practical reasons). . Also tagging friendly bug rotator @cjllanwarne",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295561066
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295561066:349,Availability,error,error,349,"@ruchim Were you able to get to the bottom of the ""Could not find suitable filesystem among Default to parse gs://..."" errors you mention in the review thread? I'm hitting the same error when running a WDL on Google's wdl_runner, but the same WDL with the same inputs runs fine on the DSDE Methods Cromwell server (both C24 and C25). In my case the error occurs when using read_lines() on a gs:// filepath that points to a text file (list of file paths that I'd rather not include in the json for practical reasons). . Also tagging friendly bug rotator @cjllanwarne",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295561066
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165:275,Deployability,pipeline,pipelines-api-examples,275,"@vdauwera I spotted the issue but it was @kshakir who ended up resolving it. I believe this had to do with the auth that was used to perform the read_* function, and not having access to the proper google credentials. I checked the [config](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/cromwell_launcher/jes_template.conf) wdl_runner uses and I believe it's missing the goolge.auths key and the engine.filesystem.gcs.auth key in the config, which is probably what Cromwell requires to parse gcs files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165:233,Modifiability,config,config,233,"@vdauwera I spotted the issue but it was @kshakir who ended up resolving it. I believe this had to do with the auth that was used to perform the read_* function, and not having access to the proper google credentials. I checked the [config](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/cromwell_launcher/jes_template.conf) wdl_runner uses and I believe it's missing the goolge.auths key and the engine.filesystem.gcs.auth key in the config, which is probably what Cromwell requires to parse gcs files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165:468,Modifiability,config,config,468,"@vdauwera I spotted the issue but it was @kshakir who ended up resolving it. I believe this had to do with the auth that was used to perform the read_* function, and not having access to the proper google credentials. I checked the [config](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/cromwell_launcher/jes_template.conf) wdl_runner uses and I believe it's missing the goolge.auths key and the engine.filesystem.gcs.auth key in the config, which is probably what Cromwell requires to parse gcs files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165:133,Performance,perform,perform,133,"@vdauwera I spotted the issue but it was @kshakir who ended up resolving it. I believe this had to do with the auth that was used to perform the read_* function, and not having access to the proper google credentials. I checked the [config](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/cromwell_launcher/jes_template.conf) wdl_runner uses and I believe it's missing the goolge.auths key and the engine.filesystem.gcs.auth key in the config, which is probably what Cromwell requires to parse gcs files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165:177,Security,access,access,177,"@vdauwera I spotted the issue but it was @kshakir who ended up resolving it. I believe this had to do with the auth that was used to perform the read_* function, and not having access to the proper google credentials. I checked the [config](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/cromwell_launcher/jes_template.conf) wdl_runner uses and I believe it's missing the goolge.auths key and the engine.filesystem.gcs.auth key in the config, which is probably what Cromwell requires to parse gcs files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:715,Availability,echo,echo,715,"I have a generic (run-anywhere) wdl now that exhibits the behaviour (runs under previous develop and release 22 & 23 versions, but not with 340a5cf): ; ```; workflow dna_mapping_38 {. call createInputs. scatter (arg in createInputs.alignedReadGroup) {; call mapping { input: inFile=arg }; }. call groupItemsByKey as groupArgsByLibrary {; input:; keys=createInputs.library,; items=mapping.outFile; }. scatter (libset in groupArgsByLibrary.groups) {; call markDup as libraryMerge {; input:; inputBams=libset.right,; outputBam=""library_${libset.left}.bam""; }; }. output {; Array[File] libMerged = libraryMerge.markDupedBam; }; }. #########; # TASKS #; #########. task createInputs {; command {; for i in `seq 1 5`; do echo ""lib1""; touch arg$i; done; }; output {; Array[File] alignedReadGroup = glob(""arg*""); Array[String] library = read_lines(stdout()); }; }. task mapping {; File inFile; command {; echo ""dummy mapping""; }; output {; File outFile=inFile; }; }. task groupItemsByKey {. Array[String] keys; Array[String] items. meta {; description: ""return pairs of (key, all-items-with-that-key)""; }. command <<<; python <<CODE; import itertools; import sys; keys = ""${sep='\t' keys}"".split(""\t""); items = ""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:897,Availability,echo,echo,897,"I have a generic (run-anywhere) wdl now that exhibits the behaviour (runs under previous develop and release 22 & 23 versions, but not with 340a5cf): ; ```; workflow dna_mapping_38 {. call createInputs. scatter (arg in createInputs.alignedReadGroup) {; call mapping { input: inFile=arg }; }. call groupItemsByKey as groupArgsByLibrary {; input:; keys=createInputs.library,; items=mapping.outFile; }. scatter (libset in groupArgsByLibrary.groups) {; call markDup as libraryMerge {; input:; inputBams=libset.right,; outputBam=""library_${libset.left}.bam""; }; }. output {; Array[File] libMerged = libraryMerge.markDupedBam; }; }. #########; # TASKS #; #########. task createInputs {; command {; for i in `seq 1 5`; do echo ""lib1""; touch arg$i; done; }; output {; Array[File] alignedReadGroup = glob(""arg*""); Array[String] library = read_lines(stdout()); }; }. task mapping {; File inFile; command {; echo ""dummy mapping""; }; output {; File outFile=inFile; }; }. task groupItemsByKey {. Array[String] keys; Array[String] items. meta {; description: ""return pairs of (key, all-items-with-that-key)""; }. command <<<; python <<CODE; import itertools; import sys; keys = ""${sep='\t' keys}"".split(""\t""); items = ""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:1695,Availability,echo,echo,1695,"cho ""lib1""; touch arg$i; done; }; output {; Array[File] alignedReadGroup = glob(""arg*""); Array[String] library = read_lines(stdout()); }; }. task mapping {; File inFile; command {; echo ""dummy mapping""; }; output {; File outFile=inFile; }; }. task groupItemsByKey {. Array[String] keys; Array[String] items. meta {; description: ""return pairs of (key, all-items-with-that-key)""; }. command <<<; python <<CODE; import itertools; import sys; keys = ""${sep='\t' keys}"".split(""\t""); items = ""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-340a5cf-SNAP.jar run scatterTest.wdl - - - -; ```. consistently (repeatably) dies with:; ```; [2016-12-21 13:01:37,48] [error] WorkflowManagerActor Workflow 28b55884-8fd1-43aa-92ea-eb4891c2c5ff failed (during ExecutingWorkflowState): Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:2158,Availability,error,error,2158,"""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-340a5cf-SNAP.jar run scatterTest.wdl - - - -; ```. consistently (repeatably) dies with:; ```; [2016-12-21 13:01:37,48] [error] WorkflowManagerActor Workflow 28b55884-8fd1-43aa-92ea-eb4891c2c5ff failed (during ExecutingWorkflowState): Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvalu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:3357,Availability,Failure,Failure,3357,luation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:3365,Availability,recover,recoverWith,3365,Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.Fo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:101,Deployability,release,release,101,"I have a generic (run-anywhere) wdl now that exhibits the behaviour (runs under previous develop and release 22 & 23 versions, but not with 340a5cf): ; ```; workflow dna_mapping_38 {. call createInputs. scatter (arg in createInputs.alignedReadGroup) {; call mapping { input: inFile=arg }; }. call groupItemsByKey as groupArgsByLibrary {; input:; keys=createInputs.library,; items=mapping.outFile; }. scatter (libset in groupArgsByLibrary.groups) {; call markDup as libraryMerge {; input:; inputBams=libset.right,; outputBam=""library_${libset.left}.bam""; }; }. output {; Array[File] libMerged = libraryMerge.markDupedBam; }; }. #########; # TASKS #; #########. task createInputs {; command {; for i in `seq 1 5`; do echo ""lib1""; touch arg$i; done; }; output {; Array[File] alignedReadGroup = glob(""arg*""); Array[String] library = read_lines(stdout()); }; }. task mapping {; File inFile; command {; echo ""dummy mapping""; }; output {; File outFile=inFile; }; }. task groupItemsByKey {. Array[String] keys; Array[String] items. meta {; description: ""return pairs of (key, all-items-with-that-key)""; }. command <<<; python <<CODE; import itertools; import sys; keys = ""${sep='\t' keys}"".split(""\t""); items = ""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:2612,Modifiability,Variab,VariableLookupException,2612,"[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-340a5cf-SNAP.jar run scatterTest.wdl - - - -; ```. consistently (repeatably) dies with:; ```; [2016-12-21 13:01:37,48] [error] WorkflowManagerActor Workflow 28b55884-8fd1-43aa-92ea-eb4891c2c5ff failed (during ExecutingWorkflowState): Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:5275,Modifiability,Variab,VariableLookupException,5275,ool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:35); 		... 12 more; 		Suppressed: wdl4s.exception.VariableLookupException: inputBams:; Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 			at wdl4s.Call.wdl4s$Call$$lookup$2(Call.scala:176); 			at wdl4s.Call$$anonfun$lookupFunction$1.apply(Call.scala:181); 			at wdl4s.Call$$anonfun$lookupFunction$1.apply(Call.scala:181); 			at wdl4s.Call$$anonfun$4$$anonfun$5.apply(Call.scala:103); 			at wdl4s.Call$$anonfun$4$$anonfun$5.apply(Call.scala:103); 			at scala.util.Try$.apply(Try.scala:192); 			at wdl4s.Call$$anonfun$4.apply(Call.scala:103); 			at wdl4s.Call$$anonfun$4.apply(Call.scala:101); 			at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 			at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 			at scala.collection.Iterator$class.foreach(Iterator.scala:893); 			at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 			at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 			at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 			at scala.collection.Tra,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:8005,Modifiability,Variab,VariableLookupException,8005, 				at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); 				at wdl4s.expression.ValueEvaluator$$anonfun$evaluate$1.apply(ValueEvaluator.scala:45); 				at scala.util.Try$.apply(Try.scala:192); 				at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:45); 				at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:112); 				at wdl4s.WdlExpression$.evaluate(WdlExpression.scala:85); 				at wdl4s.WdlExpression.evaluate(WdlExpression.scala:161); 				at wdl4s.Call$$anonfun$12$$anonfun$apply$7.apply(Call.scala:146); 				at wdl4s.Call$$anonfun$12$$anonfun$apply$7.apply(Call.scala:145); 				at scala.util.Success.flatMap(Try.scala:231); 				at wdl4s.Call$$anonfun$12.apply(Call.scala:145); 				at wdl4s.Call$$anonfun$12.apply(Call.scala:144); 				at scala.util.Success.flatMap(Try.scala:231); 				at wdl4s.Call.wdl4s$Call$$lookup$2(Call.scala:144); 				... 32 more; 		Suppressed: wdl4s.exception.VariableLookupException: outputBam:; Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 			at wdl4s.Call.wdl4s$Call$$lookup$2(Call.scala:176); 			at wdl4s.Call$$anonfun$lookupFunction$1.apply(Call.scala:181); 			at wdl4s.Call$$anonfun$lookupFunction$1.apply(Call.scala:181); 			at wdl4s.Call$$anonfun$4$$anonfun$5.apply(Call.scala:103); 			at wdl4s.Call$$anonfun$4$$anonfun$5.apply(Call.scala:103); 			at scala.util.Try$.apply(Try.scala:192); 			at wdl4s.Call$$anonfun$4.apply(Call.scala:103); 			at wdl4s.Call$$anonfun$4.apply(Call.scala:101); 			at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 			at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 			at scala.collection.Iterator$class.foreach(Iterator.scala:893); 			at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 			at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 			at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 			at scala.collection.Tra,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:4113,Performance,concurren,concurrent,4113,well.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.en,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:4187,Performance,concurren,concurrent,4187,olveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateI,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:4273,Performance,concurren,concurrent,4273,bstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:35); 		... 12 more; 		Suppressed: wdl4s.exception.Vari,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:4351,Performance,concurren,concurrent,4351,.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:35); 		... 12 more; 		Suppressed: wdl4s.exception.VariableLookupException: inputBams:; Could not find the shard mapping to this scat,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:3365,Safety,recover,recoverWith,3365,Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.Fo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:4457,Security,Validat,ValidationException,4457,JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:35); 		... 12 more; 		Suppressed: wdl4s.exception.VariableLookupException: inputBams:; Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 			at wdl4s.Call.wdl4s$Call$$lookup$2(Call.scala:176); 			at wdl4s.Call$$anonfun$lookupFunction$1.ap,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:1236,Testability,assert,assert,1236,"ByKey as groupArgsByLibrary {; input:; keys=createInputs.library,; items=mapping.outFile; }. scatter (libset in groupArgsByLibrary.groups) {; call markDup as libraryMerge {; input:; inputBams=libset.right,; outputBam=""library_${libset.left}.bam""; }; }. output {; Array[File] libMerged = libraryMerge.markDupedBam; }; }. #########; # TASKS #; #########. task createInputs {; command {; for i in `seq 1 5`; do echo ""lib1""; touch arg$i; done; }; output {; Array[File] alignedReadGroup = glob(""arg*""); Array[String] library = read_lines(stdout()); }; }. task mapping {; File inFile; command {; echo ""dummy mapping""; }; output {; File outFile=inFile; }; }. task groupItemsByKey {. Array[String] keys; Array[String] items. meta {; description: ""return pairs of (key, all-items-with-that-key)""; }. command <<<; python <<CODE; import itertools; import sys; keys = ""${sep='\t' keys}"".split(""\t""); items = ""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-340a5cf-SNAP.jar run scatterTest.wdl - - - -; ```. consistently (repeatably) dies with:; ```; [2016-12-21 13:01:37,48] [error] WorkflowManagerActor Workflow 28b55884-8fd1-43aa-92ea-eb4891c2c5ff failed (during ExecutingWorkflowState): Couldn't resolve all inputs for dna_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512
https://github.com/broadinstitute/cromwell/pull/1803#issuecomment-268541263:10,Testability,test,testable,10,👍 Is this testable easily (maybe in centaur using delocalizer's wdl) so we can prevent this regression in the future ?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1803/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1803#issuecomment-268541263
https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268610787:122,Usability,simpl,simplistic,122,"@kcibul - I've been increasingly thinking that we should rethink runtime attrs altogether, they've always seemed a little simplistic for what can be a fairly complex situation (e.g. what if 2 backends interpret the same RA differently and we're in a multi-backend situation?).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268610787
https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268615173:414,Usability,simpl,simplistic,414,":+1: I was worried about interpretation across backends. Since I just; found out that the SGE backend does not support docker. That could have; been an unpleasant surprise... On Wed, Dec 21, 2016 at 2:01 PM, Jeff Gentry <notifications@github.com>; wrote:. > @kcibul <https://github.com/kcibul> - I've been increasingly thinking; > that we should rethink runtime attrs altogether, they've always seemed a; > little simplistic for what can be a fairly complex situation (e.g. what if; > 2 backends interpret the same RA differently and we're in a multi-backend; > situation?).; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268610787>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2BYpbGzGuj7izW7zJZsKRrwE0Bcks5rKXeagaJpZM4LTPm1>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268615173
https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268638416:17,Availability,ping,ping,17,"Sure thing. Just ping me when you are back. Have a fun break. On Wed, Dec 21, 2016 at 3:28 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Let's touch base after the; > break (since I'm already on mine). I have a feeling this is one of those; > things where everyone present will say ""I agree that the current solution; > is right but I disagree with everyone else's vision"" :) So it'd probably; > take a while before getting it right.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268630429>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk73KvoC7sKtWEqWT2w88t7Qh8h5Eks5rKYvRgaJpZM4LTPm1>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268638416
https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-330607992:32,Deployability,release,released,32,"@geoffjentry I believe recently released a change with ""null"", is that related here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-330607992
https://github.com/broadinstitute/cromwell/pull/1805#issuecomment-268824680:83,Deployability,update,updated,83,Why not just reference the appropriate doc in the wdl spec (which also needs to be updated)?. I don't think of a changelog as a place for in depth documentation but rather a thing that tells me what happened that I might be interested in,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1805#issuecomment-268824680
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648:61,Performance,perform,performance,61,@geoffjentry have you continued to see many logs as drowning performance? It sounds like optimizing logs could give us a large bump in scale!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648:89,Performance,optimiz,optimizing,89,@geoffjentry have you continued to see many logs as drowning performance? It sounds like optimizing logs could give us a large bump in scale!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648:44,Testability,log,logs,44,@geoffjentry have you continued to see many logs as drowning performance? It sounds like optimizing logs could give us a large bump in scale!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648:100,Testability,log,logs,100,@geoffjentry have you continued to see many logs as drowning performance? It sounds like optimizing logs could give us a large bump in scale!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:69,Integrability,message,messages,69,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:147,Integrability,message,messages,147,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:383,Integrability,message,messages,383,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:91,Performance,perform,performance,91,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:408,Performance,perform,performance,408,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:337,Safety,risk,risk,337,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:424,Safety,risk,risk,424,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:465,Safety,risk,risk,465,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:65,Testability,log,log,65,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:206,Testability,Log,Logging,206,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:379,Testability,log,log,379,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:521,Testability,log,logs,521,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674
https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-414066983:16,Testability,log,logging,16,Closing this as logging needs to be rethought entirely anyways,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-414066983
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328885838:32,Integrability,message,messages,32,"@geoffjentry what are unhandled messages, and why do we explicitly log them?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328885838
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328885838:67,Testability,log,log,67,"@geoffjentry what are unhandled messages, and why do we explicitly log them?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328885838
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526:146,Availability,error,error,146,"When an actor sends a message to another actor and that actor isn't expecting to handle that type of message, akka has its own way of logging the error. Some well intentioned Cromwellians often partake in a pattern where they explicitly catch these and log the error. It's unnecessary and adds to LOC. Really this is tech debt, i'll relabel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526:261,Availability,error,error,261,"When an actor sends a message to another actor and that actor isn't expecting to handle that type of message, akka has its own way of logging the error. Some well intentioned Cromwellians often partake in a pattern where they explicitly catch these and log the error. It's unnecessary and adds to LOC. Really this is tech debt, i'll relabel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526:22,Integrability,message,message,22,"When an actor sends a message to another actor and that actor isn't expecting to handle that type of message, akka has its own way of logging the error. Some well intentioned Cromwellians often partake in a pattern where they explicitly catch these and log the error. It's unnecessary and adds to LOC. Really this is tech debt, i'll relabel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526:101,Integrability,message,message,101,"When an actor sends a message to another actor and that actor isn't expecting to handle that type of message, akka has its own way of logging the error. Some well intentioned Cromwellians often partake in a pattern where they explicitly catch these and log the error. It's unnecessary and adds to LOC. Really this is tech debt, i'll relabel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526:134,Testability,log,logging,134,"When an actor sends a message to another actor and that actor isn't expecting to handle that type of message, akka has its own way of logging the error. Some well intentioned Cromwellians often partake in a pattern where they explicitly catch these and log the error. It's unnecessary and adds to LOC. Really this is tech debt, i'll relabel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526:253,Testability,log,log,253,"When an actor sends a message to another actor and that actor isn't expecting to handle that type of message, akka has its own way of logging the error. Some well intentioned Cromwellians often partake in a pattern where they explicitly catch these and log the error. It's unnecessary and adds to LOC. Really this is tech debt, i'll relabel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328963748:56,Integrability,protocol,protocols,56,"As a **Cromwell dev**, I want **Cromwell to follow akka protocols of handling unexpected messages**, so that I can **avoid excessive LinesOfCode**.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328963748
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328963748:89,Integrability,message,messages,89,"As a **Cromwell dev**, I want **Cromwell to follow akka protocols of handling unexpected messages**, so that I can **avoid excessive LinesOfCode**.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328963748
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328963748:117,Safety,avoid,avoid,117,"As a **Cromwell dev**, I want **Cromwell to follow akka protocols of handling unexpected messages**, so that I can **avoid excessive LinesOfCode**.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328963748
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430020117:73,Testability,log,logging,73,"@geoffjentry Looking for some clarification; are you suggesting that all logging be removed from `whenUnhandled` handlers, or logging in specific cases within the `whenUnhandled` handler (e.g. [here](https://github.com/broadinstitute/cromwell/blob/8b4fd5d847b724d3b2383c1d1b33826006867c9c/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L207)? Or am I completely off-track?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430020117
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430020117:126,Testability,log,logging,126,"@geoffjentry Looking for some clarification; are you suggesting that all logging be removed from `whenUnhandled` handlers, or logging in specific cases within the `whenUnhandled` handler (e.g. [here](https://github.com/broadinstitute/cromwell/blob/8b4fd5d847b724d3b2383c1d1b33826006867c9c/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L207)? Or am I completely off-track?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430020117
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351:141,Integrability,message,message,141,"@rsasch that's exactly what i'm talking about. we do that everywhere, but akka can be configured to log those automatically. if an unhandled message catcher is doing something useful other than logging, that's different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351:86,Modifiability,config,configured,86,"@rsasch that's exactly what i'm talking about. we do that everywhere, but akka can be configured to log those automatically. if an unhandled message catcher is doing something useful other than logging, that's different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351:100,Testability,log,log,100,"@rsasch that's exactly what i'm talking about. we do that everywhere, but akka can be configured to log those automatically. if an unhandled message catcher is doing something useful other than logging, that's different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351:194,Testability,log,logging,194,"@rsasch that's exactly what i'm talking about. we do that everywhere, but akka can be configured to log those automatically. if an unhandled message catcher is doing something useful other than logging, that's different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467167480:55,Integrability,message,messages,55,I think we should be aware when we have actors getting messages that are unexpected as it is almost certainly a bug.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467167480
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467168303:227,Integrability,message,message,227,@danbills Sure. the point is that there's a built in way to handle it and we should be doing that instead of our ad hoc method of having some catch all on every `receive` method throughout the system that are at best logging a message and potentially slightly changing the stacktrace. . We should remove those catch alls and use the built in capabilities.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467168303
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467168303:217,Testability,log,logging,217,@danbills Sure. the point is that there's a built in way to handle it and we should be doing that instead of our ad hoc method of having some catch all on every `receive` method throughout the system that are at best logging a message and potentially slightly changing the stacktrace. . We should remove those catch alls and use the built in capabilities.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467168303
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467170389:320,Usability,clear,clear,320,"It was lost in some of the comments but the way to go (IMO) would be to have a master `whenUnhandled` to cover the cases where nothing useful is being done. . There are other cases where something useful **is** done, which isn't as bad but I think we should switch to use local `whenUnhandled` there to make things more clear",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467170389
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910:104,Integrability,message,messages,104,@geoffjentry What benefit does the AsyncAppender have for our logs? How realistic is the risk that some messages could be dropped?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910:89,Safety,risk,risk,89,@geoffjentry What benefit does the AsyncAppender have for our logs? How realistic is the risk that some messages could be dropped?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910:62,Testability,log,logs,62,@geoffjentry What benefit does the AsyncAppender have for our logs? How realistic is the risk that some messages could be dropped?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:324,Availability,down,down,324,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:479,Integrability,message,message,479,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:58,Performance,perform,performance,58,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:219,Performance,perform,performance,219,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:309,Performance,tune,tunes,309,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:341,Performance,perform,performance,341,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:257,Safety,risk,risk,257,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:319,Safety,risk,risk,319,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:37,Testability,log,logs,37,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:90,Testability,log,logs,90,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:143,Testability,log,logging,143,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:475,Testability,log,log,475,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-344952812:238,Integrability,depend,depending,238,"As a **Cromwell dev**, I want **to explore the cost/benefits of using AsyncAppender for our logs**, so that **we can decide whether we should adopt it.**; - effort: Small spike; - risk: Small to Medium; - business value: Small to Medium, depending on the results of the spike",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-344952812
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-344952812:180,Safety,risk,risk,180,"As a **Cromwell dev**, I want **to explore the cost/benefits of using AsyncAppender for our logs**, so that **we can decide whether we should adopt it.**; - effort: Small spike; - risk: Small to Medium; - business value: Small to Medium, depending on the results of the spike",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-344952812
https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-344952812:92,Testability,log,logs,92,"As a **Cromwell dev**, I want **to explore the cost/benefits of using AsyncAppender for our logs**, so that **we can decide whether we should adopt it.**; - effort: Small spike; - risk: Small to Medium; - business value: Small to Medium, depending on the results of the spike",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-344952812
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:391,Availability,down,down,391,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:441,Availability,down,down,441,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:257,Modifiability,enhance,enhancements,257,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:89,Performance,perform,performance,89,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:245,Performance,perform,performance,245,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:363,Performance,perform,performance,363,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:561,Performance,load,load,561,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:227,Testability,benchmark,benchmarking,227,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269339796:106,Usability,simpl,simple,106,"Is that a db level setting giving you bulk insert via magic? . Doing it by hand turns out to not be super simple to make it not suck when it comes to our metadata, although I think I came up with something that would work after I quit poking at it. I won't bother fiddling with it if there's potential magic though. Btw I think this might be behind a lot of the various things I've been tracking, the pattern is pretty much the same. Well, more generally I think this is a pattern we use a lot (not just the single writes but firing these things off as fast as possible due to the futures) but this is the #1 culprit by far",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269339796
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269375525:388,Availability,down,down,388,"To put some concrete numbers on this, I'm using `ab` to submit workflows using 32 threads. . Using `develop` I get a sustained ~450 requests per second for as long as my patience lasts (I've not gone beyond 200k submissions). CPU utilization remains steady throughout. Using my changes for small bursts I get ~2500 rps, if I run 200k submits I get ~1400 rps in aggregate. It starts going down pretty rapidly after a few minutes and not long after that is down to roughly the rate of develop (if that) - by 400k submissions it gets too slow for me to wait around. You can see the impact in the CPU utilization of both the JVM and MySQL as well. Using my changes and removing the metadata write I get a sustained 4200 requests per second up until at least 800k workflows. CPU utilization remains steady throughout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269375525
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269375525:455,Availability,down,down,455,"To put some concrete numbers on this, I'm using `ab` to submit workflows using 32 threads. . Using `develop` I get a sustained ~450 requests per second for as long as my patience lasts (I've not gone beyond 200k submissions). CPU utilization remains steady throughout. Using my changes for small bursts I get ~2500 rps, if I run 200k submits I get ~1400 rps in aggregate. It starts going down pretty rapidly after a few minutes and not long after that is down to roughly the rate of develop (if that) - by 400k submissions it gets too slow for me to wait around. You can see the impact in the CPU utilization of both the JVM and MySQL as well. Using my changes and removing the metadata write I get a sustained 4200 requests per second up until at least 800k workflows. CPU utilization remains steady throughout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269375525
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:195,Modifiability,config,config,195,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:29,Performance,perform,performance,29,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:281,Performance,perform,performance,281,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:306,Performance,load,load,306,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:125,Usability,simpl,simple,125,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269467766:206,Modifiability,rewrite,rewrites,206,"Cool -- so this is JDBC-batching effectively? It might be good to turn on SQL logging and check to see that the number of writes match what you think they should be. I remember @dvoet saying that the query rewrites weren't happening when they thought they should and that there was an option they had to set (either CloudSQL or Slick, can't remember). We should check in with him after break",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269467766
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269467766:78,Testability,log,logging,78,"Cool -- so this is JDBC-batching effectively? It might be good to turn on SQL logging and check to see that the number of writes match what you think they should be. I remember @dvoet saying that the query rewrites weren't happening when they thought they should and that there was an option they had to set (either CloudSQL or Slick, can't remember). We should check in with him after break",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269467766
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705:775,Modifiability,extend,extended,775,"@kcibul I don't know. Because the ticket @mcovarr linked sounded like there was a magic setting somewhere I googled around a bit and found some references to badness. I didn't check, however. Still something to look at. Also I've been using MySQL not CloudSQL, perhaps that matters. I got to the point where if I set my batch size high enough (I was generating on the order of ~15k events to write per second, FWIW) my overall performance was such that I was getting a sustained rate of ~1500-1700 (I forget exactly) requests per second on the submission side, which is certainly still a lot less than I was getting w/o metadata at all but a heck of a lot better than I was able to do otherwise. It's entirely possible that all I did was move the goalpost back and that if I extended my test even further eventually I'd see the same problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705:427,Performance,perform,performance,427,"@kcibul I don't know. Because the ticket @mcovarr linked sounded like there was a magic setting somewhere I googled around a bit and found some references to badness. I didn't check, however. Still something to look at. Also I've been using MySQL not CloudSQL, perhaps that matters. I got to the point where if I set my batch size high enough (I was generating on the order of ~15k events to write per second, FWIW) my overall performance was such that I was getting a sustained rate of ~1500-1700 (I forget exactly) requests per second on the submission side, which is certainly still a lot less than I was getting w/o metadata at all but a heck of a lot better than I was able to do otherwise. It's entirely possible that all I did was move the goalpost back and that if I extended my test even further eventually I'd see the same problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705:787,Testability,test,test,787,"@kcibul I don't know. Because the ticket @mcovarr linked sounded like there was a magic setting somewhere I googled around a bit and found some references to badness. I didn't check, however. Still something to look at. Also I've been using MySQL not CloudSQL, perhaps that matters. I got to the point where if I set my batch size high enough (I was generating on the order of ~15k events to write per second, FWIW) my overall performance was such that I was getting a sustained rate of ~1500-1700 (I forget exactly) requests per second on the submission side, which is certainly still a lot less than I was getting w/o metadata at all but a heck of a lot better than I was able to do otherwise. It's entirely possible that all I did was move the goalpost back and that if I extended my test even further eventually I'd see the same problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269831007:41,Testability,log,log,41,"Confirmed via enabling the mysql general log that these are still individual writes, so going with the ""move the goalpost back"" statement from before.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269831007
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-274632528:112,Performance,load,load,112,It's still possible to flood MySQL but that's another problem with a different solution and requires a lot more load. This was closed by #1836,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-274632528
https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-269422976:182,Integrability,message,messages,182,"It's also worth noting that `MetadataPutFailed` manages to get logged both when we send it in the metadata service and where it's used, but we don't do anything other than those log messages. It seems like one would be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-269422976
https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-269422976:63,Testability,log,logged,63,"It's also worth noting that `MetadataPutFailed` manages to get logged both when we send it in the metadata service and where it's used, but we don't do anything other than those log messages. It seems like one would be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-269422976
https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-269422976:178,Testability,log,log,178,"It's also worth noting that `MetadataPutFailed` manages to get logged both when we send it in the metadata service and where it's used, but we don't do anything other than those log messages. It seems like one would be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-269422976
https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-270128566:206,Availability,down,down,206,"example of this - in one of my tests i was generating ~1000 metadata requests per second. every single one of those in turn generated a message back to my original actor which was ignored, which would slow down the original actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-270128566
https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-270128566:136,Integrability,message,message,136,"example of this - in one of my tests i was generating ~1000 metadata requests per second. every single one of those in turn generated a message back to my original actor which was ignored, which would slow down the original actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-270128566
https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-270128566:31,Testability,test,tests,31,"example of this - in one of my tests i was generating ~1000 metadata requests per second. every single one of those in turn generated a message back to my original actor which was ignored, which would slow down the original actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-270128566
https://github.com/broadinstitute/cromwell/pull/1812#issuecomment-270995408:131,Testability,test,tests,131,"One last edit, primarily to make the JRA.dockerImage required instead of an Option. Unless I hear anything else, I'll squash after tests pass, then merge once secondary tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1812#issuecomment-270995408
https://github.com/broadinstitute/cromwell/pull/1812#issuecomment-270995408:169,Testability,test,tests,169,"One last edit, primarily to make the JRA.dockerImage required instead of an Option. Unless I hear anything else, I'll squash after tests pass, then merge once secondary tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1812#issuecomment-270995408
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-275251912:0,Deployability,UPDATE,UPDATE,0,"UPDATE:. Refactored to use the standard backend. Based on my manual crude testing, basic functionality is there. Globs definitely don't work. . More to come soon.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-275251912
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-275251912:9,Modifiability,Refactor,Refactored,9,"UPDATE:. Refactored to use the standard backend. Based on my manual crude testing, basic functionality is there. Globs definitely don't work. . More to come soon.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-275251912
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-275251912:74,Testability,test,testing,74,"UPDATE:. Refactored to use the standard backend. Based on my manual crude testing, basic functionality is there. Globs definitely don't work. . More to come soon.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-275251912
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:301,Deployability,patch,patch,301,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:484,Deployability,update,updated,484,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:1084,Deployability,patch,patches,1084,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:338,Integrability,bridg,bridge,338,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:1514,Integrability,message,messages,1514,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:1654,Safety,timeout,timeouts,1654,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:232,Testability,test,tested,232,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:862,Testability,test,testing,862,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:917,Testability,test,test,917,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:1128,Testability,test,test,1128,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:1138,Testability,test,tests,1138,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:89,Usability,feedback,feedback,89,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:152,Usability,feedback,feedback,152,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897:272,Deployability,upgrade,upgrades,272,"FYI- the failing test suite ""centaurJes"" is due to a know limitation in our test setup, but the other three look good, including the ""centaurLocal"" tests. Again, without a dockerized ""centaurTes"" we'll certainly try to avoid issues, but there won't be any guarantees that upgrades to the standard backend API don't break TES. Also, the 87 commits will need to be rebased and squashed correctly to a minimal set, on the order of 1 commit, but otherwise let us know when you're ready for re-review. Let us know if you have more questions or if we can provide any other assistance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897:219,Safety,avoid,avoid,219,"FYI- the failing test suite ""centaurJes"" is due to a know limitation in our test setup, but the other three look good, including the ""centaurLocal"" tests. Again, without a dockerized ""centaurTes"" we'll certainly try to avoid issues, but there won't be any guarantees that upgrades to the standard backend API don't break TES. Also, the 87 commits will need to be rebased and squashed correctly to a minimal set, on the order of 1 commit, but otherwise let us know when you're ready for re-review. Let us know if you have more questions or if we can provide any other assistance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897:17,Testability,test,test,17,"FYI- the failing test suite ""centaurJes"" is due to a know limitation in our test setup, but the other three look good, including the ""centaurLocal"" tests. Again, without a dockerized ""centaurTes"" we'll certainly try to avoid issues, but there won't be any guarantees that upgrades to the standard backend API don't break TES. Also, the 87 commits will need to be rebased and squashed correctly to a minimal set, on the order of 1 commit, but otherwise let us know when you're ready for re-review. Let us know if you have more questions or if we can provide any other assistance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897:76,Testability,test,test,76,"FYI- the failing test suite ""centaurJes"" is due to a know limitation in our test setup, but the other three look good, including the ""centaurLocal"" tests. Again, without a dockerized ""centaurTes"" we'll certainly try to avoid issues, but there won't be any guarantees that upgrades to the standard backend API don't break TES. Also, the 87 commits will need to be rebased and squashed correctly to a minimal set, on the order of 1 commit, but otherwise let us know when you're ready for re-review. Let us know if you have more questions or if we can provide any other assistance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897:148,Testability,test,tests,148,"FYI- the failing test suite ""centaurJes"" is due to a know limitation in our test setup, but the other three look good, including the ""centaurLocal"" tests. Again, without a dockerized ""centaurTes"" we'll certainly try to avoid issues, but there won't be any guarantees that upgrades to the standard backend API don't break TES. Also, the 87 commits will need to be rebased and squashed correctly to a minimal set, on the order of 1 commit, but otherwise let us know when you're ready for re-review. Let us know if you have more questions or if we can provide any other assistance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278432875:32,Testability,test,testing,32,"I am working on getting centuar testing up and running with a TES implementation. Rather than going through rebase hell on this branch (I have numerous merge commits), I will submit a new PR from another branch where I have squashed the commits via merge. How does that sound?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278432875
https://github.com/broadinstitute/cromwell/pull/1818#issuecomment-270945817:82,Integrability,depend,dependent,82,"If redness persists, consult your wdl4s reviewers (so that PR gets merged and the dependent artifact is published).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1818#issuecomment-270945817
https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271362663:328,Availability,echo,echo,328,"I think this is expected behavior since `array[@]` is not a valid WDL expression and so cannot be used between `${ }` in a command string. Since WDL doesn't have a way to escape the sequence, I suggest inserting the `$` in manually in a way that doesn't trigger WDL string interpolation:; ```; String dollar = ""$""; command <<<; echo ""Hello ${addressee}!""; array=(one two three); for i in ${dollar}{array[@]}; do; echo $i; done; >>>; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271362663
https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271362663:413,Availability,echo,echo,413,"I think this is expected behavior since `array[@]` is not a valid WDL expression and so cannot be used between `${ }` in a command string. Since WDL doesn't have a way to escape the sequence, I suggest inserting the `$` in manually in a way that doesn't trigger WDL string interpolation:; ```; String dollar = ""$""; command <<<; echo ""Hello ${addressee}!""; array=(one two three); for i in ${dollar}{array[@]}; do; echo $i; done; >>>; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271362663
https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271366445:190,Availability,echo,echo,190,"I verified this workaround using the following WDL file:; ```; task dollarInterpolation {. 	String dollar = ""$""; 	command <<<; 	 array=(one two three); 	 for i in ${dollar}{array[@]}; do; 	 echo $i; 	 done; 	>>>. 	output {; 		String s = read_string(stdout()); 	}; }. workflow main {; 	call dollarInterpolation; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271366445
https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802:89,Availability,down,down,89,@kcibul I created tickets related to this and scheduled a meeting on Monday to hash them down.; Let me know if that covers this ticket and if so I'll close it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802
https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802:46,Energy Efficiency,schedul,scheduled,46,@kcibul I created tickets related to this and scheduled a meeting on Monday to hash them down.; Let me know if that covers this ticket and if so I'll close it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802
https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802:79,Security,hash,hash,79,@kcibul I created tickets related to this and scheduled a meeting on Monday to hash them down.; Let me know if that covers this ticket and if so I'll close it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802
https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-273797012:62,Availability,failure,failure,62,"@kcibul now that even have a proposal doc to help reduce GOTC failure modes, it seems this spike/investigation is complete. Closing it for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-273797012
https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-273797012:50,Energy Efficiency,reduce,reduce,50,"@kcibul now that even have a proposal doc to help reduce GOTC failure modes, it seems this spike/investigation is complete. Closing it for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-273797012
https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754:460,Availability,error,error,460,"FWIW [the documentation](https://gsaweb.broadinstitute.org/wdl/devzone/) says `read_json` will do what I expect! Ctrl+F ""Array deserialization using read_json()"". Here's the WDL task I was trying to validate:. ```; # reverses a json array; task reverseArray {; Array[Int] intArray; ; command {; python -c ""import sys; print(list(map(int, sys.argv[-1:0:-1])))"" ${sep=' ' intArray}; }; ; output {; Array[Int] outArr = read_json(stdout()); }; }; ```. And got the error:. ```$ java -jar wdltool-0.4.jar validate json_things.wdl; ERROR: Could not determine type of declaration outArr:; Array[Int] outArr = read_json(stdout()); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754
https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754:525,Availability,ERROR,ERROR,525,"FWIW [the documentation](https://gsaweb.broadinstitute.org/wdl/devzone/) says `read_json` will do what I expect! Ctrl+F ""Array deserialization using read_json()"". Here's the WDL task I was trying to validate:. ```; # reverses a json array; task reverseArray {; Array[Int] intArray; ; command {; python -c ""import sys; print(list(map(int, sys.argv[-1:0:-1])))"" ${sep=' ' intArray}; }; ; output {; Array[Int] outArr = read_json(stdout()); }; }; ```. And got the error:. ```$ java -jar wdltool-0.4.jar validate json_things.wdl; ERROR: Could not determine type of declaration outArr:; Array[Int] outArr = read_json(stdout()); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754
https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754:199,Security,validat,validate,199,"FWIW [the documentation](https://gsaweb.broadinstitute.org/wdl/devzone/) says `read_json` will do what I expect! Ctrl+F ""Array deserialization using read_json()"". Here's the WDL task I was trying to validate:. ```; # reverses a json array; task reverseArray {; Array[Int] intArray; ; command {; python -c ""import sys; print(list(map(int, sys.argv[-1:0:-1])))"" ${sep=' ' intArray}; }; ; output {; Array[Int] outArr = read_json(stdout()); }; }; ```. And got the error:. ```$ java -jar wdltool-0.4.jar validate json_things.wdl; ERROR: Could not determine type of declaration outArr:; Array[Int] outArr = read_json(stdout()); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754
https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754:499,Security,validat,validate,499,"FWIW [the documentation](https://gsaweb.broadinstitute.org/wdl/devzone/) says `read_json` will do what I expect! Ctrl+F ""Array deserialization using read_json()"". Here's the WDL task I was trying to validate:. ```; # reverses a json array; task reverseArray {; Array[Int] intArray; ; command {; python -c ""import sys; print(list(map(int, sys.argv[-1:0:-1])))"" ${sep=' ' intArray}; }; ; output {; Array[Int] outArr = read_json(stdout()); }; }; ```. And got the error:. ```$ java -jar wdltool-0.4.jar validate json_things.wdl; ERROR: Could not determine type of declaration outArr:; Array[Int] outArr = read_json(stdout()); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272244992:53,Security,hash,hashing,53,"@meganshand Was this resolved by using the file-path hashing method? If so, can I close this ticket?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272244992
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272278222:326,Performance,Queue,QueuedInCromwell,326,"@cjllanwarne Unfortunately it looks like this is a separate issue. I tried running it with the file-path hashing method: When I ran it with a wide scatter (312) it hangs before it starts any tasks in the scatter. When I ran it with a small scatter (6) it ""starts"" the jobs inside the scatter but the timing diagram just says `QueuedInCromwell` for all of them. It might not be the fact that there are declarations inside of the scatter, but the fact that those declarations include declaring multiple files, which all happen at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272278222
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272278222:105,Security,hash,hashing,105,"@cjllanwarne Unfortunately it looks like this is a separate issue. I tried running it with the file-path hashing method: When I ran it with a wide scatter (312) it hangs before it starts any tasks in the scatter. When I ran it with a small scatter (6) it ""starts"" the jobs inside the scatter but the timing diagram just says `QueuedInCromwell` for all of them. It might not be the fact that there are declarations inside of the scatter, but the fact that those declarations include declaring multiple files, which all happen at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272278222
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374575:203,Modifiability,variab,variables,203,@meganshand you mentioned in your original comment you'd try running it under different conditions:. > This fails both in SGE and Local backends. There is a suspicion that this might be due to declaring variables inside of the scatter? I'm going to try extracting all variable declaration into a task to see if that works. Did moving variable declarations outside of the scatter help?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374575
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374575:268,Modifiability,variab,variable,268,@meganshand you mentioned in your original comment you'd try running it under different conditions:. > This fails both in SGE and Local backends. There is a suspicion that this might be due to declaring variables inside of the scatter? I'm going to try extracting all variable declaration into a task to see if that works. Did moving variable declarations outside of the scatter help?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374575
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374575:334,Modifiability,variab,variable,334,@meganshand you mentioned in your original comment you'd try running it under different conditions:. > This fails both in SGE and Local backends. There is a suspicion that this might be due to declaring variables inside of the scatter? I'm going to try extracting all variable declaration into a task to see if that works. Did moving variable declarations outside of the scatter help?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374575
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374768:152,Availability,avail,available,152,"Hi @meganshand, I just re-looked at the WDL and it looks like you're using string interpolation where it isn't currently supported. Right now it's only available in `command` blocks. I don't know whether that'll solve the issue you're seeing but it's almost certainly causing problems!. E.g. These `${...}` won't be expanded and will probably be passed in verbatim to bash, where who knows how they'll be interpreted: ; ```; Array[File] bams = [""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bam"", ""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bam""]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374768
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285378599:118,Testability,test,test,118,"Ahha, that is useful information. I think this WDL didn't work for multiple reasons. Is there a simple example WDL or test that uses declarations in a scatter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285378599
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285378599:96,Usability,simpl,simple,96,"Ahha, that is useful information. I think this WDL didn't work for multiple reasons. Is there a simple example WDL or test that uses declarations in a scatter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285378599
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-287464356:44,Testability,test,test,44,"@katevoss I haven't tried it. If there is a test that has declarations in a scatter then I'm happy with closing this. I'm sure I was running into some other issue, including what @cjllanwarne mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-287464356
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-289529832:28,Testability,test,test,28,"For the fixer: please add a test with declarations using string interpolations in the scatter. If it passes, then there is likely something else wrong with Megan's WDL. If it fails, then we have a bug to fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-289529832
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-289832306:19,Testability,test,test,19,There is already a test that tests a declaration used as a scatter array in centaur: https://github.com/broadinstitute/centaur/blob/develop/src/main/resources/standardTestCases/declarations_as_nodes/declarations_as_nodes.wdl#L23,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-289832306
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-289832306:29,Testability,test,tests,29,There is already a test that tests a declaration used as a scatter array in centaur: https://github.com/broadinstitute/centaur/blob/develop/src/main/resources/standardTestCases/declarations_as_nodes/declarations_as_nodes.wdl#L23,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-289832306
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-291181349:248,Testability,test,tests,248,It turns out we had a problem evaluating file outputs from array literals where internal strings needed further evaluation. This was resolved as a side-effect of my FileEvaluator changes in https://github.com/broadinstitute/wdl4s/pull/97 - and new tests in the `FileEvaluatorSpec` have been added to confirm this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-291181349
https://github.com/broadinstitute/cromwell/issues/1828#issuecomment-274830847:98,Testability,test,tested,98,"Closing as as per @kshakir this code shouldn't even really exist in the first place, much less be tested",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1828#issuecomment-274830847
https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981:160,Availability,echo,echo,160,"@cjllanwarne - Here goes... This works:; ```; workflow wf {; call tsk {; input: foo=""Hello"", bar=""World""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```; This doesn't:; ```; workflow wf {; call tsk {; input: foo=""Hello""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```. When `bar` is left out, job stays in the running state, and exceptions are continually thrown in the server logs.; The wdl validates fine.; Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981
https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981:333,Availability,echo,echo,333,"@cjllanwarne - Here goes... This works:; ```; workflow wf {; call tsk {; input: foo=""Hello"", bar=""World""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```; This doesn't:; ```; workflow wf {; call tsk {; input: foo=""Hello""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```. When `bar` is left out, job stays in the running state, and exceptions are continually thrown in the server logs.; The wdl validates fine.; Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981
https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981:508,Security,validat,validates,508,"@cjllanwarne - Here goes... This works:; ```; workflow wf {; call tsk {; input: foo=""Hello"", bar=""World""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```; This doesn't:; ```; workflow wf {; call tsk {; input: foo=""Hello""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```. When `bar` is left out, job stays in the running state, and exceptions are continually thrown in the server logs.; The wdl validates fine.; Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981
https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981:493,Testability,log,logs,493,"@cjllanwarne - Here goes... This works:; ```; workflow wf {; call tsk {; input: foo=""Hello"", bar=""World""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```; This doesn't:; ```; workflow wf {; call tsk {; input: foo=""Hello""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```. When `bar` is left out, job stays in the running state, and exceptions are continually thrown in the server logs.; The wdl validates fine.; Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981
https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-271616503:25,Performance,load,load,25,Cool!. Have you done any load/performance testing to get before and after numbers? If not let's chat as pet of this process,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-271616503
https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-271616503:30,Performance,perform,performance,30,Cool!. Have you done any load/performance testing to get before and after numbers? If not let's chat as pet of this process,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-271616503
https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-271616503:42,Testability,test,testing,42,Cool!. Have you done any load/performance testing to get before and after numbers? If not let's chat as pet of this process,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-271616503
https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-274619482:99,Integrability,rout,router,99,"I could see this being a place where workpulling from a central manager vs. pushing from a central router would be worth doing in the first go. Since these by definition are going to be slower operations although falling back to our typical ""let's see if it blows up before we get around to it"" can also work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-274619482
https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-275135813:45,Integrability,depend,depending,45,Closing for now - might be worth considering depending on what comes out of the retry meeting,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-275135813
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271612340:376,Availability,echo,echo,376,"@kcibul regarding issue #1804 .... Would my wdl and json look as follows?. ```wdl. workflow yo {; String msg; String? docker_image. call task1 {; input:; msg=msg,; docker_image=docker_image; }; }. # Run a message in an arbitrary docker container (e.g. ""broadinstitute/eval-gatk-protected:crsp_validation_latest""); task task1 {; String msg; String ? docker_image; ; command {; echo ${msg}; } ; ; runtime {; docker: ""${docker_image}""; memory: ""1GB""; }; }; ```; When I want a docker image:; ```; {; ""yo.msg"": ""foo""; ""yo.docker_image"": ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; }; ```. No docker image:; ```; {; ""yo.msg"": ""foo""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271612340
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271612340:205,Integrability,message,message,205,"@kcibul regarding issue #1804 .... Would my wdl and json look as follows?. ```wdl. workflow yo {; String msg; String? docker_image. call task1 {; input:; msg=msg,; docker_image=docker_image; }; }. # Run a message in an arbitrary docker container (e.g. ""broadinstitute/eval-gatk-protected:crsp_validation_latest""); task task1 {; String msg; String ? docker_image; ; command {; echo ${msg}; } ; ; runtime {; docker: ""${docker_image}""; memory: ""1GB""; }; }; ```; When I want a docker image:; ```; {; ""yo.msg"": ""foo""; ""yo.docker_image"": ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; }; ```. No docker image:; ```; {; ""yo.msg"": ""foo""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271612340
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271613347:115,Availability,error,error,115,"@kcibul why not support empty string as null, for docker, in the backend code? Are you worried about discerning an error where the user wants docker, but accidentally specifies empty string? . If my example wdl/json above is correct, I'd rather not have to delete json entries, since this is a bit more difficult to script around, but I can be convinced.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271613347
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-314394676:0,Availability,Ping,Pinging,0,Pinging this ticket with another requester from the forums: http://gatkforums.broadinstitute.org/wdl/discussion/9936/is-there-a-way-to-tell-cromwell-to-ignore-the-runtime-section-of-the-tasks-in-a-wdl-script#latest,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-314394676
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-329591802:258,Safety,avoid,avoid,258,"Adding this to the [Runtime Attributes improvement spec](https://docs.google.com/document/d/1EcsPrmZ6hKtz9vumhdT3357e1jdhljvs6eXXeT6HTaM/edit#). As a **workflow runner on SGE**, I want **be able to make the Docker attribute to be optional**, so that I can **avoid rewriting my WDL when I don't use Docker**.; - Effort: **@geoffjentry What do you think?**; - Risk: **Small?**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-329591802
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-329591802:358,Safety,Risk,Risk,358,"Adding this to the [Runtime Attributes improvement spec](https://docs.google.com/document/d/1EcsPrmZ6hKtz9vumhdT3357e1jdhljvs6eXXeT6HTaM/edit#). As a **workflow runner on SGE**, I want **be able to make the Docker attribute to be optional**, so that I can **avoid rewriting my WDL when I don't use Docker**.; - Effort: **@geoffjentry What do you think?**; - Risk: **Small?**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-329591802
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605:291,Deployability,configurat,configuration,291,"@ruchim I should clarify. There are a lot of people at this workshop (if not all of them) that cannot use the cloud, due to regulations, clinical requirements, etc. Or they need to be able to run WDL on their local on-prem compute cluster for testing on small cohorts, etc. This is a common configuration that prohibits docker. We really do not want these users to be forced to change the WDL that we (DSP methods) write and test. In order to stay backend-agnostic, can we implement a null option for docker as described in this issue (and #1804 )?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605:291,Modifiability,config,configuration,291,"@ruchim I should clarify. There are a lot of people at this workshop (if not all of them) that cannot use the cloud, due to regulations, clinical requirements, etc. Or they need to be able to run WDL on their local on-prem compute cluster for testing on small cohorts, etc. This is a common configuration that prohibits docker. We really do not want these users to be forced to change the WDL that we (DSP methods) write and test. In order to stay backend-agnostic, can we implement a null option for docker as described in this issue (and #1804 )?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605:243,Testability,test,testing,243,"@ruchim I should clarify. There are a lot of people at this workshop (if not all of them) that cannot use the cloud, due to regulations, clinical requirements, etc. Or they need to be able to run WDL on their local on-prem compute cluster for testing on small cohorts, etc. This is a common configuration that prohibits docker. We really do not want these users to be forced to change the WDL that we (DSP methods) write and test. In order to stay backend-agnostic, can we implement a null option for docker as described in this issue (and #1804 )?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605:425,Testability,test,test,425,"@ruchim I should clarify. There are a lot of people at this workshop (if not all of them) that cannot use the cloud, due to regulations, clinical requirements, etc. Or they need to be able to run WDL on their local on-prem compute cluster for testing on small cohorts, etc. This is a common configuration that prohibits docker. We really do not want these users to be forced to change the WDL that we (DSP methods) write and test. In order to stay backend-agnostic, can we implement a null option for docker as described in this issue (and #1804 )?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423179922:50,Deployability,configurat,configuration,50,"Singularity would require changes to the cromwell configuration file, correct? Since the docker command would change. Probably not hard, but would need documentation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423179922
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423179922:50,Modifiability,config,configuration,50,"Singularity would require changes to the cromwell configuration file, correct? Since the docker command would change. Probably not hard, but would need documentation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423179922
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:19,Deployability,configurat,configuration,19,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:288,Deployability,configurat,configuration,288,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:19,Modifiability,config,configuration,19,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:222,Modifiability,config,config,222,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:229,Modifiability,variab,variable,229,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:288,Modifiability,config,configuration,288,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
